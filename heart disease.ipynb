{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "5ea1743f-b121-4dd9-8e8c-877f4a9dda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "64ff721a-c75e-462a-bd3b-dad9935ad0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "33b4800f-5fa3-4714-aa45-5ed4adf40ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n"
     ]
    }
   ],
   "source": [
    "heart_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "0340838e-1aac-48d5-b9fe-78c7b8f55719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "1   49   F           NAP        160          180          0     Normal    156   \n",
       "2   37   M           ATA        130          283          0         ST     98   \n",
       "3   48   F           ASY        138          214          0     Normal    108   \n",
       "4   54   M           NAP        150          195          0     Normal    122   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  \n",
       "1              N      1.0     Flat             1  \n",
       "2              N      0.0       Up             0  \n",
       "3              Y      1.5     Flat             1  \n",
       "4              N      0.0       Up             0  "
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "f7193a33-5135-4cd5-8a73-2ce77adcb1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical: ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
      "Categorial: ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n"
     ]
    }
   ],
   "source": [
    "numerical = []\n",
    "categorical = []\n",
    "\n",
    "for col in heart_df.columns:\n",
    "    if col == 'HeartDisease':\n",
    "        continue\n",
    "    if type(heart_df[col][0]) == str:\n",
    "        categorical.append(col)\n",
    "    else:\n",
    "        numerical.append(col)\n",
    "\n",
    "print(\"Numerical:\", numerical)\n",
    "print(\"Categorial:\", categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "6476179e-06b8-470d-8a00-3254964ab020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age               0\n",
       "Sex               0\n",
       "ChestPainType     0\n",
       "RestingBP         0\n",
       "Cholesterol       0\n",
       "FastingBS         0\n",
       "RestingECG        0\n",
       "MaxHR             0\n",
       "ExerciseAngina    0\n",
       "Oldpeak           0\n",
       "ST_Slope          0\n",
       "HeartDisease      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "480719a7-e53f-4293-9809-0f6672b4a0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "2dd31ea1-aaea-4ed3-8cbf-7daba9bda81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', OneHotEncoder(), categorical),\n",
    "        ('numerical', StandardScaler(), numerical)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "c0a065bf-2680-4192-823d-61d922b292cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = heart_df['HeartDisease']\n",
    "X = heart_df.drop('HeartDisease', axis=1)\n",
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "e2bd8a75-e765-40d9-995c-54edf65faf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeartDisease\n",
       "1    0.553377\n",
       "0    0.446623\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "c6dd71b2-688b-47a6-be63-8a80b874e49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.value_counts().plot(kind='bar', color=['red', 'green'])\n",
    "plt.title('Class Distribution of Heart Disease')\n",
    "plt.xlabel('Heart Disease')\n",
    "plt.ylabel('Count')\n",
    "legend_patches = [\n",
    "    Patch(color='green', label='0: No Heart Disease'),\n",
    "    Patch(color='red', label='1: Heart Disease')]\n",
    "plt.legend(handles=legend_patches, loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "3ceccff6-ec1a-4443-a47a-8f039ede2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=903967749, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "172d3c51-5c92-4520-99f2-306425e501a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193764f-96e6-4f38-8b31-aab3ab6fe6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "3f724bb6-8529-4bbd-bb09-e54560e21d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find and plot for k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "18808c26-9f68-4601-b0c4-b4894f96bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,50)\n",
    "f1 = make_scorer(f1_score, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "dde58ebc-6373-4279-aae4-1615a003308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_score = validation_curve(\n",
    "    KNeighborsClassifier(),\n",
    "    X_train, y_train, \n",
    "    param_range=k_range,\n",
    "    param_name='n_neighbors',\n",
    "    scoring=f1,\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "90f07247-6fbb-40cc-9b74-34f5234d23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "73aa5d09-c13d-4402-85d9-16a3e85b7277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, train_mean, label='Training score', color='blue')\n",
    "plt.plot(k_range, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve for KNN - K vs F1 Score')\n",
    "plt.xlabel('Number of Neighbors - k')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(np.arange(1, 50, 2))\n",
    "plt.grid(which='both', axis='both', alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "c7d7733b-b068-4c68-b7d9-0a519a682c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find and plot for distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "e9ef091b-6d60-4d99-a365-a61b94deebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_metrics = ['euclidean', 'minkowski', 'manhattan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "5c8de560-ad06-419e-b2b2-2903ecf56545",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_score = validation_curve(\n",
    "    KNeighborsClassifier(n_neighbors=13),\n",
    "    X_train, y_train, \n",
    "    param_range=distance_metrics,\n",
    "    param_name='metric',\n",
    "    scoring=f1,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "d24e253e-0402-4328-94f2-357833b4d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "69bb133c-3731-4c66-99b1-f15433f884c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distance_metrics, train_mean, label='Training score', color='blue')\n",
    "plt.plot(distance_metrics, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve for KNeighborsClassifier - Distance Metric vs F1')\n",
    "plt.xlabel('Distance Metric')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "6ae088fd-3703-40ef-ba51-8579fa98fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find and plot for weight options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "de6ccd24-426c-48a0-8ce0-3a03b2934c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ['uniform', 'distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "8ca7390a-ee10-4c55-991a-dbb0a124942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_score = validation_curve(\n",
    "    KNeighborsClassifier(n_neighbors=13, metric='manhattan'),\n",
    "    X_train, y_train, \n",
    "    param_range=weights,\n",
    "    param_name='weights',\n",
    "    scoring=f1,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "f7070482-9d61-49cc-90f8-8245bbc76042",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "6b7a1afe-15c3-46b9-9d17-8a2932f3c56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(weights, train_mean, label='Training score', color='blue')\n",
    "plt.plot(weights, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve for KNeighborsClassifier - Weight Options vs F1')\n",
    "plt.xlabel('Weight Options')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "c4c15bca-c605-43f3-9780-45c42b786f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "03de7f5e-4352-4414-b230-11ba46c0434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "KNeighborsClassifier(n_neighbors=13, metric='manhattan', weights='uniform'), X_train, y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5,\n",
    "    scoring=make_scorer(f1_score, average='weighted')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "70fb90a3-7ba1-49a8-b87d-5e79f4b11c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(valid_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "25c2a8b2-c631-4e76-866d-31d0167be50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training score', color='blue')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Learning Curve for KNN Classifier')\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "3283105e-028c-4e1b-8cb2-ccac27b4f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1 score:  0.8747412008281572\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=13, metric='manhattan', weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Test set F1 score: \", test_f1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for KNN Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "e58b1a5d-fcb7-403e-954b-8b08067b9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Testing times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "60b746e8-b49f-4e0d-817d-3a1696dab48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "e622c309-93c8-4fa5-a201-a670f8a37fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation curve plottings + find hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "5e39e0cf-0088-478d-98c0-5aee6fee2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the best kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "7cc7b769-529c-4834-ab97-93ca2a83723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the appropriate C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "4dcb767d-4013-457d-baae-9ced05782abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_range = np.logspace(-4, 2, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "8fb05c91-0adb-413e-93c7-47801dcc7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    SVC(kernel='rbf'),\n",
    "    X_train, y_train, \n",
    "    param_range=c_range,\n",
    "    param_name='C',\n",
    "    scoring=f1,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "4210ccd4-9217-4afd-926d-4a220a47fc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(c_range, train_mean, label='Training score', color='blue')\n",
    "plt.semilogx(c_range, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve for SVM(RBF Kernel) - C vs F1')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "417eef34-23b4-4205-a9fd-73e6d96f288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    SVC(kernel='poly'),\n",
    "    X_train, y_train, \n",
    "    param_range=c_range,\n",
    "    param_name='C',\n",
    "    scoring=f1,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "cfd1db21-241f-455d-90eb-e4c4fce65e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(c_range, train_mean, label='Training score', color='blue')\n",
    "plt.semilogx(c_range, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve for SVM (Polynomial kernel) - C vs F1')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "59580ff5-fa92-443a-b97a-b2f7b1bc9e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    SVC(kernel='linear'),\n",
    "    X_train, y_train, \n",
    "    param_range=c_range,\n",
    "    param_name='C',\n",
    "    scoring=f1,\n",
    "    cv=5\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(c_range, train_mean, label='Training score', color='blue')\n",
    "plt.semilogx(c_range, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve for SVM(Linear) - C vs F1')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "9abc3214-9b19-4885-95c8-97439a3c5542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    SVC(kernel='sigmoid'),\n",
    "    X_train, y_train, \n",
    "    param_range=c_range,\n",
    "    param_name='C',\n",
    "    scoring=f1,\n",
    "    cv=5\n",
    ")\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(c_range, train_mean, label='Training score', color='blue')\n",
    "plt.semilogx(c_range, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve for SVM(Sigmoid kernel) - C vs F1')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1598ad8-fd52-49bc-9a89-ba63a05136b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ff1bf-65e4-415a-af2f-6c8a1bcd054f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04ff87-b4fe-4813-9a87-9a6e0e3e14b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "f8c8b25c-cb8f-4c70-9c4f-95901a35d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find appropriate degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "cd5a4749-2ff5-474b-a0ca-5138f32d5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_range = range(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "2d4cb2da-3a33-4840-b7cf-ff8dfba41fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    SVC(kernel='poly', C = 0.1),\n",
    "    X_train, y_train, \n",
    "    param_range=d_range,\n",
    "    param_name='degree',\n",
    "    scoring=f1,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "7475ad89-d5c6-40e9-a682-090a42526e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(d_range, train_mean, label='Training score', color='blue')\n",
    "plt.plot(d_range, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve for SVM - Degree of Polynomial vs F1')\n",
    "plt.xlabel('Degree of Polynomial')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "4432d166-e195-4fca-a312-536d02f4b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the appropriate Gamma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "0d7a8835-4bac-41f9-8b6b-da2ac2be30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.logspace(-4, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "83057fe2-352e-4694-8d83-81b2c3f1082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    SVC(kernel='poly', C = 0.1, degree=3),\n",
    "    X_train, y_train, \n",
    "    param_range=gamma,\n",
    "    param_name='gamma',\n",
    "    scoring=f1,\n",
    "    cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "217ad16e-483a-425e-b2df-27247ba65691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAImCAYAAABkcNoCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACKeUlEQVR4nOzdd3RU1d7G8e+k94SWECBA6L0jvUsgKFYsFy9IVcQOFri8ioKKiiI2QFRAQbGCFZGAdFCaSBVBSiiB0BMISSbJef8YZySkQ5IzmTyftbKYOfU3szMkT87e+1gMwzAQERERERGRHLmZXYCIiIiIiIizU3ASERERERHJg4KTiIiIiIhIHhScRERERERE8qDgJCIiIiIikgcFJxERERERkTwoOImIiIiIiORBwUlERERERCQPCk4iIiIiIiJ5UHASkSJz66234uvry7lz53Lc5p577sHT05MTJ07k+7gWi4XnnnvO8XzFihVYLBZWrFiR576DBg2ievXq+T7X5aZNm8acOXOyLD948CAWiyXbdcVl9erV3HnnnVSuXBkvLy+Cg4Np374906dP5+LFi6bVdS1+//13unTpQnBwMBaLhalTpxbp+U6fPs3YsWNp0KAB/v7+BAcHU69ePQYMGMC2bduAq/uetlgsWCwWBg0alO32EyZMcGxz8ODBQn5VkJCQwMsvv0ybNm0ICQnB09OTsLAwevfuzaeffkpKSkqhn1P+bfcrv8qXL+/Y5siRIzz22GN06dKFkJAQ0/8fEZHcKTiJSJEZOnQoycnJfPrpp9muP3/+PAsXLuTGG28kLCzsqs/TokUL1q9fT4sWLa76GPmRU3AKDw9n/fr13HDDDUV6/pyMHz+ezp07c/ToUSZOnEhMTAyfffYZPXr04LnnnuP//u//TKnrWg0ZMoS4uDg+++wz1q9fz913311k57pw4QJt27Zlzpw5DBs2jO+++45PPvmE++67jwMHDrB161bg6r+nAwMD+fLLL0lMTMy0vWEYzJkzh6CgoCJ5XXv37qV58+a8+OKLdOzYkY8//phffvmFt99+m8qVKzNkyBBeeOGFIjm3QL9+/Vi/fn2mr59//tmxft++fXzyySd4eXnRp08fEysVkXwxRESKSFpamlGpUiWjZcuW2a6fPn26ARjff/99gY4LGOPHj7+qmu69916jWrVqV7Vvw4YNjS5dulzVvkXliy++MABj6NChRkZGRpb1CQkJxs8//1wo57p48WKhHCe/PDw8jAceeKDQjpeammpYrdZs182aNcsAjF9++SXb9enp6YZhXN33NGD897//NXx9fY2ZM2dm2n7p0qUGYAwfPtwAjAMHDlzFK8ue1Wo1GjRoYISEhBi7du3KdpuDBw8aCxcuLLRzyr8A48EHH8x1G/v3lWEYxsaNGw3AmD17dhFXJiJXS1ecRKTIuLu7c++997J582a2b9+eZf3s2bMJDw8nOjqakydPMnLkSBo0aEBAQAChoaF0796d1atX53menLrqzZkzh7p16+Lt7U39+vX5+OOPs93/+eefp02bNpQtW5agoCBatGjBhx9+iGEYjm2qV6/Ozp07WblypaPLjb3LX05d9dasWUOPHj0IDAzEz8+P9u3b8+OPP2ap0WKxsHz5ch544AHKly9PuXLluO222zh27Fier33ChAmUKVOGt956C4vFkmV9YGAgUVFRudYJWbs/Pvfcc1gsFrZs2UK/fv0oU6YMNWvWZOrUqVgsFvbt25flGE8//TReXl6cOnXKsWzp0qX06NGDoKAg/Pz86NChA8uWLcv1Ndnfk7S0NKZPn+54v+127NjBzTffTJkyZfDx8aFZs2Z89NFHmY5h/56YO3cuo0ePpnLlynh7e2dbN9i66YHt6mF23NxsPy4L8j19ueDgYG699VZmzZqVafmsWbPo0KEDderUyfU9uRoLFy5k165djBs3jvr162e7TbVq1bjlllscz5OTkxk9ejTNmjUjODiYsmXL0q5dO7799tss+1osFh566CFmz55N3bp18fX1pVWrVvz6668YhsHkyZOJjIwkICCA7t27Z3nvu3btSqNGjVi/fj3t27fH19eX6tWrM3v2bAB+/PFHWrRogZ+fH40bN2bx4sWZ9t+3bx+DBw+mdu3a+Pn5UblyZfr27Zttu1ypefPmdOrUKcvy9PR0KleuzG233eZYNn36dJo2bUpAQACBgYHUq1eP//3vf3meIz/s31ciUjLoEysiRWrIkCFYLJYsvzDu2rWLDRs2cO+99+Lu7s6ZM2cAW7ezH3/8kdmzZ1OjRg26du2ar7FLV5ozZw6DBw+mfv36fP311/zf//0fEydO5Jdffsmy7cGDB7n//vv54osvWLBgAbfddhsPP/wwEydOdGyzcOFCatSoQfPmzR1dbhYuXJjj+VeuXEn37t05f/48H374IfPnzycwMJC+ffvy+eefZ9l+2LBheHp68umnn/Lqq6+yYsUK/vvf/+b6GuPi4tixYwdRUVH4+fkV4N3Jv9tuu41atWrx5ZdfMmPGDP773//i5eWVJXylp6czb948+vbt6xjDMW/ePKKioggKCuKjjz7iiy++oGzZsvTq1SvX8HTDDTewfv16IHNXJ4A9e/bQvn17du7cyVtvvcWCBQto0KABgwYN4tVXX81yrLFjxxIbG8uMGTP4/vvvCQ0Nzfac7dq1A2DgwIF88803jiCVnfx+T19p6NCh/Prrr+zevRuAc+fOsWDBAoYOHZrjua5FTEwMADfddFO+90lJSeHMmTM88cQTfPPNN8yfP5+OHTty2223ZfuHhx9++IEPPviAl19+mfnz55OYmMgNN9zA6NGjWbt2Le+88w4zZ85k165d3H777Zn+GAFw/PhxBg8ezLBhw/j2229p3LgxQ4YMYcKECYwdO5annnqKr7/+moCAAG655ZZMf0w4duwY5cqV4+WXX2bx4sW8++67eHh40KZNG/bs2ZPr6xw8eDBr1qxh7969mZYvWbKEY8eOMXjwYAA+++wzRo4cSZcuXVi4cCHffPMNjz/+eL7HDRqGQVpaWqavK98DESlBTL3eJSKlQpcuXYzy5csbqampjmWjR482AOOvv/7Kdp+0tDTDarUaPXr0MG699dZM67iiq97y5csNwFi+fLlhGLbuL5UqVTJatGiRqfvawYMHDU9Pz1y76qWnpxtWq9WYMGGCUa5cuUz759RV78CBA1m62LRt29YIDQ01EhMTM72mRo0aGVWqVHEcd/bs2QZgjBw5MtMxX331VQMw4uLicqz1119/NQBjzJgxOW6TV512V76n48ePNwDj2WefzbLtbbfdZlSpUiVTN6NFixZl6qJ28eJFo2zZskbfvn0z7Zuenm40bdrUuO666/Ksl2y6Ot19992Gt7e3ERsbm2l5dHS04efnZ5w7d84wjH+/Jzp37pzneewmTJhgeHl5GYABGJGRkcaIESOMP/74I8u2Bfmetr+OjIwMIzIy0njiiScMwzCMd9991wgICDASExONyZMnF3pXvd69exuAkZycnGl5RkaGYbVaHV9paWk5HsP+ORw6dKjRvHnzLK+rYsWKxoULFxzLvvnmGwMwmjVrlumzM3XqVAMwtm3b5ljWpUsXAzA2bdrkWHb69GnD3d3d8PX1NY4ePepYvnXrVgMw3nrrrVxrTU1NNWrXrm08/vjjubwzhnHq1CnDy8vL+N///pdp+Z133mmEhYU5unQ+9NBDRkhISK7Hyon9++jKr/fffz/b7dVVT8T56YqTiBS5oUOHcurUKb777jsA0tLSmDdvHp06daJ27dqO7WbMmEGLFi3w8fHBw8MDT09Pli1b5vgLfX7t2bOHY8eO0b9//0xdvKpVq0b79u2zbP/LL79w/fXXExwcjLu7O56enjz77LOcPn2a+Pj4Ar/eixcv8ttvv9GvXz8CAgIcy93d3RkwYABHjhzJ8hfxK68KNGnSBIBDhw4V+PyF6fbbb8+ybPDgwRw5coSlS5c6ls2ePZuKFSs6uqitW7eOM2fOcO+992b6a3tGRga9e/dm48aNVzXb3y+//EKPHj2IiIjItHzQoEEkJSU5rkzlVn9OnnnmGWJjY5k1axb3338/AQEBzJgxg5YtWzJ//vxM2+b3e/py9pn15s6dS1paGh9++CF33nlnpu+RvBTG1Ys333wTT09Px1fTpk0zrf/yyy/p0KEDAQEBjs/hhx9+mO3nsFu3bvj7+zue27sERkdHZ/rs2Zdf+f0cHh5Oy5YtHc/Lli1LaGgozZo1o1KlSrnun5aWxksvvUSDBg3w8vLCw8MDLy8v9u7dm+f/GeXKlaNv37589NFHZGRkAHD27Fm+/fZbBg4ciIeHBwDXXXcd586d4z//+Q/ffvttpm6o+XHnnXeycePGTF+Xd40UkZJFwUlEily/fv0IDg52jF1YtGgRJ06cyNRFacqUKTzwwAO0adOGr7/+ml9//ZWNGzfSu3dvLl26VKDz2btZVaxYMcu6K5dt2LDBMQbo/fffZ+3atWzcuJFx48YBFPjcYPsFzDCMbMfL2H8ZvLIrWLly5TI99/b2zvP8VatWBeDAgQMFrjG/snsN0dHRhIeHO9rz7NmzfPfddwwcONDRRc0+FXe/fv0y/ZLu6enJK6+8gmEYju6ZBXH69OkCva85jVnKSVhYGIMHD2bGjBls27aNlStX4uXlxaOPPpppu/x8T2dn8ODBnDx5kpdeeoktW7YUqJvewYMHs7yXK1euzHF7+/fHlWGlf//+jl/ir5yJcsGCBY5p7efNm8f69evZuHEjQ4YMITk5Ocs5ypYtm+m5l5dXrsuvPMaV29m3zc/+o0aN4plnnuGWW27h+++/57fffmPjxo00bdo0X5/bIUOGcPToUUeXxvnz55OSkpJp2vgBAwYwa9YsDh06xO23305oaCht2rRx7JOXChUq0KpVq0xfl09HLiIli4fZBYiI6/P19eU///kP77//PnFxccyaNYvAwEDuuOMOxzbz5s2ja9euTJ8+PdO+V07fnB/2EHL8+PEs665c9tlnn+Hp6ckPP/yAj4+PY/k333xT4PPalSlTBjc3N+Li4rKss4/RKIxfnsLDw2ncuDFLliwhKSkpz3FO9td35X17chvPk92EE/YrZ2+99Rbnzp1z3AvIPi4E/n19b7/9Nm3bts322FczBX25cuUK9L5mV39BdO7cmaioKL755hvi4+MdY6Ty8z2dnYiICK6//nqef/556tatm+0V0JxUqlSJjRs3ZlpWt27dHLfv2bMnM2fO5LvvvuOJJ55wLA8NDXW8jsDAwEzfD/PmzSMyMpLPP/8803vnjPd6mjdvHgMHDuSll17KtPzUqVOEhITkuX+vXr2oVKkSs2fPplevXsyePZs2bdrQoEGDTNsNHjyYwYMHc/HiRVatWsX48eO58cYb+euvv6hWrVphviQRcXK64iQixWLo0KGkp6czefJkFi1axN13353pF32LxeK4ymK3bdu2LF2v8qNu3bqEh4czf/78TF2ZDh06xLp16zJta7FY8PDwyDSY/9KlS8ydOzfLcb29vfP1l2x/f3/atGnDggULMm2fkZHBvHnzqFKlSqHNovbMM89w9uxZHnnkkWy7bV24cIElS5YAtqDi4+PjuJmrXXYzpuVl8ODBJCcnM3/+fObMmUO7du2oV6+eY32HDh0ICQlh165dWf7ibv+yX0UoiB49evDLL79kmXHw448/xs/PL8eQlpcTJ044umxdLj09nb179+Ln55fll/G8vqdzMnr0aPr27cszzzxToBq9vLyyvIeBgYE5bn/rrbfSoEEDXnrpJf788898ncNiseDl5ZUpNB0/fvyqvkeKWnb/Z/z4448cPXo0X/vb/wDwzTffsHr1ajZt2sSQIUNy3N7f35/o6GjGjRtHamoqO3fuvKb6RaTk0RUnESkWrVq1okmTJkydOhXDMLJ0UbrxxhuZOHEi48ePp0uXLuzZs4cJEyYQGRlJWlpagc7l5ubGxIkTGTZsGLfeeivDhw/n3LlzPPfcc1m66t1www1MmTKF/v37c99993H69Glee+21LL+QATRu3JjPPvuMzz//nBo1auDj40Pjxo2zrWHSpEn07NmTbt268cQTT+Dl5cW0adPYsWMH8+fPv+YrIXZ33HEHzzzzDBMnTuTPP/9k6NCh1KxZk6SkJH777Tfee+897rrrLqKiorBYLPz3v/9l1qxZ1KxZk6ZNm7Jhw4Ycb+aam3r16tGuXTsmTZrE4cOHmTlzZqb1AQEBvP3229x7772cOXOGfv36ERoaysmTJ/njjz84efJklquL+TF+/Hh++OEHunXrxrPPPkvZsmX55JNP+PHHH3n11VcJDg4u8DEB5s6dy3vvvUf//v1p3bo1wcHBHDlyhA8++ICdO3fy7LPPZgl6eX1P5yQqKsrRPbQoubu7880339CrVy+uu+46hg8fTteuXSlTpgznzp3jt99+448//sg0VfmNN97IggULGDlyJP369ePw4cNMnDiR8PDwLDPQme3GG29kzpw51KtXjyZNmrB582YmT55MlSpV8n2MIUOG8Morr9C/f398fX256667Mq0fPnw4vr6+dOjQgfDwcI4fP86kSZMIDg6mdevWhfI6vvrqKwD2798PwKZNmxzj3vr161co5xCRQmLixBQiUsq8+eabBmA0aNAgy7qUlBTjiSeeMCpXrmz4+PgYLVq0ML755ptsb1hLHrPq2X3wwQdG7dq1DS8vL6NOnTrGrFmzsj3erFmzjLp16xre3t5GjRo1jEmTJhkffvhhllnODh48aERFRRmBgYEG4DhOTrPVrV692ujevbvh7+9v+Pr6Gm3bts1ys1/7rHobN27MtDyn15STlStXGv369TPCw8MNT09PIygoyGjXrp0xefJkIyEhwbHd+fPnjWHDhhlhYWGGv7+/0bdvX+PgwYM5zqp38uTJHM85c+ZMAzB8fX2N8+fP51jXDTfcYJQtW9bw9PQ0KleubNxwww3Gl19+medrIocbiG7fvt3o27evERwcbHh5eRlNmzbN8t7b37/8nMcwDGPXrl3G6NGjjVatWhkVKlQwPDw8jDJlyhhdunQx5s6dm+N+uX1P5/U6LlcUs+rZnT9/3njppZeM1q1bG0FBQYaHh4cRGhpq9OzZ03j33Xez3Nj45ZdfNqpXr254e3sb9evXN95//33H98Plsntd9s/C5MmTMy3Prj26dOliNGzYMEu91apVM2644YYsy68839mzZ42hQ4caoaGhhp+fn9GxY0dj9erVRpcuXQp0o+r27dsbgHHPPfdkWffRRx8Z3bp1M8LCwgwvLy+jUqVKxp133plpdsCc5Kfd7dvl9CUizsViGLqhgIiIiIiISG40xklERERERCQPCk4iIiIiIiJ5UHASERERERHJg4KTiIiIiIhIHhScRERERERE8qDgJCIiIiIikodSdwPcjIwMjh07RmBgYKHdgFJEREREREoewzBITEykUqVKuLnlfk2p1AWnY8eOERERYXYZIiIiIiLiJA4fPkyVKlVy3abUBafAwEDA9uYEBQWZXA1YrVaWLFlCVFQUnp6eZpcjhUBt6nrUpq5J7ep61KauSe3qepypTRMSEoiIiHBkhNyUuuBk754XFBTkNMHJz8+PoKAg079xpHCoTV2P2tQ1qV1dj9rUNaldXY8ztml+hvBocggREREREZE8KDiJiIiIiIjkQcFJREREREQkD6VujFN+paenY7Vai/w8VqsVDw8PkpOTSU9PL/LzSdFz9Tb19PTE3d3d7DJEREREipWC0xUMw+D48eOcO3eu2M5XsWJFDh8+rPtKuYjS0KYhISFUrFjRZV+fiIiIyJUUnK5gD02hoaH4+fkV+S+GGRkZXLhwgYCAgDxvuiUlgyu3qWEYJCUlER8fD0B4eLjJFYmIiIgUDwWny6SnpztCU7ly5YrlnBkZGaSmpuLj4+Nyv2SXVq7epr6+vgDEx8cTGhqqbnsiIiJSKrjeb3XXwD6myc/Pz+RKRJyb/TNSHOMARURERJyBglM2NG5DJHf6jIiIiEhpo+AkIiIiIiKSBwUnyVHXrl157LHH8r39wYMHsVgsbN26tchqEhERERExgyaHcAF5dZu69957mTNnToGPu2DBAjw9PfO9fUREBHFxcZQvX77A5xIRERERcWYKTi4gLi7O8fjzzz/n2WefZc+ePY5l9lnQ7KxWa74CUdmyZQtUh7u7OxUrVizQPiVBft8vEREREXFd6qrnAipWrOj4Cg4OxmKxOJ4nJycTEhLCF198QdeuXfHx8WHevHmcPn2a//znP1SpUgU/Pz8aN27M/PnzMx33yq561atX56WXXmLIkCEEBgZStWpVZs6c6Vh/ZVe9FStWYLFYWLZsGa1atcLPz4/27dtnCnUAL7zwAqGhoQQGBjJs2DDGjBlDs2bNcny9Z8+e5Z577qFChQr4+vpSu3ZtZs+e7Vh/5MgR7r77bsqWLYu/vz+tWrXit99+c6yfPn06NWvWxMvLi7p16zJ37txMx7dYLMyYMYObb74Zf39/XnjhBQC+//57WrZsiY+PDzVq1OD5558nLS0tX20kIiIiIiWbqcFp1apV9O3bl0qVKmGxWPjmm2/y3GflypWZfnmdMWNGkdZoGHDxojlfhlF4r+Ppp5/mkUceYffu3fTq1Yvk5GRatmzJDz/8wI4dO7jvvvsYMGBApoCRnddff51WrVrx+++/M3LkSB544AH+/PPPXPcZN24cr7/+Ops2bcLDw4MhQ4Y41n3yySe8+OKLvPLKK2zevJmqVasyffr0XI/3zDPPsGvXLn766Sd2797N9OnTHd0DL1y4QJcuXTh27Bjfffcdf/zxB0899RQZGRkALFy4kEcffZTRo0ezY8cO7r//fgYPHszy5csznWP8+PHcfPPNbN++nSFDhvDzzz/z3//+l0ceeYRdu3bx3nvvMWfOHF588cVcaxURERER12BqV72LFy/StGlTBg8ezO23357n9gcOHKBPnz4MHz6cefPmsXbtWkaOHEmFChXytf/VSEqCgIAiOfQ/3ICQbNdcuAD+/oVzlscee4zbbrst07InnnjC8fjhhx9m8eLFfPnll7Rp0ybH4/Tp04eRI0cCtjD2xhtvsGLFCurVq5fjPi+++CJdunQBYMyYMdxwww0kJyfj4+PD22+/zdChQxk8eDAAzz77LEuWLOHChQs5Hi82NpbmzZvTqlUrwHYlzO7TTz/l5MmTbNy40dHVsFatWo71r732GoMGDXK8hlGjRvHrr7/y2muv0a1bN8d2/fv3zxTwBgwYwJgxY7j33nsBqFGjBhMnTuSpp55i/PjxOdYqIiIiIq7B1OAUHR1NdHR0vrefMWMGVatWZerUqQDUr1+fTZs28dprrxVZcHIV9pBhl56ezssvv8znn3/O0aNHSUlJISUlBf88klqTJk0cj+1dAuPj4/O9T3h4OADx8fFUrVqVPXv2OEKM3XXXXccvv/yS4/EeeOABbr/9drZs2UJUVBS33HIL7du3B2Dr1q00b948x/FZu3fv5r777su0rEOHDrz55puZll35fm3evJmNGzdmusKUnp5OcnIySUlJummyiIiIiIsrUZNDrF+/nqioqEzLevXqxYcffpjjAH57ILBLSEgAbAP+rVZrpm2tViuGYZCRkeHo2uXjA//sUiQMwyAxMZHAwMAss+P5+MA/ZeSbve4r//X19XU8BtuVlzfeeIMpU6bQuHFj/P39efzxx0lJScm0nf39sPPw8Mj03GKxkJ6enuk9sz+2P3d3d3c8Nv7pf5iWlpZp2eXHvLL2K/Xq1YsDBw7w448/smzZMnr06MHIkSOZPHkyPj4+ue6b0/ksFkumZVe+XxkZGTz33HPceuutWY7n5eWV5T3L7jyuJCMjA8MwsFqtuLu7m11OkbP/X3Hl/xlSsqldXY/a1MZqhXPn4MwZOHvW8s+/WR+fPWvb5swZC+fPF/x3juLjQWpqb7y8PIBCHMcgJrK16Y8/ppNLR6diUZD/L0pUcDp+/DhhYWGZloWFhZGWlsapU6ccVzMuN2nSJJ5//vksy5csWZLlKoGHhwcVK1bkwoULpKamFm7xufD3h4yMxCzLE7MuylNycjKGYTgCor3L28WLFx3LAJYvX050dDQ33XQTYPtF+K+//qJOnTqO7dLS0khNTXU8z8jIIDk5OdNx0tPTSUlJISEhIcu5kpKS/nkdibi5uTnW2etKSEigVq1arF27lptvvtlxzN9++4309PRM57mSt7c3t912G7fddhutWrVi/PjxPPPMM9SuXZsPPviAQ4cOUaZMmSz71a5dmxUrVnDLLbc4lq1atYpatWplOt+lS5cyPW/SpIljTNSVcupWmHg1DVhCpKamcunSJVatWlWqJsiIiYkxuwQpAmpX1+MKbWoYkJLiTmKiJxcueHHhwr//JiZ6cfGi7d/snl+65IozwXqbXYAUOm/WrVvF6dNnTa3C/vtqfpSo4ARZ71lk/+t+TvcyGjt2LKNGjXI8T0hIICIigqioKIKCgjJtm5yczOHDhwkICHBcuShquV1xuho+Pj5YLBbHawv4Z4CWv79/ptdbr149FixYwI4dOyhTpgxvvPEG8fHxNGjQwLGdh4cHXl5ejudubm74+PhkOo67uzve3t4EBQVlOZc9mAYGBjr2sXcFDAgIICgoiEceeYT777+fdu3a0b59e7744gt27dpFjRo1srSP3fjx42nRogUNGzYkJSWFZcuWUb9+fYKCghg8eDBTp07l3nvv5cUXXyQ8PJzff/+dSpUq0a5dO55++mnuvvturrvuOnr06MEPP/zA999/z5IlSzKdz9fXN9Pz5557jptuuokaNWrQr18/3Nzc2LZtGzt27GDixImZ6ivsNnVGycnJ+Pr60rlz52L7rJjJarUSExNDz549NTW9C1G7uh5nbNP0dNvVn8uv+Jw5A+fO/Xv158wZyz/rMz9OTb22nyEhIQZlykCZMgZly+J4XKYMlC0LZcsahITYHgcHG3g46W+FaWlprF+/nnbt2uHhrEVKgdjb9O672xIcbO5nNbc/1F+pRH33VaxYkePHj2daFh8fj4eHB+XKlct2H29vb7y9s/6VwtPTM8t/qunp6VgsFtzc3BxXSIqavSuX/bzXyn6M7P69/PjPPvssBw8eJDo6Gj8/P+677z5uueUWzp8/n2m7K+vKrs4r3zP74+zOfeWyAQMGcPDgQZ566imSk5O58847GTRoEBs2bMjx/fD29mbcuHEcPHgQX19fOnXqxGeffeYIdkuWLGH06NHceOONpKWl0aBBA959913c3Ny47bbbePPNN3nttdd47LHHiIyMZPbs2XTv3j3L+3j5+aOjo/nhhx+YMGECkydPxtPTk3r16jFs2LAsdRZ2mzojNzc3LBZLtp8jV1baXm9poXZ1PUXRppcucVnXtsyPc3t+7ty1ndfT0x5ycISf/DwPCQF3d3vwKtl/xLNa4ciRRJo08dBn1UXY2zQ42Pz/fwtyfothFOak11fPYrGwcOHCTF2orvT000/z/fffs2vXLseyBx54gK1bt7J+/fp8nSchIYHg4GDOnz+f7RWnAwcOEBkZWWx/Rc/IyCAhIYGgoCCX/SW7oHr27EnFihWz3F+ppCgNbWrGZ8VMVquVRYsW0adPH9P/g5fCo3Z1PXm1aUaGbdxydmEnryCUnHxttQUGFiz42L/8/MBFOy/kmz6rrseZ2jS3bHAlU684XbhwgX379jmeHzhwgK1bt1K2bFmqVq3K2LFjOXr0KB9//DEAI0aM4J133mHUqFEMHz6c9evX8+GHH2a5cauUHElJScyYMYNevXrh7u7O/PnzWbp0qUv0TxcRkeL1++8wf35dfv7ZjfPnMwcf+9Wfa5kAwd09c7C5MuTkFITKlLFdORKRks3U4LRp06ZM986xj0W69957mTNnDnFxccTGxjrWR0ZGsmjRIh5//HHeffddKlWqxFtvvaWpyEswi8XCokWLeOGFF0hJSaFu3bp8/fXXXH/99WaXJiIiJcTBg/B//weffOIJ5HxfQTs/v6u7+hMYqKs/IqWZqcGpa9eu5NZTcM6cOVmWdenShS1bthRhVVKcfH19Wbp0qdlliIhICXTmDLz0Erz9Ntgnw23b9hjduoVRoYJ7jld/SkEPYxEpAiVqcggRERGR5GR45x148cV/J1/o3h0mTbISF7fxn3ETrn+POREpXq45cl1ERERcTkYGzJsHdevCk0/aQlPjxvDTT7B0KTRvbnaFIuLKdMVJREREnN7SpbawtHWr7XmVKjBxIgwYYJu0QUSkqCk4iYiIiNP64w94+mn4+Wfb86Ag+N//4JFHwNfX3NpEpHRRcBIRERGnExsLzzwDc+eCYdim837wQRg3DsqXN7s6ESmNFJxERETEaZw7B5MmwZtvQkqKbdndd9smgqhRw9TSRKSU0+QQUuLMmTOHkJAQx/PnnnuOZs2a5brPoEGDuOWWW6753IV1HBERySwlBd54A2rWhFdftT3v0gU2bID58xWaRMR8Ck4u5Pjx4zz88MPUqFEDb29vIiIi6Nu3L8uWLTO7tCL1xBNPFPprPHjwIBaLha32Ucj/ePPNN7O9v5iIiFydjAxbMKpXD0aNst2bqUED+OEHWL4cWrc2u0IRERt11XMRBw8epEOHDoSEhPDqq6/SpEkTrFYrP//8Mw8++CB//vlntvtZrVY8PT2LudrCFRAQQEBAQLGcKzg4uFjOU5xSU1Px8vIyuwwRKYWWL7fNlLd5s+15eLhtprx77wUP/YYiIk5GV5xcxMiRI7FYLGzYsIF+/fpRp04dGjZsyKhRo/j1118d21ksFmbMmMHNN9+Mv78/L7zwAgDTp0+nZs2aeHl5UbduXebOnZvp+M899xxVq1bF29ubSpUq8cgjjzjWTZs2jdq1a+Pj40NYWBj9+vXLtsaMjAyqVKnCjBkzMi3fsmULFouF/fv3AzBlyhQaN26Mv78/ERERjBw5kgsXLuT42q/sqpeens6oUaMICQmhXLlyPPXUUxiGkWmfxYsX07FjR8c2N954I3///bdjfWRkJADNmzfHYrHQtWtXIGtXvZSUFB555BFCQ0Px8fGhY8eObNy40bF+xYoVWCwWli1bRqtWrfDz86N9+/bs2bMnx9eTmprKQw89RHh4OD4+PlSvXp1JkyY51p87d4777ruPsLAwfHx8aNSoET/88INj/ddff03Dhg3x9vamevXqvP7665mOX716dV544QUGDRpEcHAww4cPB2DdunV07twZX19fIiIieOSRR7h48WKOdYqIXK0dO+CGG2w3rd28GQID4YUXYO9eGDpUoUlEnJOCUx4Mw+Bi6sWi/bJmv/zKX/ZzcubMGRYvXsyDDz6Iv79/lvWXjwcCGD9+PDfffDPbt29nyJAhLFy4kEcffZTRo0ezY8cO7r//fgYPHszy5csB+Oqrr3jjjTd477332Lt3L9988w2NGzcGYNOmTTzyyCNMmDCBPXv2sHjxYjp37pxtnW5ubtx999188sknmZZ/+umntGvXjhr/dGB3c3PjrbfeYseOHXz00Uf88ssvPPXUU/l6LwBef/11Zs2axYcffsiaNWs4c+YMCxcuzLTNxYsXGTVqFBs3bmTZsmW4ublx6623kpGRAcCGDRsAWLp0KXFxcSxYsCDbcz311FN8/fXXfPTRR2zZsoVatWoRHR3N2bNnM203btw4Xn/9dTZt2oSHhwdDhgzJsf633nqL7777ji+++II9e/Ywb948qlevDtjCZ3R0NOvWrWPevHns2rWLl19+Gfd/bmKyefNm7rzzTu6++262b9/Oc889xzPPPJOle+HkyZNp1KgRmzdv5plnnmH79u306tWL2267jW3btvH555+zZs0aHnrooXy/7yIieTl61BaMmjaFRYtsAemhh2DfPttsedn8CBMRcRr6m04ekqxJBEwqnm5gV7ow9gL+Xnn/FNm3bx+GYVCvXr18Hbd///6ZfnHv378/gwYNYuTIkQCOq1SvvfYa3bp1IzY2looVK3L99dfj6elJ1apVue666wCIjY3F39+fG2+8kcDAQKpVq0bzXG7dfs899zBlyhQOHTpEtWrVyMjI4LPPPuN///ufY5vHHnvM8TgyMpKJEyfywAMPMG3atHy9vqlTpzJ27Fhuv/12AGbMmMHP9huA/MO+zu7DDz8kNDSUXbt20ahRIypUqABAuXLlqFixYrbnuXjxItOnT2fOnDlER0cD8P777xMTE8PcuXP5v//7P8e2L774Il26dAFgzJgx3HDDDSQnJ+Pj45PluLGxsdSuXZuOHTtisVioVq2aY93SpUvZsGEDu3fvpk6dOgCOwAm2q3U9evTgmWeeAaBOnTrs2rWLyZMnM2jQIMd23bt354knnnA8HzhwIP3793e897Vr1+att96iS5cuTJ8+Pds6RUTy6/x524QPb7wBly7ZlvXrBy+9BLVrm1ubiEh+6YqTC7BfmbJYLPnavlWrVpme7969mw4dOmRa1qFDB3bv3g3AHXfcwaVLl6hRowbDhw9n4cKFpKWlAdCzZ0+qVatGjRo1GDBgAJ988glJSUkAfPLJJ47xRwEBAaxevZrmzZtTr1495s+fD8DKlSuJj4/nzjvvdJx7+fLl9OzZk8qVKxMYGMjAgQM5ffp0vrqNnT9/nri4ONq1a+dY5uHhkeU1//333/Tv358aNWoQFBTk6JoXGxubr/fQfgyr1ZrpvfP09KR169b89ddfmbZt0qSJ43F4eDgA8fHx2R530KBBbN26lbp16/LII4+wZMkSx7qtW7dSpUoVR2i6Uk5tuXfvXtLT0x3Lrnw/Nm/ezJw5czK1V69evcjIyODAgQO5vQ0iIjlKTYW334ZatWwh6dIl6NgR1q+HL79UaBKRkkVXnPLg5+nHhbE5j6+5VhkZGSQkJhAUGISbW+Yc6+fpl69j1K5dG4vFwu7du/M1VXZ23fmuDF2GYTiWRUREsGfPHmJiYli6dCkjR45k8uTJrFy5ksDAQLZs2cKKFStYsmQJzz77LM899xwbN27kpptuok2bNo5jVq5cGbBddfr0008ZM2YMn376Kb169aL8P3czPHToEH369GHEiBFMnDiRsmXLsmbNGoYOHYrVas3X+5Efffv2JSIigvfff59KlSqRkZFBo0aNSE1Nzfcxcgqsl793dpdPwGFfZ+8WeKUWLVpw4MABfvrpJ5YuXcqdd97J9ddfz1dffYWvr2+eNWVXz5Wu/B7IyMjg/vvvzzR2za5q1aq5nlNE5EqGAV99BWPHgn34aN268MorcNNNkM+/84mIOBVdccqDxWLB38u/aL88s1+e3ytIZcuWpVevXrz77rvZXpU5d+5crvvXr1+fNWvWZFq2bt066tev73ju6+vLTTfdxFtvvcWKFStYv34927dvB2xXdK6//npeffVVtm3bxsGDB/nll18IDAykVq1aji/7L/39+/dn+/btbN68ma+++op77rnHcZ5NmzaRlpbG66+/Ttu2balTpw7Hjh3L1/sAtlnvwsPDM02IkZaWxmb7lE3A6dOn2b17N//3f/9Hjx49qF+/fpYxSfZZ5i6/SnOlWrVq4eXllem9s1qtbN68OccrQvkVFBTEXXfdxfvvv8/nn3/O119/zZkzZ2jSpAlHjhzJckXLrkGDBtm2ZZ06dRzjoLLTokULdu7cmam97F+acU9ECmLVKmjbFu680xaawsJgxgzbhBA336zQJCIll644uYhp06bRvn17rrvuOiZMmECTJk1IS0sjJiaG6dOnO7rdZefJJ5/kzjvvpEWLFvTo0YPvv/+eBQsWsHTpUsB2w9n09HTatGmDn58fc+fOxdfXl2rVqvHDDz+wf/9+OnfuTJkyZVi0aBEZGRnUrVs3x/NFRkbSvn17hg4dSlpaGjfffLNjXc2aNUlLS+Ptt9+mb9++rF27NsssfHl59NFHefnll6lduzb169dnypQpmcJjmTJlKFeuHDNnziQ8PJzY2FjGjBmT6RihoaH4+vqyePFiqlSpgo+PT5apyP39/XnggQd48sknKVu2LFWrVuXVV18lKSmJAQMGFKjmy73xxhuEh4fTrFkz3Nzc+PLLL6lYsSIhISF06dKFzp07c/vttzNlyhRq1arFn3/+icVioXfv3owePZrWrVszceJE7rrrLtavX88777yT5/iwp59+mrZt2/Lggw8yfPhw/P392b17NzExMbz99ttX/VpEpPTYvRvGjIHvvrM99/eHp56y3ZupmO4YISJSpHTFyUVERkayZcsWunXrxujRo2nUqBE9e/Zk2bJlTJ8+Pdd9b7nlFt58800mT55Mw4YNee+995g9e7ZjCu6QkBDef/99OnToQJMmTVi2bBnff/895cqVIyQkhAULFtC9e3fq16/PjBkzmD9/Pg0bNsz1nPfccw9//PEHt912W6buZ82aNWPKlCm88sorNGrUiE8++STTVNz5MXr0aAYOHMigQYNo164dgYGB3HrrrY71bm5ufPbZZ2zevJlGjRrx+OOPM3ny5EzH8PDw4K233uK9996jUqVKmcLd5V5++WVuv/12BgwYQIsWLdi3bx8//fRTlpkMCyIgIIBXXnmFVq1a0bp1aw4ePMiiRYscXTm//vprWrduzX/+8x8aNGjAU0895bgy1qJFC7744gs+++wzGjVqxLPPPsuECRMyTQyRnSZNmrBy5Ur27t1Lp06daN68Oc8884xjPJaISE7i4uC++6BRI1tocneHBx6wzZT37LMKTSLiOixGfue8dhEJCQkEBwdz/vx5goKCMq1LTk7mwIEDREZGFtssYhkZGSQkJBAUlHWMk5RMpaFNzfismMlqtbJo0SL69OlT4m8YLf9Su16bxESYPBlefx3+mROIW26BSZMgn5O8Fjq1qWtSu7oeZ2rT3LLBldRVT0RERPLNaoX334fnnwf75KDt2tlC1BWTeoqIuBQFJxEREcmTYcDChbZxTHv32pbVrg0vvwy33qpJH0TE9Sk4iYiISK7WroUnn7TdfwmgQgV47jkYPhzUc0pESgsFJxEREcnWnj22ezEtXGh77ucHo0fDE09AHkMBRERcjoJTNkrZfBkiBabPiIhrO3HCNoZp5kxITwc3Nxg61HaVqVIls6sTETGHgtNl7LN6JCUlZZoiW0QyS/pnCi2zZ8IRkcJ14QJMmQKvvgr2+6n37Wsbx9Sggbm1iYiYTcHpMu7u7oSEhBD/zzRBfn5+WIp4tGtGRgapqakkJye77NTVpY0rt6lhGCQlJREfH09ISAju7u5mlyQihSAtDWbNgvHj4fhx27LWrW0z5XXpYm5tIiLOQsHpChUrVgRwhKeiZhgGly5dwtfXt8hDmhSP0tCmISEhjs+KiJRchmG7ae2YMfDnn7ZlNWrY7sV0xx2aKU9E5HIKTlewWCyEh4cTGhqK1Wot8vNZrVZWrVpF586d1e3JRbh6m3p6eupKk4gL+PVX20x5a9bYnpcrB88+CyNGgJeXubWJiDgjBaccuLu7F8svh+7u7qSlpeHj4+OSv2SXRmpTEXFme/fC//4HX31le+7jA48/Dk8/DcHB5tYmIuLMFJxERERKgZMnYcIEmDHDNqbJYoFBg2zLqlQxuzoREeen4CQiIuLCkpLgjTfglVcgMdG2LDra9rxxY3NrExEpSRScREREXFB6OsyZYxu3dOyYbVmLFraZ8rp3N7U0EZESScFJRETEhRgGLFpkG7O0c6dtWfXq8NJLcNddtpvZiohIwSk4iYiIuIhNm2wz5a1YYXtepgw88wyMHAne3qaWJiJS4ik4iYiIlHD798O4cfDZZ7bn3t7w6KO2+zOVKWNubSIirkLBSUREpIQ6fRpeeAHefResVttMeQMGwMSJULWq2dWJiLgWBScREZES5tIleOstmDQJzp+3LYuKss2U16yZqaWJiLgsBScREZESIj0d5s61jVs6csS2rGlTePVVW3ASEZGio+AkIiLi5AwDfv4ZnnoKtm+3LYuIgBdfhHvu0Ux5IiLFQcFJRETEiW3ZYgtMy5bZngcH2yaCePhh8PExtzYRkdJEwUlERMQJHTpkC0iffGJ77uUFDz0E//sflCtnbm0iIqWRgpOIiIiTeeMN21Tiqam25/3722bPi4w0ty4RkdJMwUlERMSJnD4No0fbxjV16waTJ0PLlmZXJSIiCk4iIiJOZNUqW2iqX982rsliMbsiEREB0Dw8IiIiTmTVKtu/XbsqNImIOBMFJxERESeycqXt386dza1DREQyU3ASERFxEufOwdattsdduphZiYiIXEnBSURExEmsWWMb31S7NoSHm12NiIhcTsFJRETESdjHN+lqk4iI81FwEhERcRIa3yQi4rwUnERERJxAYiJs3mx7rCtOIiLOR8FJRETECaxbB+npUL06VK1qdjUiInIlBScREREnoPFNIiLOTcFJRETECWh8k4iIc1NwEhERMVlSEmzYYHusK04iIs5JwUlERMRkv/4KVitUrgw1aphdjYiIZEfBSURExGSXj2+yWMytRUREsqfgJCIiYjKNbxIRcX4KTiIiIiZKSbF11QONbxIRcWamB6dp06YRGRmJj48PLVu2ZPXq1blu/+6771K/fn18fX2pW7cuH3/8cTFVKiIiUvg2bIDkZAgLg7p1za5GRERy4mHmyT///HMee+wxpk2bRocOHXjvvfeIjo5m165dVM3m7n/Tp09n7NixvP/++7Ru3ZoNGzYwfPhwypQpQ9++fU14BSIiItfGPr6pc2eNbxIRcWamXnGaMmUKQ4cOZdiwYdSvX5+pU6cSERHB9OnTs91+7ty53H///dx1113UqFGDu+++m6FDh/LKK68Uc+UiIiKFQ+ObRERKBtOuOKWmprJ582bGjBmTaXlUVBTr1q3Ldp+UlBR8fHwyLfP19WXDhg1YrVY8PT2z3SclJcXxPCEhAQCr1YrVar3Wl3HN7DU4Qy1SONSmrkdt6pqcoV2tVli3zgOw0L69FX2LXRtnaFMpfGpX1+NMbVqQGkwLTqdOnSI9PZ2wsLBMy8PCwjh+/Hi2+/Tq1YsPPviAW265hRYtWrB582ZmzZqF1Wrl1KlThIeHZ9ln0qRJPP/881mWL1myBD8/v8J5MYUgJibG7BKkkKlNXY/a1DWZ2a579pTh4sXOBAamcujQTxw+bFopLkWfVdekdnU9ztCmSUlJ+d7W1DFOAJYrOnQbhpFlmd0zzzzD8ePHadu2LYZhEBYWxqBBg3j11Vdxd3fPdp+xY8cyatQox/OEhAQiIiKIiooiKCio8F7IVbJarcTExNCzZ89sr5hJyaM2dT1qU9fkDO26a5etx3y3bh7ceGMfU2pwJc7QplL41K6ux5na1N4bLT9MC07ly5fH3d09y9Wl+Pj4LFeh7Hx9fZk1axbvvfceJ06cIDw8nJkzZxIYGEj58uWz3cfb2xtvb+8syz09PU1vqMs5Wz1y7dSmrkdt6prMbNc1a2z/du3qhqen6RPdugx9Vl2T2tX1OEObFuT8pv0v7eXlRcuWLbNcoouJiaF9+/a57uvp6UmVKlVwd3fns88+48Ybb8TNTT9wRESk5EhP/zc46f5NIiLOz9SueqNGjWLAgAG0atWKdu3aMXPmTGJjYxkxYgRg62Z39OhRx72a/vrrLzZs2ECbNm04e/YsU6ZMYceOHXz00UdmvgwREZEC27oVEhIgOBiaNjW7GhERyYupwemuu+7i9OnTTJgwgbi4OBo1asSiRYuoVq0aAHFxccTGxjq2T09P5/XXX2fPnj14enrSrVs31q1bR/Xq1U16BSIiIlfHfv+mjh0hh2G6IiLiREyfHGLkyJGMHDky23Vz5szJ9Lx+/fr8/vvvxVCViIhI0dL9m0REShYNDBIRESlmGRmwerXtscY3iYiUDApOIiIixWzHDjhzBvz9oUULs6sREZH8UHASEREpZvbxTR06gGZXFhEpGRScREREipnGN4mIlDwKTiIiIsXIMP694qTxTSIiJYeCk4iISDH680+IjwcfH2jd2uxqREQkvxScREREipH9alO7duDtbW4tIiKSfwpOIiIixUjjm0RESiYFJxERkWJiGP8GJ41vEhEpWRScREREisnff8OxY+DlBW3bml2NiIgUhIKTiIhIMbGPb7ruOvD1NbcWEREpGAUnERGRYqLxTSIiJZeCk4iISDHR+CYRkZJLwUlERKQYHDpk+3J3h/btza5GREQKSsFJRESkGNjHN7VqBQEB5tYiIiIFp+AkIiJSDDS+SUSkZFNwEhERKQYa3yQiUrIpOImIiBSxY8dg3z5wc4OOHc2uRkREroaCk4iISBGzj29q1gyCg00tRURErpKCk4iISBHT+CYRkZJPwUlERKSIaXyTiEjJp+AkIiJShOLjYfdu2+NOncytRURErp6Ck4iISBFavdr2b+PGUK5c/vb5bs93fLjlQ9Iz0ouuMBERKRAFJxERkSJU0PFNZy+dpd8X/Rj2/TA6zu7InlN7iq44ERHJNwUnERGRIlTQ8U0x+2OwZlgB+PXIrzR7rxlTf51KhpFRRBWKiEh+KDiJiIgUkTNnYPt22+P8XnFatHcRAPc0voeeNXqSnJbM4z8/Ttc5Xfn7zN9FVKmIiORFwUlERKSIrFkDhgH16kFYWN7bZxgZ/LTvJwCGNB/Cz//9mRk3zMDf05/VsatpMqMJ0zZO09UnERETKDiJiIgUkYKOb9oSt4X4i/EEeAXQsWpHLBYL97e6n+0PbKdr9a4kWZN4cNGD9Jzbk0PnDhVd4SIikoWCk4iISBEp6Pgmeze9njV64uXu5VgeWSaSZQOX8Vbvt/D18OWXA7/QeHpjPtjyAYZhFHbZIiKSDQUnERGRInD+PPz+u+1xQYNTn9p9sqxzs7jxcJuH2fbANjpEdCAxNZHh3w8n+pNojiQcKayyRUQkBwpOIiIiRWDdOsjIgJo1oXLlvLc/lXSKDUc3ABBdKzrH7WqVrcXKQSt5Pep1vN29+fnvn2k0rREfbf1IV59ERIqQgpOIiEgRKOj4pp/3/YyBQdOwplQOyj1pubu5M6rdKLaO2Mp1la/jfMp5Bn07iJs/u5njF45fY+UiIpIdBScREZEiUODxTfty7qaXk3rl67F2yFom9ZiEl7sX3//1PQ2nNWT+9vm6+iQiUsgUnERERArZxYuwaZPtcX6CU3pGOov3LQYKFpwAPNw8GNNxDJvv20yL8BacuXSG/gv6c8eXdxB/Mb6gpYuISA4UnERERArZ+vWQlgZVq0L16nlvv+HoBs5cOkOITwhtq7S9qnM2Cm3Er0N/5fmuz+Ph5sHXu7+m0bRGfL3r66s6noiIZKbgJCIiUsgKOr7JPpter5q98HDzuOrzerp78myXZ9kwbAONQxtzMukk/b7sR/+v+3M66fRVH1dERBScRERECl1xjG/KTfPw5my6bxPjOo3D3eLO/B3zaTitId/t+a5Qji8iUhopOImIiBSiS5fgt99sj/MTnOIS49gStwWA3rV6F1odXu5evND9BdYPXU/98vU5cfEEN392M/d+cy/nks8V2nlEREoLBScREZFCtGEDpKZCeDjUqpX39vZJIVpXak2of2ih19O6cmu23L+FJ9s/iQULH//xMY2mNXKcV0RE8kfBSUREpBBdPr7JYsl7+8LuppcdHw8fXu35KmuGrKF22docTTxK9CfRDP9uOAkpCUV2XhERV6LgJCIiUogKMr7Jmm5lyd9LgKINTnbtI9qzdcRWHm3zKAAf/P4Bjac3Ztn+ZUV+bhGRkk7BSUREpJCkptqmIof8Bad1h9eRkJJABb8KtKrUqmiL+4efpx9Te09lxb0riAyJJPZ8LNfPvZ4Hf3yQC6kXiqUGEZGSSMFJRESkkGzaZJsconx5qF8/7+0d05DX6oWbpXh/JHep3oVtD2xjZKuRAEzbNI2mM5qy6tCqYq1DRKSkUHASEREpJFc9vqlW0XfTy06AVwDv3vAuMQNiqBpclf1n99N1TlceX/w4SdYkU2oSEXFWCk4iIiKFpCDjm2LPx7IjfgduFjeiakYVbWF5uL7G9Wx/YDvDmg/DwGDqb1Np/l5z1h9eb2pdIiLORMFJRESkEKSlwdq1tsf5CU4/7f0JgLZV2lLOr1wRVpY/Qd5BvH/T+yzqv4hKgZX46/RfdJzdkadjniY5Ldns8kRETKfgJCIiUgh+/x0uXICQEGjcOO/tze6ml5Po2tHseGAHA5sOJMPI4NV1r9JyZks2HdtkdmkiIqZScBIRESkE9m56nTqBWx4/XVPSUli6fylQPNOQF1QZ3zJ8dMtHfHPXN4T5h7Hr5C7aftCWZ355htT0VLPLExExhYKTiIhIISjI+KZVh1aRZE0iPCCcZhWbFWld1+Lmejezc+RO7m50N+lGOi+sfoHW77dm6/GtZpcmIlLsFJxERESuUXo6rF5te5yf4GSfhjy6VjSW/Ey/Z6JyfuWYf/t8vrzjS8r7lWfbiW20fr81E1ZOwJpuNbs8EZFio+AkIiJyjbZvh/PnITAQmjXLe3vH+CYn7KaXk34N+rFz5E5uq38baRlpjF8xnnYftmNH/A6zSxMRKRYKTiIiItfI3k2vQwfw8Mh927/P/M1fp//Cw82D62tcX/TFFaJQ/1C+uuMrPr3tU8r4lGFz3GZazmzJy2teJi0jzezyRESKlIKTiIjINSrI+Kaf9tmmIe9YtSPBPsFFWFXRsFgs/Kfxf9g5cic31rmR1PRUxi4bS8dZHfnz1J9mlyciUmQUnERERK5BRgasWmV7XJDxTc42DXlBhQeG893d3zHn5jkEewfz29HfaP5ec6asn0J6RrrZ5YmIFDoFJxERkWuwezecPg1+ftCyZe7bJlmTWH5wOVCyxjflxGKxcG+ze9kxcge9avYiOS2Z0UtG0/Wjruw7s8/s8kRECpWCk4iIyDWwd9Nr1w68vHLfdsXBFSSnJVM1uCoNKjQo+uKKSZWgKvx0z0/MvHEmAV4BrIldQ5PpTXj7t7fJMDLMLk9EpFAoOImIiFyDgoxvurybnrNPQ15QFouF4S2Hs/2B7XSr3o1LaZd4ZPEjXP/x9Rw8d9Ds8kRErpmCk4iIyFUyjPwHJ8Mw+HHvj4BrdNPLSfWQ6iwduJR3ot/Bz9OP5QeX03h6Y2ZunolhGGaXJyJy1UwPTtOmTSMyMhIfHx9atmzJavsdBHPwySef0LRpU/z8/AgPD2fw4MGcPn26mKoVERH51969cOIEeHvDddflvu2e03s4eO4gXu5edI/sXjwFmsTN4saD1z3IHyP+oGPVjlxIvcD9P9xPr3m9OHz+sNnliYhcFVOD0+eff85jjz3GuHHj+P333+nUqRPR0dHExsZmu/2aNWsYOHAgQ4cOZefOnXz55Zds3LiRYcOGFXPlIiIi/15tatMGfHxy39beTa9r9a74e/kXcWXOoVbZWqy4dwVToqbg4+FDzP4YGk1vxOzfZ+vqk4iUOKYGpylTpjB06FCGDRtG/fr1mTp1KhEREUyfPj3b7X/99VeqV6/OI488QmRkJB07duT+++9n06ZNxVy5iIjI1Y9vKk3c3dx5vN3jbL1/K22rtCUhJYEh3w3hps9u4ljiMbPLExHJN9OCU2pqKps3byYqKirT8qioKNatW5ftPu3bt+fIkSMsWrQIwzA4ceIEX331FTfccENxlCwiIuJQkPFNiSmJrDpku9mTK49vyk3d8nVZM3gNr1z/Cl7uXvzw1w80mtaIT7Z9oqtPIlIieJh14lOnTpGenk5YWFim5WFhYRw/fjzbfdq3b88nn3zCXXfdRXJyMmlpadx00028/fbbOZ4nJSWFlJQUx/OEhAQArFYrVqu1EF7JtbHX4Ay1SOFQm7oetalrutZ23b8fjhzxxMPDoFWrNHI7zM97f8aaYaVWmVpUD6peqr+XHr/ucaIioxj6/VC2HN/Cfxf+ly93fsk7vd8hLCAs7wPkQp9V16R2dT3O1KYFqcFimPRnnmPHjlG5cmXWrVtHu3btHMtffPFF5s6dy59//plln127dnH99dfz+OOP06tXL+Li4njyySdp3bo1H374Ybbnee6553j++eezLP/000/x8/MrvBckIiKlyrJlEbz9dgvq1j3DK6/kPrHRu4ffJeZ0DDeWv5FhVTQuFyDNSGPBiQV8ceIL0ow0gtyDuD/ifjqEdDC7NBEpRZKSkujfvz/nz58nKCgo121NC06pqan4+fnx5ZdfcuuttzqWP/roo2zdupWV9v4PlxkwYADJycl8+eWXjmVr1qyhU6dOHDt2jPDw8Cz7ZHfFKSIiglOnTuX55hQHq9VKTEwMPXv2xNPT0+xypBCoTV2P2tQ1XWu7Dhvmzscfu/Hkk+m8+GLON3k1DIMa79TgaOJRfrj7B6JqROW4bWn0x4k/GPr9ULbFbwPgjvp38GavNynvV77Ax9Jn1TWpXV2PM7VpQkIC5cuXz1dwMq2rnpeXFy1btiQmJiZTcIqJieHmm2/Odp+kpCQ8PDKX7O7uDpBj/2hvb2+8vb2zLPf09DS9oS7nbPXItVObuh61qWu62na13z2je3d3PD3dc9xu24ltHE08iq+HLz1q9sDTQ99Dl2tVpRUb79vIC6te4KXVL/Hl7i9ZGbuSmTfO5OZ62f8+kBd9Vl2T2tX1OEObFuT8ps6qN2rUKD744ANmzZrF7t27efzxx4mNjWXEiBEAjB07loEDBzq279u3LwsWLGD69Ons37+ftWvX8sgjj3DddddRqVIls16GiIiUMocPw4ED4OYGHfLoWWafTa97ZHd8PPKYs7yU8nL3YkK3Cfw67FcaVGhA/MV4bvn8FgYuHMjZS2fNLk9EBDA5ON11111MnTqVCRMm0KxZM1atWsWiRYuoVq0aAHFxcZnu6TRo0CCmTJnCO++8Q6NGjbjjjjuoW7cuCxYsMOsliIhIKbTKNkEeLVpAYGDu2zqmIS+ls+kVRKtKrdh832ae7vA0bhY35m6bS6PpjRzvoYiImUzrqmc3cuRIRo4cme26OXPmZFn28MMP8/DDDxdxVSIiIjnL7zTkZy+dZd1h2y02omtFF3FVrsHHw4eXr3+ZW+rdwr3f3Mtfp//ihk9vYEizIUzpNYVgn2CzSxSRUsrUK04iIiIlUX6DU8z+GNKNdOqXr09kmciiL8yFtK3Slq33b+Xxto9jwcKsrbNoPL0xMX/HmF2aiJRSCk4iIiIFEBcHf/0FFgt07Jj7tuqmd218PX2Z0msKKwetpGaZmhxOOEzUvCge+OEBLqReMLs8ESllFJxEREQKwD6bXpMmUKZMzttlGBn8tO8nQMHpWnWq1ok/RvzBg60fBGDG5hk0nt6YFQdXmFuYiJQqCk4iIiIFkN9uelvithB/MZ4ArwA6Vs3j0pTkyd/Ln3f6vMOygcuoFlyNg+cO0u2jbjzy0yMkWZPMLk9ESgEFJxERkQLIb3Cyd9PrWaMnXu5eRVxV6dE9sjvbHtjGfS3uA+DtDW/TdEZT1sauNbkyEXF1Ck4iIiL5dOoU7Nxpe9y5c+7banxT0QnyDuK9vu+x+J7FVA6szL4z++g0uxNPL3ualIwUs8sTERdl+nTkIiIiJYV9fFPDhlC+fM7bnUo6xYajGwBNQ16UetXqxY6RO3j858eZs3UOb/z2BmFeYXyT/g3Vy1SnWnA1qgZXpVqI7d8ArwCzSxaREkzBSUREJJ/s3fTyutr0876fMTBoGtaUykGVi76wUizEJ4TZN8/mtnq3cd/393H84nHm75yf7bZlfcvaglRwtSyhqlpwNUL9Q7FYLMX8CkSkpFBwEhERyad8j2/ap256xa1v3b5sq7SNV79+lXI1y3Ek8QiHzh8i9nwsh84d4nzKec5cOsOZS2fYenxrtsfwdvemanBVR5C6MlhFBEdovJpIKabgJCIikg9nz8Iff9ge5xac0jPSWbxvMaDgVNxCfELoWKYjfdr2wdPTM9O688nniT0fawtS9kB1WbA6lniMlPQU9p7Zy94ze7M9vgULFQMqZgpTV4asYO9gXbUScVEKTiIiIvmwdi0YBtSpAxUr5rzdhqMbOHPpDCE+IbSt0rb4CpRcBfsE09inMY3DGme7PjU9laMJRzOFqStDVnJaMnEX4oi7EMevR37N9jiBXoGZgtWVV67CA8Jxd3MvypcqIkVEwUlERCQf8ju+yT6bXq+avfBw04/ZksLL3YvIMpFElonMdr1hGJxKOpUpWF155epU0ikSUxPZEb+DHfE7sj2Oh5sHVYKqZHu1yt5N0M/TryhfqohcJf2PLiIikg8a31S6WSwWKvhXoIJ/BVpVapXtNhdTL3I44XC2V6sOnTvEkYQjpGWkcfDcQQ6eO5jjuSr4Vfg3TAVVzdI1sLxfeXUHFDGBgpOIiEgeEhNhyxbb49yCU1xiHFvibBv2rtW7GCoTZ+Lv5U+98vWoV75etuvTM9I5lngsy/iq2IR/r2BdSL3AyaSTnEw6yea4zdkex9fDN9dgVSWoCp7untnuKyJXT8FJREQkD+vWQXo6REZCRETO29knhWhdqTWh/qHFVJ2UFO5u7kQERxARHEEHOmRZbxgG55LPZbpKdeWVq+MXjnMp7RJ7Tu9hz+k92Z7HzeJGpcBKOU5gUTW4KkHeQUX9ckVcjoKTiIhIHvI9vknd9OQaWCwWyviWoYxvGZpWbJrtNilpKRxOOJzjBBax52NJTU/lSMIRjiQcYd3hddkeJ8QnJFOgigiKwMfDpyhf3lVLT09n18ld/L3hb9zdNbGGK7C3afMLzalapqrZ5eSbgpOIiEge8jO+yZpuZcnfSwAFJyk63h7e1Cpbi1pla2W7PsPIIP5ifI6h6tC5Q5xNPsu55HOcSz7HthPbivkVXIOjZhcghW3Q+UEKTiIiIq4iKQk2brQ9zi04rTu8joSUBCr45Tx5gEhRc7O4UTGgIhUDKtKmSptst0lMScwSpo4k2iaucEYZGRnEHYsjvFI4bm5uZpcjhcDepmV9y5pdSoEoOImIiOTi11/BaoUqVWxjnHJin4a8d63euFn0y504r0DvQBqGNqRhaEOzS8kXq9XKokWL6NMn642NpWSyt2ntsrXNLqVA9D+7iIhILi4f35TbDNAa3yQi4toUnERERHKRn/FNsedj2RG/AzeLG1E1o4qnMBERKVYKTiIiIjlITrZ11YPcg9NPe38CoG2VtiWuz76IiOSPgpOIiEgONm6ElBQIC4M6dXLeztFNr5a66YmIuCoFJxERkRzkZ3xTSloKS/cvBTS+SUTElSk4iYiI5CA/45tWHVpFkjWJ8IBwmlVsVix1iYhI8VNwEhERyYbVCuvW2R7nFpzs05BH14rGktu0eyIiUqIpOImIiGRj82bbzW/LlYMGDXLeTtOQi4iUDgpOIiIi2bB30+vUCdxy+Gm578w+/jr9Fx5uHlxf4/riK05ERIqdgpOIiEg28jO+yT4NeceqHQn2CS6GqkRExCwKTiIiIldIS4M1a2yPcw1O+2zBSdOQi4i4PgUnERGRK/zxByQmQnAwNGmS/TZJ1iSWH1wOaHyTiEhpoOAkIiJyBXs3vY4dwd09+21WHFxBcloyVYOr0qBCLrNHiIiIS1BwEhERuUJ+xjfZpyHvU6uPpiEXESkFFJxEREQuk5EBq1fbHucUnAzD4Me9PwLqpiciUlooOImIiFxmxw44exb8/aFFi+y32XN6DwfPHcTL3Yvukd2Lt0ARETGFgpOIiMhl7N30OnQAD4/st7F30+tavSv+Xv7FVJmIiJjpqoJTWloaS5cu5b333iMxMRGAY8eOceHChUItTkREpLgVdHyTiIiUDjn8LS1nhw4donfv3sTGxpKSkkLPnj0JDAzk1VdfJTk5mRkzZhRFnSIiIkXOMGDVKtvjnIJTYkoiqw7ZNtL4JhGR0qPAV5weffRRWrVqxdmzZ/H19XUsv/XWW1m2bFmhFiciIlKc/vwTTp4EHx9o3Tr7bZYdWIY1w0qtsrWoXa528RYoIiKmKfAVpzVr1rB27Vq8vLwyLa9WrRpHjx4ttMJERESKm72bXrt2cMWPOQd10xMRKZ0KfMUpIyOD9PT0LMuPHDlCYGBgoRQlIiJihrzGNxmG8W9wUjc9EZFSpcDBqWfPnkydOtXx3GKxcOHCBcaPH0+fPvohIiIiJZNh5B2ctsdv52jiUXw9fOlSPZfZI0RExOUUuKvelClT6N69Ow0aNCA5OZn+/fuzd+9eypcvz/z584uiRhERkSL3998QF2frotemTfbb2K829ajRAx8Pn2KsTkREzFbg4FS5cmW2bt3KZ599xubNm8nIyGDo0KHcc889mSaLEBERKUnsV5uuuw5y+nGm8U0iIqVXgYKT1Wqlbt26/PDDDwwePJjBgwcXVV0iIiLFKq9uemcvnWXd4XUARNeOLqaqRETEWRRojJOnpycpKSlYLJaiqkdERMQUeQWnmP0xpBvp1C9fn+oh1YutLhERcQ4Fnhzi4Ycf5pVXXiEtLa0o6hERESl2hw5BbCx4eED79tlvo9n0RERKtwKPcfrtt99YtmwZS5YsoXHjxvj7+2dav2DBgkIrTkREpDjYrza1bAlX/FgDIMPI4Kd9PwEKTiIipVWBg1NISAi33357UdQiIiJiiry66W2J20L8xXgCvALoWLVj8RUmIiJOo8DBafbs2UVRh4iIiGnyCk72bno9a/TEy92rmKoSERFnUuDgZHfy5En27NmDxWKhTp06VKhQoTDrEhERKRZHj9ru4eTmBh06ZL+NxjeJiEiBJ4e4ePEiQ4YMITw8nM6dO9OpUycqVarE0KFDSUpKKooaRUREiszq1baZYps1g+DgrOtPJZ1iw9ENAETX0jTkIiKlVYGD06hRo1i5ciXff/89586d49y5c3z77besXLmS0aNHF0WNIiIiRcYenHLqpvfzvp8xMGga1pTKQZWLsTIREXEmBe6q9/XXX/PVV1/RtWtXx7I+ffrg6+vLnXfeyfTp0wuzPhERkSK1apXtb4g5jm/ap256IiJyFVeckpKSCAsLy7I8NDRUXfVERKREOXfOiz17bFecOnXKuj49I53F+xYDCk4iIqVdgYNTu3btGD9+PMnJyY5lly5d4vnnn6ddu3aFWpyIiEhR2rWrHACNG0PZslnXbzi6gTOXzhDiE0LbKm2LuToREXEmBe6q9+abb9K7d2+qVKlC06ZNsVgsbN26FR8fH37++eeiqFFERKRI7NhRHsh7GvJeNXvh4XbVE9GKiIgLKPBPgUaNGrF3717mzZvHn3/+iWEY3H333dxzzz34+voWRY0iIiJFYudO2xUnjW8SEZG8XNWfz3x9fRk+fHhh1yIiIlJszpyBQ4ds84937px1fVxiHFvitgDQu1bv4ixNREScUIHHOE2aNIlZs2ZlWT5r1ixeeeWVAhcwbdo0IiMj8fHxoWXLlqxevTrHbQcNGoTFYsny1bBhwwKfV0RESrc1a2yTQtStaxAamnW9fVKI1pVaE+qfzQYiIlKqFDg4vffee9SrVy/L8oYNGzJjxowCHevzzz/nscceY9y4cfz+++906tSJ6OhoYmNjs93+zTffJC4uzvF1+PBhypYtyx133FHQlyEiIqWc/f5NnTtnZLte3fRERORyBQ5Ox48fJzw8PMvyChUqEBcXV6BjTZkyhaFDhzJs2DDq16/P1KlTiYiIyPFeUMHBwVSsWNHxtWnTJs6ePcvgwYML+jJERKSUs9+/qVMnI8s6a7qVJX8vARScRETEpsBjnCIiIli7di2RkZGZlq9du5ZKlSrl+zipqals3ryZMWPGZFoeFRXFunXr8nWMDz/8kOuvv55q1arluE1KSgopKSmO5wkJCQBYrVasVmu+6y0q9hqcoRYpHGpT16M2dT3nz8Mff9h+BLZrZ8VqzRyeVh1aRUJKAhX8KtC0QlO1fQmhz6prUru6Hmdq04LUUODgNGzYMB577DGsVivdu3cHYNmyZTz11FOMHj0638c5deoU6enpWW6mGxYWxvHjx/PcPy4ujp9++olPP/001+0mTZrE888/n2X5kiVL8PPzy3e9RS0mJsbsEqSQqU1dj9rUdWzaFEpGRjsqVrzAzp3L2Lkz8/qPjn0EQEPvhiz+abEJFcq10GfVNaldXY8ztGlSUlK+ty1wcHrqqac4c+YMI0eOJDU1FQAfHx+efvppxo4dW9DDYbFYMj03DCPLsuzMmTOHkJAQbrnllly3Gzt2LKNGjXI8T0hIICIigqioKIKCggpcb2GzWq3ExMTQs2dPPD09zS5HCoHa1PWoTV3P6tW2bnoNG57Otl3HvT8OgKGdh9KnobrqlRT6rLomtavrcaY2tfdGy48CByeLxcIrr7zCM888w+7du/H19aV27dp4e3sX6Djly5fH3d09y9Wl+Pj4LFehrmQYBrNmzWLAgAF4eXnluq23t3e2tXl6epreUJdztnrk2qlNXY/a1HWsWWP7t1Gj03h6VsrUrrHnY9l5ciduFjf61O2jNi+B9Fl1TWpX1+MMbVqQ8xd4cgi7gIAAWrduTWBgIH///TcZGdnPSpQTLy8vWrZsmeUSXUxMDO3bt89135UrV7Jv3z6GDh1a4LpFRKR0u3ABNm+2PW7Y8FSW9T/t/QmAdlXaUda3bHGWJiIiTizfwemjjz5i6tSpmZbdd9991KhRg8aNG9OoUSMOHz5coJOPGjWKDz74gFmzZrF7924ef/xxYmNjGTFiBGDrZjdw4MAs+3344Ye0adOGRo0aFeh8IiIi69dDWhpUrWoQGnopy3pNQy4iItnJd3CaMWMGwcHBjueLFy9m9uzZfPzxx2zcuJGQkJBsJ2HIzV133cXUqVOZMGECzZo1Y9WqVSxatMgxS15cXFyWezqdP3+er7/+WlebRETkqqxcafs3u2nIU9JSWLp/KaDgJCIimeV7jNNff/1Fq1atHM+//fZbbrrpJu655x4AXnrppau6n9LIkSMZOXJktuvmzJmTZVlwcHCBZr8QERG5nD04ZXfj21WHVpFkTSI8IJymYU2LuTIREXFm+b7idOnSpUyz0K1bt47OnTs7nteoUSNf04iLiIiY5dIl2LDB9ji7K06L9tq66UXXis7XDK8iIlJ65Ds4VatWjc3/jKY9deoUO3fupGPHjo71x48fz9SVT0RExNn89hukpkJ4ONSsmXW9xjeJiEhO8t1Vb+DAgTz44IPs3LmTX375hXr16tGyZUvH+nXr1mmyBhERcWr2bnpdusCVF5T2ndnHX6f/wsPNg+trXF/8xYmIiFPLd3B6+umnSUpKYsGCBVSsWJEvv/wy0/q1a9fyn//8p9ALFBERKSyXB6cr2ach71i1I8E+6kEhIiKZ5Ts4ubm5MXHiRCZOnJjt+iuDlIiIiDNJTbVNRQ45BKd9tuDUp5a66YmISFZXfQNcERGRkmTjRkhOhgoVoF69zOuSrEksP7gc0PgmERHJnoKTiIiUCv9OQ551fNOKgytITkumanBVGlRoUPzFiYiI01NwEhGRUiG38U32acj71OqjachFRCRbCk4iIuLy0tJg7Vrb4yuDk2EY/Lj3R0Dd9EREJGcKTiIi4vK2bIGLF6FMGbjyzhl7Tu/h4LmDeLl70T2yuzkFioiI0yu04HT48GGGDBlSWIcTEREpNPZuep06gdsVP/kW/70YgK7Vu+Lv5V/MlYmISElRaMHpzJkzfPTRR4V1OBERkUKT2/gme3DSNOQiIpKbfN/H6bvvvst1/f79+6+5GBERkcKWng5r1tgeXxmcLqVfYnXsakDjm0REJHf5Dk633HILFosFwzBy3EYzEYmIiLPZtg3On4fAQGjaNPO6PxL/wJphpVbZWtQuV9ucAkVEpETId1e98PBwvv76azIyMrL92rJlS1HWKSIiclXs3fQ6dgSPK/5cuDlxM6BueiIikrd8B6eWLVvmGo7yuholIiJihpzGNxmGwZYE2881ddMTEZG85Lur3pNPPsnFixdzXF+rVi2WL19eKEWJiIgUhowMWG0bwpQlOG0/uZ3T1tP4evjSpXo2s0aIiIhcJt/BqVOnTrmu9/f3p0t20xWJiIiYZNcuOH0a/PygZcvM6xbvs82m1616N3w8fEyoTkRESpJ8d9Xbv3+/uuKJiEiJYu+m1749eHpmXmefhjy6ZnQxVyUiIiVRvoNT7dq1OXnypOP5XXfdxYkTJ4qkKBERkcKQ0/ims5fOsv7IegB61exVzFWJiEhJlO/gdOXVpkWLFuU65klERMRMhgGrVtkeXxmcYvbHkG6kE+ETQfWQ6sVem4iIlDz5Dk4iIiIlyV9/wYkT4O0NrVtnXrdo7yIAWga2zGZPERGRrPIdnCwWS5Yb3OqGtyIi4qzs3fTatgWfy+Z+yDAy+GnfTwC0DFJwEhGR/Mn3rHqGYTBo0CC8vb0BSE5OZsSIEfj7+2fabsGCBYVboYiIyFXIaXzTlrgtxF+MJ8ArgHr+9Yq/MBERKZHyHZzuvffeTM//+9//FnoxIiIihcEwcg5O9m56Par3wNPtiqn2REREcpDv4DR79uyirENERKTQHDgAR4/apiBv2zbzOntwiq4VDcdMKE5EREokTQ4hIiIux361qXVr281v7U5ePMmGoxsATUMuIiIFo+AkIiIuJ6duekv+XoKBQdOwplQOrFz8hYmISIml4CQiIi4np/s3Ldpn66bXp3afYq5IRERKOgUnERFxKYcP28Y4ubtD+/b/Lk/PSGfxvsWAgpOIiBScgpOIiLgUeze9Fi0gMPDf5RuObuDMpTOE+ITQtkrb7HcWERHJgYKTiIi4lLymIe9VsxcebvmeVFZERARQcBIRERej8U0iIlIUFJxERMRlxMXBX3+BxQIdO162PDGOLXFbAOhdq7dJ1YmISEmm4CQiIi7DfrWpaVMICfl3uX1SiNaVWhPqH1r8hYmISImn4CQiIi4jx/FN6qYnIiLXSMFJRERcRnbjm6zpVpb8vQRQcBIRkaun4CQiIi7h1CnYudP2uFOnf5evO7yOhJQEKvhVoFWlVuYUJyIiJZ6Ck4iIuAT71aaGDaF8+X+X26ch712rN24W/dgTEZGro58gIiLiEjS+SUREipKCk4iIuITsxjfFno9lR/wO3CxuRNWMMqcwERFxCQpOIiJS4p09C3/8YXvcufO/y3/a+xMA7aq0o6xvWRMqExERV6HgJCIiJd6aNWAYUKcOVKz473J10xMRkcKi4CQiIiVeduObUtJSWLp/KaDgJCIi107BSURESrzsxjetOrSKJGsS4QHhNA1rak5hIiLiMhScRESkREtMhC1bbI8vH99kn4a8T+0+WCwWEyoTERFXouAkIiIl2tq1kJ4OkZEQEfHvco1vEhGRwqTgJCIiJVp245v2ndnHX6f/wsPNg+trXG9OYSIi4lIUnEREpETLbnyTfRryjlU7EuQdZEJVIiLiahScRESkxEpKgo0bbY8z3b9pny049amlbnoiIlI4FJxERKTEWr8erFaoUsU2xgkgyZrE8oPLAY1vEhGRwqPgJCIiJdbl45vsE+etOLiC5LRkqgZXpUGFBuYVJyIiLkXBSURESqzsxjc5piGvpWnIRUSk8Cg4iYhIiZScDL/+antsH99kGAY/7v0RUDc9EREpXApOIiJSIm3YACkpEBYGderYlu05vYeD5w7i5e5F98ju5hYoIiIuRcFJRERKpOzGN9m76XWt3hV/L3+TKhMREVek4CQiIiVSXuObRERECpOCk4iIlDhWK6xbZ3tsH9+UmJLIqkO2NKXxTSIiUthMD07Tpk0jMjISHx8fWrZsyerVq3PdPiUlhXHjxlGtWjW8vb2pWbMms2bNKqZqRUTEGWzaZLv5bbly0OCfGceXHViGNcNKrbK1qF2utrkFioiIy/Ew8+Sff/45jz32GNOmTaNDhw689957REdHs2vXLqpWrZrtPnfeeScnTpzgww8/pFatWsTHx5OWllbMlYuIiJns45s6dwa3f/4EqG56IiJSlEwNTlOmTGHo0KEMGzYMgKlTp/Lzzz8zffp0Jk2alGX7xYsXs3LlSvbv30/ZsmUBqF69enGWLCIiTuDK8U2GYfwbnNRNT0REioBpwSk1NZXNmzczZsyYTMujoqJYZ++4foXvvvuOVq1a8eqrrzJ37lz8/f256aabmDhxIr6+vtnuk5KSQkpKiuN5QkICAFarFavVWkiv5urZa3CGWqRwqE1dj9rUuaSlwZo1HoCF9u2tWK2wLX4bRxOP4uvhS/vK7fPVVmpX16M2dU1qV9fjTG1akBpMC06nTp0iPT2dsLCwTMvDwsI4fvx4tvvs37+fNWvW4OPjw8KFCzl16hQjR47kzJkzOY5zmjRpEs8//3yW5UuWLMHPz+/aX0ghiYmJMbsEKWRqU9ejNnUO+/aFkJjYBT8/K4cPL+LYMfj6xNcANPRryC9LfinQ8dSurkdt6prUrq7HGdo0KSkp39ua2lUPwGK/+cY/DMPIsswuIyMDi8XCJ598QnBwMGDr7tevXz/efffdbK86jR07llGjRjmeJyQkEBERQVRUFEFBQYX4Sq6O1WolJiaGnj174unpaXY5UgjUpq5Hbepc3njDNqipa1d3+va1dct7be5rANzb7l76tMxfVz21q+tRm7omtavrcaY2tfdGyw/TglP58uVxd3fPcnUpPj4+y1Uou/DwcCpXruwITQD169fHMAyOHDlC7dpZZ1Hy9vbG29s7y3JPT0/TG+pyzlaPXDu1qetRmzqHtWtt/3br5oanpxtnL51l/ZH1ANxY78YCt5Ha1fWoTV2T2tX1OEObFuT8pk1H7uXlRcuWLbNcoouJiaF9+/bZ7tOhQweOHTvGhQsXHMv++usv3NzcqFKlSpHWKyIi5svIAPtdK+wTQ8TsjyHdSKdBhQZUD6luWm0iIuLaTL2P06hRo/jggw+YNWsWu3fv5vHHHyc2NpYRI0YAtm52AwcOdGzfv39/ypUrx+DBg9m1axerVq3iySefZMiQITlODiEiIq5j+3Y4exYCAqB5c9syTUMuIiLFwdQxTnfddRenT59mwoQJxMXF0ahRIxYtWkS1atUAiIuLIzY21rF9QEAAMTExPPzww7Rq1Ypy5cpx55138sILL5j1EkREpBjZ79/UoQN4eECGkcFP+34CNA25iIgULdMnhxg5ciQjR47Mdt2cOXOyLKtXr55TzMAhIiLF78r7N22J20L8xXgCvQLpULWDeYWJiIjLM7WrnoiISH4ZRtbgZO+m17NmT7zcvUyqTERESgMFJxERKRF274aTJ8HXF1q1si3T+CYRESkuCk4iIlIi2Mc3tWsHXl5w8uJJNhzdAEDvWr1NrExEREoDBScRESkRruymt+TvJRgYNA1rSuWgyuYVJiIipYKCk4iIOD3D+PeKk2N8075/uulpNj0RESkGCk4iIuL09u2DuDhbF73rroP0jHQW71sMKDiJiEjxUHASERGnZ7/a1KaNbXKIDUc3cObSGUJ8Qmhbpa25xYmISKmg4CQiIk4vp2nIe9XshYeb6bckFBGRUkDBSUREnJ7GN4mIiNkUnERExKkdPAixseDhYZuKPC4xji1xWwBNQy4iIsVHwUlERJya/WpTq1bg749jUojWlVoT6h9qYmUiIlKaKDiJiIhTyzK+Sd30RETEBApOIiLi1C4f32RNt7Lk7yWAgpOIiBQvBScREXFaR4/C33+Dmxt06ADrDq8jISWBCn4VaFWpldnliYhIKaLgJCIiTst+tal5cwgK+nca8t61euNm0Y8wEREpPvqpIyIiTkvjm0RExFkoOImIiNO6fHxT7PlYdsTvwM3iRlTNKHMLExGRUkfBSUREnNKJE/Dnn2CxQMeO8NPenwBoV6UdZX3LmlydiIiUNgpOIiLilOzd9Bo3hrJl1U1PRETMpeAkIiJO6fLxTSlpKSzdvxRQcBIREXMoOImIiFO6fHzTqkOrSLImER4QTtOwpuYWJiIipZKCk4iIOJ3Tp2H7dtvjTp3+nYa8T+0+WCwWEysTEZHSSsFJRESczurVtn/r14fQUI1vEhER8yk4iYiI07l8fNO+M/v46/RfeLh5cH2N680tTERESi0FJxERcTqXj2+yT0PeqWongryDTKxKRERKMwUnERFxKufPw9attsedO6ubnoiIOAcFJxERcSpr1kBGBtSqBSEVklhxcAUA0bWizS1MRERKNQUnERFxKpePb1pxcAXJaclUDa5KgwoNzC1MRERKNQUnERFxKpePb3JMQ15L05CLiIi5FJxERMRpXLgAmzbZHnfqZPDj3h8BjW8SERHzKTiJiIjTWLcO0tOhWjVIDtjDwXMH8XL3ontkd7NLExGRUk7BSUREnEZ23fS6Vu+Kv5e/iVWJiIgoOImIiBO5fGKIy8c3iYiImE3BSUREnMKlS7Bhg+1xi3aJrDpkS1Ea3yQiIs5AwUlERJzCr79CaipUqgQHWIY1w0qtsrWoXa622aWJiIgoOImIiHO4fHzTT/vUTU9ERJyLgpOIiDgF+/imzp2Nf8c3qZueiIg4CQUnERExXUoKrF9ve1yx6XaOJh7F18OXLtW7mFuYiIjIPxScRETEdBs3QnIyhIbC7jTb1aYeNXrg4+FjcmUiIiI2Ck4iImI6+/imzp01vklERJyTgpOIiJjOPr6pdaezrDu8DoDo2tEmViQiIpKZgpOIiJjKaoW1a22PLbViSDfSaVChAdVDqptal4iIyOUUnERExFRbtsDFi1C2LOxIUTc9ERFxTgpOIiJiKvv4po6dMlj890+ApiEXERHno+AkIiKmso9vqtlxC/EX4wn0CqRD1Q7mFiUiInIFBScRETFNejqsXm17nFTZ1k2vZ82eeLl7mViViIhIVgpOIiJimj/+gIQECAqC3y9qfJOIiDgvBScRETGNfXxT664n2Xh0A6BpyEVExDkpOImIiGns45sqtFmCgUGzis2oFFjJ3KJERESyoeAkIiKmyMj4NzidKa9ueiIi4twUnERExBQ7d8KZM+AXkM6mc4sBddMTERHnpeAkIiKmsI9vahi1gTOXzhDiE0LbKm3NLUpERCQHCk4iImIKezc9/6a2bnq9avbCw83DxIpERERypuAkIiLFzjD+veJ0LOCf8U21Nb5JREScl4KTiIgUuz17ID4evMvF8VfiFgB61+ptclUiIiI5U3ASEZFiZ7/aVL2nbVKI1pVaE+ofamJFIiIiuVNwEhGRYmcf3+ReV930RESkZFBwEhGRYuUY3+Rm5ZDHEkDBSUREnJ/pwWnatGlERkbi4+NDy5YtWb16dY7brlixAovFkuXrzz//LMaKRUTkWuzfD0ePgnvkOi6mJ1DBrwKtKrUyuywREZFcmRqcPv/8cx577DHGjRvH77//TqdOnYiOjiY2NjbX/fbs2UNcXJzjq3bt2sVUsYiIXCv7+KaKHW3d9HrX6o2bxfS/44mIiOTK1J9UU6ZMYejQoQwbNoz69eszdepUIiIimD59eq77hYaGUrFiRceXu7t7MVUsIiLXyj6+KbWaxjeJiEjJYVpwSk1NZfPmzURFRWVaHhUVxbp163Ldt3nz5oSHh9OjRw+WL19elGWKiEghW7kSCI7lpNsO3CxuRNWMynMfERERs5l2i/ZTp06Rnp5OWFhYpuVhYWEcP348233Cw8OZOXMmLVu2JCUlhblz59KjRw9WrFhB586ds90nJSWFlJQUx/OEhAQArFYrVqu1kF7N1bPX4Ay1SOFQm7oetWnhiY2Fgwc9sbRehAG0rdyWQI9AU95btavrUZu6JrWr63GmNi1IDaYFJzuLxZLpuWEYWZbZ1a1bl7p16zqet2vXjsOHD/Paa6/lGJwmTZrE888/n2X5kiVL8PPzu4bKC1dMTIzZJUghU5u6HrXptVu+vArQEr8m33ERiEyPZNGiRabWpHZ1PWpT16R2dT3O0KZJSUn53ta04FS+fHnc3d2zXF2Kj4/PchUqN23btmXevHk5rh87diyjRo1yPE9ISCAiIoKoqCiCgoIKXnghs1qtxMTE0LNnTzw9Pc0uRwqB2tT1qE0Lz3ffuYN7CilVbDNEPNrnUZqFNTOlFrWr61Gbuia1q+txpja190bLD9OCk5eXFy1btiQmJoZbb73VsTwmJoabb74538f5/fffCQ8Pz3G9t7c33t7eWZZ7enqa3lCXc7Z65NqpTV2P2vTarV4NVFtFmiWJ8IBwWlVulWMvg+KidnU9alPXpHZ1Pc7QpgU5v6ld9UaNGsWAAQNo1aoV7dq1Y+bMmcTGxjJixAjAdrXo6NGjfPzxxwBMnTqV6tWr07BhQ1JTU5k3bx5ff/01X3/9tZkvQ0RE8iEuDvbuBXr/O5ue2aFJREQkv0wNTnfddRenT59mwoQJxMXF0ahRIxYtWkS1atUAiIuLy3RPp9TUVJ544gmOHj2Kr68vDRs25Mcff6RPH01lKyLi7Oz3b/JutIgUNA25iIiULKZPDjFy5EhGjhyZ7bo5c+Zkev7UU0/x1FNPFUNVIiJS2FatAsruIyXgLzzcPLi+xvVmlyQiIpJvulW7iIgUi5UrgVo/AdCpaieCvM2foEdERCS/FJxERKTInTwJu3YBtf8d3yQiIlKSKDiJiEiRW7UK8EzCUmMFoOAkIiIlj4KTiIgUuVWrgOorMNyTqRZcjfrl65tdkoiISIEoOImISJFbuZJM3fQ0DbmIiJQ0Ck4iIlKkzp6FP7YZUPtHAKJrRZtckYiISMEpOImISJFavRootwfKHMTL3Yvukd3NLklERKTAFJxERKRIrVqFo5te1+pd8ffyN7cgERGRq6DgJCIiRSrT+KZamk1PRERKJgUnEREpMgkJsHlHIlRbBWgachERKbkUnEREpMisXQtG9WXgbqVW2VrULlfb7JJERESuioKTiIgUmcvHN6mbnoiIlGQKTiIiUmRWrDQy3b9JRESkpFJwEhGRInHxImw8tB2CjuLj7kuX6l3MLklEROSqKTiJiEihO3oU7rgD0mvYrjZdX6MHPh4+JlclIiJy9RScRESk0BgGzJkDDRvCTz+BpY666YmIiGtQcBIRkUJx7Bj07QuDB8P589Cs43Hcqq0DILp2tMnViYiIXBsFJxERuSaGAXPn2q4y/fgjeHoZ/OfFzzl2c1PSjXQahzamekh1s8sUERG5Jh5mFyAiIiVXXBzcfz98/73teeNOhwi5ZyTzjy8CK9QvX5+Pb/3Y3CJFREQKgYKTiIgUmGHAp5/Cww/D2bPg4ZVO1P+9zUr3/+Pi8Yt4uXsxrtM4nu7wNN4e3maXKyIics0UnEREpECOH4cRI+Dbb23P63fbitstw1l0dhNkQKeqnZjZdyb1ytczt1AREZFCpOAkIiL5Yhjw2Wfw0ENw5gx4+CbRdszzrOd10s+mE+wdzOSekxnaYihuFg2hFRER16LgJCIieTpxAkaOhAULbM9r9owhJWoEay7uB+COBnfwZu83CQ8MN7FKERGRoqPgJCIiufriC1toOn0a3ANP0Wj0KP5gLlyEKkFVmNZnGn3r9jW7TBERkSKl4CQiItk6edIWmL76CsAg4sZ5JLZ/nD9ST2PBwsPXPcwL3V8g0DvQ7FJFRESKnIKTiIhk8eWXttB06hS4l99PtQdHsN8SA6nQOLQxH9z0AddVvs7sMkVERIqNgpOIiDicOgUPPmjrnoeblYq3v8HZps+xP+MSPh4+jO8yntHtRuPp7ml2qSIiIsVKwUlERADbxA8PPADx8eBWZRMVhg7juOUPyIDukd1578b3qFW2ltllioiImELBSUSklDt92nYj2/nzAa8LlO3/DOfqvMUJMijrW5YpUVMY2HQgFovF7FJFRERMo+AkIlKKffON7Wa2J06Apc4iAv/zAGcssQDc0/gepvSaQqh/qLlFioiIOAEFJxGRUujMGXjkEfjkE8D/BEFDHiOh6mckANVDqjP9hun0rtXb7DJFRESchoKTiEgp8913cP/9cPy4gaXFLLz6PkGC5RxuFjdGtR3Fc12fw9/L3+wyRUREnIqCk4hIKXH2LDz6KMydC5T7C9+R93EpdCUpQIvwFrzf931ahLcwu0wRERGnpOAkIlIK/PAD3HcfxMWnYunyKm5dX+CSJQU/Tz8mdpvII20ewcNNPxJERERyop+SIiIu7Nw5eOwx+OgjoMp6vB4eTmrITtKBXjV7Mf2G6USWiTS3SBERkRJAwUlExEX99BMMHw5HTyVAn/9B62mkWgwq+FVgau+p/KfRfzTFuIiISD4pOImIuJjz52HUKJg1C6j3DR73PESa31EABjcbzOSekynnV87cIkVEREoYBScRERfy888wbBgcOX8M7nwYGiwgDahVthbv3fge3SO7m12iiIhIieRmdgEiInLtEhJs3fJ6R2dwpOJ03B6uDw0W4OHmwf86/o9tI7YpNImIiFwDXXESESnhYmJg6FA4nLwLBg+HquvIAK6rfB3v932fJmFNzC5RRESkxFNwEhEpoRIT4YknYOasZOg4CTpNAncrAV4BvNT9JUa2Hom7m7vZZYqIiLgEBScRkRJo2TLbVaZDrIIR90H5PQD0rdOXd/u8S0RwhMkVioiIuBYFJxGREuTCBXjqKZg++yz0fBpavg9AxYCKvB39NrfXv11TjIuIiBQBBScRkRJi+XIYPMTgkN9X8NDDEHACgPtb3s/L179MiE+IuQWKiIi4MAUnEREnd+ECjBkD7849DDeMhLo/AFCvfD1m3jiTTtU6mVyhiIiI61NwEhFxYitXwqAh6Rys8C48OA68L+Dp5sn/Ov2PsR3H4u3hbXaJIiIipYKCk4iIE7p4EcaOhbe/2AZ9h0OVDQB0rNqRmTfOpH6F+iZXKCIiUrooOImIOJnVq+HeYZc4EDEB7nsN3NMI8gri1Z6vMrzlcNwsune5iIhIcVNwEhFxEklJMG4cTP1uGdx4P5T9G4B+DfrxZu83qRRYyeQKRURESi8FJxERJ7B2LQy8/zT7a4+GgR8BUCmgMtNvnMZNdW8yuToRERFRcBIRMdGlSzDu/wzeWPop9HkM/E9hwcKDrR/kxR4vEuQdZHaJIiIigoKTiIhp1q+Hex48wIGGD8BtPwNQv1wjZt3yPm2rtDW5OhEREbmcgpOISDG7dAn+79k03lg/FaPPs+B5CU+LN891e5Yn2z+Jp7un2SWKiIjIFRScRESK0a+/wn9GbeZgk+HQ83cAOlXpxoe3vEftcrVNrk5ERERyouAkIlIMkpNh7PiLTP3jWeg5FdwyCHAvw1s3vM6gZoOwWCxmlygiIiK5UHASESliGzfC7U8v5nDTEdDuEAC31/kP026aSqh/qMnViYiISH4oOImIFJGUFHjyuXje3vs4dPkUgAqe1fjojulE1442uToREREpCN1+XkSkCGzcaFDj9tm8bdSDxp9iMdx4oNkoDjyxU6FJRESkBNIVJxGRQpSSAo9P3Mv0I/dD6+UARPo058sB79OyUkuTqxMREZGrZfoVp2nTphEZGYmPjw8tW7Zk9erV+dpv7dq1eHh40KxZs6ItUEQkn37bZKXqf19iuqUxRC7HPcOX59pP5q8nNyg0iYiIlHCmBqfPP/+cxx57jHHjxvH777/TqVMnoqOjiY2NzXW/8+fPM3DgQHr06FFMlYqI5Cw1FYY++xttZ7cgvtE48EihaUAUex/byfieT+Dhpov7IiIiJZ2pwWnKlCkMHTqUYcOGUb9+faZOnUpERATTp0/Pdb/777+f/v37065du2KqVEQke2s3JlJp6CPMcmsHoTvwSivPtOvn8fuoxUSWiTS7PBERESkkpv0ZNDU1lc2bNzNmzJhMy6Oioli3bl2O+82ePZu///6befPm8cILL+R5npSUFFJSUhzPExISALBarVit1qusvvDYa3CGWqRwqE1dT3ZtarXC4Jd/5IuLD0OtIwB0ChzAF0NfpZxfOdLS0kypVfJPn1XXozZ1TWpX1+NMbVqQGkwLTqdOnSI9PZ2wsLBMy8PCwjh+/Hi2++zdu5cxY8awevVqPDzyV/qkSZN4/vnnsyxfsmQJfn5+BS+8iMTExJhdghQytanrsbfp1r+tvLp9HknVv4Ug8E6qzqOR99E+vAG/rfjN5CqloPRZdT1qU9ekdnU9ztCmSUlJ+d7W9I73Fosl03PDMLIsA0hPT6d///48//zz1KlTJ9/HHzt2LKNGjXI8T0hIICIigqioKIKCgq6+8EJitVqJiYmhZ8+eeHp6ml2OFAK1qeuxt2nnLj0YOHUeP6SMgernIcOdG8uOZt4T4/Dz8jW7TCkgfVZdj9rUNaldXY8ztam9N1p+mBacypcvj7u7e5arS/Hx8VmuQgEkJiayadMmfv/9dx566CEAMjIyMAwDDw8PlixZQvfu3bPs5+3tjbe3d5blnp6epjfU5ZytHrl2alPX8uvfZ7hnWTQXy60BHwi52Jqv7n2fHg2bml2aXCN9Vl2P2tQ1qV1djzO0aUHOb1pw8vLyomXLlsTExHDrrbc6lsfExHDzzTdn2T4oKIjt27dnWjZt2jR++eUXvvrqKyIjS94g7CFvz2bO0SdtT9abW4sUAbWpSzG8z0G5dEj1Z0Dll5j1fw/i4e5udlkiIiJSTEztqjdq1CgGDBhAq1ataNeuHTNnziQ2NpYRI0YAtm52R48e5eOPP8bNzY1GjRpl2j80NBQfH58sy0uKS6nJGL6nzS5DRPIp9Fwffhg5nda1q5pdioiIiBQzU4PTXXfdxenTp5kwYQJxcXE0atSIRYsWUa1aNQDi4uLyvKdTSfbif+7mzv0d2LxpMy1btcTDQ5efXUFamlVt6mLS0qz8ue0PRo25C29vL7PLEREREROYPjnEyJEjGTlyZLbr5syZk+u+zz33HM8991zhF1VMalQqQ0SFANxOH6BPm/qm9/GUwmG1WtWmLsbepm5uWSeuERERkdLB1BvgioiIiIiIlAQKTiIiIiIiInlQcBIREREREcmDgpOIiIiIiEgeFJxERERERETyoOAkIiIiIiKSBwUnERERERGRPCg4iYiIiIiI5EHBSUREREREJA8KTiIiIiIiInlQcBIREREREcmDgpOIiIiIiEgeFJxERERERETyoOAkIiIiIiKSBwUnERERERGRPCg4iYiIiIiI5EHBSUREREREJA8eZhdQ3AzDACAhIcHkSmysVitJSUkkJCTg6elpdjlSCNSmrkdt6prUrq5Hbeqa1K6ux5na1J4J7BkhN6UuOCUmJgIQERFhciUiIiIiIuIMEhMTCQ4OznUbi5GfeOVCMjIyOHbsGIGBgVgslly3bd26NRs3biy0ddktT0hIICIigsOHDxMUFJTPV1F0cntdxX3Mgu6Xn+3z2qYw2lVtWnj7qk1z5kztqs9q4XCmNi3ovmrTnBV2uzpTm+Znu4L8TpTTcmdrV2f6rOr/32tnGAaJiYlUqlQJN7fcRzGVuitObm5uVKlSJV/buru759iYV7Mut32CgoJM/8aB3Gss7mMWdL/8bJ/XNoXZrmrTa99XbZozZ2pXfVYLhzO1aUH3VZvmrLDb1ZnaND/bFbTt9LtS0e6nz2r28rrSZKfJIXLx4IMPFuq63PZxFkVR49Ues6D75Wf7vLZxxXZ1pjYt6L5q05w5U7vqs1o4nKlNC7qv2jRnhV2jM7VpfrYraNuVxja9lmPq/9/iVeq66jmbhIQEgoODOX/+vFMkbrl2alPXozZ1TWpX16M2dU1qV9dTUttUV5xM5u3tzfjx4/H29ja7FCkkalPXozZ1TWpX16M2dU1qV9dTUttUV5xERERERETyoCtOIiIiIiIieVBwEhERERERyYOCk4iIiIiISB4UnERERERERPKg4CQiIiIiIpIHBacSJikpiWrVqvHEE0+YXYpco8TERFq3bk2zZs1o3Lgx77//vtklSSE4fPgwXbt2pUGDBjRp0oQvv/zS7JKkENx6662UKVOGfv36mV2KXIMffviBunXrUrt2bT744AOzy5FCoM+m63Hmn6OajryEGTduHHv37qVq1aq89tprZpcj1yA9PZ2UlBT8/PxISkqiUaNGbNy4kXLlypldmlyDuLg4Tpw4QbNmzYiPj6dFixbs2bMHf39/s0uTa7B8+XIuXLjARx99xFdffWV2OXIV0tLSaNCgAcuXLycoKIgWLVrw22+/UbZsWbNLk2ugz6brceafo7riVILs3buXP//8kz59+phdihQCd3d3/Pz8AEhOTiY9PR39HaPkCw8Pp1mzZgCEhoZStmxZzpw5Y25Rcs26detGYGCg2WXINdiwYQMNGzakcuXKBAYG0qdPH37++Wezy5JrpM+m63Hmn6MKToVk1apV9O3bl0qVKmGxWPjmm2+ybDNt2jQiIyPx8fGhZcuWrF69ukDneOKJJ5g0aVIhVSx5KY42PXfuHE2bNqVKlSo89dRTlC9fvpCql5wUR7vabdq0iYyMDCIiIq6xaslNcbapmOda2/nYsWNUrlzZ8bxKlSocPXq0OEqXHOiz65oKs12d7eeoglMhuXjxIk2bNuWdd97Jdv3nn3/OY489xrhx4/j999/p1KkT0dHRxMbGOrZp2bIljRo1yvJ17Ngxvv32W+rUqUOdOnWK6yWVekXdpgAhISH88ccfHDhwgE8//ZQTJ04Uy2srzYqjXQFOnz7NwIEDmTlzZpG/ptKuuNpUzHWt7ZzdFX2LxVKkNUvuCuOzK86nsNrVKX+OGlLoAGPhwoWZll133XXGiBEjMi2rV6+eMWbMmHwdc8yYMUaVKlWMatWqGeXKlTOCgoKM559/vrBKljwURZteacSIEcYXX3xxtSXKVSiqdk1OTjY6depkfPzxx4VRphRAUX5Wly9fbtx+++3XWqIUgqtp57Vr1xq33HKLY90jjzxifPLJJ0Veq+TPtXx29dl0Xlfbrs76c1RXnIpBamoqmzdvJioqKtPyqKgo1q1bl69jTJo0icOHD3Pw4EFee+01hg8fzrPPPlsU5Uo+FEabnjhxgoSEBAASEhJYtWoVdevWLfRaJf8Ko10Nw2DQoEF0796dAQMGFEWZUgCF0abi/PLTztdddx07duzg6NGjJCYmsmjRInr16mVGuZIP+uy6pvy0qzP/HPUwu4DS4NSpU6SnpxMWFpZpeVhYGMePHzepKrkWhdGmR44cYejQoRiGgWEYPPTQQzRp0qQoypV8Kox2Xbt2LZ9//jlNmjRx9OueO3cujRs3LuxyJR8K6//fXr16sWXLFi5evEiVKlVYuHAhrVu3Luxy5Srlp509PDx4/fXX6datGxkZGTz11FOaxdSJ5fezq89myZKfdnXmn6MKTsXoyr7UhmFcVf/qQYMGFVJFcq2upU1btmzJ1q1bi6AquVbX0q4dO3YkIyOjKMqSa3Ct//9q9rWSIa92vummm7jpppuKuyy5Bnm1qT6bJVNu7erMP0fVVa8YlC9fHnd39yx/3YyPj8+SuKVkUJu6pv9v7+5Cmu77OI5/1oPU3IE9gCsqhxuF4DJ0B5oFHRRGUvZEELlFJ2VKBBFCB55UWEIqaIZ4EMmKkh4gMIkGWgSRsdIDSSsxCiqzPAknW013H4Tj3p3X/efi8mprvl/ggT//v/n1/wHhw29/JdfkQ6azAzknHzJNTn96rhSn3yAlJUV5eXny+Xwx6z6fT+vXr4/TVPgnyDQ5kWvyIdPZgZyTD5kmpz89V96qN0PGxsY0ODgY/fzt27fq7e3V4sWLtWrVKp04cUJut1sul0sFBQVqaWnR+/fvVVZWFsep8f+QaXIi1+RDprMDOScfMk1OSZ1rnP6aX9Lp6uqKSPrl4+DBg9FrmpqaIhkZGZGUlJRIbm5u5NGjR/EbGIbINDmRa/Ih09mBnJMPmSanZM7VFIlM8x/hAAAAAABRPOMEAAAAAAYoTgAAAABggOIEAAAAAAYoTgAAAABggOIEAAAAAAYoTgAAAABggOIEAAAAAAYoTgAAAABggOIEAAAAAAYoTgCAhDQ8PKzjx4/L4XBowYIFSk9P14YNG9Tc3Kzx8fF4jwcAmGXmxXsAAAD+19DQkAoLC5WWlqbq6mo5nU6Fw2G9fv1aly9f1vLly7Vjx454jwkAmEU4cQIAJJzy8nLNmzdPfr9f+/btU1ZWlpxOp/bs2aN79+5p+/btkqS6ujo5nU6lpqZq5cqVKi8v19jYWPR1rly5orS0NLW3t2vNmjUym83au3evAoGAWltbZbPZtGjRIh07dkwTExPRfTabTWfPnpXH45HFYlFGRobu3r2rL1++qKSkRBaLRU6nU36/P7pndHRU+/fv14oVK2Q2m+V0OnX9+vXfd9MAAP8qihMAIKGMjo7qwYMHqqioUGpq6rTXmEwmSdKcOXPU0NCgvr4+tba2qrOzU5WVlTHXjo+Pq6GhQTdu3ND9+/f18OFD7d69Wx0dHero6JDX61VLS4tu3boVs6++vl6FhYXq6elRcXGx3G63PB6PSktL9eLFCzkcDnk8HkUiEUlSMBhUXl6e2tvb1dfXp8OHD8vtdqu7u/tfuEsAgN/NFJn6jQ8AQALo7u5Wfn6+7ty5o127dkXXly5dqmAwKEmqqKhQTU3NL3tv3rypo0eP6uvXr5J+njgdOnRIg4ODstvtkqSysjJ5vV59/vxZFotFkrR161bZbDY1NzdL+nnitHHjRnm9Xkk/n7datmyZqqqqdPr0aUnS06dPVVBQoE+fPslqtU77sxQXFysrK0sXLlyYiVsDAIgjnnECACSkqVOlKc+ePdPk5KQOHDigUCgkSerq6lJ1dbVevnypb9++KRwOKxgMKhAIRE+rzGZztDRJUnp6umw2W7Q0Ta2NjIzEfL+1a9fGfF2SnE7nL2sjIyOyWq2amJjQ+fPn1dbWpg8fPigUCikUCv3lqRkA4M/CW/UAAAnF4XDIZDJpYGAgZj0zM1MOh0MLFy6UJL17907btm1Tdna2bt++refPn6upqUmS9OPHj+i++fPnx7yOyWSadm1ycjJm7b+vmSpx061N7autrVV9fb0qKyvV2dmp3t5eFRUV6fv373//JgAAEg7FCQCQUJYsWaItW7bo4sWLCgQCf3md3+9XOBxWbW2t8vPztXr1an38+PE3Thrr8ePHKikpUWlpqXJycpSZmak3b97EbR4AwMyiOAEAEs6lS5cUDoflcrnU1tam/v5+vXr1SlevXtXAwIDmzp0ru92ucDisxsZGDQ0Nyev1Rp9RigeHwyGfz6cnT56ov79fR44c0fDwcNzmAQDMLIoTACDh2O129fT0aPPmzTp16pRycnLkcrnU2NiokydP6syZM1q3bp3q6upUU1Oj7OxsXbt2TefOnYvbzFVVVcrNzVVRUZE2bdokq9WqnTt3xm0eAMDM4q/qAQAAAIABTpwAAAAAwADFCQAAAAAMUJwAAAAAwADFCQAAAAAMUJwAAAAAwADFCQAAAAAMUJwAAAAAwADFCQAAAAAMUJwAAAAAwADFCQAAAAAMUJwAAAAAwADFCQAAAAAM/AdCbGH+AZkNkQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(gamma, train_mean, label='Training score', color='blue')\n",
    "plt.semilogx(gamma, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve for SVM - Gamma vs F1')\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "1124dd4b-0746-4d46-bc21-59fecb5072df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "0952ba08-5456-4a7c-9131-dbfa02ad483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(26640) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26641) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26642) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26643) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26644) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26645) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26646) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26647) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='poly', C=0.1, degree=3, gamma=0.1008)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    svc, X_train, y_train, cv=5, scoring='f1', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), random_state=903967749\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "176979af-feea-4ca1-9dab-6f0baa25ca9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training score', color='blue')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Learning Curve of SVM')\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "2132076d-58a4-4dc4-8b5c-6f27c0447b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 0.1, 'coef0': 1.0, 'degree': 3, 'gamma': 0.1008, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'kernel': ['poly',],\n",
    "    'C':[0.1],\n",
    "    'degree': [3],\n",
    "    'gamma': [0.1008],\n",
    "    'coef0':  np.logspace(-4, 2, 7),\n",
    "}\n",
    "knn = SVC()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "0b4a06df-51e7-4368-81ea-41daa88a6928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1 score:  0.8857978387732663\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svc = SVC(kernel='poly', C=0.1, degree=3, gamma=0.1008, coef0=1.0)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Test set F1 score: \", test_f1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1d614-121f-4065-95e7-8bd8f0dae08c",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "b0de7602-6418-4aed-a18e-ec01ed092a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,), (50, 50), (100, 50), (100, 100)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'adaptive','invscaling'],\n",
    "    'max_iter': [3000],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "2f2a78e9-79de-4894-b6c5-88449519fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=903967749)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "b81b2137-ae28-4909-8341-1fd6b9bbe6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(58236) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(58237) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(58238) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(58239) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(58240) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(58241) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(58242) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(58243) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:545: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'sgd', 'max_iter': 3000, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (100, 50), 'alpha': 0.001, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    mlp, param_distributions=param_dist, n_iter=100, cv=5, random_state=903967749, n_jobs=-1, verbose=2\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "52fc9798-a870-4d30-a5a7-e2cc3c84626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = np.logspace(-4,1,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "ceb9c6d8-3829-498e-8626-828bab3baecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "        MLPClassifier(solver='sgd', max_iter=3000, learning_rate='constant', hidden_layer_sizes=(100, 50), activation='relu', random_state=903967749),\n",
    "        X_train, y_train, param_range=alpha_range, param_name='alpha', cv=5,\n",
    "        scoring=f1, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "243e1a6e-1331-4022-bf24-833fd00034c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(alpha_range, train_mean, label='Training score', color='blue')\n",
    "plt.semilogx(alpha_range, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve of Neural Network - alpha vs f1')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "c8fc8417-7b07-4a2f-b0ff-c42f6a8ae314",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes_range = [\n",
    "    (50,), (100,), (150,),\n",
    "    (50, 50), (100, 50), (100, 100), (150, 100),\n",
    "    (50, 50, 50), (100, 100, 100), (150, 150, 150),\n",
    "    (100, 100, 50), (150, 100, 50), (50,50,50,50)\n",
    "]\n",
    "hidden_layer_sizes_labels = [str(hls) for hls in hidden_layer_sizes_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd78db-28ee-42eb-a840-048e66969eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   1.8s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   1.3s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   3.9s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.1s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.1s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   2.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   2.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   3.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.1s\n",
      "Iteration 1, loss = 0.80893322\n",
      "Iteration 2, loss = 0.80078032\n",
      "Iteration 3, loss = 0.78874821\n",
      "Iteration 4, loss = 0.77461988\n",
      "Iteration 5, loss = 0.75877958\n",
      "Iteration 6, loss = 0.74275418\n",
      "Iteration 7, loss = 0.72649607\n",
      "Iteration 8, loss = 0.71128691\n",
      "Iteration 9, loss = 0.69634058\n",
      "Iteration 10, loss = 0.68217053\n",
      "Iteration 11, loss = 0.66904906\n",
      "Iteration 12, loss = 0.65727999\n",
      "Iteration 13, loss = 0.64612818\n",
      "Iteration 14, loss = 0.63531729\n",
      "Iteration 15, loss = 0.62564519\n",
      "Iteration 16, loss = 0.61670768\n",
      "Iteration 17, loss = 0.60843460\n",
      "Iteration 18, loss = 0.60103463\n",
      "Iteration 19, loss = 0.59365664\n",
      "Iteration 20, loss = 0.58653421\n",
      "Iteration 21, loss = 0.58053428\n",
      "Iteration 22, loss = 0.57427689\n",
      "Iteration 23, loss = 0.56915847\n",
      "Iteration 24, loss = 0.56348593\n",
      "Iteration 25, loss = 0.55844908\n",
      "Iteration 26, loss = 0.55370210\n",
      "Iteration 27, loss = 0.54918661\n",
      "Iteration 28, loss = 0.54488188\n",
      "Iteration 29, loss = 0.54071618\n",
      "Iteration 30, loss = 0.53679223\n",
      "Iteration 31, loss = 0.53288146\n",
      "Iteration 32, loss = 0.52908463\n",
      "Iteration 33, loss = 0.52556414\n",
      "Iteration 34, loss = 0.52211084\n",
      "Iteration 35, loss = 0.51891630\n",
      "Iteration 36, loss = 0.51558909\n",
      "Iteration 37, loss = 0.51258389\n",
      "Iteration 38, loss = 0.50953237\n",
      "Iteration 39, loss = 0.50682729\n",
      "Iteration 40, loss = 0.50387774\n",
      "Iteration 41, loss = 0.50121272\n",
      "Iteration 42, loss = 0.49860958\n",
      "Iteration 43, loss = 0.49601267\n",
      "Iteration 44, loss = 0.49357344\n",
      "Iteration 45, loss = 0.49111410\n",
      "Iteration 46, loss = 0.48877089\n",
      "Iteration 47, loss = 0.48650374\n",
      "Iteration 48, loss = 0.48425503\n",
      "Iteration 49, loss = 0.48210609\n",
      "Iteration 50, loss = 0.47994500\n",
      "Iteration 51, loss = 0.47794398\n",
      "Iteration 52, loss = 0.47588973\n",
      "Iteration 53, loss = 0.47389614\n",
      "Iteration 54, loss = 0.47209137\n",
      "Iteration 55, loss = 0.47015773\n",
      "Iteration 56, loss = 0.46838897\n",
      "Iteration 57, loss = 0.46660537\n",
      "Iteration 58, loss = 0.46486196\n",
      "Iteration 59, loss = 0.46318875\n",
      "Iteration 60, loss = 0.46149331\n",
      "Iteration 61, loss = 0.45992704\n",
      "Iteration 62, loss = 0.45835943\n",
      "Iteration 63, loss = 0.45682500\n",
      "Iteration 64, loss = 0.45532321\n",
      "Iteration 65, loss = 0.45385902\n",
      "Iteration 66, loss = 0.45243673\n",
      "Iteration 67, loss = 0.45101654\n",
      "Iteration 68, loss = 0.44967188\n",
      "Iteration 69, loss = 0.44830767\n",
      "Iteration 70, loss = 0.44697859\n",
      "Iteration 71, loss = 0.44575977\n",
      "Iteration 72, loss = 0.44446739\n",
      "Iteration 73, loss = 0.44325101\n",
      "Iteration 74, loss = 0.44204130\n",
      "Iteration 75, loss = 0.44085282\n",
      "Iteration 76, loss = 0.43970944\n",
      "Iteration 77, loss = 0.43861601\n",
      "Iteration 78, loss = 0.43746246\n",
      "Iteration 79, loss = 0.43636079\n",
      "Iteration 80, loss = 0.43532922\n",
      "Iteration 81, loss = 0.43435269\n",
      "Iteration 82, loss = 0.43331259\n",
      "Iteration 83, loss = 0.43232302\n",
      "Iteration 84, loss = 0.43135914\n",
      "Iteration 85, loss = 0.43038893\n",
      "Iteration 86, loss = 0.42944075\n",
      "Iteration 87, loss = 0.42853311\n",
      "Iteration 88, loss = 0.42764525\n",
      "Iteration 89, loss = 0.42672846\n",
      "Iteration 90, loss = 0.42592342\n",
      "Iteration 91, loss = 0.42506466\n",
      "Iteration 92, loss = 0.42425919\n",
      "Iteration 93, loss = 0.42338373\n",
      "Iteration 94, loss = 0.42262742\n",
      "Iteration 95, loss = 0.42183200\n",
      "Iteration 96, loss = 0.42107349\n",
      "Iteration 97, loss = 0.42030470\n",
      "Iteration 98, loss = 0.41952793\n",
      "Iteration 99, loss = 0.41883291\n",
      "Iteration 100, loss = 0.41812625\n",
      "Iteration 101, loss = 0.41744670\n",
      "Iteration 102, loss = 0.41676720\n",
      "Iteration 103, loss = 0.41605603\n",
      "Iteration 104, loss = 0.41540436\n",
      "Iteration 105, loss = 0.41472709\n",
      "Iteration 106, loss = 0.41408842\n",
      "Iteration 107, loss = 0.41348013\n",
      "Iteration 108, loss = 0.41289398\n",
      "Iteration 109, loss = 0.41225549\n",
      "Iteration 110, loss = 0.41162193\n",
      "Iteration 111, loss = 0.41107732\n",
      "Iteration 112, loss = 0.41046696\n",
      "Iteration 113, loss = 0.40993382\n",
      "Iteration 114, loss = 0.40932977\n",
      "Iteration 115, loss = 0.40880741\n",
      "Iteration 116, loss = 0.40827061\n",
      "Iteration 117, loss = 0.40770117\n",
      "Iteration 118, loss = 0.40717190\n",
      "Iteration 119, loss = 0.40667650\n",
      "Iteration 120, loss = 0.40614495\n",
      "Iteration 121, loss = 0.40564320\n",
      "Iteration 122, loss = 0.40518524\n",
      "Iteration 123, loss = 0.40468238\n",
      "Iteration 124, loss = 0.40418458\n",
      "Iteration 125, loss = 0.40371345\n",
      "Iteration 126, loss = 0.40325422\n",
      "Iteration 127, loss = 0.40277365\n",
      "Iteration 128, loss = 0.40234898\n",
      "Iteration 129, loss = 0.40190351\n",
      "Iteration 130, loss = 0.40143410\n",
      "Iteration 131, loss = 0.40099692\n",
      "Iteration 132, loss = 0.40054894\n",
      "Iteration 133, loss = 0.40017598\n",
      "Iteration 134, loss = 0.39973052\n",
      "Iteration 135, loss = 0.39931917\n",
      "Iteration 136, loss = 0.39889307\n",
      "Iteration 137, loss = 0.39850193\n",
      "Iteration 138, loss = 0.39813097\n",
      "Iteration 139, loss = 0.39774433\n",
      "Iteration 140, loss = 0.39732835\n",
      "Iteration 141, loss = 0.39696285\n",
      "Iteration 142, loss = 0.39654127\n",
      "Iteration 143, loss = 0.39619769\n",
      "Iteration 144, loss = 0.39584302\n",
      "Iteration 145, loss = 0.39544996\n",
      "Iteration 146, loss = 0.39510846\n",
      "Iteration 147, loss = 0.39473606\n",
      "Iteration 148, loss = 0.39438627\n",
      "Iteration 149, loss = 0.39403763\n",
      "Iteration 150, loss = 0.39369640\n",
      "Iteration 151, loss = 0.39334136\n",
      "Iteration 152, loss = 0.39302496\n",
      "Iteration 153, loss = 0.39268943\n",
      "Iteration 154, loss = 0.39234274\n",
      "Iteration 155, loss = 0.39201936\n",
      "Iteration 156, loss = 0.39170221\n",
      "Iteration 157, loss = 0.39137601\n",
      "Iteration 158, loss = 0.39104466\n",
      "Iteration 159, loss = 0.39073080\n",
      "Iteration 160, loss = 0.39041882\n",
      "Iteration 161, loss = 0.39012119\n",
      "Iteration 162, loss = 0.38981869\n",
      "Iteration 163, loss = 0.38951305\n",
      "Iteration 164, loss = 0.38923548\n",
      "Iteration 165, loss = 0.38889992\n",
      "Iteration 166, loss = 0.38862415\n",
      "Iteration 167, loss = 0.38832426\n",
      "Iteration 168, loss = 0.38804764\n",
      "Iteration 169, loss = 0.38776957\n",
      "Iteration 170, loss = 0.38749422\n",
      "Iteration 171, loss = 0.38721509\n",
      "Iteration 172, loss = 0.38694194\n",
      "Iteration 173, loss = 0.38664883\n",
      "Iteration 174, loss = 0.38637416\n",
      "Iteration 175, loss = 0.38612380\n",
      "Iteration 176, loss = 0.38586055\n",
      "Iteration 177, loss = 0.38560135\n",
      "Iteration 178, loss = 0.38532855\n",
      "Iteration 179, loss = 0.38508009\n",
      "Iteration 180, loss = 0.38482678\n",
      "Iteration 181, loss = 0.38458854\n",
      "Iteration 182, loss = 0.38432435\n",
      "Iteration 183, loss = 0.38407372\n",
      "Iteration 184, loss = 0.38382696\n",
      "Iteration 185, loss = 0.38357516\n",
      "Iteration 186, loss = 0.38334307\n",
      "Iteration 187, loss = 0.38310232\n",
      "Iteration 188, loss = 0.38285108\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.8s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   4.1s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   1.2s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.1s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   6.8s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.1s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   2.7s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.2s\n",
      "Iteration 1, loss = 0.78682503\n",
      "Iteration 2, loss = 0.78251825\n",
      "Iteration 3, loss = 0.77595714\n",
      "Iteration 4, loss = 0.76791687\n",
      "Iteration 5, loss = 0.75877458\n",
      "Iteration 6, loss = 0.74943629\n",
      "Iteration 7, loss = 0.73922949\n",
      "Iteration 8, loss = 0.72936364\n",
      "Iteration 9, loss = 0.71915696\n",
      "Iteration 10, loss = 0.70961018\n",
      "Iteration 11, loss = 0.69964539\n",
      "Iteration 12, loss = 0.69033590\n",
      "Iteration 13, loss = 0.68146963\n",
      "Iteration 14, loss = 0.67242794\n",
      "Iteration 15, loss = 0.66397999\n",
      "Iteration 16, loss = 0.65562024\n",
      "Iteration 17, loss = 0.64769019\n",
      "Iteration 18, loss = 0.64011385\n",
      "Iteration 19, loss = 0.63255343\n",
      "Iteration 20, loss = 0.62551557\n",
      "Iteration 21, loss = 0.61869389\n",
      "Iteration 22, loss = 0.61204137\n",
      "Iteration 23, loss = 0.60553444\n",
      "Iteration 24, loss = 0.59933218\n",
      "Iteration 25, loss = 0.59361478\n",
      "Iteration 26, loss = 0.58771504\n",
      "Iteration 27, loss = 0.58216649\n",
      "Iteration 28, loss = 0.57684589\n",
      "Iteration 29, loss = 0.57168633\n",
      "Iteration 30, loss = 0.56659900\n",
      "Iteration 31, loss = 0.56197670\n",
      "Iteration 32, loss = 0.55714152\n",
      "Iteration 33, loss = 0.55263949\n",
      "Iteration 34, loss = 0.54825195\n",
      "Iteration 35, loss = 0.54402510\n",
      "Iteration 36, loss = 0.53994347\n",
      "Iteration 37, loss = 0.53589513\n",
      "Iteration 38, loss = 0.53214621\n",
      "Iteration 39, loss = 0.52836327\n",
      "Iteration 40, loss = 0.52472564\n",
      "Iteration 41, loss = 0.52127620\n",
      "Iteration 42, loss = 0.51792022\n",
      "Iteration 43, loss = 0.51466283\n",
      "Iteration 44, loss = 0.51143651\n",
      "Iteration 45, loss = 0.50829005\n",
      "Iteration 46, loss = 0.50535754\n",
      "Iteration 47, loss = 0.50243781\n",
      "Iteration 48, loss = 0.49960368\n",
      "Iteration 49, loss = 0.49683116\n",
      "Iteration 50, loss = 0.49424914\n",
      "Iteration 51, loss = 0.49154081\n",
      "Iteration 52, loss = 0.48907514\n",
      "Iteration 53, loss = 0.48665220\n",
      "Iteration 54, loss = 0.48422815\n",
      "Iteration 55, loss = 0.48192211\n",
      "Iteration 56, loss = 0.47965790\n",
      "Iteration 57, loss = 0.47747595\n",
      "Iteration 58, loss = 0.47536474\n",
      "Iteration 59, loss = 0.47336223\n",
      "Iteration 60, loss = 0.47124839\n",
      "Iteration 61, loss = 0.46924664\n",
      "Iteration 62, loss = 0.46736973\n",
      "Iteration 63, loss = 0.46546308\n",
      "Iteration 64, loss = 0.46363516\n",
      "Iteration 65, loss = 0.46189378\n",
      "Iteration 66, loss = 0.46011676\n",
      "Iteration 67, loss = 0.45846304\n",
      "Iteration 68, loss = 0.45682668\n",
      "Iteration 69, loss = 0.45513650\n",
      "Iteration 70, loss = 0.45363672\n",
      "Iteration 71, loss = 0.45211516\n",
      "Iteration 72, loss = 0.45058445\n",
      "Iteration 73, loss = 0.44912822\n",
      "Iteration 74, loss = 0.44766645\n",
      "Iteration 75, loss = 0.44635614\n",
      "Iteration 76, loss = 0.44489141\n",
      "Iteration 77, loss = 0.44358611\n",
      "Iteration 78, loss = 0.44234131\n",
      "Iteration 79, loss = 0.44106517\n",
      "Iteration 80, loss = 0.43980006\n",
      "Iteration 81, loss = 0.43858510\n",
      "Iteration 82, loss = 0.43738819\n",
      "Iteration 83, loss = 0.43624876\n",
      "Iteration 84, loss = 0.43512192\n",
      "Iteration 85, loss = 0.43399143\n",
      "Iteration 86, loss = 0.43289967\n",
      "Iteration 87, loss = 0.43185579\n",
      "Iteration 88, loss = 0.43079964\n",
      "Iteration 89, loss = 0.42980552\n",
      "Iteration 90, loss = 0.42876921\n",
      "Iteration 91, loss = 0.42782085\n",
      "Iteration 92, loss = 0.42682636\n",
      "Iteration 93, loss = 0.42591912\n",
      "Iteration 94, loss = 0.42499821\n",
      "Iteration 95, loss = 0.42409738\n",
      "Iteration 96, loss = 0.42322190\n",
      "Iteration 97, loss = 0.42234801\n",
      "Iteration 98, loss = 0.42148309\n",
      "Iteration 99, loss = 0.42065633\n",
      "Iteration 100, loss = 0.41979592\n",
      "Iteration 101, loss = 0.41900060\n",
      "Iteration 102, loss = 0.41827223\n",
      "Iteration 103, loss = 0.41747392\n",
      "Iteration 104, loss = 0.41668890\n",
      "Iteration 105, loss = 0.41592725\n",
      "Iteration 106, loss = 0.41522327\n",
      "Iteration 107, loss = 0.41449189\n",
      "Iteration 108, loss = 0.41376639\n",
      "Iteration 109, loss = 0.41311006\n",
      "Iteration 110, loss = 0.41245263\n",
      "Iteration 111, loss = 0.41172135\n",
      "Iteration 112, loss = 0.41102942\n",
      "Iteration 113, loss = 0.41040453\n",
      "Iteration 114, loss = 0.40975815\n",
      "Iteration 115, loss = 0.40912275\n",
      "Iteration 116, loss = 0.40851546\n",
      "Iteration 117, loss = 0.40788970\n",
      "Iteration 118, loss = 0.40725404\n",
      "Iteration 119, loss = 0.40672115\n",
      "Iteration 120, loss = 0.40609999\n",
      "Iteration 121, loss = 0.40552773\n",
      "Iteration 122, loss = 0.40491108\n",
      "Iteration 123, loss = 0.40441021\n",
      "Iteration 124, loss = 0.40385990\n",
      "Iteration 125, loss = 0.40329153\n",
      "Iteration 126, loss = 0.40277529\n",
      "Iteration 127, loss = 0.40222665\n",
      "Iteration 128, loss = 0.40171455\n",
      "Iteration 129, loss = 0.40120435\n",
      "Iteration 130, loss = 0.40069273\n",
      "Iteration 131, loss = 0.40018913\n",
      "Iteration 132, loss = 0.39967703\n",
      "Iteration 133, loss = 0.39919428\n",
      "Iteration 134, loss = 0.39872941\n",
      "Iteration 135, loss = 0.39826807\n",
      "Iteration 136, loss = 0.39779865\n",
      "Iteration 137, loss = 0.39733466\n",
      "Iteration 138, loss = 0.39685942\n",
      "Iteration 139, loss = 0.39644500\n",
      "Iteration 140, loss = 0.39598842\n",
      "Iteration 141, loss = 0.39554394\n",
      "Iteration 142, loss = 0.39509393\n",
      "Iteration 143, loss = 0.39471537\n",
      "Iteration 144, loss = 0.39425793\n",
      "Iteration 145, loss = 0.39384738\n",
      "Iteration 146, loss = 0.39344348\n",
      "Iteration 147, loss = 0.39301808\n",
      "Iteration 148, loss = 0.39260444\n",
      "Iteration 149, loss = 0.39220905\n",
      "Iteration 150, loss = 0.39183610\n",
      "Iteration 151, loss = 0.39144190\n",
      "Iteration 152, loss = 0.39105207\n",
      "Iteration 153, loss = 0.39068721\n",
      "Iteration 154, loss = 0.39031976\n",
      "Iteration 155, loss = 0.38991988\n",
      "Iteration 156, loss = 0.38955552\n",
      "Iteration 157, loss = 0.38921532\n",
      "Iteration 158, loss = 0.38885436\n",
      "Iteration 159, loss = 0.38851840\n",
      "Iteration 160, loss = 0.38814913\n",
      "Iteration 161, loss = 0.38778363\n",
      "Iteration 162, loss = 0.38744741\n",
      "Iteration 163, loss = 0.38712151\n",
      "Iteration 164, loss = 0.38675981\n",
      "Iteration 165, loss = 0.38643747\n",
      "Iteration 166, loss = 0.38609889\n",
      "Iteration 167, loss = 0.38578563\n",
      "Iteration 168, loss = 0.38545549\n",
      "Iteration 169, loss = 0.38512321\n",
      "Iteration 170, loss = 0.38482269\n",
      "Iteration 171, loss = 0.38449420\n",
      "Iteration 172, loss = 0.38419611\n",
      "Iteration 173, loss = 0.38386027\n",
      "Iteration 174, loss = 0.38358761\n",
      "Iteration 175, loss = 0.38326876\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   3.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.2s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   1.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   1.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.3s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   7.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   2.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "Iteration 1, loss = 0.70135552\n",
      "Iteration 2, loss = 0.69922144\n",
      "Iteration 3, loss = 0.69603503\n",
      "Iteration 4, loss = 0.69196680\n",
      "Iteration 5, loss = 0.68740875\n",
      "Iteration 6, loss = 0.68263922\n",
      "Iteration 7, loss = 0.67769424\n",
      "Iteration 8, loss = 0.67257405\n",
      "Iteration 9, loss = 0.66750155\n",
      "Iteration 10, loss = 0.66235927\n",
      "Iteration 11, loss = 0.65738823\n",
      "Iteration 12, loss = 0.65241654\n",
      "Iteration 13, loss = 0.64758538\n",
      "Iteration 14, loss = 0.64268452\n",
      "Iteration 15, loss = 0.63808824\n",
      "Iteration 16, loss = 0.63361631\n",
      "Iteration 17, loss = 0.62909846\n",
      "Iteration 18, loss = 0.62475960\n",
      "Iteration 19, loss = 0.62050093\n",
      "Iteration 20, loss = 0.61632271\n",
      "Iteration 21, loss = 0.61220763\n",
      "Iteration 22, loss = 0.60825691\n",
      "Iteration 23, loss = 0.60416994\n",
      "Iteration 24, loss = 0.60029509\n",
      "Iteration 25, loss = 0.59645355\n",
      "Iteration 26, loss = 0.59271689\n",
      "Iteration 27, loss = 0.58899339\n",
      "Iteration 28, loss = 0.58525584\n",
      "Iteration 29, loss = 0.58166680\n",
      "Iteration 30, loss = 0.57821493\n",
      "Iteration 31, loss = 0.57474262\n",
      "Iteration 32, loss = 0.57129910\n",
      "Iteration 33, loss = 0.56785587\n",
      "Iteration 34, loss = 0.56465759\n",
      "Iteration 35, loss = 0.56132570\n",
      "Iteration 36, loss = 0.55814413\n",
      "Iteration 37, loss = 0.55486588\n",
      "Iteration 38, loss = 0.55178220\n",
      "Iteration 39, loss = 0.54864743\n",
      "Iteration 40, loss = 0.54565556\n",
      "Iteration 41, loss = 0.54253406\n",
      "Iteration 42, loss = 0.53965473\n",
      "Iteration 43, loss = 0.53671281\n",
      "Iteration 44, loss = 0.53375630\n",
      "Iteration 45, loss = 0.53093956\n",
      "Iteration 46, loss = 0.52811925\n",
      "Iteration 47, loss = 0.52525785\n",
      "Iteration 48, loss = 0.52254535\n",
      "Iteration 49, loss = 0.51984471\n",
      "Iteration 50, loss = 0.51717198\n",
      "Iteration 51, loss = 0.51448235\n",
      "Iteration 52, loss = 0.51193906\n",
      "Iteration 53, loss = 0.50934630\n",
      "Iteration 54, loss = 0.50687730\n",
      "Iteration 55, loss = 0.50436882\n",
      "Iteration 56, loss = 0.50193848\n",
      "Iteration 57, loss = 0.49956494\n",
      "Iteration 58, loss = 0.49716456\n",
      "Iteration 59, loss = 0.49473123\n",
      "Iteration 60, loss = 0.49252940\n",
      "Iteration 61, loss = 0.49026038\n",
      "Iteration 62, loss = 0.48801012\n",
      "Iteration 63, loss = 0.48588010\n",
      "Iteration 64, loss = 0.48370662\n",
      "Iteration 65, loss = 0.48153155\n",
      "Iteration 66, loss = 0.47948809\n",
      "Iteration 67, loss = 0.47753483\n",
      "Iteration 68, loss = 0.47546542\n",
      "Iteration 69, loss = 0.47350231\n",
      "Iteration 70, loss = 0.47154003\n",
      "Iteration 71, loss = 0.46962722\n",
      "Iteration 72, loss = 0.46785350\n",
      "Iteration 73, loss = 0.46597532\n",
      "Iteration 74, loss = 0.46421214\n",
      "Iteration 75, loss = 0.46244727\n",
      "Iteration 76, loss = 0.46069259\n",
      "Iteration 77, loss = 0.45903773\n",
      "Iteration 78, loss = 0.45734108\n",
      "Iteration 79, loss = 0.45571031\n",
      "Iteration 80, loss = 0.45412491\n",
      "Iteration 81, loss = 0.45254305\n",
      "Iteration 82, loss = 0.45105361\n",
      "Iteration 83, loss = 0.44952104\n",
      "Iteration 84, loss = 0.44802014\n",
      "Iteration 85, loss = 0.44662450\n",
      "Iteration 86, loss = 0.44511973\n",
      "Iteration 87, loss = 0.44377168\n",
      "Iteration 88, loss = 0.44239797\n",
      "Iteration 89, loss = 0.44102989\n",
      "Iteration 90, loss = 0.43977259\n",
      "Iteration 91, loss = 0.43847019\n",
      "Iteration 92, loss = 0.43721380\n",
      "Iteration 93, loss = 0.43595090\n",
      "Iteration 94, loss = 0.43475428\n",
      "Iteration 95, loss = 0.43352759\n",
      "Iteration 96, loss = 0.43246649\n",
      "Iteration 97, loss = 0.43120889\n",
      "Iteration 98, loss = 0.43013621\n",
      "Iteration 99, loss = 0.42907521\n",
      "Iteration 100, loss = 0.42795626\n",
      "Iteration 101, loss = 0.42693630\n",
      "Iteration 102, loss = 0.42589603\n",
      "Iteration 103, loss = 0.42486648\n",
      "Iteration 104, loss = 0.42389689\n",
      "Iteration 105, loss = 0.42288520\n",
      "Iteration 106, loss = 0.42197523\n",
      "Iteration 107, loss = 0.42098958\n",
      "Iteration 108, loss = 0.42005100\n",
      "Iteration 109, loss = 0.41919317\n",
      "Iteration 110, loss = 0.41830566\n",
      "Iteration 111, loss = 0.41742231\n",
      "Iteration 112, loss = 0.41659552\n",
      "Iteration 113, loss = 0.41571660\n",
      "Iteration 114, loss = 0.41490293\n",
      "Iteration 115, loss = 0.41411493\n",
      "Iteration 116, loss = 0.41332620\n",
      "Iteration 117, loss = 0.41255841\n",
      "Iteration 118, loss = 0.41176612\n",
      "Iteration 119, loss = 0.41103718\n",
      "Iteration 120, loss = 0.41030232\n",
      "Iteration 121, loss = 0.40955441\n",
      "Iteration 122, loss = 0.40883812\n",
      "Iteration 123, loss = 0.40814057\n",
      "Iteration 124, loss = 0.40749636\n",
      "Iteration 125, loss = 0.40680311\n",
      "Iteration 126, loss = 0.40612967\n",
      "Iteration 127, loss = 0.40550100\n",
      "Iteration 128, loss = 0.40480549\n",
      "Iteration 129, loss = 0.40419220\n",
      "Iteration 130, loss = 0.40358277\n",
      "Iteration 131, loss = 0.40294795\n",
      "Iteration 132, loss = 0.40234866\n",
      "Iteration 133, loss = 0.40173585\n",
      "Iteration 134, loss = 0.40115524\n",
      "Iteration 135, loss = 0.40060143\n",
      "Iteration 136, loss = 0.40000889\n",
      "Iteration 137, loss = 0.39940595\n",
      "Iteration 138, loss = 0.39893794\n",
      "Iteration 139, loss = 0.39833855\n",
      "Iteration 140, loss = 0.39782386\n",
      "Iteration 141, loss = 0.39731068\n",
      "Iteration 142, loss = 0.39674435\n",
      "Iteration 143, loss = 0.39626719\n",
      "Iteration 144, loss = 0.39576625\n",
      "Iteration 145, loss = 0.39523535\n",
      "Iteration 146, loss = 0.39477063\n",
      "Iteration 147, loss = 0.39428812\n",
      "Iteration 148, loss = 0.39378644\n",
      "Iteration 149, loss = 0.39334629\n",
      "Iteration 150, loss = 0.39284210\n",
      "Iteration 151, loss = 0.39241281\n",
      "Iteration 152, loss = 0.39196647\n",
      "Iteration 153, loss = 0.39150583\n",
      "Iteration 154, loss = 0.39104804\n",
      "Iteration 155, loss = 0.39062647\n",
      "Iteration 156, loss = 0.39019423\n",
      "Iteration 157, loss = 0.38978785\n",
      "Iteration 158, loss = 0.38934497\n",
      "Iteration 159, loss = 0.38893764\n",
      "Iteration 160, loss = 0.38851701\n",
      "Iteration 161, loss = 0.38813672\n",
      "Iteration 162, loss = 0.38772493\n",
      "Iteration 163, loss = 0.38732551\n",
      "Iteration 164, loss = 0.38694374\n",
      "Iteration 165, loss = 0.38654582\n",
      "Iteration 166, loss = 0.38616690\n",
      "Iteration 167, loss = 0.38578973\n",
      "Iteration 168, loss = 0.38542552\n",
      "Iteration 169, loss = 0.38507155\n",
      "Iteration 170, loss = 0.38469971\n",
      "Iteration 171, loss = 0.38432488\n",
      "Iteration 172, loss = 0.38398193\n",
      "Iteration 173, loss = 0.38364290\n",
      "Iteration 174, loss = 0.38326298\n",
      "Iteration 175, loss = 0.38292018\n",
      "Iteration 176, loss = 0.38260061\n",
      "Iteration 177, loss = 0.38224330\n",
      "Iteration 178, loss = 0.38193755\n",
      "Iteration 179, loss = 0.38157829\n",
      "Iteration 180, loss = 0.38123760\n",
      "Iteration 181, loss = 0.38094705\n",
      "Iteration 182, loss = 0.38064474\n",
      "Iteration 183, loss = 0.38030363\n",
      "Iteration 184, loss = 0.37995930\n",
      "Iteration 185, loss = 0.37965711\n",
      "Iteration 186, loss = 0.37938195\n",
      "Iteration 187, loss = 0.37902995\n",
      "Iteration 188, loss = 0.37874556\n",
      "Iteration 189, loss = 0.37847049\n",
      "Iteration 190, loss = 0.37813919\n",
      "Iteration 191, loss = 0.37786478\n",
      "Iteration 192, loss = 0.37755667\n",
      "Iteration 193, loss = 0.37728547\n",
      "Iteration 194, loss = 0.37700108\n",
      "Iteration 195, loss = 0.37670852\n",
      "Iteration 196, loss = 0.37643433\n",
      "Iteration 197, loss = 0.37615413\n",
      "Iteration 198, loss = 0.37584872\n",
      "Iteration 199, loss = 0.37560051\n",
      "Iteration 200, loss = 0.37531755\n",
      "Iteration 201, loss = 0.37506568\n",
      "Iteration 202, loss = 0.37478358\n",
      "Iteration 203, loss = 0.37453633\n",
      "Iteration 204, loss = 0.37427515\n",
      "Iteration 205, loss = 0.37401316\n",
      "Iteration 206, loss = 0.37374529\n",
      "Iteration 207, loss = 0.37349222\n",
      "Iteration 208, loss = 0.37325018\n",
      "Iteration 209, loss = 0.37300684\n",
      "Iteration 210, loss = 0.37274996\n",
      "Iteration 211, loss = 0.37250754\n",
      "Iteration 212, loss = 0.37225852\n",
      "Iteration 213, loss = 0.37200318\n",
      "Iteration 214, loss = 0.37177230\n",
      "Iteration 215, loss = 0.37152880\n",
      "Iteration 216, loss = 0.37128642\n",
      "Iteration 217, loss = 0.37105489\n",
      "Iteration 218, loss = 0.37081613\n",
      "Iteration 219, loss = 0.37059939\n",
      "Iteration 220, loss = 0.37036735\n",
      "Iteration 221, loss = 0.37012863\n",
      "Iteration 222, loss = 0.36989482\n",
      "Iteration 223, loss = 0.36967132\n",
      "Iteration 224, loss = 0.36944213\n",
      "Iteration 225, loss = 0.36924559\n",
      "Iteration 226, loss = 0.36899928\n",
      "Iteration 227, loss = 0.36880463\n",
      "Iteration 228, loss = 0.36855987\n",
      "Iteration 229, loss = 0.36834372\n",
      "Iteration 230, loss = 0.36812757\n",
      "Iteration 231, loss = 0.36791214\n",
      "Iteration 232, loss = 0.36769954\n",
      "Iteration 233, loss = 0.36751574\n",
      "Iteration 234, loss = 0.36727325\n",
      "Iteration 235, loss = 0.36707376\n",
      "Iteration 236, loss = 0.36686918\n",
      "Iteration 237, loss = 0.36666247\n",
      "Iteration 238, loss = 0.36643594\n",
      "Iteration 239, loss = 0.36627652\n",
      "Iteration 240, loss = 0.36603545\n",
      "Iteration 241, loss = 0.36581212\n",
      "Iteration 242, loss = 0.36564464\n",
      "Iteration 243, loss = 0.36541419\n",
      "Iteration 244, loss = 0.36522554\n",
      "Iteration 245, loss = 0.36504226\n",
      "Iteration 246, loss = 0.36483901\n",
      "Iteration 247, loss = 0.36463030\n",
      "Iteration 248, loss = 0.36446682\n",
      "Iteration 249, loss = 0.36425575\n",
      "Iteration 250, loss = 0.36405454\n",
      "Iteration 251, loss = 0.36388549\n",
      "Iteration 252, loss = 0.36368853\n",
      "Iteration 253, loss = 0.36350047\n",
      "Iteration 254, loss = 0.36330847\n",
      "Iteration 255, loss = 0.36311110\n",
      "Iteration 256, loss = 0.36293559\n",
      "Iteration 257, loss = 0.36274814\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   1.7s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   3.9s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   5.6s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   9.9s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.1s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   2.3s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.3s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.3s\n",
      "Iteration 1, loss = 0.74115507\n",
      "Iteration 2, loss = 0.73711524\n",
      "Iteration 3, loss = 0.73107155\n",
      "Iteration 4, loss = 0.72372827\n",
      "Iteration 5, loss = 0.71555255\n",
      "Iteration 6, loss = 0.70670645\n",
      "Iteration 7, loss = 0.69802480\n",
      "Iteration 8, loss = 0.68916851\n",
      "Iteration 9, loss = 0.68033447\n",
      "Iteration 10, loss = 0.67183375\n",
      "Iteration 11, loss = 0.66347026\n",
      "Iteration 12, loss = 0.65535366\n",
      "Iteration 13, loss = 0.64771182\n",
      "Iteration 14, loss = 0.64026272\n",
      "Iteration 15, loss = 0.63313853\n",
      "Iteration 16, loss = 0.62619628\n",
      "Iteration 17, loss = 0.61956000\n",
      "Iteration 18, loss = 0.61321268\n",
      "Iteration 19, loss = 0.60709750\n",
      "Iteration 20, loss = 0.60122783\n",
      "Iteration 21, loss = 0.59539083\n",
      "Iteration 22, loss = 0.59001701\n",
      "Iteration 23, loss = 0.58478767\n",
      "Iteration 24, loss = 0.57956156\n",
      "Iteration 25, loss = 0.57458952\n",
      "Iteration 26, loss = 0.56996311\n",
      "Iteration 27, loss = 0.56533183\n",
      "Iteration 28, loss = 0.56084346\n",
      "Iteration 29, loss = 0.55659426\n",
      "Iteration 30, loss = 0.55229316\n",
      "Iteration 31, loss = 0.54821662\n",
      "Iteration 32, loss = 0.54438840\n",
      "Iteration 33, loss = 0.54043248\n",
      "Iteration 34, loss = 0.53680120\n",
      "Iteration 35, loss = 0.53319803\n",
      "Iteration 36, loss = 0.52966018\n",
      "Iteration 37, loss = 0.52633021\n",
      "Iteration 38, loss = 0.52297983\n",
      "Iteration 39, loss = 0.51987012\n",
      "Iteration 40, loss = 0.51665675\n",
      "Iteration 41, loss = 0.51363683\n",
      "Iteration 42, loss = 0.51058948\n",
      "Iteration 43, loss = 0.50777590\n",
      "Iteration 44, loss = 0.50495363\n",
      "Iteration 45, loss = 0.50222393\n",
      "Iteration 46, loss = 0.49964987\n",
      "Iteration 47, loss = 0.49695532\n",
      "Iteration 48, loss = 0.49440918\n",
      "Iteration 49, loss = 0.49194154\n",
      "Iteration 50, loss = 0.48955614\n",
      "Iteration 51, loss = 0.48723515\n",
      "Iteration 52, loss = 0.48489353\n",
      "Iteration 53, loss = 0.48276441\n",
      "Iteration 54, loss = 0.48051501\n",
      "Iteration 55, loss = 0.47839489\n",
      "Iteration 56, loss = 0.47623316\n",
      "Iteration 57, loss = 0.47424998\n",
      "Iteration 58, loss = 0.47227742\n",
      "Iteration 59, loss = 0.47033230\n",
      "Iteration 60, loss = 0.46843580\n",
      "Iteration 61, loss = 0.46657470\n",
      "Iteration 62, loss = 0.46477242\n",
      "Iteration 63, loss = 0.46306351\n",
      "Iteration 64, loss = 0.46134275\n",
      "Iteration 65, loss = 0.45956948\n",
      "Iteration 66, loss = 0.45793566\n",
      "Iteration 67, loss = 0.45635447\n",
      "Iteration 68, loss = 0.45471915\n",
      "Iteration 69, loss = 0.45318538\n",
      "Iteration 70, loss = 0.45170364\n",
      "Iteration 71, loss = 0.45014020\n",
      "Iteration 72, loss = 0.44874114\n",
      "Iteration 73, loss = 0.44736151\n",
      "Iteration 74, loss = 0.44586946\n",
      "Iteration 75, loss = 0.44453533\n",
      "Iteration 76, loss = 0.44320858\n",
      "Iteration 77, loss = 0.44187493\n",
      "Iteration 78, loss = 0.44057846\n",
      "Iteration 79, loss = 0.43932002\n",
      "Iteration 80, loss = 0.43812426\n",
      "Iteration 81, loss = 0.43687280\n",
      "Iteration 82, loss = 0.43568997\n",
      "Iteration 83, loss = 0.43455966\n",
      "Iteration 84, loss = 0.43338207\n",
      "Iteration 85, loss = 0.43226945\n",
      "Iteration 86, loss = 0.43114361\n",
      "Iteration 87, loss = 0.43006848\n",
      "Iteration 88, loss = 0.42901252\n",
      "Iteration 89, loss = 0.42803329\n",
      "Iteration 90, loss = 0.42698457\n",
      "Iteration 91, loss = 0.42596368\n",
      "Iteration 92, loss = 0.42498822\n",
      "Iteration 93, loss = 0.42404463\n",
      "Iteration 94, loss = 0.42309154\n",
      "Iteration 95, loss = 0.42214232\n",
      "Iteration 96, loss = 0.42126406\n",
      "Iteration 97, loss = 0.42039221\n",
      "Iteration 98, loss = 0.41946088\n",
      "Iteration 99, loss = 0.41860912\n",
      "Iteration 100, loss = 0.41775640\n",
      "Iteration 101, loss = 0.41687555\n",
      "Iteration 102, loss = 0.41612003\n",
      "Iteration 103, loss = 0.41529861\n",
      "Iteration 104, loss = 0.41450384\n",
      "Iteration 105, loss = 0.41369343\n",
      "Iteration 106, loss = 0.41295000\n",
      "Iteration 107, loss = 0.41215538\n",
      "Iteration 108, loss = 0.41146788\n",
      "Iteration 109, loss = 0.41068623\n",
      "Iteration 110, loss = 0.40998452\n",
      "Iteration 111, loss = 0.40925780\n",
      "Iteration 112, loss = 0.40856968\n",
      "Iteration 113, loss = 0.40791065\n",
      "Iteration 114, loss = 0.40719880\n",
      "Iteration 115, loss = 0.40651401\n",
      "Iteration 116, loss = 0.40586809\n",
      "Iteration 117, loss = 0.40518237\n",
      "Iteration 118, loss = 0.40459437\n",
      "Iteration 119, loss = 0.40393861\n",
      "Iteration 120, loss = 0.40336850\n",
      "Iteration 121, loss = 0.40272679\n",
      "Iteration 122, loss = 0.40209916\n",
      "Iteration 123, loss = 0.40153143\n",
      "Iteration 124, loss = 0.40094174\n",
      "Iteration 125, loss = 0.40039474\n",
      "Iteration 126, loss = 0.39980522\n",
      "Iteration 127, loss = 0.39924249\n",
      "Iteration 128, loss = 0.39871565\n",
      "Iteration 129, loss = 0.39815155\n",
      "Iteration 130, loss = 0.39762954\n",
      "Iteration 131, loss = 0.39709215\n",
      "Iteration 132, loss = 0.39658812\n",
      "Iteration 133, loss = 0.39603884\n",
      "Iteration 134, loss = 0.39556020\n",
      "Iteration 135, loss = 0.39507542\n",
      "Iteration 136, loss = 0.39456453\n",
      "Iteration 137, loss = 0.39408200\n",
      "Iteration 138, loss = 0.39359800\n",
      "Iteration 139, loss = 0.39313412\n",
      "Iteration 140, loss = 0.39265683\n",
      "Iteration 141, loss = 0.39218616\n",
      "Iteration 142, loss = 0.39175249\n",
      "Iteration 143, loss = 0.39129282\n",
      "Iteration 144, loss = 0.39087132\n",
      "Iteration 145, loss = 0.39040983\n",
      "Iteration 146, loss = 0.38998064\n",
      "Iteration 147, loss = 0.38951868\n",
      "Iteration 148, loss = 0.38910361\n",
      "Iteration 149, loss = 0.38871214\n",
      "Iteration 150, loss = 0.38831303\n",
      "Iteration 151, loss = 0.38788082\n",
      "Iteration 152, loss = 0.38746910\n",
      "Iteration 153, loss = 0.38708550\n",
      "Iteration 154, loss = 0.38666648\n",
      "Iteration 155, loss = 0.38630355\n",
      "Iteration 156, loss = 0.38588865\n",
      "Iteration 157, loss = 0.38551256\n",
      "Iteration 158, loss = 0.38511986\n",
      "Iteration 159, loss = 0.38475524\n",
      "Iteration 160, loss = 0.38439932\n",
      "Iteration 161, loss = 0.38404090\n",
      "Iteration 162, loss = 0.38363556\n",
      "Iteration 163, loss = 0.38329539\n",
      "Iteration 164, loss = 0.38295424\n",
      "Iteration 165, loss = 0.38258823\n",
      "Iteration 166, loss = 0.38223341\n",
      "Iteration 167, loss = 0.38189674\n",
      "Iteration 168, loss = 0.38154409\n",
      "Iteration 169, loss = 0.38122110\n",
      "Iteration 170, loss = 0.38089139\n",
      "Iteration 171, loss = 0.38056584\n",
      "Iteration 172, loss = 0.38022673\n",
      "Iteration 173, loss = 0.37991411\n",
      "Iteration 174, loss = 0.37958583\n",
      "Iteration 175, loss = 0.37926826\n",
      "Iteration 176, loss = 0.37896655\n",
      "Iteration 177, loss = 0.37864712\n",
      "Iteration 178, loss = 0.37833638\n",
      "Iteration 179, loss = 0.37803033\n",
      "Iteration 180, loss = 0.37774616\n",
      "Iteration 181, loss = 0.37743812\n",
      "Iteration 182, loss = 0.37712286\n",
      "Iteration 183, loss = 0.37683382\n",
      "Iteration 184, loss = 0.37654418\n",
      "Iteration 185, loss = 0.37627256\n",
      "Iteration 186, loss = 0.37596971\n",
      "Iteration 187, loss = 0.37571898\n",
      "Iteration 188, loss = 0.37542141\n",
      "Iteration 189, loss = 0.37514021\n",
      "Iteration 190, loss = 0.37485837\n",
      "Iteration 191, loss = 0.37459455\n",
      "Iteration 192, loss = 0.37433795\n",
      "Iteration 193, loss = 0.37404412\n",
      "Iteration 194, loss = 0.37378870\n",
      "Iteration 195, loss = 0.37351966\n",
      "Iteration 196, loss = 0.37326860\n",
      "Iteration 197, loss = 0.37301961\n",
      "Iteration 198, loss = 0.37274105\n",
      "Iteration 199, loss = 0.37249637\n",
      "Iteration 200, loss = 0.37225069\n",
      "Iteration 201, loss = 0.37200060\n",
      "Iteration 202, loss = 0.37174262\n",
      "Iteration 203, loss = 0.37149845\n",
      "Iteration 204, loss = 0.37125443\n",
      "Iteration 205, loss = 0.37103216\n",
      "Iteration 206, loss = 0.37077424\n",
      "Iteration 207, loss = 0.37053771\n",
      "Iteration 208, loss = 0.37031056\n",
      "Iteration 209, loss = 0.37006592\n",
      "Iteration 210, loss = 0.36983020\n",
      "Iteration 211, loss = 0.36960459\n",
      "Iteration 212, loss = 0.36937320\n",
      "Iteration 213, loss = 0.36914537\n",
      "Iteration 214, loss = 0.36892253\n",
      "Iteration 215, loss = 0.36869012\n",
      "Iteration 216, loss = 0.36846281\n",
      "Iteration 217, loss = 0.36824807\n",
      "Iteration 218, loss = 0.36803092\n",
      "Iteration 219, loss = 0.36781624\n",
      "Iteration 220, loss = 0.36759102\n",
      "Iteration 221, loss = 0.36739703\n",
      "Iteration 222, loss = 0.36716464\n",
      "Iteration 223, loss = 0.36699256\n",
      "Iteration 224, loss = 0.36673782\n",
      "Iteration 225, loss = 0.36653214\n",
      "Iteration 226, loss = 0.36633990\n",
      "Iteration 227, loss = 0.36614320\n",
      "Iteration 228, loss = 0.36591291\n",
      "Iteration 229, loss = 0.36572413\n",
      "Iteration 230, loss = 0.36553118\n",
      "Iteration 231, loss = 0.36531727\n",
      "Iteration 232, loss = 0.36510540\n",
      "Iteration 233, loss = 0.36491569\n",
      "Iteration 234, loss = 0.36471917\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   1.8s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.7s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.7s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.9s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.9s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   9.0s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   2.9s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   2.2s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   2.1s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   2.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "Iteration 1, loss = 0.75073101\n",
      "Iteration 2, loss = 0.74737071\n",
      "Iteration 3, loss = 0.74225460\n",
      "Iteration 4, loss = 0.73610596\n",
      "Iteration 5, loss = 0.73030111\n",
      "Iteration 6, loss = 0.72371728\n",
      "Iteration 7, loss = 0.71785322\n",
      "Iteration 8, loss = 0.71209065\n",
      "Iteration 9, loss = 0.70666640\n",
      "Iteration 10, loss = 0.70167887\n",
      "Iteration 11, loss = 0.69712677\n",
      "Iteration 12, loss = 0.69298631\n",
      "Iteration 13, loss = 0.68886172\n",
      "Iteration 14, loss = 0.68512392\n",
      "Iteration 15, loss = 0.68161758\n",
      "Iteration 16, loss = 0.67812534\n",
      "Iteration 17, loss = 0.67477919\n",
      "Iteration 18, loss = 0.67163777\n",
      "Iteration 19, loss = 0.66851898\n",
      "Iteration 20, loss = 0.66559192\n",
      "Iteration 21, loss = 0.66249926\n",
      "Iteration 22, loss = 0.65956125\n",
      "Iteration 23, loss = 0.65668581\n",
      "Iteration 24, loss = 0.65379294\n",
      "Iteration 25, loss = 0.65099205\n",
      "Iteration 26, loss = 0.64814369\n",
      "Iteration 27, loss = 0.64527087\n",
      "Iteration 28, loss = 0.64244936\n",
      "Iteration 29, loss = 0.63957574\n",
      "Iteration 30, loss = 0.63683364\n",
      "Iteration 31, loss = 0.63396992\n",
      "Iteration 32, loss = 0.63113581\n",
      "Iteration 33, loss = 0.62828140\n",
      "Iteration 34, loss = 0.62545292\n",
      "Iteration 35, loss = 0.62265812\n",
      "Iteration 36, loss = 0.61974065\n",
      "Iteration 37, loss = 0.61686697\n",
      "Iteration 38, loss = 0.61408312\n",
      "Iteration 39, loss = 0.61108882\n",
      "Iteration 40, loss = 0.60823040\n",
      "Iteration 41, loss = 0.60532105\n",
      "Iteration 42, loss = 0.60245909\n",
      "Iteration 43, loss = 0.59951885\n",
      "Iteration 44, loss = 0.59666065\n",
      "Iteration 45, loss = 0.59367908\n",
      "Iteration 46, loss = 0.59079638\n",
      "Iteration 47, loss = 0.58794146\n",
      "Iteration 48, loss = 0.58495574\n",
      "Iteration 49, loss = 0.58211133\n",
      "Iteration 50, loss = 0.57923130\n",
      "Iteration 51, loss = 0.57633475\n",
      "Iteration 52, loss = 0.57338940\n",
      "Iteration 53, loss = 0.57059009\n",
      "Iteration 54, loss = 0.56767637\n",
      "Iteration 55, loss = 0.56470167\n",
      "Iteration 56, loss = 0.56188957\n",
      "Iteration 57, loss = 0.55899472\n",
      "Iteration 58, loss = 0.55610763\n",
      "Iteration 59, loss = 0.55328690\n",
      "Iteration 60, loss = 0.55038041\n",
      "Iteration 61, loss = 0.54757226\n",
      "Iteration 62, loss = 0.54472720\n",
      "Iteration 63, loss = 0.54188022\n",
      "Iteration 64, loss = 0.53896819\n",
      "Iteration 65, loss = 0.53621668\n",
      "Iteration 66, loss = 0.53338062\n",
      "Iteration 67, loss = 0.53054487\n",
      "Iteration 68, loss = 0.52773002\n",
      "Iteration 69, loss = 0.52493145\n",
      "Iteration 70, loss = 0.52222215\n",
      "Iteration 71, loss = 0.51956641\n",
      "Iteration 72, loss = 0.51670857\n",
      "Iteration 73, loss = 0.51407380\n",
      "Iteration 74, loss = 0.51130639\n",
      "Iteration 75, loss = 0.50878090\n",
      "Iteration 76, loss = 0.50615368\n",
      "Iteration 77, loss = 0.50344929\n",
      "Iteration 78, loss = 0.50096523\n",
      "Iteration 79, loss = 0.49839657\n",
      "Iteration 80, loss = 0.49594881\n",
      "Iteration 81, loss = 0.49341478\n",
      "Iteration 82, loss = 0.49099575\n",
      "Iteration 83, loss = 0.48852378\n",
      "Iteration 84, loss = 0.48620937\n",
      "Iteration 85, loss = 0.48390212\n",
      "Iteration 86, loss = 0.48155732\n",
      "Iteration 87, loss = 0.47927182\n",
      "Iteration 88, loss = 0.47706331\n",
      "Iteration 89, loss = 0.47491750\n",
      "Iteration 90, loss = 0.47268634\n",
      "Iteration 91, loss = 0.47062459\n",
      "Iteration 92, loss = 0.46857575\n",
      "Iteration 93, loss = 0.46641130\n",
      "Iteration 94, loss = 0.46448536\n",
      "Iteration 95, loss = 0.46253003\n",
      "Iteration 96, loss = 0.46065080\n",
      "Iteration 97, loss = 0.45885448\n",
      "Iteration 98, loss = 0.45690033\n",
      "Iteration 99, loss = 0.45511596\n",
      "Iteration 100, loss = 0.45332481\n",
      "Iteration 101, loss = 0.45170465\n",
      "Iteration 102, loss = 0.44997012\n",
      "Iteration 103, loss = 0.44831117\n",
      "Iteration 104, loss = 0.44676739\n",
      "Iteration 105, loss = 0.44526522\n",
      "Iteration 106, loss = 0.44365170\n",
      "Iteration 107, loss = 0.44217922\n",
      "Iteration 108, loss = 0.44061506\n",
      "Iteration 109, loss = 0.43930050\n",
      "Iteration 110, loss = 0.43782779\n",
      "Iteration 111, loss = 0.43657407\n",
      "Iteration 112, loss = 0.43518922\n",
      "Iteration 113, loss = 0.43389418\n",
      "Iteration 114, loss = 0.43270036\n",
      "Iteration 115, loss = 0.43142827\n",
      "Iteration 116, loss = 0.43025527\n",
      "Iteration 117, loss = 0.42909933\n",
      "Iteration 118, loss = 0.42797327\n",
      "Iteration 119, loss = 0.42693853\n",
      "Iteration 120, loss = 0.42583294\n",
      "Iteration 121, loss = 0.42473256\n",
      "Iteration 122, loss = 0.42373822\n",
      "Iteration 123, loss = 0.42274368\n",
      "Iteration 124, loss = 0.42175204\n",
      "Iteration 125, loss = 0.42078389\n",
      "Iteration 126, loss = 0.41983321\n",
      "Iteration 127, loss = 0.41904084\n",
      "Iteration 128, loss = 0.41814155\n",
      "Iteration 129, loss = 0.41725807\n",
      "Iteration 130, loss = 0.41636927\n",
      "Iteration 131, loss = 0.41558217\n",
      "Iteration 132, loss = 0.41472635\n",
      "Iteration 133, loss = 0.41398196\n",
      "Iteration 134, loss = 0.41320543\n",
      "Iteration 135, loss = 0.41240112\n",
      "Iteration 136, loss = 0.41162600\n",
      "Iteration 137, loss = 0.41093957\n",
      "Iteration 138, loss = 0.41021278\n",
      "Iteration 139, loss = 0.40958043\n",
      "Iteration 140, loss = 0.40881112\n",
      "Iteration 141, loss = 0.40814655\n",
      "Iteration 142, loss = 0.40749994\n",
      "Iteration 143, loss = 0.40686005\n",
      "Iteration 144, loss = 0.40623036\n",
      "Iteration 145, loss = 0.40556615\n",
      "Iteration 146, loss = 0.40500712\n",
      "Iteration 147, loss = 0.40437935\n",
      "Iteration 148, loss = 0.40380299\n",
      "Iteration 149, loss = 0.40320334\n",
      "Iteration 150, loss = 0.40265993\n",
      "Iteration 151, loss = 0.40209713\n",
      "Iteration 152, loss = 0.40152573\n",
      "Iteration 153, loss = 0.40097929\n",
      "Iteration 154, loss = 0.40050517\n",
      "Iteration 155, loss = 0.39995962\n",
      "Iteration 156, loss = 0.39944990\n",
      "Iteration 157, loss = 0.39898816\n",
      "Iteration 158, loss = 0.39844123\n",
      "Iteration 159, loss = 0.39800801\n",
      "Iteration 160, loss = 0.39751218\n",
      "Iteration 161, loss = 0.39703929\n",
      "Iteration 162, loss = 0.39663547\n",
      "Iteration 163, loss = 0.39610787\n",
      "Iteration 164, loss = 0.39565430\n",
      "Iteration 165, loss = 0.39526217\n",
      "Iteration 166, loss = 0.39478386\n",
      "Iteration 167, loss = 0.39434597\n",
      "Iteration 168, loss = 0.39389791\n",
      "Iteration 169, loss = 0.39351562\n",
      "Iteration 170, loss = 0.39307442\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   1.3s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   3.2s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=  10.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   4.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.1s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   2.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.2s\n",
      "Iteration 1, loss = 0.70458312\n",
      "Iteration 2, loss = 0.70199832\n",
      "Iteration 3, loss = 0.69806014\n",
      "Iteration 4, loss = 0.69329298\n",
      "Iteration 5, loss = 0.68768737\n",
      "Iteration 6, loss = 0.68188749\n",
      "Iteration 7, loss = 0.67575761\n",
      "Iteration 8, loss = 0.66948568\n",
      "Iteration 9, loss = 0.66313996\n",
      "Iteration 10, loss = 0.65668736\n",
      "Iteration 11, loss = 0.65067534\n",
      "Iteration 12, loss = 0.64437847\n",
      "Iteration 13, loss = 0.63844839\n",
      "Iteration 14, loss = 0.63247123\n",
      "Iteration 15, loss = 0.62665251\n",
      "Iteration 16, loss = 0.62083526\n",
      "Iteration 17, loss = 0.61532602\n",
      "Iteration 18, loss = 0.61009413\n",
      "Iteration 19, loss = 0.60466360\n",
      "Iteration 20, loss = 0.59951029\n",
      "Iteration 21, loss = 0.59450300\n",
      "Iteration 22, loss = 0.58962595\n",
      "Iteration 23, loss = 0.58478974\n",
      "Iteration 24, loss = 0.58008646\n",
      "Iteration 25, loss = 0.57554419\n",
      "Iteration 26, loss = 0.57089641\n",
      "Iteration 27, loss = 0.56662495\n",
      "Iteration 28, loss = 0.56234269\n",
      "Iteration 29, loss = 0.55817724\n",
      "Iteration 30, loss = 0.55402467\n",
      "Iteration 31, loss = 0.55010125\n",
      "Iteration 32, loss = 0.54623190\n",
      "Iteration 33, loss = 0.54234252\n",
      "Iteration 34, loss = 0.53860778\n",
      "Iteration 35, loss = 0.53504869\n",
      "Iteration 36, loss = 0.53146217\n",
      "Iteration 37, loss = 0.52805916\n",
      "Iteration 38, loss = 0.52466487\n",
      "Iteration 39, loss = 0.52119951\n",
      "Iteration 40, loss = 0.51802728\n",
      "Iteration 41, loss = 0.51485955\n",
      "Iteration 42, loss = 0.51175788\n",
      "Iteration 43, loss = 0.50871001\n",
      "Iteration 44, loss = 0.50583009\n",
      "Iteration 45, loss = 0.50297381\n",
      "Iteration 46, loss = 0.50006869\n",
      "Iteration 47, loss = 0.49744839\n",
      "Iteration 48, loss = 0.49472447\n",
      "Iteration 49, loss = 0.49206770\n",
      "Iteration 50, loss = 0.48951562\n",
      "Iteration 51, loss = 0.48701714\n",
      "Iteration 52, loss = 0.48456001\n",
      "Iteration 53, loss = 0.48228792\n",
      "Iteration 54, loss = 0.47996603\n",
      "Iteration 55, loss = 0.47767015\n",
      "Iteration 56, loss = 0.47548646\n",
      "Iteration 57, loss = 0.47331162\n",
      "Iteration 58, loss = 0.47117170\n",
      "Iteration 59, loss = 0.46920247\n",
      "Iteration 60, loss = 0.46716560\n",
      "Iteration 61, loss = 0.46520013\n",
      "Iteration 62, loss = 0.46328335\n",
      "Iteration 63, loss = 0.46141798\n",
      "Iteration 64, loss = 0.45954344\n",
      "Iteration 65, loss = 0.45770581\n",
      "Iteration 66, loss = 0.45600442\n",
      "Iteration 67, loss = 0.45422611\n",
      "Iteration 68, loss = 0.45262383\n",
      "Iteration 69, loss = 0.45100313\n",
      "Iteration 70, loss = 0.44935951\n",
      "Iteration 71, loss = 0.44784574\n",
      "Iteration 72, loss = 0.44628240\n",
      "Iteration 73, loss = 0.44477857\n",
      "Iteration 74, loss = 0.44335086\n",
      "Iteration 75, loss = 0.44189705\n",
      "Iteration 76, loss = 0.44050985\n",
      "Iteration 77, loss = 0.43914480\n",
      "Iteration 78, loss = 0.43784702\n",
      "Iteration 79, loss = 0.43652518\n",
      "Iteration 80, loss = 0.43528498\n",
      "Iteration 81, loss = 0.43398746\n",
      "Iteration 82, loss = 0.43277591\n",
      "Iteration 83, loss = 0.43161738\n",
      "Iteration 84, loss = 0.43042759\n",
      "Iteration 85, loss = 0.42930147\n",
      "Iteration 86, loss = 0.42817847\n",
      "Iteration 87, loss = 0.42712056\n",
      "Iteration 88, loss = 0.42611073\n",
      "Iteration 89, loss = 0.42493606\n",
      "Iteration 90, loss = 0.42394431\n",
      "Iteration 91, loss = 0.42292180\n",
      "Iteration 92, loss = 0.42189500\n",
      "Iteration 93, loss = 0.42096586\n",
      "Iteration 94, loss = 0.41996920\n",
      "Iteration 95, loss = 0.41908366\n",
      "Iteration 96, loss = 0.41810019\n",
      "Iteration 97, loss = 0.41723067\n",
      "Iteration 98, loss = 0.41634776\n",
      "Iteration 99, loss = 0.41542221\n",
      "Iteration 100, loss = 0.41460393\n",
      "Iteration 101, loss = 0.41376978\n",
      "Iteration 102, loss = 0.41289540\n",
      "Iteration 103, loss = 0.41205978\n",
      "Iteration 104, loss = 0.41131757\n",
      "Iteration 105, loss = 0.41051596\n",
      "Iteration 106, loss = 0.40978869\n",
      "Iteration 107, loss = 0.40900789\n",
      "Iteration 108, loss = 0.40829422\n",
      "Iteration 109, loss = 0.40751959\n",
      "Iteration 110, loss = 0.40685374\n",
      "Iteration 111, loss = 0.40608760\n",
      "Iteration 112, loss = 0.40541681\n",
      "Iteration 113, loss = 0.40479104\n",
      "Iteration 114, loss = 0.40408618\n",
      "Iteration 115, loss = 0.40338344\n",
      "Iteration 116, loss = 0.40275387\n",
      "Iteration 117, loss = 0.40208372\n",
      "Iteration 118, loss = 0.40150307\n",
      "Iteration 119, loss = 0.40084265\n",
      "Iteration 120, loss = 0.40023513\n",
      "Iteration 121, loss = 0.39961688\n",
      "Iteration 122, loss = 0.39902227\n",
      "Iteration 123, loss = 0.39843988\n",
      "Iteration 124, loss = 0.39785561\n",
      "Iteration 125, loss = 0.39725102\n",
      "Iteration 126, loss = 0.39668945\n",
      "Iteration 127, loss = 0.39617032\n",
      "Iteration 128, loss = 0.39559808\n",
      "Iteration 129, loss = 0.39499741\n",
      "Iteration 130, loss = 0.39449252\n",
      "Iteration 131, loss = 0.39393916\n",
      "Iteration 132, loss = 0.39341283\n",
      "Iteration 133, loss = 0.39289685\n",
      "Iteration 134, loss = 0.39236861\n",
      "Iteration 135, loss = 0.39186060\n",
      "Iteration 136, loss = 0.39131385\n",
      "Iteration 137, loss = 0.39088862\n",
      "Iteration 138, loss = 0.39035995\n",
      "Iteration 139, loss = 0.38987678\n",
      "Iteration 140, loss = 0.38939304\n",
      "Iteration 141, loss = 0.38888517\n",
      "Iteration 142, loss = 0.38840499\n",
      "Iteration 143, loss = 0.38795192\n",
      "Iteration 144, loss = 0.38751486\n",
      "Iteration 145, loss = 0.38702175\n",
      "Iteration 146, loss = 0.38658012\n",
      "Iteration 147, loss = 0.38612483\n",
      "Iteration 148, loss = 0.38569120\n",
      "Iteration 149, loss = 0.38520645\n",
      "Iteration 150, loss = 0.38480079\n",
      "Iteration 151, loss = 0.38437237\n",
      "Iteration 152, loss = 0.38392942\n",
      "Iteration 153, loss = 0.38347455\n",
      "Iteration 154, loss = 0.38304798\n",
      "Iteration 155, loss = 0.38263043\n",
      "Iteration 156, loss = 0.38220044\n",
      "Iteration 157, loss = 0.38181132\n",
      "Iteration 158, loss = 0.38138622\n",
      "Iteration 159, loss = 0.38094404\n",
      "Iteration 160, loss = 0.38056252\n",
      "Iteration 161, loss = 0.38015232\n",
      "Iteration 162, loss = 0.37974944\n",
      "Iteration 163, loss = 0.37938556\n",
      "Iteration 164, loss = 0.37897638\n",
      "Iteration 165, loss = 0.37859414\n",
      "Iteration 166, loss = 0.37823554\n",
      "Iteration 167, loss = 0.37781039\n",
      "Iteration 168, loss = 0.37748369\n",
      "Iteration 169, loss = 0.37710111\n",
      "Iteration 170, loss = 0.37675466\n",
      "Iteration 171, loss = 0.37640061\n",
      "Iteration 172, loss = 0.37600435\n",
      "Iteration 173, loss = 0.37565680\n",
      "Iteration 174, loss = 0.37528457\n",
      "Iteration 175, loss = 0.37495539\n",
      "Iteration 176, loss = 0.37461926\n",
      "Iteration 177, loss = 0.37427043\n",
      "Iteration 178, loss = 0.37392982\n",
      "Iteration 179, loss = 0.37358966\n",
      "Iteration 180, loss = 0.37324021\n",
      "Iteration 181, loss = 0.37296921\n",
      "Iteration 182, loss = 0.37260650\n",
      "Iteration 183, loss = 0.37231240\n",
      "Iteration 184, loss = 0.37197972\n",
      "Iteration 185, loss = 0.37165101\n",
      "Iteration 186, loss = 0.37134934\n",
      "Iteration 187, loss = 0.37104604\n",
      "Iteration 188, loss = 0.37072334\n",
      "Iteration 189, loss = 0.37042003\n",
      "Iteration 190, loss = 0.37011303\n",
      "Iteration 191, loss = 0.36983652\n",
      "Iteration 192, loss = 0.36952991\n",
      "Iteration 193, loss = 0.36924447\n",
      "Iteration 194, loss = 0.36897647\n",
      "Iteration 195, loss = 0.36866278\n",
      "Iteration 196, loss = 0.36839342\n",
      "Iteration 197, loss = 0.36811700\n",
      "Iteration 198, loss = 0.36779948\n",
      "Iteration 199, loss = 0.36753971\n",
      "Iteration 200, loss = 0.36726009\n",
      "Iteration 201, loss = 0.36699296\n",
      "Iteration 202, loss = 0.36671371\n",
      "Iteration 203, loss = 0.36646158\n",
      "Iteration 204, loss = 0.36615360\n",
      "Iteration 205, loss = 0.36590847\n",
      "Iteration 206, loss = 0.36564369\n",
      "Iteration 207, loss = 0.36538368\n",
      "Iteration 208, loss = 0.36513251\n",
      "Iteration 209, loss = 0.36489352\n",
      "Iteration 210, loss = 0.36462334\n",
      "Iteration 211, loss = 0.36438568\n",
      "Iteration 212, loss = 0.36414202\n",
      "Iteration 213, loss = 0.36387954\n",
      "Iteration 214, loss = 0.36362848\n",
      "Iteration 215, loss = 0.36337878\n",
      "Iteration 216, loss = 0.36313480\n",
      "Iteration 217, loss = 0.36289324\n",
      "Iteration 218, loss = 0.36268397\n",
      "Iteration 219, loss = 0.36241389\n",
      "Iteration 220, loss = 0.36219162\n",
      "Iteration 221, loss = 0.36198798\n",
      "Iteration 222, loss = 0.36173650\n",
      "Iteration 223, loss = 0.36151643\n",
      "Iteration 224, loss = 0.36128890\n",
      "Iteration 225, loss = 0.36104936\n",
      "Iteration 226, loss = 0.36084103\n",
      "Iteration 227, loss = 0.36062842\n",
      "Iteration 228, loss = 0.36041379\n",
      "Iteration 229, loss = 0.36018548\n",
      "Iteration 230, loss = 0.35996964\n",
      "Iteration 231, loss = 0.35975061\n",
      "Iteration 232, loss = 0.35954583\n",
      "Iteration 233, loss = 0.35933655\n",
      "Iteration 234, loss = 0.35912615\n",
      "Iteration 235, loss = 0.35890473\n",
      "Iteration 236, loss = 0.35869032\n",
      "Iteration 237, loss = 0.35851071\n",
      "Iteration 238, loss = 0.35829610\n",
      "Iteration 239, loss = 0.35812590\n",
      "Iteration 240, loss = 0.35788791\n",
      "Iteration 241, loss = 0.35768276\n",
      "Iteration 242, loss = 0.35751656\n",
      "Iteration 243, loss = 0.35729491\n",
      "Iteration 244, loss = 0.35709518\n",
      "Iteration 245, loss = 0.35688224\n",
      "Iteration 246, loss = 0.35671498\n",
      "Iteration 247, loss = 0.35651253\n",
      "Iteration 248, loss = 0.35632846\n",
      "Iteration 249, loss = 0.35612985\n",
      "Iteration 250, loss = 0.35594574\n",
      "Iteration 251, loss = 0.35575159\n",
      "Iteration 252, loss = 0.35556218\n",
      "Iteration 253, loss = 0.35539104\n",
      "Iteration 254, loss = 0.35521210\n",
      "Iteration 255, loss = 0.35503839\n",
      "Iteration 256, loss = 0.35480518\n",
      "Iteration 257, loss = 0.35462258\n",
      "Iteration 258, loss = 0.35446628\n",
      "Iteration 259, loss = 0.35428261\n",
      "Iteration 260, loss = 0.35409366\n",
      "Iteration 261, loss = 0.35390991\n",
      "Iteration 262, loss = 0.35372274\n",
      "Iteration 263, loss = 0.35357127\n",
      "Iteration 264, loss = 0.35340372\n",
      "Iteration 265, loss = 0.35321056\n",
      "Iteration 266, loss = 0.35304752\n",
      "Iteration 267, loss = 0.35286328\n",
      "Iteration 268, loss = 0.35268502\n",
      "Iteration 269, loss = 0.35253195\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   1.8s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.7s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=  11.5s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   2.0s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   2.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.7s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   2.9s\n",
      "Iteration 1, loss = 0.77369997\n",
      "Iteration 2, loss = 0.77080825\n",
      "Iteration 3, loss = 0.76630577\n",
      "Iteration 4, loss = 0.76070705\n",
      "Iteration 5, loss = 0.75455025\n",
      "Iteration 6, loss = 0.74795399\n",
      "Iteration 7, loss = 0.74108375\n",
      "Iteration 8, loss = 0.73420899\n",
      "Iteration 9, loss = 0.72702355\n",
      "Iteration 10, loss = 0.72014022\n",
      "Iteration 11, loss = 0.71318438\n",
      "Iteration 12, loss = 0.70644079\n",
      "Iteration 13, loss = 0.69976814\n",
      "Iteration 14, loss = 0.69333468\n",
      "Iteration 15, loss = 0.68713681\n",
      "Iteration 16, loss = 0.68077445\n",
      "Iteration 17, loss = 0.67483035\n",
      "Iteration 18, loss = 0.66891583\n",
      "Iteration 19, loss = 0.66310198\n",
      "Iteration 20, loss = 0.65754016\n",
      "Iteration 21, loss = 0.65204221\n",
      "Iteration 22, loss = 0.64648674\n",
      "Iteration 23, loss = 0.64142876\n",
      "Iteration 24, loss = 0.63629550\n",
      "Iteration 25, loss = 0.63124210\n",
      "Iteration 26, loss = 0.62630623\n",
      "Iteration 27, loss = 0.62141897\n",
      "Iteration 28, loss = 0.61682848\n",
      "Iteration 29, loss = 0.61226243\n",
      "Iteration 30, loss = 0.60766154\n",
      "Iteration 31, loss = 0.60326772\n",
      "Iteration 32, loss = 0.59882111\n",
      "Iteration 33, loss = 0.59463180\n",
      "Iteration 34, loss = 0.59046842\n",
      "Iteration 35, loss = 0.58630563\n",
      "Iteration 36, loss = 0.58224803\n",
      "Iteration 37, loss = 0.57821720\n",
      "Iteration 38, loss = 0.57439886\n",
      "Iteration 39, loss = 0.57053297\n",
      "Iteration 40, loss = 0.56687116\n",
      "Iteration 41, loss = 0.56320776\n",
      "Iteration 42, loss = 0.55957769\n",
      "Iteration 43, loss = 0.55605730\n",
      "Iteration 44, loss = 0.55258230\n",
      "Iteration 45, loss = 0.54911772\n",
      "Iteration 46, loss = 0.54576569\n",
      "Iteration 47, loss = 0.54262857\n",
      "Iteration 48, loss = 0.53939369\n",
      "Iteration 49, loss = 0.53633723\n",
      "Iteration 50, loss = 0.53322306\n",
      "Iteration 51, loss = 0.53017120\n",
      "Iteration 52, loss = 0.52730250\n",
      "Iteration 53, loss = 0.52438505\n",
      "Iteration 54, loss = 0.52156853\n",
      "Iteration 55, loss = 0.51883204\n",
      "Iteration 56, loss = 0.51614833\n",
      "Iteration 57, loss = 0.51341164\n",
      "Iteration 58, loss = 0.51088981\n",
      "Iteration 59, loss = 0.50851082\n",
      "Iteration 60, loss = 0.50597195\n",
      "Iteration 61, loss = 0.50351701\n",
      "Iteration 62, loss = 0.50116018\n",
      "Iteration 63, loss = 0.49880870\n",
      "Iteration 64, loss = 0.49662583\n",
      "Iteration 65, loss = 0.49441248\n",
      "Iteration 66, loss = 0.49222503\n",
      "Iteration 67, loss = 0.49014649\n",
      "Iteration 68, loss = 0.48805716\n",
      "Iteration 69, loss = 0.48606256\n",
      "Iteration 70, loss = 0.48409344\n",
      "Iteration 71, loss = 0.48226715\n",
      "Iteration 72, loss = 0.48030029\n",
      "Iteration 73, loss = 0.47854035\n",
      "Iteration 74, loss = 0.47681458\n",
      "Iteration 75, loss = 0.47493543\n",
      "Iteration 76, loss = 0.47333560\n",
      "Iteration 77, loss = 0.47161822\n",
      "Iteration 78, loss = 0.46999491\n",
      "Iteration 79, loss = 0.46837538\n",
      "Iteration 80, loss = 0.46686709\n",
      "Iteration 81, loss = 0.46535547\n",
      "Iteration 82, loss = 0.46394884\n",
      "Iteration 83, loss = 0.46251717\n",
      "Iteration 84, loss = 0.46108665\n",
      "Iteration 85, loss = 0.45970715\n",
      "Iteration 86, loss = 0.45838849\n",
      "Iteration 87, loss = 0.45708437\n",
      "Iteration 88, loss = 0.45580178\n",
      "Iteration 89, loss = 0.45449131\n",
      "Iteration 90, loss = 0.45333881\n",
      "Iteration 91, loss = 0.45215874\n",
      "Iteration 92, loss = 0.45096829\n",
      "Iteration 93, loss = 0.44984029\n",
      "Iteration 94, loss = 0.44881149\n",
      "Iteration 95, loss = 0.44765348\n",
      "Iteration 96, loss = 0.44654518\n",
      "Iteration 97, loss = 0.44555312\n",
      "Iteration 98, loss = 0.44453239\n",
      "Iteration 99, loss = 0.44353600\n",
      "Iteration 100, loss = 0.44252548\n",
      "Iteration 101, loss = 0.44157352\n",
      "Iteration 102, loss = 0.44061076\n",
      "Iteration 103, loss = 0.43969513\n",
      "Iteration 104, loss = 0.43879225\n",
      "Iteration 105, loss = 0.43794133\n",
      "Iteration 106, loss = 0.43697140\n",
      "Iteration 107, loss = 0.43617615\n",
      "Iteration 108, loss = 0.43531426\n",
      "Iteration 109, loss = 0.43446764\n",
      "Iteration 110, loss = 0.43365354\n",
      "Iteration 111, loss = 0.43283331\n",
      "Iteration 112, loss = 0.43205534\n",
      "Iteration 113, loss = 0.43125305\n",
      "Iteration 114, loss = 0.43049932\n",
      "Iteration 115, loss = 0.42973117\n",
      "Iteration 116, loss = 0.42901330\n",
      "Iteration 117, loss = 0.42828364\n",
      "Iteration 118, loss = 0.42757770\n",
      "Iteration 119, loss = 0.42683979\n",
      "Iteration 120, loss = 0.42615999\n",
      "Iteration 121, loss = 0.42546213\n",
      "Iteration 122, loss = 0.42476774\n",
      "Iteration 123, loss = 0.42413207\n",
      "Iteration 124, loss = 0.42351695\n",
      "Iteration 125, loss = 0.42284496\n",
      "Iteration 126, loss = 0.42220910\n",
      "Iteration 127, loss = 0.42155642\n",
      "Iteration 128, loss = 0.42096748\n",
      "Iteration 129, loss = 0.42033964\n",
      "Iteration 130, loss = 0.41974527\n",
      "Iteration 131, loss = 0.41915048\n",
      "Iteration 132, loss = 0.41859396\n",
      "Iteration 133, loss = 0.41800010\n",
      "Iteration 134, loss = 0.41744154\n",
      "Iteration 135, loss = 0.41693036\n",
      "Iteration 136, loss = 0.41632728\n",
      "Iteration 137, loss = 0.41578671\n",
      "Iteration 138, loss = 0.41520536\n",
      "Iteration 139, loss = 0.41472251\n",
      "Iteration 140, loss = 0.41417786\n",
      "Iteration 141, loss = 0.41366141\n",
      "Iteration 142, loss = 0.41315574\n",
      "Iteration 143, loss = 0.41264015\n",
      "Iteration 144, loss = 0.41215147\n",
      "Iteration 145, loss = 0.41163883\n",
      "Iteration 146, loss = 0.41117864\n",
      "Iteration 147, loss = 0.41067920\n",
      "Iteration 148, loss = 0.41020780\n",
      "Iteration 149, loss = 0.40971213\n",
      "Iteration 150, loss = 0.40927538\n",
      "Iteration 151, loss = 0.40879035\n",
      "Iteration 152, loss = 0.40833015\n",
      "Iteration 153, loss = 0.40791720\n",
      "Iteration 154, loss = 0.40743660\n",
      "Iteration 155, loss = 0.40702089\n",
      "Iteration 156, loss = 0.40657437\n",
      "Iteration 157, loss = 0.40615297\n",
      "Iteration 158, loss = 0.40570588\n",
      "Iteration 159, loss = 0.40527443\n",
      "Iteration 160, loss = 0.40484936\n",
      "Iteration 161, loss = 0.40444896\n",
      "Iteration 162, loss = 0.40399999\n",
      "Iteration 163, loss = 0.40360682\n",
      "Iteration 164, loss = 0.40319872\n",
      "Iteration 165, loss = 0.40277576\n",
      "Iteration 166, loss = 0.40240416\n",
      "Iteration 167, loss = 0.40202012\n",
      "Iteration 168, loss = 0.40161659\n",
      "Iteration 169, loss = 0.40121012\n",
      "Iteration 170, loss = 0.40080476\n",
      "Iteration 171, loss = 0.40044590\n",
      "Iteration 172, loss = 0.40005344\n",
      "Iteration 173, loss = 0.39967702\n",
      "Iteration 174, loss = 0.39931792\n",
      "Iteration 175, loss = 0.39892755\n",
      "Iteration 176, loss = 0.39857627\n",
      "Iteration 177, loss = 0.39819650\n",
      "Iteration 178, loss = 0.39783544\n",
      "Iteration 179, loss = 0.39749117\n",
      "Iteration 180, loss = 0.39712391\n",
      "Iteration 181, loss = 0.39678530\n",
      "Iteration 182, loss = 0.39641023\n",
      "Iteration 183, loss = 0.39606856\n",
      "Iteration 184, loss = 0.39574000\n",
      "Iteration 185, loss = 0.39537754\n",
      "Iteration 186, loss = 0.39506115\n",
      "Iteration 187, loss = 0.39469905\n",
      "Iteration 188, loss = 0.39438373\n",
      "Iteration 189, loss = 0.39404147\n",
      "Iteration 190, loss = 0.39373613\n",
      "Iteration 191, loss = 0.39336616\n",
      "Iteration 192, loss = 0.39309636\n",
      "Iteration 193, loss = 0.39273138\n",
      "Iteration 194, loss = 0.39243652\n",
      "Iteration 195, loss = 0.39212131\n",
      "Iteration 196, loss = 0.39179736\n",
      "Iteration 197, loss = 0.39146961\n",
      "Iteration 198, loss = 0.39118009\n",
      "Iteration 199, loss = 0.39085672\n",
      "Iteration 200, loss = 0.39056783\n",
      "Iteration 201, loss = 0.39027457\n",
      "Iteration 202, loss = 0.38997622\n",
      "Iteration 203, loss = 0.38967643\n",
      "Iteration 204, loss = 0.38937474\n",
      "Iteration 205, loss = 0.38906631\n",
      "Iteration 206, loss = 0.38878784\n",
      "Iteration 207, loss = 0.38851277\n",
      "Iteration 208, loss = 0.38823499\n",
      "Iteration 209, loss = 0.38792082\n",
      "Iteration 210, loss = 0.38764806\n",
      "Iteration 211, loss = 0.38740195\n",
      "Iteration 212, loss = 0.38709654\n",
      "Iteration 213, loss = 0.38683570\n",
      "Iteration 214, loss = 0.38655609\n",
      "Iteration 215, loss = 0.38627100\n",
      "Iteration 216, loss = 0.38602095\n",
      "Iteration 217, loss = 0.38574177\n",
      "Iteration 218, loss = 0.38547896\n",
      "Iteration 219, loss = 0.38520676\n",
      "Iteration 220, loss = 0.38497284\n",
      "Iteration 221, loss = 0.38469000\n",
      "Iteration 222, loss = 0.38444841\n",
      "Iteration 223, loss = 0.38417511\n",
      "Iteration 224, loss = 0.38392489\n",
      "Iteration 225, loss = 0.38370131\n",
      "Iteration 226, loss = 0.38339333\n",
      "Iteration 227, loss = 0.38315147\n",
      "Iteration 228, loss = 0.38295106\n",
      "Iteration 229, loss = 0.38265603\n",
      "Iteration 230, loss = 0.38241051\n",
      "Iteration 231, loss = 0.38216621\n",
      "Iteration 232, loss = 0.38193914\n",
      "Iteration 233, loss = 0.38167940\n",
      "Iteration 234, loss = 0.38142530\n",
      "Iteration 235, loss = 0.38120289\n",
      "Iteration 236, loss = 0.38098588\n",
      "Iteration 237, loss = 0.38073524\n",
      "Iteration 238, loss = 0.38048333\n",
      "Iteration 239, loss = 0.38023740\n",
      "Iteration 240, loss = 0.38001383\n",
      "Iteration 241, loss = 0.37977240\n",
      "Iteration 242, loss = 0.37956816\n",
      "Iteration 243, loss = 0.37933378\n",
      "Iteration 244, loss = 0.37909788\n",
      "Iteration 245, loss = 0.37889337\n",
      "Iteration 246, loss = 0.37867665\n",
      "Iteration 247, loss = 0.37841714\n",
      "Iteration 248, loss = 0.37822509\n",
      "Iteration 249, loss = 0.37799531\n",
      "Iteration 250, loss = 0.37777084\n",
      "Iteration 251, loss = 0.37755564\n",
      "Iteration 252, loss = 0.37732721\n",
      "Iteration 253, loss = 0.37713332\n",
      "Iteration 254, loss = 0.37689007\n",
      "Iteration 255, loss = 0.37670273\n",
      "Iteration 256, loss = 0.37647954\n",
      "Iteration 257, loss = 0.37628463\n",
      "Iteration 258, loss = 0.37604760\n",
      "Iteration 259, loss = 0.37584475\n",
      "Iteration 260, loss = 0.37562707\n",
      "Iteration 261, loss = 0.37543936\n",
      "Iteration 262, loss = 0.37524631\n",
      "Iteration 263, loss = 0.37501907\n",
      "Iteration 264, loss = 0.37480298\n",
      "Iteration 265, loss = 0.37462441\n",
      "Iteration 266, loss = 0.37441259\n",
      "Iteration 267, loss = 0.37421191\n",
      "Iteration 268, loss = 0.37402855\n",
      "Iteration 269, loss = 0.37382523\n",
      "Iteration 270, loss = 0.37358977\n",
      "Iteration 271, loss = 0.37340654\n",
      "Iteration 272, loss = 0.37319862\n",
      "Iteration 273, loss = 0.37300314\n",
      "Iteration 274, loss = 0.37284393\n",
      "Iteration 275, loss = 0.37262972\n",
      "Iteration 276, loss = 0.37243060\n",
      "Iteration 277, loss = 0.37223388\n",
      "Iteration 278, loss = 0.37201538\n",
      "Iteration 279, loss = 0.37184399\n",
      "Iteration 280, loss = 0.37166473\n",
      "Iteration 281, loss = 0.37146065\n",
      "Iteration 282, loss = 0.37126749\n",
      "Iteration 283, loss = 0.37107269\n",
      "Iteration 284, loss = 0.37089535\n",
      "Iteration 285, loss = 0.37069977\n",
      "Iteration 286, loss = 0.37054200\n",
      "Iteration 287, loss = 0.37032196\n",
      "Iteration 288, loss = 0.37013840\n",
      "Iteration 289, loss = 0.36996103\n",
      "Iteration 290, loss = 0.36978964\n",
      "Iteration 291, loss = 0.36959306\n",
      "Iteration 292, loss = 0.36941462\n",
      "Iteration 293, loss = 0.36922527\n",
      "Iteration 294, loss = 0.36904261\n",
      "Iteration 295, loss = 0.36887457\n",
      "Iteration 296, loss = 0.36869460\n",
      "Iteration 297, loss = 0.36851592\n",
      "Iteration 298, loss = 0.36835899\n",
      "Iteration 299, loss = 0.36815971\n",
      "Iteration 300, loss = 0.36800182\n",
      "Iteration 301, loss = 0.36781819\n",
      "Iteration 302, loss = 0.36764285\n",
      "Iteration 303, loss = 0.36747097\n",
      "Iteration 304, loss = 0.36730870\n",
      "Iteration 305, loss = 0.36712714\n",
      "Iteration 306, loss = 0.36695337\n",
      "Iteration 307, loss = 0.36680272\n",
      "Iteration 308, loss = 0.36662041\n",
      "Iteration 309, loss = 0.36645348\n",
      "Iteration 310, loss = 0.36628426\n",
      "Iteration 311, loss = 0.36611321\n",
      "Iteration 312, loss = 0.36593919\n",
      "Iteration 313, loss = 0.36578723\n",
      "Iteration 314, loss = 0.36565170\n",
      "Iteration 315, loss = 0.36546048\n",
      "Iteration 316, loss = 0.36530673\n",
      "Iteration 317, loss = 0.36515807\n",
      "Iteration 318, loss = 0.36496864\n",
      "Iteration 319, loss = 0.36480578\n",
      "Iteration 320, loss = 0.36467009\n",
      "Iteration 321, loss = 0.36449132\n",
      "Iteration 322, loss = 0.36436763\n",
      "Iteration 323, loss = 0.36417608\n",
      "Iteration 324, loss = 0.36402356\n",
      "Iteration 325, loss = 0.36386535\n",
      "Iteration 326, loss = 0.36371399\n",
      "Iteration 327, loss = 0.36355562\n",
      "Iteration 328, loss = 0.36339812\n",
      "Iteration 171, loss = 0.39269299\n",
      "Iteration 172, loss = 0.39224747\n",
      "Iteration 173, loss = 0.39189862\n",
      "Iteration 174, loss = 0.39148191\n",
      "Iteration 175, loss = 0.39106045\n",
      "Iteration 176, loss = 0.39066611\n",
      "Iteration 177, loss = 0.39029979\n",
      "Iteration 178, loss = 0.38989764\n",
      "Iteration 179, loss = 0.38954172\n",
      "Iteration 180, loss = 0.38918409\n",
      "Iteration 181, loss = 0.38881102\n",
      "Iteration 182, loss = 0.38842508\n",
      "Iteration 183, loss = 0.38809908\n",
      "Iteration 184, loss = 0.38770442\n",
      "Iteration 185, loss = 0.38737585\n",
      "Iteration 186, loss = 0.38699948\n",
      "Iteration 187, loss = 0.38669477\n",
      "Iteration 188, loss = 0.38630161\n",
      "Iteration 189, loss = 0.38602170\n",
      "Iteration 190, loss = 0.38568911\n",
      "Iteration 191, loss = 0.38533900\n",
      "Iteration 192, loss = 0.38503739\n",
      "Iteration 193, loss = 0.38472000\n",
      "Iteration 194, loss = 0.38435759\n",
      "Iteration 195, loss = 0.38403913\n",
      "Iteration 196, loss = 0.38373104\n",
      "Iteration 197, loss = 0.38344034\n",
      "Iteration 198, loss = 0.38310501\n",
      "Iteration 199, loss = 0.38280745\n",
      "Iteration 200, loss = 0.38250644\n",
      "Iteration 201, loss = 0.38218289\n",
      "Iteration 202, loss = 0.38189390\n",
      "Iteration 203, loss = 0.38158675\n",
      "Iteration 204, loss = 0.38129181\n",
      "Iteration 205, loss = 0.38102460\n",
      "Iteration 206, loss = 0.38074904\n",
      "Iteration 207, loss = 0.38043754\n",
      "Iteration 208, loss = 0.38020560\n",
      "Iteration 209, loss = 0.37987741\n",
      "Iteration 210, loss = 0.37961781\n",
      "Iteration 211, loss = 0.37932860\n",
      "Iteration 212, loss = 0.37904576\n",
      "Iteration 213, loss = 0.37884170\n",
      "Iteration 214, loss = 0.37851830\n",
      "Iteration 215, loss = 0.37824304\n",
      "Iteration 216, loss = 0.37798465\n",
      "Iteration 217, loss = 0.37776160\n",
      "Iteration 218, loss = 0.37748550\n",
      "Iteration 219, loss = 0.37720835\n",
      "Iteration 220, loss = 0.37693251\n",
      "Iteration 221, loss = 0.37670078\n",
      "Iteration 222, loss = 0.37641335\n",
      "Iteration 223, loss = 0.37620083\n",
      "Iteration 224, loss = 0.37595055\n",
      "Iteration 225, loss = 0.37569901\n",
      "Iteration 226, loss = 0.37546139\n",
      "Iteration 227, loss = 0.37518789\n",
      "Iteration 228, loss = 0.37494456\n",
      "Iteration 229, loss = 0.37471900\n",
      "Iteration 230, loss = 0.37449103\n",
      "Iteration 231, loss = 0.37424256\n",
      "Iteration 232, loss = 0.37399375\n",
      "Iteration 233, loss = 0.37379081\n",
      "Iteration 234, loss = 0.37355396\n",
      "Iteration 235, loss = 0.37333997\n",
      "Iteration 236, loss = 0.37311567\n",
      "Iteration 237, loss = 0.37289487\n",
      "Iteration 238, loss = 0.37265410\n",
      "Iteration 239, loss = 0.37244690\n",
      "Iteration 240, loss = 0.37222727\n",
      "Iteration 241, loss = 0.37199499\n",
      "Iteration 242, loss = 0.37180646\n",
      "Iteration 243, loss = 0.37161509\n",
      "Iteration 244, loss = 0.37136239\n",
      "Iteration 245, loss = 0.37115872\n",
      "Iteration 246, loss = 0.37097418\n",
      "Iteration 247, loss = 0.37076663\n",
      "Iteration 248, loss = 0.37057269\n",
      "Iteration 249, loss = 0.37034227\n",
      "Iteration 250, loss = 0.37013463\n",
      "Iteration 251, loss = 0.36993758\n",
      "Iteration 252, loss = 0.36973330\n",
      "Iteration 253, loss = 0.36959902\n",
      "Iteration 254, loss = 0.36935282\n",
      "Iteration 255, loss = 0.36913405\n",
      "Iteration 256, loss = 0.36898894\n",
      "Iteration 257, loss = 0.36874934\n",
      "Iteration 258, loss = 0.36857343\n",
      "Iteration 259, loss = 0.36838404\n",
      "Iteration 260, loss = 0.36820930\n",
      "Iteration 261, loss = 0.36801478\n",
      "Iteration 262, loss = 0.36782577\n",
      "Iteration 263, loss = 0.36769946\n",
      "Iteration 264, loss = 0.36744273\n",
      "Iteration 265, loss = 0.36728436\n",
      "Iteration 266, loss = 0.36717066\n",
      "Iteration 267, loss = 0.36690435\n",
      "Iteration 268, loss = 0.36674359\n",
      "Iteration 269, loss = 0.36656684\n",
      "Iteration 270, loss = 0.36638271\n",
      "Iteration 271, loss = 0.36624975\n",
      "Iteration 272, loss = 0.36602137\n",
      "Iteration 273, loss = 0.36584829\n",
      "Iteration 274, loss = 0.36566743\n",
      "Iteration 275, loss = 0.36550946\n",
      "Iteration 276, loss = 0.36533234\n",
      "Iteration 277, loss = 0.36521527\n",
      "Iteration 278, loss = 0.36500703\n",
      "Iteration 279, loss = 0.36481435\n",
      "Iteration 280, loss = 0.36464925\n",
      "Iteration 281, loss = 0.36446721\n",
      "Iteration 282, loss = 0.36429283\n",
      "Iteration 283, loss = 0.36413453\n",
      "Iteration 284, loss = 0.36398921\n",
      "Iteration 285, loss = 0.36379972\n",
      "Iteration 286, loss = 0.36363747\n",
      "Iteration 287, loss = 0.36347142\n",
      "Iteration 288, loss = 0.36329776\n",
      "Iteration 289, loss = 0.36314768\n",
      "Iteration 290, loss = 0.36306960\n",
      "Iteration 291, loss = 0.36284527\n",
      "Iteration 292, loss = 0.36267541\n",
      "Iteration 293, loss = 0.36252665\n",
      "Iteration 294, loss = 0.36236786\n",
      "Iteration 295, loss = 0.36217675\n",
      "Iteration 296, loss = 0.36206284\n",
      "Iteration 297, loss = 0.36187724\n",
      "Iteration 298, loss = 0.36170260\n",
      "Iteration 299, loss = 0.36161271\n",
      "Iteration 300, loss = 0.36140309\n",
      "Iteration 301, loss = 0.36125051\n",
      "Iteration 302, loss = 0.36115697\n",
      "Iteration 303, loss = 0.36092755\n",
      "Iteration 304, loss = 0.36079451\n",
      "Iteration 305, loss = 0.36062232\n",
      "Iteration 306, loss = 0.36050266\n",
      "Iteration 307, loss = 0.36032318\n",
      "Iteration 308, loss = 0.36018684\n",
      "Iteration 309, loss = 0.36002454\n",
      "Iteration 310, loss = 0.35994175\n",
      "Iteration 311, loss = 0.35972010\n",
      "Iteration 312, loss = 0.35959137\n",
      "Iteration 313, loss = 0.35943887\n",
      "Iteration 314, loss = 0.35929408\n",
      "Iteration 315, loss = 0.35916541\n",
      "Iteration 316, loss = 0.35902467\n",
      "Iteration 317, loss = 0.35884394\n",
      "Iteration 318, loss = 0.35872801\n",
      "Iteration 319, loss = 0.35856811\n",
      "Iteration 320, loss = 0.35843076\n",
      "Iteration 321, loss = 0.35830085\n",
      "Iteration 322, loss = 0.35818670\n",
      "Iteration 323, loss = 0.35803444\n",
      "Iteration 324, loss = 0.35787390\n",
      "Iteration 325, loss = 0.35774539\n",
      "Iteration 326, loss = 0.35762446\n",
      "Iteration 327, loss = 0.35753437\n",
      "Iteration 328, loss = 0.35732200\n",
      "Iteration 329, loss = 0.35720043\n",
      "Iteration 330, loss = 0.35704196\n",
      "Iteration 331, loss = 0.35691566\n",
      "Iteration 332, loss = 0.35679994\n",
      "Iteration 333, loss = 0.35666221\n",
      "Iteration 334, loss = 0.35653441\n",
      "Iteration 335, loss = 0.35637817\n",
      "Iteration 336, loss = 0.35623397\n",
      "Iteration 337, loss = 0.35608971\n",
      "Iteration 338, loss = 0.35596685\n",
      "Iteration 339, loss = 0.35582842\n",
      "Iteration 340, loss = 0.35570445\n",
      "Iteration 341, loss = 0.35555995\n",
      "Iteration 342, loss = 0.35545212\n",
      "Iteration 343, loss = 0.35531397\n",
      "Iteration 344, loss = 0.35516110\n",
      "Iteration 345, loss = 0.35512694\n",
      "Iteration 346, loss = 0.35495464\n",
      "Iteration 347, loss = 0.35477921\n",
      "Iteration 348, loss = 0.35473076\n",
      "Iteration 349, loss = 0.35453599\n",
      "Iteration 350, loss = 0.35439065\n",
      "Iteration 351, loss = 0.35428773\n",
      "Iteration 352, loss = 0.35415725\n",
      "Iteration 353, loss = 0.35401556\n",
      "Iteration 354, loss = 0.35386076\n",
      "Iteration 355, loss = 0.35379753\n",
      "Iteration 356, loss = 0.35360856\n",
      "Iteration 357, loss = 0.35349630\n",
      "Iteration 358, loss = 0.35335963\n",
      "Iteration 359, loss = 0.35324911\n",
      "Iteration 360, loss = 0.35311511\n",
      "Iteration 361, loss = 0.35299163\n",
      "Iteration 362, loss = 0.35287914\n",
      "Iteration 363, loss = 0.35272827\n",
      "Iteration 364, loss = 0.35261814\n",
      "Iteration 365, loss = 0.35248402\n",
      "Iteration 366, loss = 0.35236009\n",
      "Iteration 367, loss = 0.35227377\n",
      "Iteration 368, loss = 0.35211707\n",
      "Iteration 369, loss = 0.35199868\n",
      "Iteration 370, loss = 0.35185749\n",
      "Iteration 371, loss = 0.35174817\n",
      "Iteration 372, loss = 0.35161778\n",
      "Iteration 373, loss = 0.35151619\n",
      "Iteration 374, loss = 0.35143202\n",
      "Iteration 375, loss = 0.35125324\n",
      "Iteration 376, loss = 0.35112608\n",
      "Iteration 377, loss = 0.35106330\n",
      "Iteration 378, loss = 0.35088741\n",
      "Iteration 379, loss = 0.35077290\n",
      "Iteration 380, loss = 0.35069052\n",
      "Iteration 381, loss = 0.35054775\n",
      "Iteration 382, loss = 0.35043133\n",
      "Iteration 383, loss = 0.35028253\n",
      "Iteration 384, loss = 0.35016813\n",
      "Iteration 385, loss = 0.35006919\n",
      "Iteration 386, loss = 0.34994950\n",
      "Iteration 387, loss = 0.34979556\n",
      "Iteration 388, loss = 0.34967964\n",
      "Iteration 389, loss = 0.34961038\n",
      "Iteration 390, loss = 0.34945322\n",
      "Iteration 391, loss = 0.34932338\n",
      "Iteration 392, loss = 0.34921176\n",
      "Iteration 393, loss = 0.34907572\n",
      "Iteration 394, loss = 0.34895849\n",
      "Iteration 395, loss = 0.34885176\n",
      "Iteration 396, loss = 0.34871581\n",
      "Iteration 397, loss = 0.34864402\n",
      "Iteration 398, loss = 0.34848320\n",
      "Iteration 399, loss = 0.34836817\n",
      "Iteration 400, loss = 0.34825022\n",
      "Iteration 401, loss = 0.34814014\n",
      "Iteration 402, loss = 0.34801976\n",
      "Iteration 403, loss = 0.34791094\n",
      "Iteration 404, loss = 0.34781829\n",
      "Iteration 405, loss = 0.34768180\n",
      "Iteration 406, loss = 0.34755154\n",
      "Iteration 407, loss = 0.34747683\n",
      "Iteration 408, loss = 0.34735915\n",
      "Iteration 409, loss = 0.34721217\n",
      "Iteration 410, loss = 0.34712736\n",
      "Iteration 411, loss = 0.34700700\n",
      "Iteration 412, loss = 0.34687045\n",
      "Iteration 413, loss = 0.34674961\n",
      "Iteration 414, loss = 0.34665118\n",
      "Iteration 415, loss = 0.34651513\n",
      "Iteration 416, loss = 0.34640845\n",
      "Iteration 417, loss = 0.34630514\n",
      "Iteration 418, loss = 0.34622820\n",
      "Iteration 419, loss = 0.34605509\n",
      "Iteration 420, loss = 0.34596679\n",
      "Iteration 421, loss = 0.34583133\n",
      "Iteration 422, loss = 0.34575061\n",
      "Iteration 423, loss = 0.34560612\n",
      "Iteration 424, loss = 0.34549886\n",
      "Iteration 425, loss = 0.34538268\n",
      "Iteration 426, loss = 0.34528025\n",
      "Iteration 427, loss = 0.34519729\n",
      "Iteration 428, loss = 0.34506194\n",
      "Iteration 429, loss = 0.34495356\n",
      "Iteration 430, loss = 0.34482216\n",
      "Iteration 431, loss = 0.34472191\n",
      "Iteration 432, loss = 0.34460476\n",
      "Iteration 433, loss = 0.34452187\n",
      "Iteration 434, loss = 0.34445118\n",
      "Iteration 435, loss = 0.34429795\n",
      "Iteration 436, loss = 0.34416464\n",
      "Iteration 437, loss = 0.34405757\n",
      "Iteration 438, loss = 0.34402188\n",
      "Iteration 439, loss = 0.34384255\n",
      "Iteration 440, loss = 0.34370475\n",
      "Iteration 441, loss = 0.34362312\n",
      "Iteration 442, loss = 0.34349884\n",
      "Iteration 443, loss = 0.34337559\n",
      "Iteration 444, loss = 0.34326715\n",
      "Iteration 445, loss = 0.34321946\n",
      "Iteration 446, loss = 0.34306562\n",
      "Iteration 447, loss = 0.34294652\n",
      "Iteration 448, loss = 0.34280626\n",
      "Iteration 449, loss = 0.34269612\n",
      "Iteration 450, loss = 0.34264294\n",
      "Iteration 451, loss = 0.34253621\n",
      "Iteration 452, loss = 0.34234881\n",
      "Iteration 453, loss = 0.34225087\n",
      "Iteration 454, loss = 0.34212855\n",
      "Iteration 455, loss = 0.34203362\n",
      "Iteration 456, loss = 0.34190801\n",
      "Iteration 457, loss = 0.34180796\n",
      "Iteration 458, loss = 0.34172084\n",
      "Iteration 459, loss = 0.34157168\n",
      "Iteration 460, loss = 0.34147902\n",
      "Iteration 461, loss = 0.34136536\n",
      "Iteration 462, loss = 0.34123411\n",
      "Iteration 463, loss = 0.34113676\n",
      "Iteration 464, loss = 0.34104804\n",
      "Iteration 465, loss = 0.34090709\n",
      "Iteration 466, loss = 0.34080632\n",
      "Iteration 467, loss = 0.34068183\n",
      "Iteration 468, loss = 0.34059358\n",
      "Iteration 469, loss = 0.34047608\n",
      "Iteration 470, loss = 0.34038562\n",
      "Iteration 471, loss = 0.34028913\n",
      "Iteration 472, loss = 0.34017938\n",
      "Iteration 473, loss = 0.34006112\n",
      "Iteration 474, loss = 0.33994547\n",
      "Iteration 475, loss = 0.33983743\n",
      "Iteration 476, loss = 0.33971777\n",
      "Iteration 477, loss = 0.33960923\n",
      "Iteration 478, loss = 0.33951585\n",
      "Iteration 479, loss = 0.33939265\n",
      "Iteration 480, loss = 0.33934294\n",
      "Iteration 481, loss = 0.33918191\n",
      "Iteration 482, loss = 0.33905835\n",
      "Iteration 483, loss = 0.33897696\n",
      "Iteration 484, loss = 0.33886165\n",
      "Iteration 485, loss = 0.33879277\n",
      "Iteration 486, loss = 0.33865729\n",
      "Iteration 487, loss = 0.33851063\n",
      "Iteration 488, loss = 0.33841623\n",
      "Iteration 489, loss = 0.33832403\n",
      "Iteration 490, loss = 0.33822144\n",
      "Iteration 491, loss = 0.33812570\n",
      "Iteration 492, loss = 0.33800461\n",
      "Iteration 493, loss = 0.33787297\n",
      "Iteration 494, loss = 0.33777674\n",
      "Iteration 495, loss = 0.33765762\n",
      "Iteration 496, loss = 0.33756517\n",
      "Iteration 497, loss = 0.33748975\n",
      "Iteration 498, loss = 0.33738837\n",
      "Iteration 499, loss = 0.33724504\n",
      "Iteration 500, loss = 0.33715428\n",
      "Iteration 501, loss = 0.33703930\n",
      "Iteration 502, loss = 0.33693154\n",
      "Iteration 503, loss = 0.33681936\n",
      "Iteration 504, loss = 0.33671603\n",
      "Iteration 505, loss = 0.33662206\n",
      "Iteration 506, loss = 0.33650063\n",
      "Iteration 507, loss = 0.33641607\n",
      "Iteration 508, loss = 0.33630990\n",
      "Iteration 509, loss = 0.33621557\n",
      "Iteration 510, loss = 0.33610026\n",
      "Iteration 511, loss = 0.33598801\n",
      "Iteration 512, loss = 0.33588998\n",
      "Iteration 513, loss = 0.33582018\n",
      "Iteration 514, loss = 0.33568583\n",
      "Iteration 515, loss = 0.33556807\n",
      "Iteration 516, loss = 0.33546820\n",
      "Iteration 517, loss = 0.33536527\n",
      "Iteration 518, loss = 0.33527167\n",
      "Iteration 519, loss = 0.33519550\n",
      "Iteration 520, loss = 0.33508205\n",
      "Iteration 521, loss = 0.33499298\n",
      "Iteration 522, loss = 0.33485533\n",
      "Iteration 523, loss = 0.33474010\n",
      "Iteration 524, loss = 0.33468024\n",
      "Iteration 525, loss = 0.33453917\n",
      "Iteration 526, loss = 0.33446261\n",
      "Iteration 527, loss = 0.33436360\n",
      "Iteration 528, loss = 0.33424777\n",
      "Iteration 529, loss = 0.33412729\n",
      "Iteration 530, loss = 0.33405477\n",
      "Iteration 531, loss = 0.33393185\n",
      "Iteration 532, loss = 0.33384086\n",
      "Iteration 533, loss = 0.33377760\n",
      "Iteration 534, loss = 0.33362421\n",
      "Iteration 535, loss = 0.33352402\n",
      "Iteration 536, loss = 0.33344130\n",
      "Iteration 537, loss = 0.33334009\n",
      "Iteration 538, loss = 0.33323856\n",
      "Iteration 539, loss = 0.33311129\n",
      "Iteration 540, loss = 0.33304110\n",
      "Iteration 541, loss = 0.33295843\n",
      "Iteration 542, loss = 0.33281844\n",
      "Iteration 543, loss = 0.33271161\n",
      "Iteration 544, loss = 0.33264140\n",
      "Iteration 545, loss = 0.33251395\n",
      "Iteration 546, loss = 0.33241807\n",
      "Iteration 547, loss = 0.33232869\n",
      "Iteration 548, loss = 0.33219963\n",
      "Iteration 549, loss = 0.33215574\n",
      "Iteration 550, loss = 0.33202028\n",
      "Iteration 551, loss = 0.33192487\n",
      "Iteration 552, loss = 0.33181970\n",
      "Iteration 553, loss = 0.33172962\n",
      "Iteration 554, loss = 0.33161001\n",
      "Iteration 555, loss = 0.33152942\n",
      "Iteration 556, loss = 0.33141722\n",
      "Iteration 557, loss = 0.33131727\n",
      "Iteration 558, loss = 0.33123176\n",
      "Iteration 559, loss = 0.33111542\n",
      "Iteration 560, loss = 0.33102300\n",
      "Iteration 561, loss = 0.33094265\n",
      "Iteration 562, loss = 0.33083714\n",
      "Iteration 563, loss = 0.33075360\n",
      "Iteration 564, loss = 0.33065451\n",
      "Iteration 565, loss = 0.33053037\n",
      "Iteration 566, loss = 0.33044202\n",
      "Iteration 567, loss = 0.33040037\n",
      "Iteration 568, loss = 0.33026194\n",
      "Iteration 569, loss = 0.33014594\n",
      "Iteration 570, loss = 0.33005385\n",
      "Iteration 571, loss = 0.32995076\n",
      "Iteration 572, loss = 0.32985040\n",
      "Iteration 573, loss = 0.32980043\n",
      "Iteration 574, loss = 0.32967361\n",
      "Iteration 575, loss = 0.32956100\n",
      "Iteration 576, loss = 0.32947283\n",
      "Iteration 577, loss = 0.32943058\n",
      "Iteration 578, loss = 0.32925523\n",
      "Iteration 579, loss = 0.32920676\n",
      "Iteration 580, loss = 0.32906749\n",
      "Iteration 581, loss = 0.32895590\n",
      "Iteration 582, loss = 0.32883411\n",
      "Iteration 583, loss = 0.32877772\n",
      "Iteration 584, loss = 0.32864989\n",
      "Iteration 585, loss = 0.32855773\n",
      "Iteration 586, loss = 0.32844325\n",
      "Iteration 587, loss = 0.32840816\n",
      "Iteration 588, loss = 0.32823868\n",
      "Iteration 589, loss = 0.32814662\n",
      "Iteration 590, loss = 0.32813014\n",
      "Iteration 591, loss = 0.32793795\n",
      "Iteration 592, loss = 0.32785834\n",
      "Iteration 593, loss = 0.32774820\n",
      "Iteration 594, loss = 0.32764934\n",
      "Iteration 595, loss = 0.32752482\n",
      "Iteration 596, loss = 0.32747460\n",
      "Iteration 597, loss = 0.32737669\n",
      "Iteration 598, loss = 0.32726042\n",
      "Iteration 599, loss = 0.32712455\n",
      "Iteration 600, loss = 0.32705500\n",
      "Iteration 601, loss = 0.32693690\n",
      "Iteration 602, loss = 0.32684645\n",
      "Iteration 603, loss = 0.32676370\n",
      "Iteration 604, loss = 0.32661891\n",
      "Iteration 605, loss = 0.32651768\n",
      "Iteration 606, loss = 0.32642043\n",
      "Iteration 607, loss = 0.32634825\n",
      "Iteration 608, loss = 0.32621857\n",
      "Iteration 609, loss = 0.32610089\n",
      "Iteration 610, loss = 0.32602294\n",
      "Iteration 611, loss = 0.32595042\n",
      "Iteration 612, loss = 0.32579713\n",
      "Iteration 613, loss = 0.32568401\n",
      "Iteration 614, loss = 0.32562485\n",
      "Iteration 615, loss = 0.32548540\n",
      "Iteration 616, loss = 0.32538055\n",
      "Iteration 617, loss = 0.32533837\n",
      "Iteration 618, loss = 0.32519481\n",
      "Iteration 619, loss = 0.32507399\n",
      "Iteration 620, loss = 0.32497558\n",
      "Iteration 621, loss = 0.32486381\n",
      "Iteration 622, loss = 0.32476986\n",
      "Iteration 623, loss = 0.32465981\n",
      "Iteration 624, loss = 0.32455732\n",
      "Iteration 625, loss = 0.32447901\n",
      "Iteration 626, loss = 0.32437052\n",
      "Iteration 627, loss = 0.32425239\n",
      "Iteration 628, loss = 0.32417830\n",
      "Iteration 629, loss = 0.32405063\n",
      "Iteration 630, loss = 0.32395668\n",
      "Iteration 631, loss = 0.32387224\n",
      "Iteration 632, loss = 0.32375187\n",
      "Iteration 633, loss = 0.32368514\n",
      "Iteration 634, loss = 0.32353091\n",
      "Iteration 635, loss = 0.32346267\n",
      "Iteration 636, loss = 0.32333509\n",
      "Iteration 637, loss = 0.32332545\n",
      "Iteration 638, loss = 0.32311982\n",
      "Iteration 639, loss = 0.32305547\n",
      "Iteration 640, loss = 0.32297175\n",
      "Iteration 641, loss = 0.32281348\n",
      "Iteration 642, loss = 0.32272210\n",
      "Iteration 643, loss = 0.32261916\n",
      "Iteration 644, loss = 0.32251858\n",
      "Iteration 645, loss = 0.32240802\n",
      "Iteration 646, loss = 0.32231398\n",
      "Iteration 647, loss = 0.32223136\n",
      "Iteration 648, loss = 0.32211064\n",
      "Iteration 649, loss = 0.32201432\n",
      "Iteration 650, loss = 0.32190671\n",
      "Iteration 651, loss = 0.32180910\n",
      "Iteration 652, loss = 0.32176980\n",
      "Iteration 653, loss = 0.32159920\n",
      "Iteration 654, loss = 0.32148717\n",
      "Iteration 655, loss = 0.32138278\n",
      "Iteration 656, loss = 0.32131181\n",
      "Iteration 657, loss = 0.32120749\n",
      "Iteration 658, loss = 0.32110166\n",
      "Iteration 659, loss = 0.32099160\n",
      "Iteration 660, loss = 0.32090386\n",
      "Iteration 661, loss = 0.32079295\n",
      "Iteration 662, loss = 0.32068533\n",
      "Iteration 663, loss = 0.32060143\n",
      "Iteration 664, loss = 0.32047484\n",
      "Iteration 665, loss = 0.32039001\n",
      "Iteration 666, loss = 0.32027944\n",
      "Iteration 329, loss = 0.36325921\n",
      "Iteration 330, loss = 0.36310235\n",
      "Iteration 331, loss = 0.36295957\n",
      "Iteration 332, loss = 0.36279677\n",
      "Iteration 333, loss = 0.36264762\n",
      "Iteration 334, loss = 0.36250731\n",
      "Iteration 335, loss = 0.36234357\n",
      "Iteration 336, loss = 0.36219180\n",
      "Iteration 337, loss = 0.36205249\n",
      "Iteration 338, loss = 0.36188990\n",
      "Iteration 339, loss = 0.36175099\n",
      "Iteration 340, loss = 0.36160843\n",
      "Iteration 341, loss = 0.36145627\n",
      "Iteration 342, loss = 0.36130895\n",
      "Iteration 343, loss = 0.36119832\n",
      "Iteration 344, loss = 0.36104322\n",
      "Iteration 345, loss = 0.36090061\n",
      "Iteration 346, loss = 0.36072766\n",
      "Iteration 347, loss = 0.36061367\n",
      "Iteration 348, loss = 0.36044180\n",
      "Iteration 349, loss = 0.36032107\n",
      "Iteration 350, loss = 0.36017281\n",
      "Iteration 351, loss = 0.36003117\n",
      "Iteration 352, loss = 0.35988797\n",
      "Iteration 353, loss = 0.35973614\n",
      "Iteration 354, loss = 0.35960018\n",
      "Iteration 355, loss = 0.35947110\n",
      "Iteration 356, loss = 0.35930876\n",
      "Iteration 357, loss = 0.35916535\n",
      "Iteration 358, loss = 0.35904756\n",
      "Iteration 359, loss = 0.35890788\n",
      "Iteration 360, loss = 0.35875061\n",
      "Iteration 361, loss = 0.35862271\n",
      "Iteration 362, loss = 0.35848442\n",
      "Iteration 363, loss = 0.35840341\n",
      "Iteration 364, loss = 0.35822533\n",
      "Iteration 365, loss = 0.35807203\n",
      "Iteration 366, loss = 0.35794941\n",
      "Iteration 367, loss = 0.35780124\n",
      "Iteration 368, loss = 0.35767705\n",
      "Iteration 369, loss = 0.35753955\n",
      "Iteration 370, loss = 0.35741077\n",
      "Iteration 371, loss = 0.35729364\n",
      "Iteration 372, loss = 0.35713555\n",
      "Iteration 373, loss = 0.35701042\n",
      "Iteration 374, loss = 0.35687382\n",
      "Iteration 375, loss = 0.35676845\n",
      "Iteration 376, loss = 0.35661706\n",
      "Iteration 377, loss = 0.35649891\n",
      "Iteration 378, loss = 0.35635331\n",
      "Iteration 379, loss = 0.35622746\n",
      "Iteration 380, loss = 0.35612580\n",
      "Iteration 381, loss = 0.35595641\n",
      "Iteration 382, loss = 0.35583723\n",
      "Iteration 383, loss = 0.35571553\n",
      "Iteration 384, loss = 0.35556496\n",
      "Iteration 385, loss = 0.35545211\n",
      "Iteration 386, loss = 0.35535453\n",
      "Iteration 387, loss = 0.35521500\n",
      "Iteration 388, loss = 0.35506026\n",
      "Iteration 389, loss = 0.35494822\n",
      "Iteration 390, loss = 0.35481684\n",
      "Iteration 391, loss = 0.35470291\n",
      "Iteration 392, loss = 0.35457728\n",
      "Iteration 393, loss = 0.35442918\n",
      "Iteration 394, loss = 0.35429497\n",
      "Iteration 395, loss = 0.35419124\n",
      "Iteration 396, loss = 0.35406168\n",
      "Iteration 397, loss = 0.35393959\n",
      "Iteration 398, loss = 0.35384643\n",
      "Iteration 399, loss = 0.35370273\n",
      "Iteration 400, loss = 0.35356668\n",
      "Iteration 401, loss = 0.35343688\n",
      "Iteration 402, loss = 0.35331866\n",
      "Iteration 403, loss = 0.35320927\n",
      "Iteration 404, loss = 0.35307312\n",
      "Iteration 405, loss = 0.35297671\n",
      "Iteration 406, loss = 0.35283816\n",
      "Iteration 407, loss = 0.35271328\n",
      "Iteration 408, loss = 0.35258595\n",
      "Iteration 409, loss = 0.35246724\n",
      "Iteration 410, loss = 0.35235695\n",
      "Iteration 411, loss = 0.35222946\n",
      "Iteration 412, loss = 0.35211838\n",
      "Iteration 413, loss = 0.35199922\n",
      "Iteration 414, loss = 0.35193121\n",
      "Iteration 415, loss = 0.35175397\n",
      "Iteration 416, loss = 0.35167316\n",
      "Iteration 417, loss = 0.35154153\n",
      "Iteration 418, loss = 0.35141408\n",
      "Iteration 419, loss = 0.35129652\n",
      "Iteration 420, loss = 0.35117279\n",
      "Iteration 421, loss = 0.35105700\n",
      "Iteration 422, loss = 0.35094944\n",
      "Iteration 423, loss = 0.35082818\n",
      "Iteration 424, loss = 0.35071776\n",
      "Iteration 425, loss = 0.35060139\n",
      "Iteration 426, loss = 0.35050908\n",
      "Iteration 427, loss = 0.35036242\n",
      "Iteration 428, loss = 0.35025967\n",
      "Iteration 429, loss = 0.35017045\n",
      "Iteration 430, loss = 0.35003438\n",
      "Iteration 431, loss = 0.34991766\n",
      "Iteration 432, loss = 0.34981233\n",
      "Iteration 433, loss = 0.34968883\n",
      "Iteration 434, loss = 0.34959677\n",
      "Iteration 435, loss = 0.34947405\n",
      "Iteration 436, loss = 0.34936976\n",
      "Iteration 437, loss = 0.34926622\n",
      "Iteration 438, loss = 0.34913452\n",
      "Iteration 439, loss = 0.34904871\n",
      "Iteration 440, loss = 0.34892283\n",
      "Iteration 441, loss = 0.34881475\n",
      "Iteration 442, loss = 0.34870263\n",
      "Iteration 443, loss = 0.34859820\n",
      "Iteration 444, loss = 0.34847477\n",
      "Iteration 445, loss = 0.34838076\n",
      "Iteration 446, loss = 0.34826954\n",
      "Iteration 447, loss = 0.34815973\n",
      "Iteration 448, loss = 0.34805088\n",
      "Iteration 449, loss = 0.34795757\n",
      "Iteration 450, loss = 0.34784391\n",
      "Iteration 451, loss = 0.34775003\n",
      "Iteration 452, loss = 0.34765086\n",
      "Iteration 453, loss = 0.34751213\n",
      "Iteration 454, loss = 0.34741243\n",
      "Iteration 455, loss = 0.34731025\n",
      "Iteration 456, loss = 0.34722318\n",
      "Iteration 457, loss = 0.34710322\n",
      "Iteration 458, loss = 0.34698730\n",
      "Iteration 459, loss = 0.34688578\n",
      "Iteration 460, loss = 0.34677902\n",
      "Iteration 461, loss = 0.34667302\n",
      "Iteration 462, loss = 0.34659282\n",
      "Iteration 463, loss = 0.34646755\n",
      "Iteration 464, loss = 0.34636505\n",
      "Iteration 465, loss = 0.34626503\n",
      "Iteration 466, loss = 0.34616804\n",
      "Iteration 467, loss = 0.34605788\n",
      "Iteration 468, loss = 0.34595894\n",
      "Iteration 469, loss = 0.34587003\n",
      "Iteration 470, loss = 0.34576344\n",
      "Iteration 471, loss = 0.34564991\n",
      "Iteration 472, loss = 0.34556834\n",
      "Iteration 473, loss = 0.34546186\n",
      "Iteration 474, loss = 0.34539214\n",
      "Iteration 475, loss = 0.34526755\n",
      "Iteration 476, loss = 0.34514553\n",
      "Iteration 477, loss = 0.34506990\n",
      "Iteration 478, loss = 0.34495634\n",
      "Iteration 479, loss = 0.34486391\n",
      "Iteration 480, loss = 0.34476067\n",
      "Iteration 481, loss = 0.34466592\n",
      "Iteration 482, loss = 0.34457302\n",
      "Iteration 483, loss = 0.34446209\n",
      "Iteration 484, loss = 0.34437694\n",
      "Iteration 485, loss = 0.34426523\n",
      "Iteration 486, loss = 0.34416193\n",
      "Iteration 487, loss = 0.34409218\n",
      "Iteration 488, loss = 0.34398906\n",
      "Iteration 489, loss = 0.34388481\n",
      "Iteration 490, loss = 0.34377971\n",
      "Iteration 491, loss = 0.34368963\n",
      "Iteration 492, loss = 0.34359837\n",
      "Iteration 493, loss = 0.34349280\n",
      "Iteration 494, loss = 0.34343111\n",
      "Iteration 495, loss = 0.34329555\n",
      "Iteration 496, loss = 0.34321032\n",
      "Iteration 497, loss = 0.34312181\n",
      "Iteration 498, loss = 0.34302248\n",
      "Iteration 499, loss = 0.34291004\n",
      "Iteration 500, loss = 0.34282888\n",
      "Iteration 501, loss = 0.34274609\n",
      "Iteration 502, loss = 0.34264037\n",
      "Iteration 503, loss = 0.34255240\n",
      "Iteration 504, loss = 0.34245667\n",
      "Iteration 505, loss = 0.34238213\n",
      "Iteration 506, loss = 0.34226512\n",
      "Iteration 507, loss = 0.34216322\n",
      "Iteration 508, loss = 0.34207257\n",
      "Iteration 509, loss = 0.34198131\n",
      "Iteration 510, loss = 0.34189018\n",
      "Iteration 511, loss = 0.34179454\n",
      "Iteration 512, loss = 0.34169861\n",
      "Iteration 513, loss = 0.34161174\n",
      "Iteration 514, loss = 0.34152860\n",
      "Iteration 515, loss = 0.34142961\n",
      "Iteration 516, loss = 0.34134887\n",
      "Iteration 517, loss = 0.34125055\n",
      "Iteration 518, loss = 0.34115349\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.81642539\n",
      "Iteration 2, loss = 0.80782475\n",
      "Iteration 3, loss = 0.79522267\n",
      "Iteration 4, loss = 0.77988076\n",
      "Iteration 5, loss = 0.76310601\n",
      "Iteration 6, loss = 0.74611038\n",
      "Iteration 7, loss = 0.72871322\n",
      "Iteration 8, loss = 0.71224902\n",
      "Iteration 9, loss = 0.69715127\n",
      "Iteration 10, loss = 0.68152846\n",
      "Iteration 11, loss = 0.66767666\n",
      "Iteration 12, loss = 0.65459710\n",
      "Iteration 13, loss = 0.64298582\n",
      "Iteration 14, loss = 0.63155357\n",
      "Iteration 15, loss = 0.62055830\n",
      "Iteration 16, loss = 0.61100844\n",
      "Iteration 17, loss = 0.60223810\n",
      "Iteration 18, loss = 0.59399606\n",
      "Iteration 19, loss = 0.58598854\n",
      "Iteration 20, loss = 0.57823879\n",
      "Iteration 21, loss = 0.57154433\n",
      "Iteration 22, loss = 0.56493979\n",
      "Iteration 23, loss = 0.55912159\n",
      "Iteration 24, loss = 0.55306184\n",
      "Iteration 25, loss = 0.54745645\n",
      "Iteration 26, loss = 0.54225947\n",
      "Iteration 27, loss = 0.53717073\n",
      "Iteration 28, loss = 0.53240156\n",
      "Iteration 29, loss = 0.52782079\n",
      "Iteration 30, loss = 0.52348007\n",
      "Iteration 31, loss = 0.51913630\n",
      "Iteration 32, loss = 0.51493367\n",
      "Iteration 33, loss = 0.51104528\n",
      "Iteration 34, loss = 0.50717915\n",
      "Iteration 35, loss = 0.50369013\n",
      "Iteration 36, loss = 0.50001988\n",
      "Iteration 37, loss = 0.49663544\n",
      "Iteration 38, loss = 0.49326334\n",
      "Iteration 39, loss = 0.49024089\n",
      "Iteration 40, loss = 0.48700162\n",
      "Iteration 41, loss = 0.48401504\n",
      "Iteration 42, loss = 0.48106451\n",
      "Iteration 43, loss = 0.47821565\n",
      "Iteration 44, loss = 0.47548746\n",
      "Iteration 45, loss = 0.47278061\n",
      "Iteration 46, loss = 0.47014900\n",
      "Iteration 47, loss = 0.46758845\n",
      "Iteration 48, loss = 0.46516575\n",
      "Iteration 49, loss = 0.46275229\n",
      "Iteration 50, loss = 0.46037143\n",
      "Iteration 51, loss = 0.45808197\n",
      "Iteration 52, loss = 0.45584006\n",
      "Iteration 53, loss = 0.45364055\n",
      "Iteration 54, loss = 0.45160913\n",
      "Iteration 55, loss = 0.44948744\n",
      "Iteration 56, loss = 0.44754733\n",
      "Iteration 57, loss = 0.44553232\n",
      "Iteration 58, loss = 0.44357894\n",
      "Iteration 59, loss = 0.44177207\n",
      "Iteration 60, loss = 0.43986148\n",
      "Iteration 61, loss = 0.43814270\n",
      "Iteration 62, loss = 0.43636074\n",
      "Iteration 63, loss = 0.43462936\n",
      "Iteration 64, loss = 0.43300774\n",
      "Iteration 65, loss = 0.43136452\n",
      "Iteration 66, loss = 0.42982333\n",
      "Iteration 67, loss = 0.42824281\n",
      "Iteration 68, loss = 0.42673047\n",
      "Iteration 69, loss = 0.42522251\n",
      "Iteration 70, loss = 0.42378096\n",
      "Iteration 71, loss = 0.42232495\n",
      "Iteration 72, loss = 0.42093739\n",
      "Iteration 73, loss = 0.41957682\n",
      "Iteration 74, loss = 0.41829805\n",
      "Iteration 75, loss = 0.41694498\n",
      "Iteration 76, loss = 0.41565909\n",
      "Iteration 77, loss = 0.41443157\n",
      "Iteration 78, loss = 0.41317616\n",
      "Iteration 79, loss = 0.41199132\n",
      "Iteration 80, loss = 0.41080890\n",
      "Iteration 81, loss = 0.40973065\n",
      "Iteration 82, loss = 0.40857297\n",
      "Iteration 83, loss = 0.40749747\n",
      "Iteration 84, loss = 0.40637998\n",
      "Iteration 85, loss = 0.40537368\n",
      "Iteration 86, loss = 0.40427183\n",
      "Iteration 87, loss = 0.40329116\n",
      "Iteration 88, loss = 0.40231543\n",
      "Iteration 89, loss = 0.40131286\n",
      "Iteration 90, loss = 0.40040940\n",
      "Iteration 91, loss = 0.39944735\n",
      "Iteration 92, loss = 0.39856360\n",
      "Iteration 93, loss = 0.39762347\n",
      "Iteration 94, loss = 0.39676568\n",
      "Iteration 95, loss = 0.39590621\n",
      "Iteration 96, loss = 0.39507422\n",
      "Iteration 97, loss = 0.39425504\n",
      "Iteration 98, loss = 0.39338034\n",
      "Iteration 99, loss = 0.39263755\n",
      "Iteration 100, loss = 0.39182030\n",
      "Iteration 101, loss = 0.39108632\n",
      "Iteration 102, loss = 0.39035284\n",
      "Iteration 103, loss = 0.38955701\n",
      "Iteration 104, loss = 0.38883115\n",
      "Iteration 105, loss = 0.38810596\n",
      "Iteration 106, loss = 0.38741286\n",
      "Iteration 107, loss = 0.38674158\n",
      "Iteration 108, loss = 0.38607335\n",
      "Iteration 109, loss = 0.38539442\n",
      "Iteration 110, loss = 0.38472472\n",
      "Iteration 111, loss = 0.38411535\n",
      "Iteration 112, loss = 0.38344831\n",
      "Iteration 113, loss = 0.38285645\n",
      "Iteration 114, loss = 0.38222109\n",
      "Iteration 115, loss = 0.38163052\n",
      "Iteration 116, loss = 0.38104434\n",
      "Iteration 117, loss = 0.38041495\n",
      "Iteration 118, loss = 0.37985995\n",
      "Iteration 119, loss = 0.37930877\n",
      "Iteration 120, loss = 0.37872540\n",
      "Iteration 121, loss = 0.37817874\n",
      "Iteration 122, loss = 0.37768685\n",
      "Iteration 123, loss = 0.37714870\n",
      "Iteration 124, loss = 0.37659864\n",
      "Iteration 125, loss = 0.37608454\n",
      "Iteration 126, loss = 0.37559265\n",
      "Iteration 127, loss = 0.37506758\n",
      "Iteration 128, loss = 0.37459160\n",
      "Iteration 129, loss = 0.37411966\n",
      "Iteration 130, loss = 0.37359996\n",
      "Iteration 131, loss = 0.37315406\n",
      "Iteration 132, loss = 0.37266485\n",
      "Iteration 133, loss = 0.37224567\n",
      "Iteration 134, loss = 0.37177176\n",
      "Iteration 135, loss = 0.37130754\n",
      "Iteration 136, loss = 0.37087590\n",
      "Iteration 137, loss = 0.37045252\n",
      "Iteration 138, loss = 0.37004064\n",
      "Iteration 139, loss = 0.36960012\n",
      "Iteration 140, loss = 0.36917928\n",
      "Iteration 141, loss = 0.36877704\n",
      "Iteration 142, loss = 0.36835454\n",
      "Iteration 143, loss = 0.36796866\n",
      "Iteration 144, loss = 0.36757723\n",
      "Iteration 145, loss = 0.36716976\n",
      "Iteration 146, loss = 0.36680665\n",
      "Iteration 147, loss = 0.36640411\n",
      "Iteration 148, loss = 0.36600896\n",
      "Iteration 149, loss = 0.36566709\n",
      "Iteration 150, loss = 0.36529496\n",
      "Iteration 151, loss = 0.36491818\n",
      "Iteration 152, loss = 0.36457968\n",
      "Iteration 153, loss = 0.36422448\n",
      "Iteration 154, loss = 0.36386452\n",
      "Iteration 155, loss = 0.36350865\n",
      "Iteration 156, loss = 0.36318099\n",
      "Iteration 157, loss = 0.36284197\n",
      "Iteration 158, loss = 0.36250247\n",
      "Iteration 159, loss = 0.36217710\n",
      "Iteration 160, loss = 0.36184604\n",
      "Iteration 161, loss = 0.36153424\n",
      "Iteration 162, loss = 0.36122008\n",
      "Iteration 163, loss = 0.36089815\n",
      "Iteration 164, loss = 0.36059314\n",
      "Iteration 165, loss = 0.36027237\n",
      "Iteration 166, loss = 0.35997418\n",
      "Iteration 167, loss = 0.35968365\n",
      "Iteration 168, loss = 0.35937492\n",
      "Iteration 169, loss = 0.35908774\n",
      "Iteration 170, loss = 0.35880987\n",
      "Iteration 171, loss = 0.35850769\n",
      "Iteration 172, loss = 0.35822123\n",
      "Iteration 173, loss = 0.35793034\n",
      "Iteration 174, loss = 0.35765734\n",
      "Iteration 175, loss = 0.35740041\n",
      "Iteration 176, loss = 0.35712403\n",
      "Iteration 177, loss = 0.35685282\n",
      "Iteration 178, loss = 0.35657460\n",
      "Iteration 179, loss = 0.35632010\n",
      "Iteration 180, loss = 0.35606924\n",
      "Iteration 181, loss = 0.35582026\n",
      "Iteration 182, loss = 0.35554052\n",
      "Iteration 183, loss = 0.35531070\n",
      "Iteration 184, loss = 0.35506832\n",
      "Iteration 185, loss = 0.35480645\n",
      "Iteration 186, loss = 0.35456467\n",
      "Iteration 187, loss = 0.35430990\n",
      "Iteration 188, loss = 0.35407232\n",
      "Iteration 189, loss = 0.35383680\n",
      "Iteration 190, loss = 0.35360456\n",
      "Iteration 191, loss = 0.35339590\n",
      "Iteration 192, loss = 0.35313323\n",
      "Iteration 193, loss = 0.35291430\n",
      "Iteration 194, loss = 0.35268180\n",
      "Iteration 195, loss = 0.35246224\n",
      "Iteration 196, loss = 0.35225196\n",
      "Iteration 197, loss = 0.35202876\n",
      "Iteration 198, loss = 0.35180278\n",
      "Iteration 199, loss = 0.35157711\n",
      "Iteration 200, loss = 0.35137803\n",
      "Iteration 201, loss = 0.35115594\n",
      "Iteration 202, loss = 0.35095002\n",
      "Iteration 203, loss = 0.35075046\n",
      "Iteration 204, loss = 0.35052918\n",
      "Iteration 205, loss = 0.35031427\n",
      "Iteration 206, loss = 0.35013132\n",
      "Iteration 207, loss = 0.34992056\n",
      "Iteration 208, loss = 0.34972568\n",
      "Iteration 209, loss = 0.34952298\n",
      "Iteration 210, loss = 0.34931378\n",
      "Iteration 211, loss = 0.34913730\n",
      "Iteration 212, loss = 0.34892419\n",
      "Iteration 213, loss = 0.34872640\n",
      "Iteration 214, loss = 0.34854931\n",
      "Iteration 215, loss = 0.34835052\n",
      "Iteration 216, loss = 0.34817341\n",
      "Iteration 217, loss = 0.34797152\n",
      "Iteration 218, loss = 0.34779983\n",
      "Iteration 219, loss = 0.34761614\n",
      "Iteration 220, loss = 0.34742659\n",
      "Iteration 221, loss = 0.34725715\n",
      "Iteration 222, loss = 0.34706654\n",
      "Iteration 223, loss = 0.34689644\n",
      "Iteration 224, loss = 0.34670774\n",
      "Iteration 225, loss = 0.34654158\n",
      "Iteration 226, loss = 0.34636854\n",
      "Iteration 227, loss = 0.34618809\n",
      "Iteration 228, loss = 0.34601481\n",
      "Iteration 229, loss = 0.34584947\n",
      "Iteration 230, loss = 0.34568865\n",
      "Iteration 231, loss = 0.34551971\n",
      "Iteration 232, loss = 0.34534539\n",
      "Iteration 233, loss = 0.34518538\n",
      "Iteration 234, loss = 0.34501309\n",
      "Iteration 235, loss = 0.34486018\n",
      "Iteration 236, loss = 0.34470653\n",
      "Iteration 237, loss = 0.34452880\n",
      "Iteration 238, loss = 0.34437162\n",
      "Iteration 239, loss = 0.34422413\n",
      "Iteration 240, loss = 0.34407798\n",
      "Iteration 241, loss = 0.34390264\n",
      "Iteration 242, loss = 0.34374456\n",
      "Iteration 243, loss = 0.34358893\n",
      "Iteration 244, loss = 0.34343686\n",
      "Iteration 245, loss = 0.34329278\n",
      "Iteration 246, loss = 0.34313085\n",
      "Iteration 247, loss = 0.34297809\n",
      "Iteration 248, loss = 0.34286285\n",
      "Iteration 249, loss = 0.34269262\n",
      "Iteration 250, loss = 0.34254436\n",
      "Iteration 251, loss = 0.34239626\n",
      "Iteration 252, loss = 0.34224311\n",
      "Iteration 253, loss = 0.34210622\n",
      "Iteration 254, loss = 0.34196009\n",
      "Iteration 255, loss = 0.34180740\n",
      "Iteration 256, loss = 0.34167658\n",
      "Iteration 257, loss = 0.34153877\n",
      "Iteration 258, loss = 0.34139411\n",
      "Iteration 259, loss = 0.34126116\n",
      "Iteration 260, loss = 0.34111956\n",
      "Iteration 261, loss = 0.34098940\n",
      "Iteration 262, loss = 0.34084783\n",
      "Iteration 263, loss = 0.34073204\n",
      "Iteration 264, loss = 0.34057654\n",
      "Iteration 265, loss = 0.34044609\n",
      "Iteration 266, loss = 0.34031125\n",
      "Iteration 267, loss = 0.34017640\n",
      "Iteration 268, loss = 0.34006183\n",
      "Iteration 269, loss = 0.33991459\n",
      "Iteration 270, loss = 0.33978538\n",
      "Iteration 271, loss = 0.33965888\n",
      "Iteration 272, loss = 0.33952379\n",
      "Iteration 273, loss = 0.33939050\n",
      "Iteration 274, loss = 0.33928150\n",
      "Iteration 275, loss = 0.33914230\n",
      "Iteration 276, loss = 0.33901253\n",
      "Iteration 277, loss = 0.33888554\n",
      "Iteration 278, loss = 0.33875349\n",
      "Iteration 279, loss = 0.33863800\n",
      "Iteration 280, loss = 0.33850875\n",
      "Iteration 281, loss = 0.33839942\n",
      "Iteration 282, loss = 0.33827016\n",
      "Iteration 283, loss = 0.33814804\n",
      "Iteration 284, loss = 0.33803222\n",
      "Iteration 285, loss = 0.33790384\n",
      "Iteration 286, loss = 0.33778721\n",
      "Iteration 287, loss = 0.33766148\n",
      "Iteration 288, loss = 0.33753921\n",
      "Iteration 289, loss = 0.33742398\n",
      "Iteration 290, loss = 0.33732995\n",
      "Iteration 291, loss = 0.33718860\n",
      "Iteration 292, loss = 0.33708839\n",
      "Iteration 293, loss = 0.33696001\n",
      "Iteration 294, loss = 0.33685211\n",
      "Iteration 295, loss = 0.33673386\n",
      "Iteration 296, loss = 0.33662365\n",
      "Iteration 297, loss = 0.33650948\n",
      "Iteration 298, loss = 0.33638870\n",
      "Iteration 299, loss = 0.33628198\n",
      "Iteration 300, loss = 0.33616858\n",
      "Iteration 301, loss = 0.33605293\n",
      "Iteration 302, loss = 0.33594277\n",
      "Iteration 303, loss = 0.33583536\n",
      "Iteration 304, loss = 0.33573102\n",
      "Iteration 305, loss = 0.33562181\n",
      "Iteration 306, loss = 0.33550856\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.3s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.7s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.6s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   3.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=lbfgs; total time=   0.0s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   8.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   0.4s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.2s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   9.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=invscaling, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.3s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=identity, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.1s\n",
      "[CV] END activation=logistic, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=lbfgs; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   2.1s\n",
      "[CV] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=lbfgs; total time=   0.1s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=invscaling, max_iter=3000, solver=adam; total time=   2.6s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.3s\n",
      "Iteration 1, loss = 0.73217872\n",
      "Iteration 2, loss = 0.73000458\n",
      "Iteration 3, loss = 0.72673413\n",
      "Iteration 4, loss = 0.72271476\n",
      "Iteration 5, loss = 0.71807067\n",
      "Iteration 6, loss = 0.71319144\n",
      "Iteration 7, loss = 0.70808180\n",
      "Iteration 8, loss = 0.70305045\n",
      "Iteration 9, loss = 0.69776019\n",
      "Iteration 10, loss = 0.69265291\n",
      "Iteration 11, loss = 0.68746625\n",
      "Iteration 12, loss = 0.68235610\n",
      "Iteration 13, loss = 0.67741200\n",
      "Iteration 14, loss = 0.67253488\n",
      "Iteration 15, loss = 0.66765454\n",
      "Iteration 16, loss = 0.66298977\n",
      "Iteration 17, loss = 0.65836051\n",
      "Iteration 18, loss = 0.65393610\n",
      "Iteration 19, loss = 0.64936164\n",
      "Iteration 20, loss = 0.64510259\n",
      "Iteration 21, loss = 0.64073864\n",
      "Iteration 22, loss = 0.63652212\n",
      "Iteration 23, loss = 0.63240310\n",
      "Iteration 24, loss = 0.62830325\n",
      "Iteration 25, loss = 0.62427258\n",
      "Iteration 26, loss = 0.62027882\n",
      "Iteration 27, loss = 0.61642463\n",
      "Iteration 28, loss = 0.61250257\n",
      "Iteration 29, loss = 0.60872037\n",
      "Iteration 30, loss = 0.60494178\n",
      "Iteration 31, loss = 0.60128849\n",
      "Iteration 32, loss = 0.59759083\n",
      "Iteration 33, loss = 0.59395818\n",
      "Iteration 34, loss = 0.59028932\n",
      "Iteration 35, loss = 0.58693434\n",
      "Iteration 36, loss = 0.58338471\n",
      "Iteration 37, loss = 0.58000210\n",
      "Iteration 38, loss = 0.57660108\n",
      "Iteration 39, loss = 0.57324538\n",
      "Iteration 40, loss = 0.56998634\n",
      "Iteration 41, loss = 0.56674088\n",
      "Iteration 42, loss = 0.56354442\n",
      "Iteration 43, loss = 0.56035626\n",
      "Iteration 44, loss = 0.55732189\n",
      "Iteration 45, loss = 0.55423235\n",
      "Iteration 46, loss = 0.55126410\n",
      "Iteration 47, loss = 0.54822779\n",
      "Iteration 48, loss = 0.54530737\n",
      "Iteration 49, loss = 0.54237492\n",
      "Iteration 50, loss = 0.53961787\n",
      "Iteration 51, loss = 0.53681469\n",
      "Iteration 52, loss = 0.53403862\n",
      "Iteration 53, loss = 0.53138334\n",
      "Iteration 54, loss = 0.52870932\n",
      "Iteration 55, loss = 0.52605156\n",
      "Iteration 56, loss = 0.52341714\n",
      "Iteration 57, loss = 0.52093606\n",
      "Iteration 58, loss = 0.51843320\n",
      "Iteration 59, loss = 0.51598398\n",
      "Iteration 60, loss = 0.51351435\n",
      "Iteration 61, loss = 0.51107499\n",
      "Iteration 62, loss = 0.50881975\n",
      "Iteration 63, loss = 0.50644956\n",
      "Iteration 64, loss = 0.50423890\n",
      "Iteration 65, loss = 0.50201963\n",
      "Iteration 66, loss = 0.49973775\n",
      "Iteration 67, loss = 0.49766630\n",
      "Iteration 68, loss = 0.49554536\n",
      "Iteration 69, loss = 0.49345701\n",
      "Iteration 70, loss = 0.49134257\n",
      "Iteration 71, loss = 0.48941791\n",
      "Iteration 72, loss = 0.48748796\n",
      "Iteration 73, loss = 0.48549189\n",
      "Iteration 74, loss = 0.48363890\n",
      "Iteration 75, loss = 0.48169990\n",
      "Iteration 76, loss = 0.47995871\n",
      "Iteration 77, loss = 0.47811926\n",
      "Iteration 78, loss = 0.47641634\n",
      "Iteration 79, loss = 0.47465995\n",
      "Iteration 80, loss = 0.47294862\n",
      "Iteration 81, loss = 0.47134648\n",
      "Iteration 82, loss = 0.46961315\n",
      "Iteration 83, loss = 0.46806971\n",
      "Iteration 84, loss = 0.46653285\n",
      "Iteration 85, loss = 0.46496875\n",
      "Iteration 86, loss = 0.46355096\n",
      "Iteration 87, loss = 0.46199003\n",
      "Iteration 88, loss = 0.46053790\n",
      "Iteration 89, loss = 0.45922745\n",
      "Iteration 90, loss = 0.45779251\n",
      "Iteration 91, loss = 0.45634088\n",
      "Iteration 92, loss = 0.45504753\n",
      "Iteration 93, loss = 0.45371406\n",
      "Iteration 94, loss = 0.45246635\n",
      "Iteration 95, loss = 0.45119126\n",
      "Iteration 96, loss = 0.44997406\n",
      "Iteration 97, loss = 0.44874197\n",
      "Iteration 98, loss = 0.44757958\n",
      "Iteration 99, loss = 0.44637951\n",
      "Iteration 100, loss = 0.44528465\n",
      "Iteration 101, loss = 0.44413566\n",
      "Iteration 102, loss = 0.44298935\n",
      "Iteration 103, loss = 0.44196579\n",
      "Iteration 104, loss = 0.44089843\n",
      "Iteration 105, loss = 0.43982940\n",
      "Iteration 106, loss = 0.43884040\n",
      "Iteration 107, loss = 0.43786613\n",
      "Iteration 108, loss = 0.43684178\n",
      "Iteration 109, loss = 0.43587725\n",
      "Iteration 110, loss = 0.43498129\n",
      "Iteration 111, loss = 0.43404423\n",
      "Iteration 112, loss = 0.43309906\n",
      "Iteration 113, loss = 0.43226209\n",
      "Iteration 114, loss = 0.43137009\n",
      "Iteration 115, loss = 0.43051547\n",
      "Iteration 116, loss = 0.42972597\n",
      "Iteration 117, loss = 0.42887944\n",
      "Iteration 118, loss = 0.42800921\n",
      "Iteration 119, loss = 0.42725278\n",
      "Iteration 120, loss = 0.42648381\n",
      "Iteration 121, loss = 0.42570058\n",
      "Iteration 122, loss = 0.42493100\n",
      "Iteration 123, loss = 0.42417450\n",
      "Iteration 124, loss = 0.42347780\n",
      "Iteration 125, loss = 0.42277460\n",
      "Iteration 126, loss = 0.42203182\n",
      "Iteration 127, loss = 0.42137228\n",
      "Iteration 128, loss = 0.42069405\n",
      "Iteration 129, loss = 0.42000695\n",
      "Iteration 130, loss = 0.41933331\n",
      "Iteration 131, loss = 0.41868135\n",
      "Iteration 132, loss = 0.41804392\n",
      "Iteration 133, loss = 0.41742967\n",
      "Iteration 134, loss = 0.41685434\n",
      "Iteration 135, loss = 0.41620890\n",
      "Iteration 136, loss = 0.41560584\n",
      "Iteration 137, loss = 0.41502173\n",
      "Iteration 138, loss = 0.41443561\n",
      "Iteration 139, loss = 0.41385296\n",
      "Iteration 140, loss = 0.41331611\n",
      "Iteration 141, loss = 0.41276595\n",
      "Iteration 142, loss = 0.41219712\n",
      "Iteration 143, loss = 0.41165110\n",
      "Iteration 144, loss = 0.41114235\n",
      "Iteration 145, loss = 0.41062828\n",
      "Iteration 146, loss = 0.41005755\n",
      "Iteration 147, loss = 0.40955683\n",
      "Iteration 148, loss = 0.40906040\n",
      "Iteration 149, loss = 0.40852736\n",
      "Iteration 150, loss = 0.40804793\n",
      "Iteration 151, loss = 0.40758543\n",
      "Iteration 152, loss = 0.40708745\n",
      "Iteration 153, loss = 0.40659996\n",
      "Iteration 154, loss = 0.40613668\n",
      "Iteration 155, loss = 0.40564024\n",
      "Iteration 156, loss = 0.40524156\n",
      "Iteration 157, loss = 0.40473966\n",
      "Iteration 158, loss = 0.40430737\n",
      "Iteration 159, loss = 0.40387103\n",
      "Iteration 160, loss = 0.40341752\n",
      "Iteration 161, loss = 0.40300167\n",
      "Iteration 162, loss = 0.40257654\n",
      "Iteration 163, loss = 0.40216220\n",
      "Iteration 164, loss = 0.40174208\n",
      "Iteration 165, loss = 0.40133875\n",
      "Iteration 166, loss = 0.40092264\n",
      "Iteration 167, loss = 0.40050761\n",
      "Iteration 168, loss = 0.40013025\n",
      "Iteration 169, loss = 0.39974378\n",
      "Iteration 170, loss = 0.39933711\n",
      "Iteration 171, loss = 0.39897313\n",
      "Iteration 172, loss = 0.39856159\n",
      "Iteration 173, loss = 0.39819674\n",
      "Iteration 174, loss = 0.39783288\n",
      "Iteration 175, loss = 0.39746543\n",
      "Iteration 176, loss = 0.39709410\n",
      "Iteration 177, loss = 0.39673994\n",
      "Iteration 178, loss = 0.39635831\n",
      "Iteration 179, loss = 0.39599884\n",
      "Iteration 180, loss = 0.39565439\n",
      "Iteration 181, loss = 0.39530725\n",
      "Iteration 182, loss = 0.39500549\n",
      "Iteration 183, loss = 0.39464152\n",
      "Iteration 184, loss = 0.39428140\n",
      "Iteration 185, loss = 0.39394167\n",
      "Iteration 186, loss = 0.39361632\n",
      "Iteration 187, loss = 0.39329043\n",
      "Iteration 188, loss = 0.39296900\n",
      "Iteration 189, loss = 0.39266014\n",
      "Iteration 190, loss = 0.39231479\n",
      "Iteration 191, loss = 0.39199116\n",
      "Iteration 192, loss = 0.39168123\n",
      "Iteration 193, loss = 0.39136710\n",
      "Iteration 194, loss = 0.39109053\n",
      "Iteration 195, loss = 0.39073525\n",
      "Iteration 196, loss = 0.39045020\n",
      "Iteration 197, loss = 0.39014078\n",
      "Iteration 198, loss = 0.38982532\n",
      "Iteration 199, loss = 0.38954016\n",
      "Iteration 200, loss = 0.38926272\n",
      "Iteration 201, loss = 0.38900104\n",
      "Iteration 202, loss = 0.38867477\n",
      "Iteration 203, loss = 0.38836891\n",
      "Iteration 204, loss = 0.38806046\n",
      "Iteration 205, loss = 0.38781581\n",
      "Iteration 206, loss = 0.38753497\n",
      "Iteration 207, loss = 0.38723974\n",
      "Iteration 208, loss = 0.38696856\n",
      "Iteration 209, loss = 0.38667979\n",
      "Iteration 210, loss = 0.38641363\n",
      "Iteration 211, loss = 0.38615835\n",
      "Iteration 212, loss = 0.38585161\n",
      "Iteration 213, loss = 0.38561927\n",
      "Iteration 214, loss = 0.38535132\n",
      "Iteration 215, loss = 0.38508429\n",
      "Iteration 216, loss = 0.38479268\n",
      "Iteration 217, loss = 0.38453834\n",
      "Iteration 218, loss = 0.38429136\n",
      "Iteration 219, loss = 0.38402969\n",
      "Iteration 220, loss = 0.38376716\n",
      "Iteration 221, loss = 0.38350679\n",
      "Iteration 222, loss = 0.38328561\n",
      "Iteration 223, loss = 0.38302029\n",
      "Iteration 224, loss = 0.38274547\n",
      "Iteration 225, loss = 0.38250933\n",
      "Iteration 226, loss = 0.38227644\n",
      "Iteration 227, loss = 0.38203440\n",
      "Iteration 228, loss = 0.38177484\n",
      "Iteration 229, loss = 0.38154246\n",
      "Iteration 230, loss = 0.38128711\n",
      "Iteration 231, loss = 0.38106000\n",
      "Iteration 232, loss = 0.38081699\n",
      "Iteration 233, loss = 0.38058683\n",
      "Iteration 234, loss = 0.38035633\n",
      "Iteration 235, loss = 0.38011073\n",
      "Iteration 236, loss = 0.37990648\n",
      "Iteration 237, loss = 0.37967176\n",
      "Iteration 238, loss = 0.37942940\n",
      "Iteration 239, loss = 0.37921926\n",
      "Iteration 270, loss = 0.35236296\n",
      "Iteration 271, loss = 0.35218783\n",
      "Iteration 272, loss = 0.35201433\n",
      "Iteration 273, loss = 0.35185035\n",
      "Iteration 274, loss = 0.35170051\n",
      "Iteration 275, loss = 0.35151658\n",
      "Iteration 276, loss = 0.35134822\n",
      "Iteration 277, loss = 0.35118657\n",
      "Iteration 278, loss = 0.35107275\n",
      "Iteration 279, loss = 0.35087699\n",
      "Iteration 280, loss = 0.35071609\n",
      "Iteration 281, loss = 0.35055192\n",
      "Iteration 282, loss = 0.35039042\n",
      "Iteration 283, loss = 0.35022489\n",
      "Iteration 284, loss = 0.35008353\n",
      "Iteration 285, loss = 0.34994204\n",
      "Iteration 286, loss = 0.34977187\n",
      "Iteration 287, loss = 0.34961123\n",
      "Iteration 288, loss = 0.34944889\n",
      "Iteration 289, loss = 0.34929328\n",
      "Iteration 290, loss = 0.34915522\n",
      "Iteration 291, loss = 0.34900015\n",
      "Iteration 292, loss = 0.34884651\n",
      "Iteration 293, loss = 0.34870027\n",
      "Iteration 294, loss = 0.34859683\n",
      "Iteration 295, loss = 0.34840595\n",
      "Iteration 296, loss = 0.34825174\n",
      "Iteration 297, loss = 0.34810390\n",
      "Iteration 298, loss = 0.34800870\n",
      "Iteration 299, loss = 0.34780721\n",
      "Iteration 300, loss = 0.34768535\n",
      "Iteration 301, loss = 0.34752200\n",
      "Iteration 302, loss = 0.34737202\n",
      "Iteration 303, loss = 0.34723587\n",
      "Iteration 304, loss = 0.34708817\n",
      "Iteration 305, loss = 0.34697190\n",
      "Iteration 306, loss = 0.34680891\n",
      "Iteration 307, loss = 0.34666284\n",
      "Iteration 308, loss = 0.34655391\n",
      "Iteration 309, loss = 0.34638241\n",
      "Iteration 310, loss = 0.34625385\n",
      "Iteration 311, loss = 0.34610917\n",
      "Iteration 312, loss = 0.34597514\n",
      "Iteration 313, loss = 0.34583291\n",
      "Iteration 314, loss = 0.34572487\n",
      "Iteration 315, loss = 0.34557802\n",
      "Iteration 316, loss = 0.34543134\n",
      "Iteration 317, loss = 0.34529225\n",
      "Iteration 318, loss = 0.34516510\n",
      "Iteration 319, loss = 0.34505431\n",
      "Iteration 320, loss = 0.34490612\n",
      "Iteration 321, loss = 0.34477222\n",
      "Iteration 322, loss = 0.34463415\n",
      "Iteration 323, loss = 0.34450259\n",
      "Iteration 324, loss = 0.34437911\n",
      "Iteration 325, loss = 0.34426109\n",
      "Iteration 326, loss = 0.34412541\n",
      "Iteration 327, loss = 0.34398737\n",
      "Iteration 328, loss = 0.34387001\n",
      "Iteration 329, loss = 0.34373898\n",
      "Iteration 330, loss = 0.34360462\n",
      "Iteration 331, loss = 0.34347872\n",
      "Iteration 332, loss = 0.34336843\n",
      "Iteration 333, loss = 0.34323315\n",
      "Iteration 334, loss = 0.34310199\n",
      "Iteration 335, loss = 0.34297959\n",
      "Iteration 336, loss = 0.34285930\n",
      "Iteration 337, loss = 0.34272909\n",
      "Iteration 338, loss = 0.34260969\n",
      "Iteration 339, loss = 0.34249123\n",
      "Iteration 340, loss = 0.34236650\n",
      "Iteration 341, loss = 0.34226237\n",
      "Iteration 342, loss = 0.34214509\n",
      "Iteration 343, loss = 0.34199903\n",
      "Iteration 344, loss = 0.34188482\n",
      "Iteration 345, loss = 0.34176734\n",
      "Iteration 346, loss = 0.34164661\n",
      "Iteration 347, loss = 0.34152303\n",
      "Iteration 348, loss = 0.34139836\n",
      "Iteration 349, loss = 0.34130855\n",
      "Iteration 350, loss = 0.34116353\n",
      "Iteration 351, loss = 0.34105234\n",
      "Iteration 352, loss = 0.34092817\n",
      "Iteration 353, loss = 0.34080577\n",
      "Iteration 354, loss = 0.34071120\n",
      "Iteration 355, loss = 0.34057840\n",
      "Iteration 356, loss = 0.34048320\n",
      "Iteration 357, loss = 0.34036621\n",
      "Iteration 358, loss = 0.34022407\n",
      "Iteration 359, loss = 0.34009906\n",
      "Iteration 360, loss = 0.33998892\n",
      "Iteration 361, loss = 0.33988420\n",
      "Iteration 362, loss = 0.33978692\n",
      "Iteration 363, loss = 0.33965719\n",
      "Iteration 364, loss = 0.33955762\n",
      "Iteration 365, loss = 0.33945503\n",
      "Iteration 366, loss = 0.33931422\n",
      "Iteration 367, loss = 0.33918936\n",
      "Iteration 368, loss = 0.33908053\n",
      "Iteration 369, loss = 0.33897872\n",
      "Iteration 370, loss = 0.33886118\n",
      "Iteration 371, loss = 0.33874876\n",
      "Iteration 372, loss = 0.33863140\n",
      "Iteration 373, loss = 0.33853042\n",
      "Iteration 374, loss = 0.33840243\n",
      "Iteration 375, loss = 0.33831569\n",
      "Iteration 376, loss = 0.33820743\n",
      "Iteration 377, loss = 0.33810442\n",
      "Iteration 378, loss = 0.33796199\n",
      "Iteration 379, loss = 0.33785818\n",
      "Iteration 380, loss = 0.33775335\n",
      "Iteration 381, loss = 0.33764957\n",
      "Iteration 382, loss = 0.33752960\n",
      "Iteration 383, loss = 0.33743818\n",
      "Iteration 384, loss = 0.33732087\n",
      "Iteration 385, loss = 0.33720285\n",
      "Iteration 386, loss = 0.33709005\n",
      "Iteration 387, loss = 0.33701490\n",
      "Iteration 388, loss = 0.33689002\n",
      "Iteration 389, loss = 0.33678994\n",
      "Iteration 390, loss = 0.33669735\n",
      "Iteration 391, loss = 0.33655920\n",
      "Iteration 392, loss = 0.33648367\n",
      "Iteration 393, loss = 0.33637453\n",
      "Iteration 394, loss = 0.33624008\n",
      "Iteration 395, loss = 0.33614228\n",
      "Iteration 396, loss = 0.33604135\n",
      "Iteration 397, loss = 0.33592347\n",
      "Iteration 398, loss = 0.33584072\n",
      "Iteration 399, loss = 0.33573353\n",
      "Iteration 400, loss = 0.33563731\n",
      "Iteration 401, loss = 0.33553003\n",
      "Iteration 402, loss = 0.33541435\n",
      "Iteration 403, loss = 0.33531363\n",
      "Iteration 404, loss = 0.33521700\n",
      "Iteration 405, loss = 0.33510513\n",
      "Iteration 406, loss = 0.33503588\n",
      "Iteration 407, loss = 0.33493805\n",
      "Iteration 408, loss = 0.33481821\n",
      "Iteration 409, loss = 0.33470280\n",
      "Iteration 410, loss = 0.33459954\n",
      "Iteration 411, loss = 0.33450690\n",
      "Iteration 412, loss = 0.33439813\n",
      "Iteration 413, loss = 0.33431819\n",
      "Iteration 414, loss = 0.33419669\n",
      "Iteration 415, loss = 0.33410317\n",
      "Iteration 416, loss = 0.33400361\n",
      "Iteration 417, loss = 0.33389158\n",
      "Iteration 418, loss = 0.33380727\n",
      "Iteration 419, loss = 0.33370791\n",
      "Iteration 420, loss = 0.33360956\n",
      "Iteration 421, loss = 0.33352240\n",
      "Iteration 422, loss = 0.33339855\n",
      "Iteration 423, loss = 0.33332726\n",
      "Iteration 424, loss = 0.33322808\n",
      "Iteration 425, loss = 0.33314024\n",
      "Iteration 426, loss = 0.33302981\n",
      "Iteration 427, loss = 0.33292804\n",
      "Iteration 428, loss = 0.33282585\n",
      "Iteration 429, loss = 0.33273613\n",
      "Iteration 430, loss = 0.33262981\n",
      "Iteration 431, loss = 0.33254871\n",
      "Iteration 432, loss = 0.33244040\n",
      "Iteration 433, loss = 0.33234582\n",
      "Iteration 434, loss = 0.33224549\n",
      "Iteration 435, loss = 0.33216024\n",
      "Iteration 436, loss = 0.33204993\n",
      "Iteration 437, loss = 0.33195435\n",
      "Iteration 438, loss = 0.33186899\n",
      "Iteration 439, loss = 0.33176387\n",
      "Iteration 440, loss = 0.33168095\n",
      "Iteration 441, loss = 0.33160085\n",
      "Iteration 442, loss = 0.33147624\n",
      "Iteration 443, loss = 0.33139983\n",
      "Iteration 444, loss = 0.33128945\n",
      "Iteration 445, loss = 0.33121810\n",
      "Iteration 446, loss = 0.33113289\n",
      "Iteration 447, loss = 0.33101243\n",
      "Iteration 448, loss = 0.33092660\n",
      "Iteration 449, loss = 0.33082311\n",
      "Iteration 450, loss = 0.33074333\n",
      "Iteration 451, loss = 0.33064387\n",
      "Iteration 452, loss = 0.33058817\n",
      "Iteration 453, loss = 0.33046115\n",
      "Iteration 454, loss = 0.33039298\n",
      "Iteration 455, loss = 0.33028471\n",
      "Iteration 456, loss = 0.33019092\n",
      "Iteration 457, loss = 0.33010157\n",
      "Iteration 458, loss = 0.33001794\n",
      "Iteration 459, loss = 0.32993277\n",
      "Iteration 460, loss = 0.32982117\n",
      "Iteration 461, loss = 0.32974942\n",
      "Iteration 462, loss = 0.32964903\n",
      "Iteration 463, loss = 0.32957657\n",
      "Iteration 464, loss = 0.32948649\n",
      "Iteration 465, loss = 0.32937806\n",
      "Iteration 466, loss = 0.32930314\n",
      "Iteration 467, loss = 0.32921923\n",
      "Iteration 468, loss = 0.32910879\n",
      "Iteration 469, loss = 0.32902354\n",
      "Iteration 470, loss = 0.32892934\n",
      "Iteration 471, loss = 0.32884193\n",
      "Iteration 472, loss = 0.32875378\n",
      "Iteration 473, loss = 0.32867697\n",
      "Iteration 474, loss = 0.32858852\n",
      "Iteration 475, loss = 0.32850056\n",
      "Iteration 476, loss = 0.32840736\n",
      "Iteration 477, loss = 0.32831366\n",
      "Iteration 478, loss = 0.32823670\n",
      "Iteration 479, loss = 0.32813590\n",
      "Iteration 480, loss = 0.32804902\n",
      "Iteration 481, loss = 0.32795973\n",
      "Iteration 482, loss = 0.32788148\n",
      "Iteration 483, loss = 0.32779797\n",
      "Iteration 484, loss = 0.32770438\n",
      "Iteration 485, loss = 0.32760511\n",
      "Iteration 486, loss = 0.32752246\n",
      "Iteration 487, loss = 0.32743659\n",
      "Iteration 488, loss = 0.32734575\n",
      "Iteration 489, loss = 0.32725808\n",
      "Iteration 490, loss = 0.32718401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74881309\n",
      "Iteration 2, loss = 0.74793479\n",
      "Iteration 3, loss = 0.74658172\n",
      "Iteration 4, loss = 0.74518180\n",
      "Iteration 5, loss = 0.74312398\n",
      "Iteration 6, loss = 0.74112001\n",
      "Iteration 7, loss = 0.73925630\n",
      "Iteration 8, loss = 0.73736646\n",
      "Iteration 9, loss = 0.73541933\n",
      "Iteration 10, loss = 0.73365944\n",
      "Iteration 11, loss = 0.73180165\n",
      "Iteration 12, loss = 0.72987415\n",
      "Iteration 13, loss = 0.72807101\n",
      "Iteration 14, loss = 0.72644899\n",
      "Iteration 15, loss = 0.72477345\n",
      "Iteration 16, loss = 0.72308931\n",
      "Iteration 17, loss = 0.72140854\n",
      "Iteration 18, loss = 0.71983535\n",
      "Iteration 19, loss = 0.71820169\n",
      "Iteration 20, loss = 0.71657697\n",
      "Iteration 21, loss = 0.71500068\n",
      "Iteration 22, loss = 0.71332892\n",
      "Iteration 23, loss = 0.71170283\n",
      "Iteration 24, loss = 0.71004121\n",
      "Iteration 25, loss = 0.70852215\n",
      "Iteration 26, loss = 0.70679984\n",
      "Iteration 27, loss = 0.70514935\n",
      "Iteration 28, loss = 0.70348564\n",
      "Iteration 29, loss = 0.70181773\n",
      "Iteration 30, loss = 0.70017709\n",
      "Iteration 31, loss = 0.69839223\n",
      "Iteration 32, loss = 0.69664943\n",
      "Iteration 33, loss = 0.69488219\n",
      "Iteration 34, loss = 0.69301318\n",
      "Iteration 35, loss = 0.69122112\n",
      "Iteration 36, loss = 0.68934317\n",
      "Iteration 37, loss = 0.68741603\n",
      "Iteration 38, loss = 0.68553660\n",
      "Iteration 39, loss = 0.68350475\n",
      "Iteration 40, loss = 0.68155885\n",
      "Iteration 41, loss = 0.67944275\n",
      "Iteration 42, loss = 0.67740851\n",
      "Iteration 43, loss = 0.67526706\n",
      "Iteration 44, loss = 0.67312686\n",
      "Iteration 45, loss = 0.67088259\n",
      "Iteration 46, loss = 0.66864362\n",
      "Iteration 47, loss = 0.66643282\n",
      "Iteration 48, loss = 0.66408330\n",
      "Iteration 49, loss = 0.66171406\n",
      "Iteration 50, loss = 0.65930763\n",
      "Iteration 51, loss = 0.65685363\n",
      "Iteration 52, loss = 0.65435626\n",
      "Iteration 53, loss = 0.65192460\n",
      "Iteration 54, loss = 0.64937114\n",
      "Iteration 55, loss = 0.64680653\n",
      "Iteration 56, loss = 0.64422447\n",
      "Iteration 57, loss = 0.64161176\n",
      "Iteration 58, loss = 0.63893761\n",
      "Iteration 59, loss = 0.63623229\n",
      "Iteration 60, loss = 0.63354935\n",
      "Iteration 61, loss = 0.63082698\n",
      "Iteration 62, loss = 0.62811495\n",
      "Iteration 63, loss = 0.62538396\n",
      "Iteration 64, loss = 0.62249137\n",
      "Iteration 65, loss = 0.61981263\n",
      "Iteration 66, loss = 0.61694401\n",
      "Iteration 67, loss = 0.61397631\n",
      "Iteration 68, loss = 0.61109690\n",
      "Iteration 69, loss = 0.60814404\n",
      "Iteration 70, loss = 0.60524660\n",
      "Iteration 71, loss = 0.60221635\n",
      "Iteration 72, loss = 0.59921030\n",
      "Iteration 73, loss = 0.59629518\n",
      "Iteration 74, loss = 0.59318826\n",
      "Iteration 75, loss = 0.59025561\n",
      "Iteration 76, loss = 0.58716801\n",
      "Iteration 77, loss = 0.58410254\n",
      "Iteration 78, loss = 0.58107761\n",
      "Iteration 79, loss = 0.57800223\n",
      "Iteration 80, loss = 0.57499516\n",
      "Iteration 81, loss = 0.57186113\n",
      "Iteration 82, loss = 0.56880286\n",
      "Iteration 83, loss = 0.56576260\n",
      "Iteration 84, loss = 0.56266161\n",
      "Iteration 85, loss = 0.55959266\n",
      "Iteration 86, loss = 0.55662571\n",
      "Iteration 87, loss = 0.55360464\n",
      "Iteration 88, loss = 0.55065421\n",
      "Iteration 89, loss = 0.54757427\n",
      "Iteration 90, loss = 0.54461536\n",
      "Iteration 91, loss = 0.54172205\n",
      "Iteration 92, loss = 0.53874067\n",
      "Iteration 93, loss = 0.53590487\n",
      "Iteration 94, loss = 0.53287212\n",
      "Iteration 95, loss = 0.53004903\n",
      "Iteration 96, loss = 0.52722855\n",
      "Iteration 97, loss = 0.52436247\n",
      "Iteration 98, loss = 0.52157565\n",
      "Iteration 99, loss = 0.51895188\n",
      "Iteration 100, loss = 0.51616085\n",
      "Iteration 101, loss = 0.51351620\n",
      "Iteration 102, loss = 0.51090494\n",
      "Iteration 103, loss = 0.50821434\n",
      "Iteration 104, loss = 0.50565232\n",
      "Iteration 105, loss = 0.50306434\n",
      "Iteration 106, loss = 0.50068433\n",
      "Iteration 107, loss = 0.49822047\n",
      "Iteration 108, loss = 0.49583952\n",
      "Iteration 109, loss = 0.49343875\n",
      "Iteration 110, loss = 0.49120042\n",
      "Iteration 111, loss = 0.48903852\n",
      "Iteration 112, loss = 0.48670169\n",
      "Iteration 113, loss = 0.48458301\n",
      "Iteration 114, loss = 0.48246709\n",
      "Iteration 115, loss = 0.48038056\n",
      "Iteration 116, loss = 0.47828042\n",
      "Iteration 117, loss = 0.47634689\n",
      "Iteration 118, loss = 0.47441961\n",
      "Iteration 119, loss = 0.47258844\n",
      "Iteration 120, loss = 0.47072264\n",
      "Iteration 121, loss = 0.46886361\n",
      "Iteration 122, loss = 0.46718944\n",
      "Iteration 123, loss = 0.46542577\n",
      "Iteration 124, loss = 0.46378402\n",
      "Iteration 125, loss = 0.46215959\n",
      "Iteration 126, loss = 0.46064652\n",
      "Iteration 127, loss = 0.45900529\n",
      "Iteration 128, loss = 0.45758476\n",
      "Iteration 129, loss = 0.45609959\n",
      "Iteration 130, loss = 0.45470969\n",
      "Iteration 131, loss = 0.45335808\n",
      "Iteration 132, loss = 0.45202964\n",
      "Iteration 133, loss = 0.45074322\n",
      "Iteration 134, loss = 0.44937928\n",
      "Iteration 135, loss = 0.44825332\n",
      "Iteration 136, loss = 0.44701190\n",
      "Iteration 137, loss = 0.44595382\n",
      "Iteration 138, loss = 0.44473356\n",
      "Iteration 139, loss = 0.44364217\n",
      "Iteration 140, loss = 0.44261586\n",
      "Iteration 141, loss = 0.44162188\n",
      "Iteration 142, loss = 0.44052578\n",
      "Iteration 143, loss = 0.43955294\n",
      "Iteration 144, loss = 0.43854426\n",
      "Iteration 145, loss = 0.43760117\n",
      "Iteration 146, loss = 0.43671021\n",
      "Iteration 147, loss = 0.43584880\n",
      "Iteration 148, loss = 0.43495948\n",
      "Iteration 149, loss = 0.43414381\n",
      "Iteration 150, loss = 0.43322915\n",
      "Iteration 151, loss = 0.43241367\n",
      "Iteration 152, loss = 0.43168203\n",
      "Iteration 153, loss = 0.43087669\n",
      "Iteration 154, loss = 0.43012858\n",
      "Iteration 155, loss = 0.42941488\n",
      "Iteration 156, loss = 0.42863900\n",
      "Iteration 157, loss = 0.42793505\n",
      "Iteration 158, loss = 0.42720832\n",
      "Iteration 159, loss = 0.42655429\n",
      "Iteration 160, loss = 0.42588933\n",
      "Iteration 161, loss = 0.42518304\n",
      "Iteration 162, loss = 0.42465066\n",
      "Iteration 163, loss = 0.42394797\n",
      "Iteration 164, loss = 0.42332379\n",
      "Iteration 165, loss = 0.42272787\n",
      "Iteration 166, loss = 0.42215968\n",
      "Iteration 167, loss = 0.42154418\n",
      "Iteration 168, loss = 0.42096057\n",
      "Iteration 169, loss = 0.42036483\n",
      "Iteration 170, loss = 0.41984196\n",
      "Iteration 171, loss = 0.41929911\n",
      "Iteration 172, loss = 0.41877080\n",
      "Iteration 173, loss = 0.41830052\n",
      "Iteration 174, loss = 0.41766944\n",
      "Iteration 175, loss = 0.41720698\n",
      "Iteration 176, loss = 0.41665629\n",
      "Iteration 177, loss = 0.41615197\n",
      "Iteration 178, loss = 0.41565514\n",
      "Iteration 179, loss = 0.41523800\n",
      "Iteration 180, loss = 0.41473580\n",
      "Iteration 181, loss = 0.41423940\n",
      "Iteration 182, loss = 0.41377315\n",
      "Iteration 183, loss = 0.41332482\n",
      "Iteration 184, loss = 0.41280973\n",
      "Iteration 185, loss = 0.41235260\n",
      "Iteration 186, loss = 0.41188801\n",
      "Iteration 187, loss = 0.41146481\n",
      "Iteration 188, loss = 0.41098674\n",
      "Iteration 189, loss = 0.41055139\n",
      "Iteration 190, loss = 0.41013319\n",
      "Iteration 191, loss = 0.40974403\n",
      "Iteration 192, loss = 0.40927596\n",
      "Iteration 193, loss = 0.40881098\n",
      "Iteration 194, loss = 0.40839686\n",
      "Iteration 195, loss = 0.40799037\n",
      "Iteration 196, loss = 0.40755620\n",
      "Iteration 197, loss = 0.40714100\n",
      "Iteration 198, loss = 0.40674050\n",
      "Iteration 199, loss = 0.40630850\n",
      "Iteration 200, loss = 0.40588215\n",
      "Iteration 201, loss = 0.40549966\n",
      "Iteration 202, loss = 0.40504940\n",
      "Iteration 203, loss = 0.40471877\n",
      "Iteration 204, loss = 0.40427179\n",
      "Iteration 205, loss = 0.40387166\n",
      "Iteration 206, loss = 0.40349391\n",
      "Iteration 207, loss = 0.40308418\n",
      "Iteration 208, loss = 0.40272625\n",
      "Iteration 209, loss = 0.40228303\n",
      "Iteration 210, loss = 0.40192395\n",
      "Iteration 211, loss = 0.40153261\n",
      "Iteration 212, loss = 0.40117318\n",
      "Iteration 213, loss = 0.40083245\n",
      "Iteration 214, loss = 0.40041381\n",
      "Iteration 215, loss = 0.40006117\n",
      "Iteration 216, loss = 0.39977057\n",
      "Iteration 217, loss = 0.39934393\n",
      "Iteration 218, loss = 0.39901361\n",
      "Iteration 219, loss = 0.39856434\n",
      "Iteration 220, loss = 0.39833420\n",
      "Iteration 221, loss = 0.39789585\n",
      "Iteration 222, loss = 0.39755857\n",
      "Iteration 223, loss = 0.39718011\n",
      "Iteration 224, loss = 0.39682109\n",
      "Iteration 225, loss = 0.39653948\n",
      "Iteration 226, loss = 0.39620808\n",
      "Iteration 227, loss = 0.39580675\n",
      "Iteration 228, loss = 0.39551884\n",
      "Iteration 229, loss = 0.39514902\n",
      "Iteration 230, loss = 0.39481727\n",
      "Iteration 231, loss = 0.39450600\n",
      "Iteration 232, loss = 0.39418867\n",
      "Iteration 233, loss = 0.39393155\n",
      "Iteration 234, loss = 0.39354184\n",
      "Iteration 235, loss = 0.39323025\n",
      "Iteration 236, loss = 0.39287583\n",
      "Iteration 237, loss = 0.39254903\n",
      "Iteration 238, loss = 0.39223583\n",
      "Iteration 239, loss = 0.39194995\n",
      "Iteration 240, loss = 0.39163398\n",
      "Iteration 241, loss = 0.39136498\n",
      "Iteration 242, loss = 0.39109765\n",
      "Iteration 243, loss = 0.39070562\n",
      "Iteration 244, loss = 0.39037983\n",
      "Iteration 245, loss = 0.39009734\n",
      "Iteration 246, loss = 0.38984184\n",
      "Iteration 247, loss = 0.38947960\n",
      "Iteration 248, loss = 0.38923732\n",
      "Iteration 249, loss = 0.38899775\n",
      "Iteration 250, loss = 0.38859383\n",
      "Iteration 251, loss = 0.38829871\n",
      "Iteration 252, loss = 0.38801602\n",
      "Iteration 253, loss = 0.38775850\n",
      "Iteration 254, loss = 0.38742536\n",
      "Iteration 255, loss = 0.38716134\n",
      "Iteration 256, loss = 0.38687387\n",
      "Iteration 257, loss = 0.38658868\n",
      "Iteration 258, loss = 0.38630092\n",
      "Iteration 259, loss = 0.38601977\n",
      "Iteration 260, loss = 0.38579108\n",
      "Iteration 261, loss = 0.38554754\n",
      "Iteration 262, loss = 0.38520934\n",
      "Iteration 263, loss = 0.38490629\n",
      "Iteration 264, loss = 0.38461491\n",
      "Iteration 265, loss = 0.38438652\n",
      "Iteration 266, loss = 0.38415160\n",
      "Iteration 267, loss = 0.38394310\n",
      "Iteration 268, loss = 0.38354290\n",
      "Iteration 269, loss = 0.38333531\n",
      "Iteration 270, loss = 0.38301194\n",
      "Iteration 271, loss = 0.38275449\n",
      "Iteration 272, loss = 0.38254853\n",
      "Iteration 273, loss = 0.38223855\n",
      "Iteration 274, loss = 0.38198689\n",
      "Iteration 275, loss = 0.38173709\n",
      "Iteration 189, loss = 0.38262405\n",
      "Iteration 190, loss = 0.38238606\n",
      "Iteration 191, loss = 0.38217687\n",
      "Iteration 192, loss = 0.38192045\n",
      "Iteration 193, loss = 0.38169803\n",
      "Iteration 194, loss = 0.38146515\n",
      "Iteration 195, loss = 0.38124065\n",
      "Iteration 196, loss = 0.38102227\n",
      "Iteration 197, loss = 0.38080698\n",
      "Iteration 198, loss = 0.38057939\n",
      "Iteration 199, loss = 0.38036211\n",
      "Iteration 200, loss = 0.38015318\n",
      "Iteration 201, loss = 0.37992152\n",
      "Iteration 202, loss = 0.37972054\n",
      "Iteration 203, loss = 0.37952094\n",
      "Iteration 204, loss = 0.37928635\n",
      "Iteration 205, loss = 0.37907449\n",
      "Iteration 206, loss = 0.37888576\n",
      "Iteration 207, loss = 0.37867782\n",
      "Iteration 208, loss = 0.37846350\n",
      "Iteration 209, loss = 0.37826077\n",
      "Iteration 210, loss = 0.37806080\n",
      "Iteration 211, loss = 0.37786358\n",
      "Iteration 212, loss = 0.37766629\n",
      "Iteration 213, loss = 0.37745626\n",
      "Iteration 214, loss = 0.37726363\n",
      "Iteration 215, loss = 0.37707306\n",
      "Iteration 216, loss = 0.37687612\n",
      "Iteration 217, loss = 0.37667770\n",
      "Iteration 218, loss = 0.37649801\n",
      "Iteration 219, loss = 0.37630972\n",
      "Iteration 220, loss = 0.37611727\n",
      "Iteration 221, loss = 0.37593660\n",
      "Iteration 222, loss = 0.37574561\n",
      "Iteration 223, loss = 0.37556054\n",
      "Iteration 224, loss = 0.37537021\n",
      "Iteration 225, loss = 0.37518900\n",
      "Iteration 226, loss = 0.37501364\n",
      "Iteration 227, loss = 0.37483435\n",
      "Iteration 228, loss = 0.37465197\n",
      "Iteration 229, loss = 0.37447897\n",
      "Iteration 230, loss = 0.37430118\n",
      "Iteration 231, loss = 0.37412428\n",
      "Iteration 232, loss = 0.37394587\n",
      "Iteration 233, loss = 0.37377482\n",
      "Iteration 234, loss = 0.37360829\n",
      "Iteration 235, loss = 0.37342935\n",
      "Iteration 236, loss = 0.37326838\n",
      "Iteration 237, loss = 0.37310537\n",
      "Iteration 238, loss = 0.37291525\n",
      "Iteration 239, loss = 0.37276456\n",
      "Iteration 240, loss = 0.37259724\n",
      "Iteration 241, loss = 0.37242146\n",
      "Iteration 242, loss = 0.37226703\n",
      "Iteration 243, loss = 0.37209383\n",
      "Iteration 244, loss = 0.37192830\n",
      "Iteration 245, loss = 0.37178049\n",
      "Iteration 246, loss = 0.37160583\n",
      "Iteration 247, loss = 0.37144738\n",
      "Iteration 248, loss = 0.37129861\n",
      "Iteration 249, loss = 0.37113700\n",
      "Iteration 250, loss = 0.37097023\n",
      "Iteration 251, loss = 0.37082235\n",
      "Iteration 252, loss = 0.37065962\n",
      "Iteration 253, loss = 0.37050962\n",
      "Iteration 254, loss = 0.37034941\n",
      "Iteration 255, loss = 0.37018337\n",
      "Iteration 256, loss = 0.37003901\n",
      "Iteration 257, loss = 0.36989307\n",
      "Iteration 258, loss = 0.36974058\n",
      "Iteration 259, loss = 0.36958857\n",
      "Iteration 260, loss = 0.36943636\n",
      "Iteration 261, loss = 0.36929730\n",
      "Iteration 262, loss = 0.36914859\n",
      "Iteration 263, loss = 0.36901066\n",
      "Iteration 264, loss = 0.36884421\n",
      "Iteration 265, loss = 0.36870037\n",
      "Iteration 266, loss = 0.36856057\n",
      "Iteration 267, loss = 0.36841660\n",
      "Iteration 268, loss = 0.36828971\n",
      "Iteration 269, loss = 0.36813033\n",
      "Iteration 270, loss = 0.36799591\n",
      "Iteration 271, loss = 0.36784408\n",
      "Iteration 272, loss = 0.36770827\n",
      "Iteration 273, loss = 0.36756155\n",
      "Iteration 274, loss = 0.36743238\n",
      "Iteration 275, loss = 0.36728571\n",
      "Iteration 276, loss = 0.36714931\n",
      "Iteration 277, loss = 0.36700866\n",
      "Iteration 278, loss = 0.36686680\n",
      "Iteration 279, loss = 0.36674127\n",
      "Iteration 280, loss = 0.36659869\n",
      "Iteration 281, loss = 0.36647687\n",
      "Iteration 282, loss = 0.36633529\n",
      "Iteration 283, loss = 0.36620253\n",
      "Iteration 284, loss = 0.36607338\n",
      "Iteration 285, loss = 0.36593013\n",
      "Iteration 286, loss = 0.36580036\n",
      "Iteration 287, loss = 0.36566782\n",
      "Iteration 288, loss = 0.36554279\n",
      "Iteration 289, loss = 0.36542126\n",
      "Iteration 290, loss = 0.36529290\n",
      "Iteration 291, loss = 0.36515533\n",
      "Iteration 292, loss = 0.36504619\n",
      "Iteration 293, loss = 0.36489315\n",
      "Iteration 294, loss = 0.36477017\n",
      "Iteration 295, loss = 0.36465222\n",
      "Iteration 296, loss = 0.36451943\n",
      "Iteration 297, loss = 0.36439312\n",
      "Iteration 298, loss = 0.36425623\n",
      "Iteration 299, loss = 0.36414498\n",
      "Iteration 300, loss = 0.36400971\n",
      "Iteration 301, loss = 0.36389157\n",
      "Iteration 302, loss = 0.36376544\n",
      "Iteration 303, loss = 0.36365433\n",
      "Iteration 304, loss = 0.36353631\n",
      "Iteration 305, loss = 0.36341524\n",
      "Iteration 306, loss = 0.36328820\n",
      "Iteration 307, loss = 0.36316297\n",
      "Iteration 308, loss = 0.36304362\n",
      "Iteration 309, loss = 0.36291966\n",
      "Iteration 310, loss = 0.36281203\n",
      "Iteration 311, loss = 0.36270760\n",
      "Iteration 312, loss = 0.36256678\n",
      "Iteration 313, loss = 0.36245798\n",
      "Iteration 314, loss = 0.36234266\n",
      "Iteration 315, loss = 0.36221019\n",
      "Iteration 316, loss = 0.36210201\n",
      "Iteration 317, loss = 0.36198824\n",
      "Iteration 318, loss = 0.36187667\n",
      "Iteration 319, loss = 0.36175908\n",
      "Iteration 320, loss = 0.36165060\n",
      "Iteration 321, loss = 0.36154420\n",
      "Iteration 322, loss = 0.36142410\n",
      "Iteration 323, loss = 0.36129936\n",
      "Iteration 324, loss = 0.36122237\n",
      "Iteration 325, loss = 0.36109267\n",
      "Iteration 326, loss = 0.36097933\n",
      "Iteration 327, loss = 0.36086524\n",
      "Iteration 328, loss = 0.36075758\n",
      "Iteration 329, loss = 0.36063995\n",
      "Iteration 330, loss = 0.36053036\n",
      "Iteration 331, loss = 0.36043439\n",
      "Iteration 332, loss = 0.36031961\n",
      "Iteration 333, loss = 0.36021025\n",
      "Iteration 334, loss = 0.36009966\n",
      "Iteration 335, loss = 0.36000135\n",
      "Iteration 336, loss = 0.35988242\n",
      "Iteration 337, loss = 0.35978137\n",
      "Iteration 338, loss = 0.35967970\n",
      "Iteration 339, loss = 0.35958074\n",
      "Iteration 340, loss = 0.35947049\n",
      "Iteration 341, loss = 0.35937842\n",
      "Iteration 342, loss = 0.35926084\n",
      "Iteration 343, loss = 0.35915975\n",
      "Iteration 344, loss = 0.35905586\n",
      "Iteration 345, loss = 0.35895399\n",
      "Iteration 346, loss = 0.35884535\n",
      "Iteration 347, loss = 0.35874933\n",
      "Iteration 348, loss = 0.35864281\n",
      "Iteration 349, loss = 0.35854161\n",
      "Iteration 350, loss = 0.35844397\n",
      "Iteration 351, loss = 0.35835496\n",
      "Iteration 352, loss = 0.35823770\n",
      "Iteration 353, loss = 0.35813808\n",
      "Iteration 354, loss = 0.35804388\n",
      "Iteration 355, loss = 0.35793165\n",
      "Iteration 356, loss = 0.35784074\n",
      "Iteration 357, loss = 0.35775541\n",
      "Iteration 358, loss = 0.35764636\n",
      "Iteration 359, loss = 0.35754263\n",
      "Iteration 360, loss = 0.35744760\n",
      "Iteration 361, loss = 0.35735982\n",
      "Iteration 362, loss = 0.35726106\n",
      "Iteration 363, loss = 0.35715933\n",
      "Iteration 364, loss = 0.35706140\n",
      "Iteration 365, loss = 0.35696420\n",
      "Iteration 366, loss = 0.35687986\n",
      "Iteration 367, loss = 0.35678336\n",
      "Iteration 368, loss = 0.35667734\n",
      "Iteration 369, loss = 0.35660201\n",
      "Iteration 370, loss = 0.35649236\n",
      "Iteration 371, loss = 0.35641011\n",
      "Iteration 372, loss = 0.35631634\n",
      "Iteration 373, loss = 0.35621385\n",
      "Iteration 374, loss = 0.35611935\n",
      "Iteration 375, loss = 0.35603516\n",
      "Iteration 376, loss = 0.35595746\n",
      "Iteration 377, loss = 0.35585486\n",
      "Iteration 378, loss = 0.35576133\n",
      "Iteration 379, loss = 0.35566953\n",
      "Iteration 380, loss = 0.35559771\n",
      "Iteration 381, loss = 0.35548292\n",
      "Iteration 382, loss = 0.35539962\n",
      "Iteration 383, loss = 0.35531691\n",
      "Iteration 384, loss = 0.35523345\n",
      "Iteration 385, loss = 0.35512437\n",
      "Iteration 386, loss = 0.35504055\n",
      "Iteration 387, loss = 0.35495288\n",
      "Iteration 388, loss = 0.35486744\n",
      "Iteration 389, loss = 0.35477867\n",
      "Iteration 390, loss = 0.35468967\n",
      "Iteration 391, loss = 0.35459770\n",
      "Iteration 392, loss = 0.35451730\n",
      "Iteration 393, loss = 0.35443535\n",
      "Iteration 394, loss = 0.35434445\n",
      "Iteration 395, loss = 0.35425734\n",
      "Iteration 396, loss = 0.35417114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75430924\n",
      "Iteration 2, loss = 0.75255765\n",
      "Iteration 3, loss = 0.74981182\n",
      "Iteration 4, loss = 0.74657558\n",
      "Iteration 5, loss = 0.74301190\n",
      "Iteration 6, loss = 0.73892333\n",
      "Iteration 7, loss = 0.73493566\n",
      "Iteration 8, loss = 0.73082628\n",
      "Iteration 9, loss = 0.72672846\n",
      "Iteration 10, loss = 0.72279607\n",
      "Iteration 11, loss = 0.71867820\n",
      "Iteration 12, loss = 0.71470969\n",
      "Iteration 13, loss = 0.71086629\n",
      "Iteration 14, loss = 0.70693820\n",
      "Iteration 15, loss = 0.70331655\n",
      "Iteration 16, loss = 0.69952433\n",
      "Iteration 17, loss = 0.69582896\n",
      "Iteration 18, loss = 0.69232060\n",
      "Iteration 19, loss = 0.68860719\n",
      "Iteration 20, loss = 0.68511448\n",
      "Iteration 21, loss = 0.68154892\n",
      "Iteration 22, loss = 0.67801807\n",
      "Iteration 23, loss = 0.67464640\n",
      "Iteration 24, loss = 0.67110043\n",
      "Iteration 25, loss = 0.66770396\n",
      "Iteration 26, loss = 0.66425589\n",
      "Iteration 27, loss = 0.66080842\n",
      "Iteration 28, loss = 0.65738700\n",
      "Iteration 29, loss = 0.65410252\n",
      "Iteration 30, loss = 0.65057349\n",
      "Iteration 31, loss = 0.64724041\n",
      "Iteration 32, loss = 0.64375449\n",
      "Iteration 33, loss = 0.64051933\n",
      "Iteration 34, loss = 0.63706528\n",
      "Iteration 35, loss = 0.63370411\n",
      "Iteration 36, loss = 0.63033126\n",
      "Iteration 37, loss = 0.62693327\n",
      "Iteration 38, loss = 0.62369061\n",
      "Iteration 39, loss = 0.62029856\n",
      "Iteration 40, loss = 0.61692087\n",
      "Iteration 41, loss = 0.61352528\n",
      "Iteration 42, loss = 0.61025775\n",
      "Iteration 43, loss = 0.60697376\n",
      "Iteration 44, loss = 0.60368557\n",
      "Iteration 45, loss = 0.60035156\n",
      "Iteration 46, loss = 0.59713544\n",
      "Iteration 47, loss = 0.59394013\n",
      "Iteration 48, loss = 0.59068451\n",
      "Iteration 49, loss = 0.58744028\n",
      "Iteration 50, loss = 0.58435551\n",
      "Iteration 51, loss = 0.58129291\n",
      "Iteration 52, loss = 0.57810822\n",
      "Iteration 53, loss = 0.57502565\n",
      "Iteration 54, loss = 0.57196718\n",
      "Iteration 55, loss = 0.56893513\n",
      "Iteration 56, loss = 0.56594697\n",
      "Iteration 57, loss = 0.56301208\n",
      "Iteration 58, loss = 0.55999411\n",
      "Iteration 59, loss = 0.55723481\n",
      "Iteration 60, loss = 0.55428871\n",
      "Iteration 61, loss = 0.55147931\n",
      "Iteration 62, loss = 0.54868783\n",
      "Iteration 63, loss = 0.54598769\n",
      "Iteration 64, loss = 0.54327393\n",
      "Iteration 65, loss = 0.54055261\n",
      "Iteration 66, loss = 0.53794858\n",
      "Iteration 67, loss = 0.53544789\n",
      "Iteration 68, loss = 0.53285635\n",
      "Iteration 69, loss = 0.53032682\n",
      "Iteration 70, loss = 0.52792332\n",
      "Iteration 71, loss = 0.52551946\n",
      "Iteration 72, loss = 0.52310513\n",
      "Iteration 73, loss = 0.52078634\n",
      "Iteration 74, loss = 0.51850308\n",
      "Iteration 75, loss = 0.51627069\n",
      "Iteration 76, loss = 0.51397424\n",
      "Iteration 77, loss = 0.51190892\n",
      "Iteration 78, loss = 0.50975498\n",
      "Iteration 79, loss = 0.50767364\n",
      "Iteration 80, loss = 0.50564756\n",
      "Iteration 81, loss = 0.50369352\n",
      "Iteration 82, loss = 0.50173929\n",
      "Iteration 83, loss = 0.49976110\n",
      "Iteration 84, loss = 0.49788961\n",
      "Iteration 85, loss = 0.49604971\n",
      "Iteration 86, loss = 0.49424587\n",
      "Iteration 87, loss = 0.49247136\n",
      "Iteration 88, loss = 0.49077821\n",
      "Iteration 89, loss = 0.48911966\n",
      "Iteration 90, loss = 0.48739179\n",
      "Iteration 91, loss = 0.48577443\n",
      "Iteration 92, loss = 0.48418890\n",
      "Iteration 93, loss = 0.48261275\n",
      "Iteration 94, loss = 0.48121227\n",
      "Iteration 95, loss = 0.47961845\n",
      "Iteration 96, loss = 0.47817279\n",
      "Iteration 97, loss = 0.47670416\n",
      "Iteration 98, loss = 0.47536494\n",
      "Iteration 99, loss = 0.47392729\n",
      "Iteration 100, loss = 0.47259098\n",
      "Iteration 101, loss = 0.47132986\n",
      "Iteration 102, loss = 0.47000688\n",
      "Iteration 103, loss = 0.46873232\n",
      "Iteration 104, loss = 0.46751816\n",
      "Iteration 105, loss = 0.46623018\n",
      "Iteration 106, loss = 0.46512084\n",
      "Iteration 107, loss = 0.46398799\n",
      "Iteration 108, loss = 0.46284491\n",
      "Iteration 109, loss = 0.46168537\n",
      "Iteration 110, loss = 0.46061605\n",
      "Iteration 111, loss = 0.45953884\n",
      "Iteration 112, loss = 0.45848495\n",
      "Iteration 113, loss = 0.45745945\n",
      "Iteration 114, loss = 0.45643055\n",
      "Iteration 115, loss = 0.45545295\n",
      "Iteration 116, loss = 0.45446208\n",
      "Iteration 117, loss = 0.45347241\n",
      "Iteration 118, loss = 0.45257849\n",
      "Iteration 119, loss = 0.45165247\n",
      "Iteration 120, loss = 0.45072361\n",
      "Iteration 121, loss = 0.44982935\n",
      "Iteration 122, loss = 0.44889158\n",
      "Iteration 123, loss = 0.44808886\n",
      "Iteration 124, loss = 0.44725600\n",
      "Iteration 125, loss = 0.44639066\n",
      "Iteration 126, loss = 0.44554002\n",
      "Iteration 127, loss = 0.44472098\n",
      "Iteration 128, loss = 0.44393055\n",
      "Iteration 129, loss = 0.44308671\n",
      "Iteration 130, loss = 0.44237973\n",
      "Iteration 131, loss = 0.44159269\n",
      "Iteration 132, loss = 0.44080404\n",
      "Iteration 133, loss = 0.44011338\n",
      "Iteration 134, loss = 0.43934935\n",
      "Iteration 135, loss = 0.43867641\n",
      "Iteration 136, loss = 0.43793453\n",
      "Iteration 137, loss = 0.43722997\n",
      "Iteration 138, loss = 0.43653408\n",
      "Iteration 139, loss = 0.43584738\n",
      "Iteration 140, loss = 0.43512322\n",
      "Iteration 141, loss = 0.43449164\n",
      "Iteration 142, loss = 0.43380706\n",
      "Iteration 143, loss = 0.43316252\n",
      "Iteration 144, loss = 0.43250940\n",
      "Iteration 145, loss = 0.43186076\n",
      "Iteration 146, loss = 0.43126030\n",
      "Iteration 147, loss = 0.43055598\n",
      "Iteration 148, loss = 0.42997531\n",
      "Iteration 149, loss = 0.42937994\n",
      "Iteration 150, loss = 0.42877740\n",
      "Iteration 151, loss = 0.42816661\n",
      "Iteration 152, loss = 0.42755168\n",
      "Iteration 153, loss = 0.42696793\n",
      "Iteration 154, loss = 0.42641668\n",
      "Iteration 155, loss = 0.42582419\n",
      "Iteration 156, loss = 0.42531239\n",
      "Iteration 157, loss = 0.42471147\n",
      "Iteration 158, loss = 0.42416983\n",
      "Iteration 159, loss = 0.42360204\n",
      "Iteration 160, loss = 0.42305225\n",
      "Iteration 161, loss = 0.42252645\n",
      "Iteration 162, loss = 0.42203340\n",
      "Iteration 163, loss = 0.42151025\n",
      "Iteration 164, loss = 0.42100859\n",
      "Iteration 165, loss = 0.42048857\n",
      "Iteration 166, loss = 0.41996153\n",
      "Iteration 167, loss = 0.41946022\n",
      "Iteration 168, loss = 0.41893248\n",
      "Iteration 169, loss = 0.41848652\n",
      "Iteration 170, loss = 0.41798113\n",
      "Iteration 171, loss = 0.41751190\n",
      "Iteration 172, loss = 0.41704175\n",
      "Iteration 173, loss = 0.41655408\n",
      "Iteration 174, loss = 0.41608218\n",
      "Iteration 175, loss = 0.41561833\n",
      "Iteration 176, loss = 0.41516507\n",
      "Iteration 177, loss = 0.41469930\n",
      "Iteration 178, loss = 0.41425285\n",
      "Iteration 179, loss = 0.41382398\n",
      "Iteration 180, loss = 0.41341452\n",
      "Iteration 181, loss = 0.41297125\n",
      "Iteration 182, loss = 0.41253274\n",
      "Iteration 183, loss = 0.41208097\n",
      "Iteration 184, loss = 0.41167672\n",
      "Iteration 185, loss = 0.41129340\n",
      "Iteration 186, loss = 0.41086034\n",
      "Iteration 187, loss = 0.41044549\n",
      "Iteration 188, loss = 0.41006152\n",
      "Iteration 189, loss = 0.40964906\n",
      "Iteration 190, loss = 0.40929977\n",
      "Iteration 191, loss = 0.40887283\n",
      "Iteration 192, loss = 0.40846146\n",
      "Iteration 193, loss = 0.40809097\n",
      "Iteration 194, loss = 0.40772567\n",
      "Iteration 195, loss = 0.40731217\n",
      "Iteration 196, loss = 0.40697559\n",
      "Iteration 197, loss = 0.40661309\n",
      "Iteration 198, loss = 0.40623774\n",
      "Iteration 199, loss = 0.40582328\n",
      "Iteration 200, loss = 0.40549287\n",
      "Iteration 201, loss = 0.40514390\n",
      "Iteration 202, loss = 0.40478154\n",
      "Iteration 203, loss = 0.40442485\n",
      "Iteration 204, loss = 0.40406921\n",
      "Iteration 205, loss = 0.40377276\n",
      "Iteration 206, loss = 0.40340640\n",
      "Iteration 207, loss = 0.40308299\n",
      "Iteration 208, loss = 0.40276059\n",
      "Iteration 209, loss = 0.40240688\n",
      "Iteration 210, loss = 0.40204619\n",
      "Iteration 211, loss = 0.40173206\n",
      "Iteration 212, loss = 0.40138445\n",
      "Iteration 213, loss = 0.40106915\n",
      "Iteration 214, loss = 0.40076102\n",
      "Iteration 215, loss = 0.40043269\n",
      "Iteration 216, loss = 0.40013521\n",
      "Iteration 217, loss = 0.39981628\n",
      "Iteration 218, loss = 0.39949492\n",
      "Iteration 219, loss = 0.39919837\n",
      "Iteration 220, loss = 0.39888839\n",
      "Iteration 221, loss = 0.39859495\n",
      "Iteration 222, loss = 0.39826854\n",
      "Iteration 223, loss = 0.39800036\n",
      "Iteration 224, loss = 0.39768003\n",
      "Iteration 225, loss = 0.39739201\n",
      "Iteration 226, loss = 0.39709615\n",
      "Iteration 227, loss = 0.39682708\n",
      "Iteration 228, loss = 0.39653490\n",
      "Iteration 229, loss = 0.39623981\n",
      "Iteration 230, loss = 0.39596201\n",
      "Iteration 231, loss = 0.39564689\n",
      "Iteration 232, loss = 0.39540764\n",
      "Iteration 233, loss = 0.39513260\n",
      "Iteration 234, loss = 0.39482969\n",
      "Iteration 235, loss = 0.39460547\n",
      "Iteration 236, loss = 0.39432506\n",
      "Iteration 237, loss = 0.39406364\n",
      "Iteration 238, loss = 0.39378168\n",
      "Iteration 239, loss = 0.39350431\n",
      "Iteration 240, loss = 0.39324542\n",
      "Iteration 241, loss = 0.39300978\n",
      "Iteration 242, loss = 0.39272089\n",
      "Iteration 243, loss = 0.39246771\n",
      "Iteration 244, loss = 0.39223051\n",
      "Iteration 245, loss = 0.39196601\n",
      "Iteration 246, loss = 0.39169706\n",
      "Iteration 247, loss = 0.39145385\n",
      "Iteration 248, loss = 0.39124177\n",
      "Iteration 249, loss = 0.39096826\n",
      "Iteration 250, loss = 0.39073696\n",
      "Iteration 251, loss = 0.39047538\n",
      "Iteration 252, loss = 0.39025805\n",
      "Iteration 253, loss = 0.39003742\n",
      "Iteration 254, loss = 0.38976519\n",
      "Iteration 255, loss = 0.38953666\n",
      "Iteration 256, loss = 0.38929138\n",
      "Iteration 257, loss = 0.38906278\n",
      "Iteration 258, loss = 0.38885484\n",
      "Iteration 259, loss = 0.38860200\n",
      "Iteration 260, loss = 0.38843082\n",
      "Iteration 261, loss = 0.38816353\n",
      "Iteration 262, loss = 0.38796651\n",
      "Iteration 263, loss = 0.38771900\n",
      "Iteration 264, loss = 0.38750565\n",
      "Iteration 265, loss = 0.38727407\n",
      "Iteration 266, loss = 0.38709161\n",
      "Iteration 267, loss = 0.38685953\n",
      "Iteration 268, loss = 0.38663707\n",
      "Iteration 269, loss = 0.38645111\n",
      "Iteration 270, loss = 0.38622204\n",
      "Iteration 271, loss = 0.38601847\n",
      "Iteration 272, loss = 0.38581725\n",
      "Iteration 273, loss = 0.38559447\n",
      "Iteration 274, loss = 0.38537951\n",
      "Iteration 275, loss = 0.38522442\n",
      "Iteration 276, loss = 0.38497234\n",
      "Iteration 277, loss = 0.38476928\n",
      "Iteration 278, loss = 0.38455483\n",
      "Iteration 279, loss = 0.38439052\n",
      "Iteration 280, loss = 0.38416196\n",
      "Iteration 281, loss = 0.38398080\n",
      "Iteration 282, loss = 0.38379377\n",
      "Iteration 283, loss = 0.38358428\n",
      "Iteration 284, loss = 0.38339105\n",
      "Iteration 285, loss = 0.38318367\n",
      "Iteration 286, loss = 0.38298457\n",
      "Iteration 287, loss = 0.38283367\n",
      "Iteration 288, loss = 0.38263103\n",
      "Iteration 289, loss = 0.38243603\n",
      "Iteration 307, loss = 0.33539054\n",
      "Iteration 308, loss = 0.33528965\n",
      "Iteration 309, loss = 0.33517582\n",
      "Iteration 310, loss = 0.33507744\n",
      "Iteration 311, loss = 0.33497936\n",
      "Iteration 312, loss = 0.33486554\n",
      "Iteration 313, loss = 0.33476861\n",
      "Iteration 314, loss = 0.33465559\n",
      "Iteration 315, loss = 0.33454053\n",
      "Iteration 316, loss = 0.33444069\n",
      "Iteration 317, loss = 0.33433779\n",
      "Iteration 318, loss = 0.33423704\n",
      "Iteration 319, loss = 0.33413348\n",
      "Iteration 320, loss = 0.33404352\n",
      "Iteration 321, loss = 0.33394241\n",
      "Iteration 322, loss = 0.33383988\n",
      "Iteration 323, loss = 0.33372283\n",
      "Iteration 324, loss = 0.33363823\n",
      "Iteration 325, loss = 0.33354349\n",
      "Iteration 326, loss = 0.33342546\n",
      "Iteration 327, loss = 0.33332466\n",
      "Iteration 328, loss = 0.33323199\n",
      "Iteration 329, loss = 0.33313974\n",
      "Iteration 330, loss = 0.33303069\n",
      "Iteration 331, loss = 0.33294906\n",
      "Iteration 332, loss = 0.33283988\n",
      "Iteration 333, loss = 0.33274545\n",
      "Iteration 334, loss = 0.33264244\n",
      "Iteration 335, loss = 0.33255763\n",
      "Iteration 336, loss = 0.33244610\n",
      "Iteration 337, loss = 0.33235742\n",
      "Iteration 338, loss = 0.33226225\n",
      "Iteration 339, loss = 0.33217222\n",
      "Iteration 340, loss = 0.33207850\n",
      "Iteration 341, loss = 0.33199031\n",
      "Iteration 342, loss = 0.33188693\n",
      "Iteration 343, loss = 0.33180750\n",
      "Iteration 344, loss = 0.33170211\n",
      "Iteration 345, loss = 0.33161531\n",
      "Iteration 346, loss = 0.33151702\n",
      "Iteration 347, loss = 0.33143142\n",
      "Iteration 348, loss = 0.33133572\n",
      "Iteration 349, loss = 0.33124687\n",
      "Iteration 350, loss = 0.33115147\n",
      "Iteration 351, loss = 0.33108219\n",
      "Iteration 352, loss = 0.33097427\n",
      "Iteration 353, loss = 0.33088382\n",
      "Iteration 354, loss = 0.33080506\n",
      "Iteration 355, loss = 0.33070129\n",
      "Iteration 356, loss = 0.33061606\n",
      "Iteration 357, loss = 0.33053445\n",
      "Iteration 358, loss = 0.33044495\n",
      "Iteration 359, loss = 0.33035120\n",
      "Iteration 360, loss = 0.33026914\n",
      "Iteration 361, loss = 0.33020231\n",
      "Iteration 362, loss = 0.33010263\n",
      "Iteration 363, loss = 0.33001182\n",
      "Iteration 364, loss = 0.32993737\n",
      "Iteration 365, loss = 0.32984512\n",
      "Iteration 366, loss = 0.32976019\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78555878\n",
      "Iteration 2, loss = 0.78094056\n",
      "Iteration 3, loss = 0.77383276\n",
      "Iteration 4, loss = 0.76512640\n",
      "Iteration 5, loss = 0.75544012\n",
      "Iteration 6, loss = 0.74537541\n",
      "Iteration 7, loss = 0.73424933\n",
      "Iteration 8, loss = 0.72379165\n",
      "Iteration 9, loss = 0.71275096\n",
      "Iteration 10, loss = 0.70254502\n",
      "Iteration 11, loss = 0.69180634\n",
      "Iteration 12, loss = 0.68189616\n",
      "Iteration 13, loss = 0.67213609\n",
      "Iteration 14, loss = 0.66242911\n",
      "Iteration 15, loss = 0.65337946\n",
      "Iteration 16, loss = 0.64449789\n",
      "Iteration 17, loss = 0.63593822\n",
      "Iteration 18, loss = 0.62776769\n",
      "Iteration 19, loss = 0.61969800\n",
      "Iteration 20, loss = 0.61208006\n",
      "Iteration 21, loss = 0.60468184\n",
      "Iteration 22, loss = 0.59759056\n",
      "Iteration 23, loss = 0.59061384\n",
      "Iteration 24, loss = 0.58391076\n",
      "Iteration 25, loss = 0.57766413\n",
      "Iteration 26, loss = 0.57148426\n",
      "Iteration 27, loss = 0.56533635\n",
      "Iteration 28, loss = 0.55969783\n",
      "Iteration 29, loss = 0.55417005\n",
      "Iteration 30, loss = 0.54870901\n",
      "Iteration 31, loss = 0.54360591\n",
      "Iteration 32, loss = 0.53845250\n",
      "Iteration 33, loss = 0.53363899\n",
      "Iteration 34, loss = 0.52882821\n",
      "Iteration 35, loss = 0.52438753\n",
      "Iteration 36, loss = 0.51990957\n",
      "Iteration 37, loss = 0.51568573\n",
      "Iteration 38, loss = 0.51150882\n",
      "Iteration 39, loss = 0.50748305\n",
      "Iteration 40, loss = 0.50350985\n",
      "Iteration 41, loss = 0.49989936\n",
      "Iteration 42, loss = 0.49627472\n",
      "Iteration 43, loss = 0.49271679\n",
      "Iteration 44, loss = 0.48927806\n",
      "Iteration 45, loss = 0.48595449\n",
      "Iteration 46, loss = 0.48276063\n",
      "Iteration 47, loss = 0.47959633\n",
      "Iteration 48, loss = 0.47652530\n",
      "Iteration 49, loss = 0.47362228\n",
      "Iteration 50, loss = 0.47079899\n",
      "Iteration 51, loss = 0.46792501\n",
      "Iteration 52, loss = 0.46526703\n",
      "Iteration 53, loss = 0.46266898\n",
      "Iteration 54, loss = 0.46007619\n",
      "Iteration 55, loss = 0.45758089\n",
      "Iteration 56, loss = 0.45516525\n",
      "Iteration 57, loss = 0.45281665\n",
      "Iteration 58, loss = 0.45054661\n",
      "Iteration 59, loss = 0.44836142\n",
      "Iteration 60, loss = 0.44614424\n",
      "Iteration 61, loss = 0.44396426\n",
      "Iteration 62, loss = 0.44197340\n",
      "Iteration 63, loss = 0.43995273\n",
      "Iteration 64, loss = 0.43800118\n",
      "Iteration 65, loss = 0.43608631\n",
      "Iteration 66, loss = 0.43420085\n",
      "Iteration 67, loss = 0.43241666\n",
      "Iteration 68, loss = 0.43066923\n",
      "Iteration 69, loss = 0.42891736\n",
      "Iteration 70, loss = 0.42727311\n",
      "Iteration 71, loss = 0.42568742\n",
      "Iteration 72, loss = 0.42402535\n",
      "Iteration 73, loss = 0.42245590\n",
      "Iteration 74, loss = 0.42094243\n",
      "Iteration 75, loss = 0.41951650\n",
      "Iteration 76, loss = 0.41800889\n",
      "Iteration 77, loss = 0.41659641\n",
      "Iteration 78, loss = 0.41523503\n",
      "Iteration 79, loss = 0.41391939\n",
      "Iteration 80, loss = 0.41256111\n",
      "Iteration 81, loss = 0.41128211\n",
      "Iteration 82, loss = 0.41001304\n",
      "Iteration 83, loss = 0.40881652\n",
      "Iteration 84, loss = 0.40761362\n",
      "Iteration 85, loss = 0.40643866\n",
      "Iteration 86, loss = 0.40532146\n",
      "Iteration 87, loss = 0.40421184\n",
      "Iteration 88, loss = 0.40310049\n",
      "Iteration 89, loss = 0.40204493\n",
      "Iteration 90, loss = 0.40099610\n",
      "Iteration 91, loss = 0.39996125\n",
      "Iteration 92, loss = 0.39892249\n",
      "Iteration 93, loss = 0.39797941\n",
      "Iteration 94, loss = 0.39702495\n",
      "Iteration 95, loss = 0.39609828\n",
      "Iteration 96, loss = 0.39515327\n",
      "Iteration 97, loss = 0.39429392\n",
      "Iteration 98, loss = 0.39336269\n",
      "Iteration 99, loss = 0.39252170\n",
      "Iteration 100, loss = 0.39164936\n",
      "Iteration 101, loss = 0.39082039\n",
      "Iteration 102, loss = 0.39003936\n",
      "Iteration 103, loss = 0.38924513\n",
      "Iteration 104, loss = 0.38838806\n",
      "Iteration 105, loss = 0.38762214\n",
      "Iteration 106, loss = 0.38688580\n",
      "Iteration 107, loss = 0.38612171\n",
      "Iteration 108, loss = 0.38538581\n",
      "Iteration 109, loss = 0.38470325\n",
      "Iteration 110, loss = 0.38403346\n",
      "Iteration 111, loss = 0.38329966\n",
      "Iteration 112, loss = 0.38259555\n",
      "Iteration 113, loss = 0.38194127\n",
      "Iteration 114, loss = 0.38129803\n",
      "Iteration 115, loss = 0.38063182\n",
      "Iteration 116, loss = 0.38001197\n",
      "Iteration 117, loss = 0.37940136\n",
      "Iteration 118, loss = 0.37874778\n",
      "Iteration 119, loss = 0.37821414\n",
      "Iteration 120, loss = 0.37756415\n",
      "Iteration 121, loss = 0.37699483\n",
      "Iteration 122, loss = 0.37639203\n",
      "Iteration 123, loss = 0.37587398\n",
      "Iteration 124, loss = 0.37533876\n",
      "Iteration 125, loss = 0.37475734\n",
      "Iteration 126, loss = 0.37422664\n",
      "Iteration 127, loss = 0.37368369\n",
      "Iteration 128, loss = 0.37318971\n",
      "Iteration 129, loss = 0.37267826\n",
      "Iteration 130, loss = 0.37215098\n",
      "Iteration 131, loss = 0.37165237\n",
      "Iteration 132, loss = 0.37115808\n",
      "Iteration 133, loss = 0.37068887\n",
      "Iteration 134, loss = 0.37021398\n",
      "Iteration 135, loss = 0.36976221\n",
      "Iteration 136, loss = 0.36927839\n",
      "Iteration 137, loss = 0.36883069\n",
      "Iteration 138, loss = 0.36836353\n",
      "Iteration 139, loss = 0.36795691\n",
      "Iteration 140, loss = 0.36752617\n",
      "Iteration 141, loss = 0.36705537\n",
      "Iteration 142, loss = 0.36663203\n",
      "Iteration 143, loss = 0.36623711\n",
      "Iteration 144, loss = 0.36582507\n",
      "Iteration 145, loss = 0.36540694\n",
      "Iteration 146, loss = 0.36500235\n",
      "Iteration 147, loss = 0.36458854\n",
      "Iteration 148, loss = 0.36419426\n",
      "Iteration 149, loss = 0.36380270\n",
      "Iteration 150, loss = 0.36345054\n",
      "Iteration 151, loss = 0.36304406\n",
      "Iteration 152, loss = 0.36268104\n",
      "Iteration 153, loss = 0.36231658\n",
      "Iteration 154, loss = 0.36196429\n",
      "Iteration 155, loss = 0.36155831\n",
      "Iteration 156, loss = 0.36122594\n",
      "Iteration 157, loss = 0.36089145\n",
      "Iteration 158, loss = 0.36053055\n",
      "Iteration 159, loss = 0.36021246\n",
      "Iteration 160, loss = 0.35984672\n",
      "Iteration 161, loss = 0.35951118\n",
      "Iteration 162, loss = 0.35917490\n",
      "Iteration 163, loss = 0.35885941\n",
      "Iteration 164, loss = 0.35851005\n",
      "Iteration 165, loss = 0.35820928\n",
      "Iteration 166, loss = 0.35787622\n",
      "Iteration 167, loss = 0.35758269\n",
      "Iteration 168, loss = 0.35728075\n",
      "Iteration 169, loss = 0.35694991\n",
      "Iteration 170, loss = 0.35666149\n",
      "Iteration 171, loss = 0.35634196\n",
      "Iteration 172, loss = 0.35607317\n",
      "Iteration 173, loss = 0.35574237\n",
      "Iteration 174, loss = 0.35548555\n",
      "Iteration 175, loss = 0.35517843\n",
      "Iteration 176, loss = 0.35492671\n",
      "Iteration 177, loss = 0.35461597\n",
      "Iteration 178, loss = 0.35435504\n",
      "Iteration 179, loss = 0.35407890\n",
      "Iteration 180, loss = 0.35379700\n",
      "Iteration 181, loss = 0.35352702\n",
      "Iteration 182, loss = 0.35326996\n",
      "Iteration 183, loss = 0.35298421\n",
      "Iteration 184, loss = 0.35272950\n",
      "Iteration 185, loss = 0.35248039\n",
      "Iteration 186, loss = 0.35221763\n",
      "Iteration 187, loss = 0.35197452\n",
      "Iteration 188, loss = 0.35169297\n",
      "Iteration 189, loss = 0.35145743\n",
      "Iteration 190, loss = 0.35121157\n",
      "Iteration 191, loss = 0.35098334\n",
      "Iteration 192, loss = 0.35072797\n",
      "Iteration 193, loss = 0.35049207\n",
      "Iteration 194, loss = 0.35023538\n",
      "Iteration 195, loss = 0.34999898\n",
      "Iteration 196, loss = 0.34976431\n",
      "Iteration 197, loss = 0.34953823\n",
      "Iteration 198, loss = 0.34931488\n",
      "Iteration 199, loss = 0.34907867\n",
      "Iteration 200, loss = 0.34884958\n",
      "Iteration 201, loss = 0.34862707\n",
      "Iteration 202, loss = 0.34842724\n",
      "Iteration 203, loss = 0.34818126\n",
      "Iteration 204, loss = 0.34795448\n",
      "Iteration 205, loss = 0.34775814\n",
      "Iteration 206, loss = 0.34753252\n",
      "Iteration 207, loss = 0.34731260\n",
      "Iteration 208, loss = 0.34709557\n",
      "Iteration 209, loss = 0.34689676\n",
      "Iteration 210, loss = 0.34667361\n",
      "Iteration 211, loss = 0.34646065\n",
      "Iteration 212, loss = 0.34625240\n",
      "Iteration 213, loss = 0.34605098\n",
      "Iteration 214, loss = 0.34584882\n",
      "Iteration 215, loss = 0.34565065\n",
      "Iteration 216, loss = 0.34544681\n",
      "Iteration 217, loss = 0.34525322\n",
      "Iteration 218, loss = 0.34503515\n",
      "Iteration 219, loss = 0.34485110\n",
      "Iteration 220, loss = 0.34467076\n",
      "Iteration 221, loss = 0.34446310\n",
      "Iteration 222, loss = 0.34425680\n",
      "Iteration 223, loss = 0.34407619\n",
      "Iteration 224, loss = 0.34389209\n",
      "Iteration 225, loss = 0.34369187\n",
      "Iteration 226, loss = 0.34350049\n",
      "Iteration 227, loss = 0.34332578\n",
      "Iteration 228, loss = 0.34312900\n",
      "Iteration 229, loss = 0.34295351\n",
      "Iteration 230, loss = 0.34275012\n",
      "Iteration 231, loss = 0.34257918\n",
      "Iteration 232, loss = 0.34239750\n",
      "Iteration 233, loss = 0.34222865\n",
      "Iteration 234, loss = 0.34203610\n",
      "Iteration 235, loss = 0.34186329\n",
      "Iteration 236, loss = 0.34169773\n",
      "Iteration 237, loss = 0.34153346\n",
      "Iteration 238, loss = 0.34133290\n",
      "Iteration 239, loss = 0.34116696\n",
      "Iteration 240, loss = 0.34099550\n",
      "Iteration 241, loss = 0.34082370\n",
      "Iteration 242, loss = 0.34065374\n",
      "Iteration 243, loss = 0.34049846\n",
      "Iteration 244, loss = 0.34031075\n",
      "Iteration 245, loss = 0.34014897\n",
      "Iteration 246, loss = 0.34000290\n",
      "Iteration 247, loss = 0.33982152\n",
      "Iteration 248, loss = 0.33965833\n",
      "Iteration 249, loss = 0.33950226\n",
      "Iteration 250, loss = 0.33933610\n",
      "Iteration 251, loss = 0.33917345\n",
      "Iteration 252, loss = 0.33901961\n",
      "Iteration 253, loss = 0.33888732\n",
      "Iteration 254, loss = 0.33871137\n",
      "Iteration 255, loss = 0.33854938\n",
      "Iteration 256, loss = 0.33839151\n",
      "Iteration 257, loss = 0.33825145\n",
      "Iteration 258, loss = 0.33808070\n",
      "Iteration 259, loss = 0.33794386\n",
      "Iteration 260, loss = 0.33778098\n",
      "Iteration 261, loss = 0.33763324\n",
      "Iteration 262, loss = 0.33748929\n",
      "Iteration 263, loss = 0.33733164\n",
      "Iteration 264, loss = 0.33718880\n",
      "Iteration 265, loss = 0.33705181\n",
      "Iteration 266, loss = 0.33689687\n",
      "Iteration 267, loss = 0.33675595\n",
      "Iteration 268, loss = 0.33661061\n",
      "Iteration 269, loss = 0.33647831\n",
      "Iteration 270, loss = 0.33632533\n",
      "Iteration 271, loss = 0.33619240\n",
      "Iteration 272, loss = 0.33604228\n",
      "Iteration 273, loss = 0.33591053\n",
      "Iteration 274, loss = 0.33576056\n",
      "Iteration 275, loss = 0.33562693\n",
      "Iteration 276, loss = 0.33548487\n",
      "Iteration 277, loss = 0.33534232\n",
      "Iteration 278, loss = 0.33522710\n",
      "Iteration 279, loss = 0.33506885\n",
      "Iteration 280, loss = 0.33494709\n",
      "Iteration 281, loss = 0.33480633\n",
      "Iteration 282, loss = 0.33467514\n",
      "Iteration 283, loss = 0.33453707\n",
      "Iteration 284, loss = 0.33440624\n",
      "Iteration 285, loss = 0.33428789\n",
      "Iteration 286, loss = 0.33415240\n",
      "Iteration 287, loss = 0.33401732\n",
      "Iteration 288, loss = 0.33389153\n",
      "Iteration 289, loss = 0.33376010\n",
      "Iteration 290, loss = 0.33363999\n",
      "Iteration 291, loss = 0.33350571\n",
      "Iteration 292, loss = 0.33336915\n",
      "Iteration 293, loss = 0.33324745\n",
      "Iteration 294, loss = 0.33313077\n",
      "Iteration 295, loss = 0.33299975\n",
      "Iteration 296, loss = 0.33287786\n",
      "Iteration 297, loss = 0.33274841\n",
      "Iteration 298, loss = 0.33262182\n",
      "Iteration 299, loss = 0.33251378\n",
      "Iteration 300, loss = 0.33238857\n",
      "Iteration 301, loss = 0.33226963\n",
      "Iteration 302, loss = 0.33213113\n",
      "Iteration 303, loss = 0.33201240\n",
      "Iteration 304, loss = 0.33191893\n",
      "Iteration 305, loss = 0.33177629\n",
      "Iteration 306, loss = 0.33166171\n",
      "Iteration 307, loss = 0.33155346\n",
      "Iteration 308, loss = 0.33143816\n",
      "Iteration 309, loss = 0.33130687\n",
      "Iteration 310, loss = 0.33120398\n",
      "Iteration 311, loss = 0.33106701\n",
      "Iteration 312, loss = 0.33096476\n",
      "Iteration 313, loss = 0.33085622\n",
      "Iteration 314, loss = 0.33074347\n",
      "Iteration 315, loss = 0.33060577\n",
      "Iteration 316, loss = 0.33049834\n",
      "Iteration 317, loss = 0.33037348\n",
      "Iteration 318, loss = 0.33027593\n",
      "Iteration 319, loss = 0.33015544\n",
      "Iteration 320, loss = 0.33004353\n",
      "Iteration 321, loss = 0.32993861\n",
      "Iteration 322, loss = 0.32982260\n",
      "Iteration 323, loss = 0.32971991\n",
      "Iteration 324, loss = 0.32959815\n",
      "Iteration 325, loss = 0.32949801\n",
      "Iteration 326, loss = 0.32937748\n",
      "Iteration 327, loss = 0.32926762\n",
      "Iteration 328, loss = 0.32917046\n",
      "Iteration 329, loss = 0.32905078\n",
      "Iteration 330, loss = 0.32895324\n",
      "Iteration 331, loss = 0.32883925\n",
      "Iteration 332, loss = 0.32872484\n",
      "Iteration 333, loss = 0.32861438\n",
      "Iteration 334, loss = 0.32852232\n",
      "Iteration 335, loss = 0.32840533\n",
      "Iteration 336, loss = 0.32830349\n",
      "Iteration 337, loss = 0.32819520\n",
      "Iteration 338, loss = 0.32808966\n",
      "Iteration 339, loss = 0.32798472\n",
      "Iteration 340, loss = 0.32789278\n",
      "Iteration 341, loss = 0.32777386\n",
      "Iteration 342, loss = 0.32767657\n",
      "Iteration 343, loss = 0.32757646\n",
      "Iteration 344, loss = 0.32748893\n",
      "Iteration 345, loss = 0.32738832\n",
      "Iteration 346, loss = 0.32727247\n",
      "Iteration 347, loss = 0.32717423\n",
      "Iteration 348, loss = 0.32706868\n",
      "Iteration 349, loss = 0.32696306\n",
      "Iteration 350, loss = 0.32686713\n",
      "Iteration 351, loss = 0.32677577\n",
      "Iteration 352, loss = 0.32667213\n",
      "Iteration 353, loss = 0.32657395\n",
      "Iteration 354, loss = 0.32646902\n",
      "Iteration 355, loss = 0.32638408\n",
      "Iteration 356, loss = 0.32628169\n",
      "Iteration 357, loss = 0.32619710\n",
      "Iteration 358, loss = 0.32608863\n",
      "Iteration 359, loss = 0.32598815\n",
      "Iteration 360, loss = 0.32589001\n",
      "Iteration 361, loss = 0.32579527\n",
      "Iteration 362, loss = 0.32569786\n",
      "Iteration 363, loss = 0.32560147\n",
      "Iteration 364, loss = 0.32550583\n",
      "Iteration 365, loss = 0.32541209\n",
      "Iteration 366, loss = 0.32532902\n",
      "Iteration 367, loss = 0.32522292\n",
      "Iteration 368, loss = 0.32513817\n",
      "Iteration 369, loss = 0.32503607\n",
      "Iteration 370, loss = 0.32494359\n",
      "Iteration 371, loss = 0.32484904\n",
      "Iteration 372, loss = 0.32476695\n",
      "Iteration 373, loss = 0.32466597\n",
      "Iteration 374, loss = 0.32458337\n",
      "Iteration 375, loss = 0.32448777\n",
      "Iteration 376, loss = 0.32441078\n",
      "Iteration 377, loss = 0.32431424\n",
      "Iteration 378, loss = 0.32421215\n",
      "Iteration 379, loss = 0.32412426\n",
      "Iteration 380, loss = 0.32404348\n",
      "Iteration 381, loss = 0.32394849\n",
      "Iteration 382, loss = 0.32386698\n",
      "Iteration 383, loss = 0.32377824\n",
      "Iteration 384, loss = 0.32369268\n",
      "Iteration 385, loss = 0.32359731\n",
      "Iteration 386, loss = 0.32351171\n",
      "Iteration 387, loss = 0.32342464\n",
      "Iteration 388, loss = 0.32333372\n",
      "Iteration 389, loss = 0.32326968\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74786344\n",
      "Iteration 2, loss = 0.74343908\n",
      "Iteration 3, loss = 0.73683623\n",
      "Iteration 4, loss = 0.72860864\n",
      "Iteration 5, loss = 0.71972980\n",
      "Iteration 6, loss = 0.70988469\n",
      "Iteration 7, loss = 0.70041802\n",
      "Iteration 8, loss = 0.69052987\n",
      "Iteration 9, loss = 0.68096121\n",
      "Iteration 10, loss = 0.67146958\n",
      "Iteration 11, loss = 0.66219540\n",
      "Iteration 12, loss = 0.65326182\n",
      "Iteration 13, loss = 0.64475911\n",
      "Iteration 14, loss = 0.63666278\n",
      "Iteration 15, loss = 0.62867424\n",
      "Iteration 16, loss = 0.62099183\n",
      "Iteration 17, loss = 0.61371723\n",
      "Iteration 18, loss = 0.60665508\n",
      "Iteration 19, loss = 0.59980297\n",
      "Iteration 20, loss = 0.59329544\n",
      "Iteration 21, loss = 0.58696311\n",
      "Iteration 22, loss = 0.58094088\n",
      "Iteration 23, loss = 0.57509957\n",
      "Iteration 24, loss = 0.56931498\n",
      "Iteration 25, loss = 0.56374482\n",
      "Iteration 26, loss = 0.55865791\n",
      "Iteration 27, loss = 0.55355892\n",
      "Iteration 28, loss = 0.54846814\n",
      "Iteration 29, loss = 0.54377196\n",
      "Iteration 30, loss = 0.53902211\n",
      "Iteration 31, loss = 0.53452626\n",
      "Iteration 32, loss = 0.53028088\n",
      "Iteration 33, loss = 0.52593573\n",
      "Iteration 34, loss = 0.52183264\n",
      "Iteration 35, loss = 0.51783101\n",
      "Iteration 36, loss = 0.51393920\n",
      "Iteration 37, loss = 0.51019315\n",
      "Iteration 38, loss = 0.50653291\n",
      "Iteration 39, loss = 0.50302048\n",
      "Iteration 40, loss = 0.49950574\n",
      "Iteration 41, loss = 0.49614284\n",
      "Iteration 42, loss = 0.49273998\n",
      "Iteration 43, loss = 0.48965771\n",
      "Iteration 44, loss = 0.48649754\n",
      "Iteration 45, loss = 0.48349591\n",
      "Iteration 46, loss = 0.48060425\n",
      "Iteration 235, loss = 0.36452675\n",
      "Iteration 236, loss = 0.36433771\n",
      "Iteration 237, loss = 0.36415378\n",
      "Iteration 238, loss = 0.36394767\n",
      "Iteration 239, loss = 0.36375609\n",
      "Iteration 240, loss = 0.36356952\n",
      "Iteration 241, loss = 0.36338108\n",
      "Iteration 242, loss = 0.36319091\n",
      "Iteration 243, loss = 0.36300770\n",
      "Iteration 244, loss = 0.36283124\n",
      "Iteration 245, loss = 0.36264609\n",
      "Iteration 246, loss = 0.36244954\n",
      "Iteration 247, loss = 0.36229334\n",
      "Iteration 248, loss = 0.36210770\n",
      "Iteration 249, loss = 0.36191945\n",
      "Iteration 250, loss = 0.36174769\n",
      "Iteration 251, loss = 0.36158219\n",
      "Iteration 252, loss = 0.36139733\n",
      "Iteration 253, loss = 0.36122901\n",
      "Iteration 254, loss = 0.36103864\n",
      "Iteration 255, loss = 0.36088801\n",
      "Iteration 256, loss = 0.36070629\n",
      "Iteration 257, loss = 0.36054736\n",
      "Iteration 258, loss = 0.36036598\n",
      "Iteration 259, loss = 0.36020706\n",
      "Iteration 260, loss = 0.36004940\n",
      "Iteration 261, loss = 0.35988748\n",
      "Iteration 262, loss = 0.35969548\n",
      "Iteration 263, loss = 0.35954240\n",
      "Iteration 264, loss = 0.35938517\n",
      "Iteration 265, loss = 0.35922123\n",
      "Iteration 266, loss = 0.35905248\n",
      "Iteration 267, loss = 0.35890020\n",
      "Iteration 268, loss = 0.35873921\n",
      "Iteration 269, loss = 0.35859317\n",
      "Iteration 270, loss = 0.35841276\n",
      "Iteration 271, loss = 0.35826753\n",
      "Iteration 272, loss = 0.35810783\n",
      "Iteration 273, loss = 0.35794458\n",
      "Iteration 274, loss = 0.35781717\n",
      "Iteration 275, loss = 0.35765508\n",
      "Iteration 276, loss = 0.35748682\n",
      "Iteration 277, loss = 0.35735082\n",
      "Iteration 278, loss = 0.35718980\n",
      "Iteration 279, loss = 0.35704898\n",
      "Iteration 280, loss = 0.35688990\n",
      "Iteration 281, loss = 0.35674520\n",
      "Iteration 282, loss = 0.35659945\n",
      "Iteration 283, loss = 0.35645429\n",
      "Iteration 284, loss = 0.35630609\n",
      "Iteration 285, loss = 0.35616267\n",
      "Iteration 286, loss = 0.35601823\n",
      "Iteration 287, loss = 0.35588024\n",
      "Iteration 288, loss = 0.35574045\n",
      "Iteration 289, loss = 0.35558701\n",
      "Iteration 290, loss = 0.35545826\n",
      "Iteration 291, loss = 0.35530673\n",
      "Iteration 292, loss = 0.35517445\n",
      "Iteration 293, loss = 0.35503479\n",
      "Iteration 294, loss = 0.35489617\n",
      "Iteration 295, loss = 0.35475467\n",
      "Iteration 296, loss = 0.35462115\n",
      "Iteration 297, loss = 0.35448920\n",
      "Iteration 298, loss = 0.35434720\n",
      "Iteration 299, loss = 0.35421884\n",
      "Iteration 300, loss = 0.35407072\n",
      "Iteration 301, loss = 0.35394176\n",
      "Iteration 302, loss = 0.35382269\n",
      "Iteration 303, loss = 0.35368084\n",
      "Iteration 304, loss = 0.35355007\n",
      "Iteration 305, loss = 0.35342185\n",
      "Iteration 306, loss = 0.35329684\n",
      "Iteration 307, loss = 0.35316053\n",
      "Iteration 308, loss = 0.35302983\n",
      "Iteration 309, loss = 0.35291423\n",
      "Iteration 310, loss = 0.35277116\n",
      "Iteration 311, loss = 0.35265125\n",
      "Iteration 312, loss = 0.35253254\n",
      "Iteration 313, loss = 0.35239185\n",
      "Iteration 314, loss = 0.35227933\n",
      "Iteration 315, loss = 0.35214519\n",
      "Iteration 316, loss = 0.35203236\n",
      "Iteration 317, loss = 0.35190284\n",
      "Iteration 318, loss = 0.35178385\n",
      "Iteration 319, loss = 0.35165924\n",
      "Iteration 320, loss = 0.35152615\n",
      "Iteration 321, loss = 0.35140222\n",
      "Iteration 322, loss = 0.35128649\n",
      "Iteration 323, loss = 0.35116527\n",
      "Iteration 324, loss = 0.35105831\n",
      "Iteration 325, loss = 0.35092694\n",
      "Iteration 326, loss = 0.35080935\n",
      "Iteration 327, loss = 0.35070390\n",
      "Iteration 328, loss = 0.35057548\n",
      "Iteration 329, loss = 0.35045695\n",
      "Iteration 330, loss = 0.35033862\n",
      "Iteration 331, loss = 0.35022863\n",
      "Iteration 332, loss = 0.35011321\n",
      "Iteration 333, loss = 0.35000087\n",
      "Iteration 334, loss = 0.34988057\n",
      "Iteration 335, loss = 0.34977210\n",
      "Iteration 336, loss = 0.34965442\n",
      "Iteration 337, loss = 0.34955032\n",
      "Iteration 338, loss = 0.34941957\n",
      "Iteration 339, loss = 0.34931086\n",
      "Iteration 340, loss = 0.34919995\n",
      "Iteration 341, loss = 0.34909028\n",
      "Iteration 342, loss = 0.34898219\n",
      "Iteration 343, loss = 0.34886743\n",
      "Iteration 344, loss = 0.34876130\n",
      "Iteration 345, loss = 0.34865715\n",
      "Iteration 346, loss = 0.34853114\n",
      "Iteration 347, loss = 0.34843139\n",
      "Iteration 348, loss = 0.34832838\n",
      "Iteration 349, loss = 0.34821741\n",
      "Iteration 350, loss = 0.34811416\n",
      "Iteration 351, loss = 0.34800596\n",
      "Iteration 352, loss = 0.34789823\n",
      "Iteration 353, loss = 0.34780081\n",
      "Iteration 354, loss = 0.34768880\n",
      "Iteration 355, loss = 0.34757810\n",
      "Iteration 356, loss = 0.34747477\n",
      "Iteration 357, loss = 0.34737994\n",
      "Iteration 358, loss = 0.34726734\n",
      "Iteration 359, loss = 0.34717701\n",
      "Iteration 360, loss = 0.34706223\n",
      "Iteration 361, loss = 0.34696189\n",
      "Iteration 362, loss = 0.34686300\n",
      "Iteration 363, loss = 0.34676715\n",
      "Iteration 364, loss = 0.34665441\n",
      "Iteration 365, loss = 0.34655469\n",
      "Iteration 366, loss = 0.34645627\n",
      "Iteration 367, loss = 0.34636134\n",
      "Iteration 368, loss = 0.34627880\n",
      "Iteration 369, loss = 0.34617491\n",
      "Iteration 370, loss = 0.34606778\n",
      "Iteration 371, loss = 0.34596736\n",
      "Iteration 372, loss = 0.34587021\n",
      "Iteration 373, loss = 0.34578295\n",
      "Iteration 374, loss = 0.34567673\n",
      "Iteration 375, loss = 0.34560240\n",
      "Iteration 376, loss = 0.34548433\n",
      "Iteration 377, loss = 0.34540330\n",
      "Iteration 378, loss = 0.34530235\n",
      "Iteration 379, loss = 0.34520775\n",
      "Iteration 380, loss = 0.34510933\n",
      "Iteration 381, loss = 0.34503294\n",
      "Iteration 382, loss = 0.34492477\n",
      "Iteration 383, loss = 0.34483252\n",
      "Iteration 384, loss = 0.34474656\n",
      "Iteration 385, loss = 0.34464405\n",
      "Iteration 386, loss = 0.34456249\n",
      "Iteration 387, loss = 0.34447697\n",
      "Iteration 388, loss = 0.34437306\n",
      "Iteration 389, loss = 0.34428943\n",
      "Iteration 390, loss = 0.34418974\n",
      "Iteration 391, loss = 0.34411230\n",
      "Iteration 392, loss = 0.34402751\n",
      "Iteration 393, loss = 0.34392884\n",
      "Iteration 394, loss = 0.34383991\n",
      "Iteration 395, loss = 0.34376470\n",
      "Iteration 396, loss = 0.34367016\n",
      "Iteration 397, loss = 0.34358036\n",
      "Iteration 398, loss = 0.34349581\n",
      "Iteration 399, loss = 0.34341033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79133307\n",
      "Iteration 2, loss = 0.78828845\n",
      "Iteration 3, loss = 0.78366088\n",
      "Iteration 4, loss = 0.77801732\n",
      "Iteration 5, loss = 0.77190321\n",
      "Iteration 6, loss = 0.76526768\n",
      "Iteration 7, loss = 0.75853960\n",
      "Iteration 8, loss = 0.75167313\n",
      "Iteration 9, loss = 0.74498603\n",
      "Iteration 10, loss = 0.73861882\n",
      "Iteration 11, loss = 0.73201465\n",
      "Iteration 12, loss = 0.72548474\n",
      "Iteration 13, loss = 0.71934960\n",
      "Iteration 14, loss = 0.71329665\n",
      "Iteration 15, loss = 0.70763908\n",
      "Iteration 16, loss = 0.70226368\n",
      "Iteration 17, loss = 0.69677009\n",
      "Iteration 18, loss = 0.69161974\n",
      "Iteration 19, loss = 0.68667869\n",
      "Iteration 20, loss = 0.68169295\n",
      "Iteration 21, loss = 0.67686844\n",
      "Iteration 22, loss = 0.67222863\n",
      "Iteration 23, loss = 0.66783141\n",
      "Iteration 24, loss = 0.66335265\n",
      "Iteration 25, loss = 0.65930361\n",
      "Iteration 26, loss = 0.65483984\n",
      "Iteration 27, loss = 0.65097260\n",
      "Iteration 28, loss = 0.64687313\n",
      "Iteration 29, loss = 0.64297971\n",
      "Iteration 30, loss = 0.63897021\n",
      "Iteration 31, loss = 0.63533744\n",
      "Iteration 32, loss = 0.63154300\n",
      "Iteration 33, loss = 0.62781283\n",
      "Iteration 34, loss = 0.62422768\n",
      "Iteration 35, loss = 0.62058658\n",
      "Iteration 36, loss = 0.61708319\n",
      "Iteration 37, loss = 0.61364334\n",
      "Iteration 38, loss = 0.61027449\n",
      "Iteration 39, loss = 0.60675189\n",
      "Iteration 40, loss = 0.60352772\n",
      "Iteration 41, loss = 0.60016585\n",
      "Iteration 42, loss = 0.59705017\n",
      "Iteration 43, loss = 0.59390847\n",
      "Iteration 44, loss = 0.59075358\n",
      "Iteration 45, loss = 0.58769199\n",
      "Iteration 46, loss = 0.58468214\n",
      "Iteration 47, loss = 0.58169581\n",
      "Iteration 48, loss = 0.57872232\n",
      "Iteration 49, loss = 0.57588989\n",
      "Iteration 50, loss = 0.57301466\n",
      "Iteration 51, loss = 0.57019149\n",
      "Iteration 52, loss = 0.56748598\n",
      "Iteration 53, loss = 0.56479590\n",
      "Iteration 54, loss = 0.56206542\n",
      "Iteration 55, loss = 0.55940138\n",
      "Iteration 56, loss = 0.55676177\n",
      "Iteration 57, loss = 0.55426305\n",
      "Iteration 58, loss = 0.55169393\n",
      "Iteration 59, loss = 0.54916022\n",
      "Iteration 60, loss = 0.54679454\n",
      "Iteration 61, loss = 0.54428102\n",
      "Iteration 62, loss = 0.54205238\n",
      "Iteration 63, loss = 0.53952488\n",
      "Iteration 64, loss = 0.53717729\n",
      "Iteration 65, loss = 0.53501565\n",
      "Iteration 66, loss = 0.53272707\n",
      "Iteration 67, loss = 0.53054455\n",
      "Iteration 68, loss = 0.52837997\n",
      "Iteration 69, loss = 0.52623203\n",
      "Iteration 70, loss = 0.52412135\n",
      "Iteration 71, loss = 0.52199811\n",
      "Iteration 72, loss = 0.52000863\n",
      "Iteration 73, loss = 0.51802646\n",
      "Iteration 74, loss = 0.51602733\n",
      "Iteration 75, loss = 0.51408699\n",
      "Iteration 76, loss = 0.51218596\n",
      "Iteration 77, loss = 0.51031302\n",
      "Iteration 78, loss = 0.50849381\n",
      "Iteration 79, loss = 0.50667866\n",
      "Iteration 80, loss = 0.50490037\n",
      "Iteration 81, loss = 0.50309225\n",
      "Iteration 82, loss = 0.50144322\n",
      "Iteration 83, loss = 0.49972455\n",
      "Iteration 84, loss = 0.49808243\n",
      "Iteration 85, loss = 0.49646379\n",
      "Iteration 86, loss = 0.49488439\n",
      "Iteration 87, loss = 0.49333484\n",
      "Iteration 88, loss = 0.49177497\n",
      "Iteration 89, loss = 0.49025088\n",
      "Iteration 90, loss = 0.48875664\n",
      "Iteration 91, loss = 0.48728124\n",
      "Iteration 92, loss = 0.48590983\n",
      "Iteration 93, loss = 0.48444658\n",
      "Iteration 94, loss = 0.48299688\n",
      "Iteration 95, loss = 0.48162616\n",
      "Iteration 96, loss = 0.48030860\n",
      "Iteration 97, loss = 0.47899810\n",
      "Iteration 98, loss = 0.47767581\n",
      "Iteration 99, loss = 0.47637636\n",
      "Iteration 100, loss = 0.47508109\n",
      "Iteration 101, loss = 0.47387268\n",
      "Iteration 102, loss = 0.47259216\n",
      "Iteration 103, loss = 0.47142620\n",
      "Iteration 104, loss = 0.47024458\n",
      "Iteration 105, loss = 0.46913418\n",
      "Iteration 106, loss = 0.46787563\n",
      "Iteration 107, loss = 0.46678466\n",
      "Iteration 108, loss = 0.46568418\n",
      "Iteration 109, loss = 0.46453287\n",
      "Iteration 110, loss = 0.46355712\n",
      "Iteration 111, loss = 0.46245481\n",
      "Iteration 112, loss = 0.46140616\n",
      "Iteration 113, loss = 0.46038096\n",
      "Iteration 114, loss = 0.45938202\n",
      "Iteration 115, loss = 0.45840875\n",
      "Iteration 116, loss = 0.45742991\n",
      "Iteration 117, loss = 0.45647095\n",
      "Iteration 118, loss = 0.45555008\n",
      "Iteration 119, loss = 0.45461732\n",
      "Iteration 120, loss = 0.45366448\n",
      "Iteration 121, loss = 0.45275794\n",
      "Iteration 122, loss = 0.45191941\n",
      "Iteration 123, loss = 0.45108186\n",
      "Iteration 124, loss = 0.45014740\n",
      "Iteration 125, loss = 0.44932321\n",
      "Iteration 126, loss = 0.44842969\n",
      "Iteration 127, loss = 0.44762971\n",
      "Iteration 128, loss = 0.44680765\n",
      "Iteration 129, loss = 0.44597079\n",
      "Iteration 130, loss = 0.44522578\n",
      "Iteration 131, loss = 0.44441589\n",
      "Iteration 132, loss = 0.44359363\n",
      "Iteration 133, loss = 0.44290327\n",
      "Iteration 134, loss = 0.44208675\n",
      "Iteration 135, loss = 0.44131214\n",
      "Iteration 136, loss = 0.44061621\n",
      "Iteration 137, loss = 0.43982956\n",
      "Iteration 138, loss = 0.43908241\n",
      "Iteration 139, loss = 0.43840693\n",
      "Iteration 140, loss = 0.43768413\n",
      "Iteration 141, loss = 0.43702758\n",
      "Iteration 142, loss = 0.43627666\n",
      "Iteration 143, loss = 0.43561579\n",
      "Iteration 144, loss = 0.43490429\n",
      "Iteration 145, loss = 0.43427468\n",
      "Iteration 146, loss = 0.43354336\n",
      "Iteration 147, loss = 0.43293445\n",
      "Iteration 148, loss = 0.43228030\n",
      "Iteration 149, loss = 0.43163893\n",
      "Iteration 150, loss = 0.43099829\n",
      "Iteration 151, loss = 0.43034594\n",
      "Iteration 152, loss = 0.42972718\n",
      "Iteration 153, loss = 0.42912198\n",
      "Iteration 154, loss = 0.42851386\n",
      "Iteration 155, loss = 0.42790910\n",
      "Iteration 156, loss = 0.42733776\n",
      "Iteration 157, loss = 0.42672246\n",
      "Iteration 158, loss = 0.42612265\n",
      "Iteration 159, loss = 0.42554316\n",
      "Iteration 160, loss = 0.42495308\n",
      "Iteration 161, loss = 0.42435300\n",
      "Iteration 162, loss = 0.42379807\n",
      "Iteration 163, loss = 0.42325949\n",
      "Iteration 164, loss = 0.42264130\n",
      "Iteration 165, loss = 0.42211531\n",
      "Iteration 166, loss = 0.42158375\n",
      "Iteration 167, loss = 0.42098523\n",
      "Iteration 168, loss = 0.42045176\n",
      "Iteration 169, loss = 0.41988034\n",
      "Iteration 170, loss = 0.41936477\n",
      "Iteration 171, loss = 0.41882624\n",
      "Iteration 172, loss = 0.41831982\n",
      "Iteration 173, loss = 0.41777841\n",
      "Iteration 174, loss = 0.41726555\n",
      "Iteration 175, loss = 0.41680233\n",
      "Iteration 176, loss = 0.41625440\n",
      "Iteration 177, loss = 0.41571276\n",
      "Iteration 178, loss = 0.41522717\n",
      "Iteration 179, loss = 0.41472264\n",
      "Iteration 180, loss = 0.41424463\n",
      "Iteration 181, loss = 0.41374534\n",
      "Iteration 182, loss = 0.41327374\n",
      "Iteration 183, loss = 0.41277729\n",
      "Iteration 184, loss = 0.41228134\n",
      "Iteration 185, loss = 0.41180200\n",
      "Iteration 186, loss = 0.41133508\n",
      "Iteration 187, loss = 0.41085979\n",
      "Iteration 188, loss = 0.41041944\n",
      "Iteration 189, loss = 0.40994441\n",
      "Iteration 190, loss = 0.40949623\n",
      "Iteration 191, loss = 0.40901788\n",
      "Iteration 192, loss = 0.40856245\n",
      "Iteration 193, loss = 0.40812866\n",
      "Iteration 194, loss = 0.40768116\n",
      "Iteration 195, loss = 0.40725838\n",
      "Iteration 196, loss = 0.40680366\n",
      "Iteration 197, loss = 0.40641797\n",
      "Iteration 198, loss = 0.40596110\n",
      "Iteration 199, loss = 0.40550509\n",
      "Iteration 200, loss = 0.40517965\n",
      "Iteration 201, loss = 0.40465993\n",
      "Iteration 202, loss = 0.40426147\n",
      "Iteration 203, loss = 0.40383009\n",
      "Iteration 204, loss = 0.40341460\n",
      "Iteration 205, loss = 0.40299530\n",
      "Iteration 206, loss = 0.40259887\n",
      "Iteration 207, loss = 0.40223070\n",
      "Iteration 208, loss = 0.40190062\n",
      "Iteration 209, loss = 0.40142060\n",
      "Iteration 210, loss = 0.40104804\n",
      "Iteration 211, loss = 0.40067579\n",
      "Iteration 212, loss = 0.40028594\n",
      "Iteration 213, loss = 0.39985201\n",
      "Iteration 214, loss = 0.39947297\n",
      "Iteration 215, loss = 0.39912207\n",
      "Iteration 216, loss = 0.39873499\n",
      "Iteration 217, loss = 0.39837484\n",
      "Iteration 218, loss = 0.39801306\n",
      "Iteration 219, loss = 0.39762872\n",
      "Iteration 220, loss = 0.39731908\n",
      "Iteration 221, loss = 0.39694249\n",
      "Iteration 222, loss = 0.39656682\n",
      "Iteration 223, loss = 0.39625357\n",
      "Iteration 224, loss = 0.39587990\n",
      "Iteration 225, loss = 0.39553750\n",
      "Iteration 226, loss = 0.39519568\n",
      "Iteration 227, loss = 0.39486528\n",
      "Iteration 228, loss = 0.39450741\n",
      "Iteration 229, loss = 0.39415561\n",
      "Iteration 230, loss = 0.39383726\n",
      "Iteration 231, loss = 0.39351985\n",
      "Iteration 232, loss = 0.39317088\n",
      "Iteration 233, loss = 0.39287627\n",
      "Iteration 234, loss = 0.39252964\n",
      "Iteration 235, loss = 0.39218941\n",
      "Iteration 236, loss = 0.39193821\n",
      "Iteration 237, loss = 0.39154350\n",
      "Iteration 238, loss = 0.39124206\n",
      "Iteration 239, loss = 0.39089915\n",
      "Iteration 240, loss = 0.39058180\n",
      "Iteration 241, loss = 0.39029278\n",
      "Iteration 242, loss = 0.38994737\n",
      "Iteration 243, loss = 0.38965041\n",
      "Iteration 244, loss = 0.38933595\n",
      "Iteration 245, loss = 0.38907218\n",
      "Iteration 246, loss = 0.38876157\n",
      "Iteration 247, loss = 0.38841945\n",
      "Iteration 248, loss = 0.38818781\n",
      "Iteration 249, loss = 0.38782482\n",
      "Iteration 250, loss = 0.38754683\n",
      "Iteration 251, loss = 0.38723429\n",
      "Iteration 252, loss = 0.38693718\n",
      "Iteration 253, loss = 0.38664657\n",
      "Iteration 254, loss = 0.38635495\n",
      "Iteration 255, loss = 0.38605661\n",
      "Iteration 256, loss = 0.38583550\n",
      "Iteration 257, loss = 0.38548194\n",
      "Iteration 258, loss = 0.38524058\n",
      "Iteration 259, loss = 0.38494033\n",
      "Iteration 260, loss = 0.38465384\n",
      "Iteration 261, loss = 0.38441453\n",
      "Iteration 262, loss = 0.38409485\n",
      "Iteration 263, loss = 0.38381920\n",
      "Iteration 264, loss = 0.38356755\n",
      "Iteration 265, loss = 0.38330038\n",
      "Iteration 266, loss = 0.38300322\n",
      "Iteration 267, loss = 0.38273776\n",
      "Iteration 268, loss = 0.38245941\n",
      "Iteration 269, loss = 0.38222807\n",
      "Iteration 270, loss = 0.38192342\n",
      "Iteration 271, loss = 0.38165890\n",
      "Iteration 272, loss = 0.38138631\n",
      "Iteration 273, loss = 0.38117692\n",
      "Iteration 274, loss = 0.38089158\n",
      "Iteration 275, loss = 0.38065634\n",
      "Iteration 276, loss = 0.38041573\n",
      "Iteration 277, loss = 0.38008650\n",
      "Iteration 278, loss = 0.37984774\n",
      "Iteration 279, loss = 0.37959709\n",
      "Iteration 280, loss = 0.37934006\n",
      "Iteration 281, loss = 0.37910336\n",
      "Iteration 282, loss = 0.37883225\n",
      "Iteration 283, loss = 0.37864670\n",
      "Iteration 284, loss = 0.37834551\n",
      "Iteration 285, loss = 0.37810243\n",
      "Iteration 286, loss = 0.37788160\n",
      "Iteration 287, loss = 0.37762252\n",
      "Iteration 288, loss = 0.37741628\n",
      "Iteration 289, loss = 0.37714047\n",
      "Iteration 290, loss = 0.37690345\n",
      "Iteration 291, loss = 0.37667755\n",
      "Iteration 292, loss = 0.37643846\n",
      "Iteration 293, loss = 0.37621452\n",
      "Iteration 294, loss = 0.37599383\n",
      "Iteration 295, loss = 0.37574134\n",
      "Iteration 296, loss = 0.37554557\n",
      "Iteration 297, loss = 0.37530010\n",
      "Iteration 298, loss = 0.37506282\n",
      "Iteration 299, loss = 0.37482277\n",
      "Iteration 300, loss = 0.37465082\n",
      "Iteration 301, loss = 0.37440194\n",
      "Iteration 302, loss = 0.37418024\n",
      "Iteration 303, loss = 0.37397612\n",
      "Iteration 304, loss = 0.37376263\n",
      "Iteration 305, loss = 0.37354700\n",
      "Iteration 306, loss = 0.37331679\n",
      "Iteration 307, loss = 0.37310321\n",
      "Iteration 308, loss = 0.37290165\n",
      "Iteration 309, loss = 0.37269580\n",
      "Iteration 310, loss = 0.37251632\n",
      "Iteration 311, loss = 0.37227542\n",
      "Iteration 312, loss = 0.37207349\n",
      "Iteration 313, loss = 0.37187499\n",
      "Iteration 314, loss = 0.37163607\n",
      "Iteration 315, loss = 0.37144159\n",
      "Iteration 316, loss = 0.37122435\n",
      "Iteration 317, loss = 0.37101486\n",
      "Iteration 318, loss = 0.37080478\n",
      "Iteration 319, loss = 0.37062631\n",
      "Iteration 320, loss = 0.37039675\n",
      "Iteration 321, loss = 0.37019653\n",
      "Iteration 322, loss = 0.37000805\n",
      "Iteration 323, loss = 0.36980013\n",
      "Iteration 324, loss = 0.36963775\n",
      "Iteration 325, loss = 0.36944411\n",
      "Iteration 326, loss = 0.36920365\n",
      "Iteration 327, loss = 0.36900451\n",
      "Iteration 328, loss = 0.36887587\n",
      "Iteration 329, loss = 0.36863669\n",
      "Iteration 330, loss = 0.36842629\n",
      "Iteration 331, loss = 0.36823996\n",
      "Iteration 667, loss = 0.32020216\n",
      "Iteration 668, loss = 0.32016235\n",
      "Iteration 669, loss = 0.32003986\n",
      "Iteration 670, loss = 0.31985823\n",
      "Iteration 671, loss = 0.31981294\n",
      "Iteration 672, loss = 0.31967991\n",
      "Iteration 673, loss = 0.31960074\n",
      "Iteration 674, loss = 0.31955451\n",
      "Iteration 675, loss = 0.31938186\n",
      "Iteration 676, loss = 0.31926585\n",
      "Iteration 677, loss = 0.31917639\n",
      "Iteration 678, loss = 0.31911008\n",
      "Iteration 679, loss = 0.31903664\n",
      "Iteration 680, loss = 0.31887584\n",
      "Iteration 681, loss = 0.31877737\n",
      "Iteration 682, loss = 0.31868932\n",
      "Iteration 683, loss = 0.31860725\n",
      "Iteration 684, loss = 0.31847748\n",
      "Iteration 685, loss = 0.31837769\n",
      "Iteration 686, loss = 0.31832586\n",
      "Iteration 687, loss = 0.31827088\n",
      "Iteration 688, loss = 0.31819227\n",
      "Iteration 689, loss = 0.31799406\n",
      "Iteration 690, loss = 0.31791806\n",
      "Iteration 691, loss = 0.31780237\n",
      "Iteration 692, loss = 0.31769345\n",
      "Iteration 693, loss = 0.31760248\n",
      "Iteration 694, loss = 0.31752265\n",
      "Iteration 695, loss = 0.31742522\n",
      "Iteration 696, loss = 0.31731980\n",
      "Iteration 697, loss = 0.31720395\n",
      "Iteration 698, loss = 0.31714434\n",
      "Iteration 699, loss = 0.31703681\n",
      "Iteration 700, loss = 0.31692785\n",
      "Iteration 701, loss = 0.31681566\n",
      "Iteration 702, loss = 0.31671539\n",
      "Iteration 703, loss = 0.31665510\n",
      "Iteration 704, loss = 0.31652924\n",
      "Iteration 705, loss = 0.31643805\n",
      "Iteration 706, loss = 0.31650064\n",
      "Iteration 707, loss = 0.31622544\n",
      "Iteration 708, loss = 0.31615045\n",
      "Iteration 709, loss = 0.31606479\n",
      "Iteration 710, loss = 0.31595019\n",
      "Iteration 711, loss = 0.31585401\n",
      "Iteration 712, loss = 0.31578029\n",
      "Iteration 713, loss = 0.31565835\n",
      "Iteration 714, loss = 0.31558139\n",
      "Iteration 715, loss = 0.31549274\n",
      "Iteration 716, loss = 0.31539078\n",
      "Iteration 717, loss = 0.31528571\n",
      "Iteration 718, loss = 0.31517814\n",
      "Iteration 719, loss = 0.31510884\n",
      "Iteration 720, loss = 0.31500934\n",
      "Iteration 721, loss = 0.31490611\n",
      "Iteration 722, loss = 0.31480662\n",
      "Iteration 723, loss = 0.31471997\n",
      "Iteration 724, loss = 0.31459736\n",
      "Iteration 725, loss = 0.31454629\n",
      "Iteration 726, loss = 0.31440943\n",
      "Iteration 727, loss = 0.31432815\n",
      "Iteration 728, loss = 0.31424957\n",
      "Iteration 729, loss = 0.31411587\n",
      "Iteration 730, loss = 0.31402353\n",
      "Iteration 731, loss = 0.31394891\n",
      "Iteration 732, loss = 0.31384921\n",
      "Iteration 733, loss = 0.31373465\n",
      "Iteration 734, loss = 0.31365161\n",
      "Iteration 735, loss = 0.31355494\n",
      "Iteration 736, loss = 0.31344361\n",
      "Iteration 737, loss = 0.31334163\n",
      "Iteration 738, loss = 0.31325033\n",
      "Iteration 739, loss = 0.31315390\n",
      "Iteration 740, loss = 0.31306350\n",
      "Iteration 741, loss = 0.31299204\n",
      "Iteration 742, loss = 0.31288584\n",
      "Iteration 743, loss = 0.31284061\n",
      "Iteration 744, loss = 0.31268435\n",
      "Iteration 745, loss = 0.31260547\n",
      "Iteration 746, loss = 0.31248005\n",
      "Iteration 747, loss = 0.31240746\n",
      "Iteration 748, loss = 0.31230131\n",
      "Iteration 749, loss = 0.31221614\n",
      "Iteration 750, loss = 0.31209962\n",
      "Iteration 751, loss = 0.31199597\n",
      "Iteration 752, loss = 0.31189731\n",
      "Iteration 753, loss = 0.31183132\n",
      "Iteration 754, loss = 0.31172739\n",
      "Iteration 755, loss = 0.31163145\n",
      "Iteration 756, loss = 0.31156097\n",
      "Iteration 757, loss = 0.31142653\n",
      "Iteration 758, loss = 0.31131662\n",
      "Iteration 759, loss = 0.31121327\n",
      "Iteration 760, loss = 0.31113909\n",
      "Iteration 761, loss = 0.31104973\n",
      "Iteration 762, loss = 0.31091778\n",
      "Iteration 763, loss = 0.31082484\n",
      "Iteration 764, loss = 0.31071785\n",
      "Iteration 765, loss = 0.31062538\n",
      "Iteration 766, loss = 0.31052301\n",
      "Iteration 767, loss = 0.31046742\n",
      "Iteration 768, loss = 0.31035281\n",
      "Iteration 769, loss = 0.31023857\n",
      "Iteration 770, loss = 0.31014507\n",
      "Iteration 771, loss = 0.31005676\n",
      "Iteration 772, loss = 0.30994206\n",
      "Iteration 773, loss = 0.30984331\n",
      "Iteration 774, loss = 0.30976004\n",
      "Iteration 775, loss = 0.30964539\n",
      "Iteration 776, loss = 0.30954366\n",
      "Iteration 777, loss = 0.30947898\n",
      "Iteration 778, loss = 0.30941346\n",
      "Iteration 779, loss = 0.30927935\n",
      "Iteration 780, loss = 0.30920996\n",
      "Iteration 781, loss = 0.30907572\n",
      "Iteration 782, loss = 0.30896559\n",
      "Iteration 783, loss = 0.30891330\n",
      "Iteration 784, loss = 0.30882329\n",
      "Iteration 785, loss = 0.30871011\n",
      "Iteration 786, loss = 0.30868681\n",
      "Iteration 787, loss = 0.30852154\n",
      "Iteration 788, loss = 0.30841523\n",
      "Iteration 789, loss = 0.30831435\n",
      "Iteration 790, loss = 0.30820495\n",
      "Iteration 791, loss = 0.30813337\n",
      "Iteration 792, loss = 0.30804958\n",
      "Iteration 793, loss = 0.30794289\n",
      "Iteration 794, loss = 0.30784283\n",
      "Iteration 795, loss = 0.30776778\n",
      "Iteration 796, loss = 0.30765700\n",
      "Iteration 797, loss = 0.30755067\n",
      "Iteration 798, loss = 0.30747606\n",
      "Iteration 799, loss = 0.30737511\n",
      "Iteration 800, loss = 0.30732260\n",
      "Iteration 801, loss = 0.30729685\n",
      "Iteration 802, loss = 0.30707375\n",
      "Iteration 803, loss = 0.30702101\n",
      "Iteration 804, loss = 0.30698534\n",
      "Iteration 805, loss = 0.30679855\n",
      "Iteration 806, loss = 0.30670719\n",
      "Iteration 807, loss = 0.30662514\n",
      "Iteration 808, loss = 0.30652000\n",
      "Iteration 809, loss = 0.30645835\n",
      "Iteration 810, loss = 0.30633140\n",
      "Iteration 811, loss = 0.30631640\n",
      "Iteration 812, loss = 0.30614206\n",
      "Iteration 813, loss = 0.30605372\n",
      "Iteration 814, loss = 0.30597046\n",
      "Iteration 815, loss = 0.30590107\n",
      "Iteration 816, loss = 0.30583528\n",
      "Iteration 817, loss = 0.30566670\n",
      "Iteration 818, loss = 0.30557538\n",
      "Iteration 819, loss = 0.30551293\n",
      "Iteration 820, loss = 0.30543091\n",
      "Iteration 821, loss = 0.30534186\n",
      "Iteration 822, loss = 0.30517942\n",
      "Iteration 823, loss = 0.30511448\n",
      "Iteration 824, loss = 0.30500943\n",
      "Iteration 825, loss = 0.30494079\n",
      "Iteration 826, loss = 0.30481007\n",
      "Iteration 827, loss = 0.30487119\n",
      "Iteration 828, loss = 0.30463989\n",
      "Iteration 829, loss = 0.30455744\n",
      "Iteration 830, loss = 0.30444340\n",
      "Iteration 831, loss = 0.30437009\n",
      "Iteration 832, loss = 0.30428382\n",
      "Iteration 833, loss = 0.30416338\n",
      "Iteration 834, loss = 0.30407321\n",
      "Iteration 835, loss = 0.30398542\n",
      "Iteration 836, loss = 0.30387134\n",
      "Iteration 837, loss = 0.30377966\n",
      "Iteration 838, loss = 0.30369782\n",
      "Iteration 839, loss = 0.30362933\n",
      "Iteration 840, loss = 0.30351338\n",
      "Iteration 841, loss = 0.30342029\n",
      "Iteration 842, loss = 0.30329938\n",
      "Iteration 843, loss = 0.30329723\n",
      "Iteration 844, loss = 0.30311439\n",
      "Iteration 845, loss = 0.30301761\n",
      "Iteration 846, loss = 0.30294188\n",
      "Iteration 847, loss = 0.30284416\n",
      "Iteration 848, loss = 0.30272940\n",
      "Iteration 849, loss = 0.30268998\n",
      "Iteration 850, loss = 0.30256510\n",
      "Iteration 851, loss = 0.30246180\n",
      "Iteration 852, loss = 0.30238120\n",
      "Iteration 853, loss = 0.30225393\n",
      "Iteration 854, loss = 0.30215662\n",
      "Iteration 855, loss = 0.30207745\n",
      "Iteration 856, loss = 0.30197207\n",
      "Iteration 857, loss = 0.30190090\n",
      "Iteration 858, loss = 0.30179656\n",
      "Iteration 859, loss = 0.30167576\n",
      "Iteration 860, loss = 0.30158510\n",
      "Iteration 861, loss = 0.30148998\n",
      "Iteration 862, loss = 0.30138810\n",
      "Iteration 863, loss = 0.30128803\n",
      "Iteration 864, loss = 0.30122050\n",
      "Iteration 865, loss = 0.30109840\n",
      "Iteration 866, loss = 0.30101101\n",
      "Iteration 867, loss = 0.30089968\n",
      "Iteration 868, loss = 0.30078978\n",
      "Iteration 869, loss = 0.30069039\n",
      "Iteration 870, loss = 0.30062857\n",
      "Iteration 871, loss = 0.30049770\n",
      "Iteration 872, loss = 0.30039858\n",
      "Iteration 873, loss = 0.30030722\n",
      "Iteration 874, loss = 0.30021055\n",
      "Iteration 875, loss = 0.30016657\n",
      "Iteration 876, loss = 0.30000244\n",
      "Iteration 877, loss = 0.29991653\n",
      "Iteration 878, loss = 0.29981506\n",
      "Iteration 879, loss = 0.29972844\n",
      "Iteration 880, loss = 0.29966428\n",
      "Iteration 881, loss = 0.29953488\n",
      "Iteration 882, loss = 0.29942631\n",
      "Iteration 883, loss = 0.29934716\n",
      "Iteration 884, loss = 0.29923627\n",
      "Iteration 885, loss = 0.29913591\n",
      "Iteration 886, loss = 0.29905677\n",
      "Iteration 887, loss = 0.29894768\n",
      "Iteration 888, loss = 0.29882201\n",
      "Iteration 889, loss = 0.29873500\n",
      "Iteration 890, loss = 0.29862751\n",
      "Iteration 891, loss = 0.29854177\n",
      "Iteration 892, loss = 0.29842869\n",
      "Iteration 893, loss = 0.29832205\n",
      "Iteration 894, loss = 0.29823843\n",
      "Iteration 895, loss = 0.29812332\n",
      "Iteration 896, loss = 0.29802501\n",
      "Iteration 897, loss = 0.29791590\n",
      "Iteration 898, loss = 0.29782824\n",
      "Iteration 899, loss = 0.29773258\n",
      "Iteration 900, loss = 0.29766383\n",
      "Iteration 901, loss = 0.29754322\n",
      "Iteration 902, loss = 0.29744118\n",
      "Iteration 903, loss = 0.29738164\n",
      "Iteration 904, loss = 0.29724675\n",
      "Iteration 905, loss = 0.29712641\n",
      "Iteration 906, loss = 0.29714241\n",
      "Iteration 907, loss = 0.29695774\n",
      "Iteration 908, loss = 0.29685704\n",
      "Iteration 909, loss = 0.29674574\n",
      "Iteration 910, loss = 0.29664415\n",
      "Iteration 911, loss = 0.29655255\n",
      "Iteration 912, loss = 0.29646845\n",
      "Iteration 913, loss = 0.29634895\n",
      "Iteration 914, loss = 0.29625076\n",
      "Iteration 915, loss = 0.29613657\n",
      "Iteration 916, loss = 0.29604914\n",
      "Iteration 917, loss = 0.29602806\n",
      "Iteration 918, loss = 0.29595030\n",
      "Iteration 919, loss = 0.29576173\n",
      "Iteration 920, loss = 0.29566644\n",
      "Iteration 921, loss = 0.29563196\n",
      "Iteration 922, loss = 0.29557632\n",
      "Iteration 923, loss = 0.29536712\n",
      "Iteration 924, loss = 0.29528078\n",
      "Iteration 925, loss = 0.29516611\n",
      "Iteration 926, loss = 0.29507842\n",
      "Iteration 927, loss = 0.29495208\n",
      "Iteration 928, loss = 0.29486254\n",
      "Iteration 929, loss = 0.29475128\n",
      "Iteration 930, loss = 0.29464590\n",
      "Iteration 931, loss = 0.29457488\n",
      "Iteration 932, loss = 0.29447310\n",
      "Iteration 933, loss = 0.29435938\n",
      "Iteration 934, loss = 0.29425670\n",
      "Iteration 935, loss = 0.29418333\n",
      "Iteration 936, loss = 0.29406862\n",
      "Iteration 937, loss = 0.29397177\n",
      "Iteration 938, loss = 0.29389274\n",
      "Iteration 939, loss = 0.29378336\n",
      "Iteration 940, loss = 0.29366884\n",
      "Iteration 941, loss = 0.29359562\n",
      "Iteration 942, loss = 0.29347786\n",
      "Iteration 943, loss = 0.29337371\n",
      "Iteration 944, loss = 0.29330839\n",
      "Iteration 945, loss = 0.29317356\n",
      "Iteration 946, loss = 0.29306575\n",
      "Iteration 947, loss = 0.29297255\n",
      "Iteration 948, loss = 0.29286154\n",
      "Iteration 949, loss = 0.29277768\n",
      "Iteration 950, loss = 0.29266973\n",
      "Iteration 951, loss = 0.29257062\n",
      "Iteration 952, loss = 0.29246116\n",
      "Iteration 953, loss = 0.29241267\n",
      "Iteration 954, loss = 0.29226707\n",
      "Iteration 955, loss = 0.29217461\n",
      "Iteration 956, loss = 0.29208183\n",
      "Iteration 957, loss = 0.29197009\n",
      "Iteration 958, loss = 0.29187647\n",
      "Iteration 959, loss = 0.29176556\n",
      "Iteration 960, loss = 0.29174586\n",
      "Iteration 961, loss = 0.29160368\n",
      "Iteration 962, loss = 0.29146912\n",
      "Iteration 963, loss = 0.29138210\n",
      "Iteration 964, loss = 0.29127151\n",
      "Iteration 965, loss = 0.29117357\n",
      "Iteration 966, loss = 0.29106431\n",
      "Iteration 967, loss = 0.29099041\n",
      "Iteration 968, loss = 0.29086205\n",
      "Iteration 969, loss = 0.29077805\n",
      "Iteration 970, loss = 0.29067245\n",
      "Iteration 971, loss = 0.29059630\n",
      "Iteration 972, loss = 0.29052180\n",
      "Iteration 973, loss = 0.29038115\n",
      "Iteration 974, loss = 0.29027002\n",
      "Iteration 975, loss = 0.29018341\n",
      "Iteration 976, loss = 0.29007582\n",
      "Iteration 977, loss = 0.28998059\n",
      "Iteration 978, loss = 0.28991541\n",
      "Iteration 979, loss = 0.28975663\n",
      "Iteration 980, loss = 0.28966884\n",
      "Iteration 981, loss = 0.28958348\n",
      "Iteration 982, loss = 0.28947401\n",
      "Iteration 983, loss = 0.28935850\n",
      "Iteration 984, loss = 0.28929936\n",
      "Iteration 985, loss = 0.28917509\n",
      "Iteration 986, loss = 0.28905469\n",
      "Iteration 987, loss = 0.28899087\n",
      "Iteration 988, loss = 0.28885570\n",
      "Iteration 989, loss = 0.28874830\n",
      "Iteration 990, loss = 0.28865122\n",
      "Iteration 991, loss = 0.28855192\n",
      "Iteration 992, loss = 0.28846589\n",
      "Iteration 993, loss = 0.28838812\n",
      "Iteration 994, loss = 0.28825233\n",
      "Iteration 995, loss = 0.28814507\n",
      "Iteration 996, loss = 0.28805490\n",
      "Iteration 997, loss = 0.28793643\n",
      "Iteration 998, loss = 0.28793036\n",
      "Iteration 999, loss = 0.28777034\n",
      "Iteration 1000, loss = 0.28768350\n",
      "Iteration 1001, loss = 0.28755186\n",
      "Iteration 1002, loss = 0.28747007\n",
      "Iteration 1003, loss = 0.28734788\n",
      "Iteration 1004, loss = 0.28725110\n",
      "Iteration 1005, loss = 0.28719477\n",
      "Iteration 1006, loss = 0.28705550\n",
      "Iteration 1007, loss = 0.28697092\n",
      "Iteration 1008, loss = 0.28684352\n",
      "Iteration 1009, loss = 0.28674994\n",
      "Iteration 1010, loss = 0.28663708\n",
      "Iteration 1011, loss = 0.28654632\n",
      "Iteration 1012, loss = 0.28643741\n",
      "Iteration 1013, loss = 0.28633036\n",
      "Iteration 1014, loss = 0.28622507\n",
      "Iteration 1015, loss = 0.28615110\n",
      "Iteration 1016, loss = 0.28605092\n",
      "Iteration 1017, loss = 0.28591828\n",
      "Iteration 1018, loss = 0.28582934\n",
      "Iteration 1019, loss = 0.28572055\n",
      "Iteration 1020, loss = 0.28563307\n",
      "Iteration 1021, loss = 0.28556268\n",
      "Iteration 1022, loss = 0.28541424\n",
      "Iteration 1023, loss = 0.28534246\n",
      "Iteration 1024, loss = 0.28524583\n",
      "Iteration 1025, loss = 0.28517190\n",
      "Iteration 1026, loss = 0.28513698\n",
      "Iteration 1027, loss = 0.28492325\n",
      "Iteration 1028, loss = 0.28480671\n",
      "Iteration 1029, loss = 0.28474169\n",
      "Iteration 1030, loss = 0.28460258\n",
      "Iteration 1031, loss = 0.28455255\n",
      "Iteration 1032, loss = 0.28446981\n",
      "Iteration 1033, loss = 0.28430584\n",
      "Iteration 1034, loss = 0.28420548\n",
      "Iteration 1035, loss = 0.28411306\n",
      "Iteration 1036, loss = 0.28401342\n",
      "Iteration 1037, loss = 0.28392564\n",
      "Iteration 1038, loss = 0.28381325\n",
      "Iteration 1039, loss = 0.28370341\n",
      "Iteration 1040, loss = 0.28379200\n",
      "Iteration 1041, loss = 0.28349207\n",
      "Iteration 1042, loss = 0.28337377\n",
      "Iteration 1043, loss = 0.28329266\n",
      "Iteration 1044, loss = 0.28318666\n",
      "Iteration 1045, loss = 0.28310416\n",
      "Iteration 1046, loss = 0.28303305\n",
      "Iteration 1047, loss = 0.28291823\n",
      "Iteration 1048, loss = 0.28279353\n",
      "Iteration 1049, loss = 0.28273390\n",
      "Iteration 1050, loss = 0.28257565\n",
      "Iteration 1051, loss = 0.28248328\n",
      "Iteration 1052, loss = 0.28242458\n",
      "Iteration 1053, loss = 0.28228799\n",
      "Iteration 1054, loss = 0.28217146\n",
      "Iteration 1055, loss = 0.28205772\n",
      "Iteration 1056, loss = 0.28195413\n",
      "Iteration 1057, loss = 0.28186009\n",
      "Iteration 1058, loss = 0.28178396\n",
      "Iteration 1059, loss = 0.28166042\n",
      "Iteration 1060, loss = 0.28155753\n",
      "Iteration 1061, loss = 0.28143619\n",
      "Iteration 1062, loss = 0.28133833\n",
      "Iteration 1063, loss = 0.28128381\n",
      "Iteration 1064, loss = 0.28131859\n",
      "Iteration 1065, loss = 0.28102290\n",
      "Iteration 1066, loss = 0.28092390\n",
      "Iteration 1067, loss = 0.28088909\n",
      "Iteration 1068, loss = 0.28074287\n",
      "Iteration 1069, loss = 0.28062400\n",
      "Iteration 1070, loss = 0.28052614\n",
      "Iteration 1071, loss = 0.28041505\n",
      "Iteration 1072, loss = 0.28037945\n",
      "Iteration 1073, loss = 0.28021842\n",
      "Iteration 1074, loss = 0.28009010\n",
      "Iteration 1075, loss = 0.28002152\n",
      "Iteration 1076, loss = 0.28003044\n",
      "Iteration 1077, loss = 0.27978248\n",
      "Iteration 1078, loss = 0.27968552\n",
      "Iteration 1079, loss = 0.27960443\n",
      "Iteration 1080, loss = 0.27947763\n",
      "Iteration 1081, loss = 0.27937865\n",
      "Iteration 1082, loss = 0.27925925\n",
      "Iteration 1083, loss = 0.27918925\n",
      "Iteration 1084, loss = 0.27910736\n",
      "Iteration 1085, loss = 0.27895395\n",
      "Iteration 1086, loss = 0.27884786\n",
      "Iteration 1087, loss = 0.27876053\n",
      "Iteration 1088, loss = 0.27864550\n",
      "Iteration 1089, loss = 0.27853695\n",
      "Iteration 1090, loss = 0.27849125\n",
      "Iteration 1091, loss = 0.27835846\n",
      "Iteration 1092, loss = 0.27822907\n",
      "Iteration 1093, loss = 0.27812532\n",
      "Iteration 1094, loss = 0.27803644\n",
      "Iteration 1095, loss = 0.27792449\n",
      "Iteration 1096, loss = 0.27783067\n",
      "Iteration 1097, loss = 0.27774097\n",
      "Iteration 1098, loss = 0.27762192\n",
      "Iteration 1099, loss = 0.27752142\n",
      "Iteration 1100, loss = 0.27741855\n",
      "Iteration 1101, loss = 0.27731293\n",
      "Iteration 1102, loss = 0.27722804\n",
      "Iteration 1103, loss = 0.27711417\n",
      "Iteration 1104, loss = 0.27702702\n",
      "Iteration 1105, loss = 0.27689261\n",
      "Iteration 1106, loss = 0.27680516\n",
      "Iteration 1107, loss = 0.27669818\n",
      "Iteration 1108, loss = 0.27660484\n",
      "Iteration 1109, loss = 0.27650135\n",
      "Iteration 1110, loss = 0.27641797\n",
      "Iteration 1111, loss = 0.27631284\n",
      "Iteration 1112, loss = 0.27616934\n",
      "Iteration 1113, loss = 0.27617286\n",
      "Iteration 1114, loss = 0.27600617\n",
      "Iteration 1115, loss = 0.27588917\n",
      "Iteration 1116, loss = 0.27576785\n",
      "Iteration 1117, loss = 0.27568809\n",
      "Iteration 1118, loss = 0.27557929\n",
      "Iteration 1119, loss = 0.27547491\n",
      "Iteration 1120, loss = 0.27538231\n",
      "Iteration 1121, loss = 0.27528515\n",
      "Iteration 1122, loss = 0.27515975\n",
      "Iteration 1123, loss = 0.27505968\n",
      "Iteration 1124, loss = 0.27499310\n",
      "Iteration 1125, loss = 0.27484907\n",
      "Iteration 1126, loss = 0.27474693\n",
      "Iteration 1127, loss = 0.27464944\n",
      "Iteration 1128, loss = 0.27462383\n",
      "Iteration 1129, loss = 0.27452756\n",
      "Iteration 1130, loss = 0.27432107\n",
      "Iteration 1131, loss = 0.27421935\n",
      "Iteration 1132, loss = 0.27412100\n",
      "Iteration 1133, loss = 0.27403372\n",
      "Iteration 1134, loss = 0.27392979\n",
      "Iteration 1135, loss = 0.27380974\n",
      "Iteration 1136, loss = 0.27378623\n",
      "Iteration 1137, loss = 0.27361913\n",
      "Iteration 1138, loss = 0.27351089\n",
      "Iteration 1139, loss = 0.27340316\n",
      "Iteration 1140, loss = 0.27329586\n",
      "Iteration 1141, loss = 0.27318933\n",
      "Iteration 1142, loss = 0.27311119\n",
      "Iteration 1143, loss = 0.27299274\n",
      "Iteration 1144, loss = 0.27287262\n",
      "Iteration 1145, loss = 0.27279197\n",
      "Iteration 1146, loss = 0.27281199\n",
      "Iteration 1147, loss = 0.27255676\n",
      "Iteration 1148, loss = 0.27248866\n",
      "Iteration 1149, loss = 0.27235962\n",
      "Iteration 1150, loss = 0.27225444\n",
      "Iteration 1151, loss = 0.27216792\n",
      "Iteration 1152, loss = 0.27213476\n",
      "Iteration 1153, loss = 0.27196190\n",
      "Iteration 1154, loss = 0.27182764\n",
      "Iteration 1155, loss = 0.27180059\n",
      "Iteration 1156, loss = 0.27164688\n",
      "Iteration 1157, loss = 0.27152320\n",
      "Iteration 240, loss = 0.37898612\n",
      "Iteration 241, loss = 0.37882051\n",
      "Iteration 242, loss = 0.37854685\n",
      "Iteration 243, loss = 0.37832173\n",
      "Iteration 244, loss = 0.37810626\n",
      "Iteration 245, loss = 0.37788363\n",
      "Iteration 246, loss = 0.37766766\n",
      "Iteration 247, loss = 0.37747755\n",
      "Iteration 248, loss = 0.37724550\n",
      "Iteration 249, loss = 0.37702161\n",
      "Iteration 250, loss = 0.37682282\n",
      "Iteration 251, loss = 0.37662418\n",
      "Iteration 252, loss = 0.37639992\n",
      "Iteration 253, loss = 0.37622344\n",
      "Iteration 254, loss = 0.37601002\n",
      "Iteration 255, loss = 0.37580735\n",
      "Iteration 256, loss = 0.37557733\n",
      "Iteration 257, loss = 0.37540050\n",
      "Iteration 258, loss = 0.37521099\n",
      "Iteration 259, loss = 0.37498701\n",
      "Iteration 260, loss = 0.37479916\n",
      "Iteration 261, loss = 0.37459027\n",
      "Iteration 262, loss = 0.37441471\n",
      "Iteration 263, loss = 0.37421810\n",
      "Iteration 264, loss = 0.37401980\n",
      "Iteration 265, loss = 0.37381507\n",
      "Iteration 266, loss = 0.37362915\n",
      "Iteration 267, loss = 0.37343101\n",
      "Iteration 268, loss = 0.37323247\n",
      "Iteration 269, loss = 0.37306310\n",
      "Iteration 270, loss = 0.37286110\n",
      "Iteration 271, loss = 0.37268947\n",
      "Iteration 272, loss = 0.37249271\n",
      "Iteration 273, loss = 0.37234986\n",
      "Iteration 274, loss = 0.37213707\n",
      "Iteration 275, loss = 0.37194225\n",
      "Iteration 276, loss = 0.37175480\n",
      "Iteration 277, loss = 0.37159014\n",
      "Iteration 278, loss = 0.37141060\n",
      "Iteration 279, loss = 0.37123621\n",
      "Iteration 280, loss = 0.37104860\n",
      "Iteration 281, loss = 0.37087161\n",
      "Iteration 282, loss = 0.37068734\n",
      "Iteration 283, loss = 0.37051886\n",
      "Iteration 284, loss = 0.37034044\n",
      "Iteration 285, loss = 0.37017582\n",
      "Iteration 286, loss = 0.37000352\n",
      "Iteration 287, loss = 0.36982453\n",
      "Iteration 288, loss = 0.36966334\n",
      "Iteration 289, loss = 0.36949846\n",
      "Iteration 290, loss = 0.36931189\n",
      "Iteration 291, loss = 0.36914519\n",
      "Iteration 292, loss = 0.36898262\n",
      "Iteration 293, loss = 0.36882796\n",
      "Iteration 294, loss = 0.36865555\n",
      "Iteration 295, loss = 0.36850944\n",
      "Iteration 296, loss = 0.36830897\n",
      "Iteration 297, loss = 0.36816500\n",
      "Iteration 298, loss = 0.36799227\n",
      "Iteration 299, loss = 0.36783930\n",
      "Iteration 300, loss = 0.36767001\n",
      "Iteration 301, loss = 0.36750515\n",
      "Iteration 302, loss = 0.36735732\n",
      "Iteration 303, loss = 0.36719884\n",
      "Iteration 304, loss = 0.36703363\n",
      "Iteration 305, loss = 0.36687900\n",
      "Iteration 306, loss = 0.36673288\n",
      "Iteration 307, loss = 0.36656064\n",
      "Iteration 308, loss = 0.36642113\n",
      "Iteration 309, loss = 0.36624006\n",
      "Iteration 310, loss = 0.36610377\n",
      "Iteration 311, loss = 0.36594007\n",
      "Iteration 312, loss = 0.36579900\n",
      "Iteration 313, loss = 0.36565460\n",
      "Iteration 314, loss = 0.36548613\n",
      "Iteration 315, loss = 0.36533798\n",
      "Iteration 316, loss = 0.36519010\n",
      "Iteration 317, loss = 0.36504077\n",
      "Iteration 318, loss = 0.36487140\n",
      "Iteration 319, loss = 0.36472295\n",
      "Iteration 320, loss = 0.36458340\n",
      "Iteration 321, loss = 0.36442894\n",
      "Iteration 322, loss = 0.36427724\n",
      "Iteration 323, loss = 0.36414652\n",
      "Iteration 324, loss = 0.36397920\n",
      "Iteration 325, loss = 0.36384379\n",
      "Iteration 326, loss = 0.36368927\n",
      "Iteration 327, loss = 0.36355342\n",
      "Iteration 328, loss = 0.36341525\n",
      "Iteration 329, loss = 0.36325646\n",
      "Iteration 330, loss = 0.36310241\n",
      "Iteration 331, loss = 0.36295051\n",
      "Iteration 332, loss = 0.36281591\n",
      "Iteration 333, loss = 0.36272076\n",
      "Iteration 334, loss = 0.36254014\n",
      "Iteration 335, loss = 0.36238598\n",
      "Iteration 336, loss = 0.36227177\n",
      "Iteration 337, loss = 0.36209832\n",
      "Iteration 338, loss = 0.36196268\n",
      "Iteration 339, loss = 0.36181701\n",
      "Iteration 340, loss = 0.36168159\n",
      "Iteration 341, loss = 0.36155254\n",
      "Iteration 342, loss = 0.36140009\n",
      "Iteration 343, loss = 0.36128380\n",
      "Iteration 344, loss = 0.36111545\n",
      "Iteration 345, loss = 0.36098345\n",
      "Iteration 346, loss = 0.36084935\n",
      "Iteration 347, loss = 0.36071459\n",
      "Iteration 348, loss = 0.36059347\n",
      "Iteration 349, loss = 0.36044149\n",
      "Iteration 350, loss = 0.36031278\n",
      "Iteration 351, loss = 0.36017898\n",
      "Iteration 352, loss = 0.36002782\n",
      "Iteration 353, loss = 0.35989826\n",
      "Iteration 354, loss = 0.35975936\n",
      "Iteration 355, loss = 0.35962641\n",
      "Iteration 356, loss = 0.35948509\n",
      "Iteration 357, loss = 0.35935836\n",
      "Iteration 358, loss = 0.35923638\n",
      "Iteration 359, loss = 0.35909590\n",
      "Iteration 360, loss = 0.35895540\n",
      "Iteration 361, loss = 0.35882958\n",
      "Iteration 362, loss = 0.35869488\n",
      "Iteration 363, loss = 0.35857308\n",
      "Iteration 364, loss = 0.35844268\n",
      "Iteration 365, loss = 0.35831394\n",
      "Iteration 366, loss = 0.35818736\n",
      "Iteration 367, loss = 0.35805261\n",
      "Iteration 368, loss = 0.35791662\n",
      "Iteration 369, loss = 0.35778928\n",
      "Iteration 370, loss = 0.35767099\n",
      "Iteration 371, loss = 0.35754324\n",
      "Iteration 372, loss = 0.35740692\n",
      "Iteration 373, loss = 0.35728883\n",
      "Iteration 374, loss = 0.35717415\n",
      "Iteration 375, loss = 0.35702308\n",
      "Iteration 376, loss = 0.35690766\n",
      "Iteration 377, loss = 0.35677566\n",
      "Iteration 378, loss = 0.35665181\n",
      "Iteration 379, loss = 0.35652857\n",
      "Iteration 380, loss = 0.35639843\n",
      "Iteration 381, loss = 0.35627569\n",
      "Iteration 382, loss = 0.35616601\n",
      "Iteration 383, loss = 0.35603127\n",
      "Iteration 384, loss = 0.35591996\n",
      "Iteration 385, loss = 0.35577991\n",
      "Iteration 386, loss = 0.35567342\n",
      "Iteration 387, loss = 0.35553576\n",
      "Iteration 388, loss = 0.35543915\n",
      "Iteration 389, loss = 0.35530864\n",
      "Iteration 390, loss = 0.35517870\n",
      "Iteration 391, loss = 0.35505932\n",
      "Iteration 392, loss = 0.35494580\n",
      "Iteration 393, loss = 0.35480584\n",
      "Iteration 394, loss = 0.35470245\n",
      "Iteration 395, loss = 0.35457939\n",
      "Iteration 396, loss = 0.35445717\n",
      "Iteration 397, loss = 0.35435004\n",
      "Iteration 398, loss = 0.35422483\n",
      "Iteration 399, loss = 0.35410729\n",
      "Iteration 400, loss = 0.35397827\n",
      "Iteration 401, loss = 0.35387823\n",
      "Iteration 402, loss = 0.35374923\n",
      "Iteration 403, loss = 0.35363166\n",
      "Iteration 404, loss = 0.35351261\n",
      "Iteration 405, loss = 0.35340422\n",
      "Iteration 406, loss = 0.35329919\n",
      "Iteration 407, loss = 0.35316924\n",
      "Iteration 408, loss = 0.35306142\n",
      "Iteration 409, loss = 0.35294361\n",
      "Iteration 410, loss = 0.35281749\n",
      "Iteration 411, loss = 0.35271225\n",
      "Iteration 412, loss = 0.35260960\n",
      "Iteration 413, loss = 0.35248023\n",
      "Iteration 414, loss = 0.35236537\n",
      "Iteration 415, loss = 0.35225301\n",
      "Iteration 416, loss = 0.35213553\n",
      "Iteration 417, loss = 0.35203657\n",
      "Iteration 418, loss = 0.35191595\n",
      "Iteration 419, loss = 0.35179461\n",
      "Iteration 420, loss = 0.35168847\n",
      "Iteration 421, loss = 0.35159129\n",
      "Iteration 422, loss = 0.35148371\n",
      "Iteration 423, loss = 0.35136468\n",
      "Iteration 424, loss = 0.35127083\n",
      "Iteration 425, loss = 0.35115167\n",
      "Iteration 426, loss = 0.35103013\n",
      "Iteration 427, loss = 0.35091181\n",
      "Iteration 428, loss = 0.35081588\n",
      "Iteration 429, loss = 0.35072227\n",
      "Iteration 430, loss = 0.35057875\n",
      "Iteration 431, loss = 0.35047903\n",
      "Iteration 432, loss = 0.35036225\n",
      "Iteration 433, loss = 0.35025789\n",
      "Iteration 434, loss = 0.35015124\n",
      "Iteration 435, loss = 0.35006487\n",
      "Iteration 436, loss = 0.34993505\n",
      "Iteration 437, loss = 0.34981936\n",
      "Iteration 438, loss = 0.34970580\n",
      "Iteration 439, loss = 0.34962719\n",
      "Iteration 440, loss = 0.34949819\n",
      "Iteration 441, loss = 0.34939613\n",
      "Iteration 442, loss = 0.34929372\n",
      "Iteration 443, loss = 0.34917560\n",
      "Iteration 444, loss = 0.34907417\n",
      "Iteration 445, loss = 0.34896756\n",
      "Iteration 446, loss = 0.34885447\n",
      "Iteration 447, loss = 0.34879250\n",
      "Iteration 448, loss = 0.34865858\n",
      "Iteration 449, loss = 0.34855338\n",
      "Iteration 450, loss = 0.34842993\n",
      "Iteration 451, loss = 0.34834141\n",
      "Iteration 452, loss = 0.34820932\n",
      "Iteration 453, loss = 0.34811065\n",
      "Iteration 454, loss = 0.34803403\n",
      "Iteration 455, loss = 0.34791018\n",
      "Iteration 456, loss = 0.34779585\n",
      "Iteration 457, loss = 0.34769449\n",
      "Iteration 458, loss = 0.34761068\n",
      "Iteration 459, loss = 0.34747605\n",
      "Iteration 460, loss = 0.34738254\n",
      "Iteration 461, loss = 0.34727096\n",
      "Iteration 462, loss = 0.34716609\n",
      "Iteration 463, loss = 0.34707454\n",
      "Iteration 464, loss = 0.34696262\n",
      "Iteration 465, loss = 0.34685713\n",
      "Iteration 466, loss = 0.34677920\n",
      "Iteration 467, loss = 0.34666486\n",
      "Iteration 468, loss = 0.34655413\n",
      "Iteration 469, loss = 0.34645457\n",
      "Iteration 470, loss = 0.34635198\n",
      "Iteration 471, loss = 0.34623551\n",
      "Iteration 472, loss = 0.34615007\n",
      "Iteration 473, loss = 0.34603095\n",
      "Iteration 474, loss = 0.34594597\n",
      "Iteration 475, loss = 0.34587131\n",
      "Iteration 476, loss = 0.34574019\n",
      "Iteration 477, loss = 0.34563912\n",
      "Iteration 478, loss = 0.34553650\n",
      "Iteration 479, loss = 0.34542943\n",
      "Iteration 480, loss = 0.34532871\n",
      "Iteration 481, loss = 0.34522613\n",
      "Iteration 482, loss = 0.34513990\n",
      "Iteration 483, loss = 0.34503307\n",
      "Iteration 484, loss = 0.34494225\n",
      "Iteration 485, loss = 0.34486376\n",
      "Iteration 486, loss = 0.34473473\n",
      "Iteration 487, loss = 0.34463148\n",
      "Iteration 488, loss = 0.34452596\n",
      "Iteration 489, loss = 0.34442962\n",
      "Iteration 490, loss = 0.34435458\n",
      "Iteration 491, loss = 0.34422664\n",
      "Iteration 492, loss = 0.34414114\n",
      "Iteration 493, loss = 0.34403433\n",
      "Iteration 494, loss = 0.34393124\n",
      "Iteration 495, loss = 0.34386046\n",
      "Iteration 496, loss = 0.34374873\n",
      "Iteration 497, loss = 0.34364048\n",
      "Iteration 498, loss = 0.34356541\n",
      "Iteration 499, loss = 0.34346635\n",
      "Iteration 500, loss = 0.34337192\n",
      "Iteration 501, loss = 0.34324642\n",
      "Iteration 502, loss = 0.34315124\n",
      "Iteration 503, loss = 0.34304681\n",
      "Iteration 504, loss = 0.34298033\n",
      "Iteration 505, loss = 0.34286717\n",
      "Iteration 506, loss = 0.34276663\n",
      "Iteration 507, loss = 0.34267076\n",
      "Iteration 508, loss = 0.34256932\n",
      "Iteration 509, loss = 0.34246537\n",
      "Iteration 510, loss = 0.34237521\n",
      "Iteration 511, loss = 0.34228225\n",
      "Iteration 512, loss = 0.34218043\n",
      "Iteration 513, loss = 0.34208896\n",
      "Iteration 514, loss = 0.34200785\n",
      "Iteration 515, loss = 0.34191341\n",
      "Iteration 516, loss = 0.34180753\n",
      "Iteration 517, loss = 0.34173156\n",
      "Iteration 518, loss = 0.34160762\n",
      "Iteration 519, loss = 0.34153131\n",
      "Iteration 520, loss = 0.34143679\n",
      "Iteration 521, loss = 0.34132636\n",
      "Iteration 522, loss = 0.34124329\n",
      "Iteration 523, loss = 0.34116429\n",
      "Iteration 524, loss = 0.34107228\n",
      "Iteration 525, loss = 0.34095416\n",
      "Iteration 526, loss = 0.34085228\n",
      "Iteration 527, loss = 0.34080641\n",
      "Iteration 528, loss = 0.34067444\n",
      "Iteration 529, loss = 0.34057366\n",
      "Iteration 530, loss = 0.34047814\n",
      "Iteration 531, loss = 0.34038642\n",
      "Iteration 532, loss = 0.34029417\n",
      "Iteration 533, loss = 0.34019481\n",
      "Iteration 534, loss = 0.34010302\n",
      "Iteration 535, loss = 0.34001125\n",
      "Iteration 536, loss = 0.33991810\n",
      "Iteration 537, loss = 0.33981523\n",
      "Iteration 538, loss = 0.33976309\n",
      "Iteration 539, loss = 0.33963839\n",
      "Iteration 540, loss = 0.33956071\n",
      "Iteration 541, loss = 0.33946027\n",
      "Iteration 542, loss = 0.33934876\n",
      "Iteration 543, loss = 0.33927749\n",
      "Iteration 544, loss = 0.33916580\n",
      "Iteration 545, loss = 0.33906607\n",
      "Iteration 546, loss = 0.33898275\n",
      "Iteration 547, loss = 0.33888493\n",
      "Iteration 548, loss = 0.33879600\n",
      "Iteration 549, loss = 0.33872041\n",
      "Iteration 550, loss = 0.33860985\n",
      "Iteration 551, loss = 0.33851423\n",
      "Iteration 552, loss = 0.33842136\n",
      "Iteration 553, loss = 0.33832812\n",
      "Iteration 554, loss = 0.33824440\n",
      "Iteration 555, loss = 0.33815433\n",
      "Iteration 556, loss = 0.33807778\n",
      "Iteration 557, loss = 0.33796406\n",
      "Iteration 558, loss = 0.33787565\n",
      "Iteration 559, loss = 0.33777821\n",
      "Iteration 560, loss = 0.33767796\n",
      "Iteration 561, loss = 0.33759354\n",
      "Iteration 562, loss = 0.33749319\n",
      "Iteration 563, loss = 0.33741880\n",
      "Iteration 564, loss = 0.33731719\n",
      "Iteration 565, loss = 0.33722415\n",
      "Iteration 566, loss = 0.33713841\n",
      "Iteration 567, loss = 0.33706300\n",
      "Iteration 568, loss = 0.33696673\n",
      "Iteration 569, loss = 0.33687675\n",
      "Iteration 570, loss = 0.33676700\n",
      "Iteration 571, loss = 0.33668658\n",
      "Iteration 572, loss = 0.33658421\n",
      "Iteration 573, loss = 0.33649684\n",
      "Iteration 574, loss = 0.33639417\n",
      "Iteration 575, loss = 0.33631955\n",
      "Iteration 576, loss = 0.33621582\n",
      "Iteration 577, loss = 0.33613234\n",
      "Iteration 578, loss = 0.33604315\n",
      "Iteration 579, loss = 0.33595576\n",
      "Iteration 580, loss = 0.33584798\n",
      "Iteration 581, loss = 0.33576957\n",
      "Iteration 582, loss = 0.33569309\n",
      "Iteration 583, loss = 0.33558986\n",
      "Iteration 584, loss = 0.33551910\n",
      "Iteration 585, loss = 0.33547062\n",
      "Iteration 586, loss = 0.33535214\n",
      "Iteration 587, loss = 0.33522333\n",
      "Iteration 588, loss = 0.33513453\n",
      "Iteration 589, loss = 0.33505392\n",
      "Iteration 590, loss = 0.33497625\n",
      "Iteration 591, loss = 0.33489981\n",
      "Iteration 592, loss = 0.33478466\n",
      "Iteration 593, loss = 0.33471615\n",
      "Iteration 594, loss = 0.33461727\n",
      "Iteration 595, loss = 0.33453341\n",
      "Iteration 596, loss = 0.33444610\n",
      "Iteration 597, loss = 0.33434041\n",
      "Iteration 598, loss = 0.33425628\n",
      "Iteration 599, loss = 0.33416156\n",
      "Iteration 600, loss = 0.33407418\n",
      "Iteration 601, loss = 0.33400484\n",
      "Iteration 602, loss = 0.33391083\n",
      "Iteration 603, loss = 0.33384323\n",
      "Iteration 604, loss = 0.33373711\n",
      "Iteration 605, loss = 0.33368410\n",
      "Iteration 606, loss = 0.33357263\n",
      "Iteration 607, loss = 0.33348042\n",
      "Iteration 608, loss = 0.33340800\n",
      "Iteration 609, loss = 0.33330253\n",
      "Iteration 610, loss = 0.33321065\n",
      "Iteration 611, loss = 0.33313560\n",
      "Iteration 612, loss = 0.33304541\n",
      "Iteration 613, loss = 0.33295975\n",
      "Iteration 614, loss = 0.33286688\n",
      "Iteration 615, loss = 0.33279001\n",
      "Iteration 616, loss = 0.33271062\n",
      "Iteration 617, loss = 0.33260759\n",
      "Iteration 618, loss = 0.33255031\n",
      "Iteration 619, loss = 0.33245363\n",
      "Iteration 620, loss = 0.33236614\n",
      "Iteration 621, loss = 0.33228767\n",
      "Iteration 622, loss = 0.33220665\n",
      "Iteration 623, loss = 0.33211519\n",
      "Iteration 624, loss = 0.33202785\n",
      "Iteration 625, loss = 0.33194992\n",
      "Iteration 626, loss = 0.33186318\n",
      "Iteration 627, loss = 0.33179730\n",
      "Iteration 628, loss = 0.33172568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69850064\n",
      "Iteration 2, loss = 0.69618605\n",
      "Iteration 3, loss = 0.69274346\n",
      "Iteration 4, loss = 0.68837174\n",
      "Iteration 5, loss = 0.68350906\n",
      "Iteration 6, loss = 0.67843565\n",
      "Iteration 7, loss = 0.67294138\n",
      "Iteration 8, loss = 0.66743928\n",
      "Iteration 9, loss = 0.66191818\n",
      "Iteration 10, loss = 0.65634577\n",
      "Iteration 11, loss = 0.65102157\n",
      "Iteration 12, loss = 0.64561108\n",
      "Iteration 13, loss = 0.64037623\n",
      "Iteration 14, loss = 0.63514346\n",
      "Iteration 15, loss = 0.63007693\n",
      "Iteration 16, loss = 0.62520406\n",
      "Iteration 17, loss = 0.62025331\n",
      "Iteration 18, loss = 0.61552087\n",
      "Iteration 19, loss = 0.61089555\n",
      "Iteration 20, loss = 0.60631763\n",
      "Iteration 21, loss = 0.60187446\n",
      "Iteration 22, loss = 0.59742450\n",
      "Iteration 23, loss = 0.59297302\n",
      "Iteration 24, loss = 0.58872248\n",
      "Iteration 25, loss = 0.58452203\n",
      "Iteration 26, loss = 0.58031507\n",
      "Iteration 27, loss = 0.57629290\n",
      "Iteration 28, loss = 0.57217856\n",
      "Iteration 29, loss = 0.56815421\n",
      "Iteration 30, loss = 0.56439099\n",
      "Iteration 31, loss = 0.56042954\n",
      "Iteration 32, loss = 0.55666831\n",
      "Iteration 33, loss = 0.55283587\n",
      "Iteration 34, loss = 0.54928047\n",
      "Iteration 35, loss = 0.54552583\n",
      "Iteration 36, loss = 0.54194111\n",
      "Iteration 37, loss = 0.53831151\n",
      "Iteration 38, loss = 0.53486056\n",
      "Iteration 39, loss = 0.53136046\n",
      "Iteration 40, loss = 0.52796577\n",
      "Iteration 41, loss = 0.52454740\n",
      "Iteration 42, loss = 0.52118789\n",
      "Iteration 43, loss = 0.51790508\n",
      "Iteration 44, loss = 0.51461796\n",
      "Iteration 45, loss = 0.51148468\n",
      "Iteration 46, loss = 0.50827384\n",
      "Iteration 47, loss = 0.50521303\n",
      "Iteration 48, loss = 0.50206391\n",
      "Iteration 49, loss = 0.49910186\n",
      "Iteration 50, loss = 0.49617959\n",
      "Iteration 51, loss = 0.49311426\n",
      "Iteration 52, loss = 0.49031344\n",
      "Iteration 53, loss = 0.48747134\n",
      "Iteration 54, loss = 0.48470104\n",
      "Iteration 55, loss = 0.48188548\n",
      "Iteration 56, loss = 0.47919776\n",
      "Iteration 57, loss = 0.47659272\n",
      "Iteration 58, loss = 0.47396248\n",
      "Iteration 59, loss = 0.47132530\n",
      "Iteration 60, loss = 0.46882828\n",
      "Iteration 61, loss = 0.46639164\n",
      "Iteration 62, loss = 0.46394690\n",
      "Iteration 63, loss = 0.46158387\n",
      "Iteration 64, loss = 0.45912145\n",
      "Iteration 65, loss = 0.45676271\n",
      "Iteration 66, loss = 0.45458540\n",
      "Iteration 67, loss = 0.45241883\n",
      "Iteration 68, loss = 0.45016163\n",
      "Iteration 69, loss = 0.44797682\n",
      "Iteration 70, loss = 0.44585209\n",
      "Iteration 71, loss = 0.44381574\n",
      "Iteration 72, loss = 0.44186080\n",
      "Iteration 73, loss = 0.43981122\n",
      "Iteration 74, loss = 0.43790505\n",
      "Iteration 75, loss = 0.43592542\n",
      "Iteration 76, loss = 0.43405991\n",
      "Iteration 77, loss = 0.43222572\n",
      "Iteration 78, loss = 0.43039810\n",
      "Iteration 79, loss = 0.42859608\n",
      "Iteration 80, loss = 0.42689169\n",
      "Iteration 81, loss = 0.42523348\n",
      "Iteration 82, loss = 0.42357146\n",
      "Iteration 83, loss = 0.42192245\n",
      "Iteration 84, loss = 0.42034761\n",
      "Iteration 85, loss = 0.41878352\n",
      "Iteration 86, loss = 0.41721087\n",
      "Iteration 87, loss = 0.41573690\n",
      "Iteration 88, loss = 0.41427193\n",
      "Iteration 89, loss = 0.41280893\n",
      "Iteration 90, loss = 0.41140007\n",
      "Iteration 91, loss = 0.41003068\n",
      "Iteration 92, loss = 0.40864524\n",
      "Iteration 93, loss = 0.40731854\n",
      "Iteration 94, loss = 0.40607336\n",
      "Iteration 95, loss = 0.40477502\n",
      "Iteration 96, loss = 0.40358813\n",
      "Iteration 97, loss = 0.40229994\n",
      "Iteration 98, loss = 0.40114038\n",
      "Iteration 99, loss = 0.40002990\n",
      "Iteration 100, loss = 0.39891267\n",
      "Iteration 101, loss = 0.39777470\n",
      "Iteration 102, loss = 0.39667167\n",
      "Iteration 103, loss = 0.39562315\n",
      "Iteration 104, loss = 0.39462568\n",
      "Iteration 105, loss = 0.39356076\n",
      "Iteration 106, loss = 0.39259892\n",
      "Iteration 107, loss = 0.39158508\n",
      "Iteration 258, loss = 0.36257230\n",
      "Iteration 259, loss = 0.36238770\n",
      "Iteration 260, loss = 0.36219475\n",
      "Iteration 261, loss = 0.36201417\n",
      "Iteration 262, loss = 0.36183653\n",
      "Iteration 263, loss = 0.36165984\n",
      "Iteration 264, loss = 0.36147039\n",
      "Iteration 265, loss = 0.36130631\n",
      "Iteration 266, loss = 0.36114441\n",
      "Iteration 267, loss = 0.36096744\n",
      "Iteration 268, loss = 0.36079497\n",
      "Iteration 269, loss = 0.36061909\n",
      "Iteration 270, loss = 0.36043632\n",
      "Iteration 271, loss = 0.36027211\n",
      "Iteration 272, loss = 0.36011266\n",
      "Iteration 273, loss = 0.35994992\n",
      "Iteration 274, loss = 0.35975511\n",
      "Iteration 275, loss = 0.35960836\n",
      "Iteration 276, loss = 0.35943693\n",
      "Iteration 277, loss = 0.35927174\n",
      "Iteration 278, loss = 0.35910704\n",
      "Iteration 279, loss = 0.35896224\n",
      "Iteration 280, loss = 0.35878199\n",
      "Iteration 281, loss = 0.35865831\n",
      "Iteration 282, loss = 0.35846389\n",
      "Iteration 283, loss = 0.35832751\n",
      "Iteration 284, loss = 0.35816264\n",
      "Iteration 285, loss = 0.35799799\n",
      "Iteration 286, loss = 0.35786219\n",
      "Iteration 287, loss = 0.35769159\n",
      "Iteration 288, loss = 0.35754080\n",
      "Iteration 289, loss = 0.35738028\n",
      "Iteration 290, loss = 0.35722834\n",
      "Iteration 291, loss = 0.35707328\n",
      "Iteration 292, loss = 0.35694874\n",
      "Iteration 293, loss = 0.35677867\n",
      "Iteration 294, loss = 0.35662835\n",
      "Iteration 295, loss = 0.35650005\n",
      "Iteration 296, loss = 0.35634098\n",
      "Iteration 297, loss = 0.35621763\n",
      "Iteration 298, loss = 0.35606137\n",
      "Iteration 299, loss = 0.35591551\n",
      "Iteration 300, loss = 0.35578402\n",
      "Iteration 301, loss = 0.35564541\n",
      "Iteration 302, loss = 0.35550336\n",
      "Iteration 303, loss = 0.35536509\n",
      "Iteration 304, loss = 0.35522372\n",
      "Iteration 305, loss = 0.35509896\n",
      "Iteration 306, loss = 0.35494529\n",
      "Iteration 307, loss = 0.35481706\n",
      "Iteration 308, loss = 0.35467077\n",
      "Iteration 309, loss = 0.35453525\n",
      "Iteration 310, loss = 0.35441973\n",
      "Iteration 311, loss = 0.35426755\n",
      "Iteration 312, loss = 0.35417156\n",
      "Iteration 313, loss = 0.35400367\n",
      "Iteration 314, loss = 0.35388807\n",
      "Iteration 315, loss = 0.35374651\n",
      "Iteration 316, loss = 0.35360328\n",
      "Iteration 317, loss = 0.35348293\n",
      "Iteration 318, loss = 0.35334797\n",
      "Iteration 319, loss = 0.35319837\n",
      "Iteration 320, loss = 0.35308385\n",
      "Iteration 321, loss = 0.35295113\n",
      "Iteration 322, loss = 0.35283161\n",
      "Iteration 323, loss = 0.35269780\n",
      "Iteration 324, loss = 0.35257982\n",
      "Iteration 325, loss = 0.35243516\n",
      "Iteration 326, loss = 0.35232031\n",
      "Iteration 327, loss = 0.35218113\n",
      "Iteration 328, loss = 0.35206639\n",
      "Iteration 329, loss = 0.35193784\n",
      "Iteration 330, loss = 0.35181304\n",
      "Iteration 331, loss = 0.35168865\n",
      "Iteration 332, loss = 0.35157315\n",
      "Iteration 333, loss = 0.35143427\n",
      "Iteration 334, loss = 0.35131762\n",
      "Iteration 335, loss = 0.35119883\n",
      "Iteration 336, loss = 0.35107088\n",
      "Iteration 337, loss = 0.35095393\n",
      "Iteration 338, loss = 0.35084264\n",
      "Iteration 339, loss = 0.35072038\n",
      "Iteration 340, loss = 0.35058942\n",
      "Iteration 341, loss = 0.35047400\n",
      "Iteration 342, loss = 0.35035836\n",
      "Iteration 343, loss = 0.35026533\n",
      "Iteration 344, loss = 0.35011876\n",
      "Iteration 345, loss = 0.35000095\n",
      "Iteration 346, loss = 0.34988963\n",
      "Iteration 347, loss = 0.34976002\n",
      "Iteration 348, loss = 0.34966781\n",
      "Iteration 349, loss = 0.34954701\n",
      "Iteration 350, loss = 0.34941857\n",
      "Iteration 351, loss = 0.34930115\n",
      "Iteration 352, loss = 0.34919393\n",
      "Iteration 353, loss = 0.34910206\n",
      "Iteration 354, loss = 0.34897306\n",
      "Iteration 355, loss = 0.34887607\n",
      "Iteration 356, loss = 0.34874013\n",
      "Iteration 357, loss = 0.34863385\n",
      "Iteration 358, loss = 0.34851919\n",
      "Iteration 359, loss = 0.34841371\n",
      "Iteration 360, loss = 0.34829626\n",
      "Iteration 361, loss = 0.34818633\n",
      "Iteration 362, loss = 0.34807154\n",
      "Iteration 363, loss = 0.34797523\n",
      "Iteration 364, loss = 0.34786320\n",
      "Iteration 365, loss = 0.34776713\n",
      "Iteration 366, loss = 0.34764826\n",
      "Iteration 367, loss = 0.34753331\n",
      "Iteration 368, loss = 0.34743867\n",
      "Iteration 369, loss = 0.34732113\n",
      "Iteration 370, loss = 0.34724356\n",
      "Iteration 371, loss = 0.34711193\n",
      "Iteration 372, loss = 0.34701649\n",
      "Iteration 373, loss = 0.34692446\n",
      "Iteration 374, loss = 0.34679429\n",
      "Iteration 375, loss = 0.34669276\n",
      "Iteration 376, loss = 0.34661376\n",
      "Iteration 377, loss = 0.34648801\n",
      "Iteration 378, loss = 0.34639438\n",
      "Iteration 379, loss = 0.34632033\n",
      "Iteration 380, loss = 0.34618672\n",
      "Iteration 381, loss = 0.34608339\n",
      "Iteration 382, loss = 0.34597587\n",
      "Iteration 383, loss = 0.34589172\n",
      "Iteration 384, loss = 0.34579012\n",
      "Iteration 385, loss = 0.34569864\n",
      "Iteration 386, loss = 0.34558456\n",
      "Iteration 387, loss = 0.34547933\n",
      "Iteration 388, loss = 0.34538903\n",
      "Iteration 389, loss = 0.34528336\n",
      "Iteration 390, loss = 0.34521083\n",
      "Iteration 391, loss = 0.34511789\n",
      "Iteration 392, loss = 0.34504070\n",
      "Iteration 393, loss = 0.34491075\n",
      "Iteration 394, loss = 0.34479802\n",
      "Iteration 395, loss = 0.34470268\n",
      "Iteration 396, loss = 0.34463498\n",
      "Iteration 397, loss = 0.34451464\n",
      "Iteration 398, loss = 0.34441709\n",
      "Iteration 399, loss = 0.34433526\n",
      "Iteration 400, loss = 0.34422339\n",
      "Iteration 401, loss = 0.34415611\n",
      "Iteration 402, loss = 0.34404826\n",
      "Iteration 403, loss = 0.34401507\n",
      "Iteration 404, loss = 0.34387869\n",
      "Iteration 405, loss = 0.34375375\n",
      "Iteration 406, loss = 0.34365458\n",
      "Iteration 407, loss = 0.34357492\n",
      "Iteration 408, loss = 0.34349187\n",
      "Iteration 409, loss = 0.34339147\n",
      "Iteration 410, loss = 0.34330534\n",
      "Iteration 411, loss = 0.34321203\n",
      "Iteration 412, loss = 0.34311235\n",
      "Iteration 413, loss = 0.34302042\n",
      "Iteration 414, loss = 0.34293101\n",
      "Iteration 415, loss = 0.34283263\n",
      "Iteration 416, loss = 0.34274978\n",
      "Iteration 417, loss = 0.34263953\n",
      "Iteration 418, loss = 0.34254831\n",
      "Iteration 419, loss = 0.34245765\n",
      "Iteration 420, loss = 0.34238795\n",
      "Iteration 421, loss = 0.34229118\n",
      "Iteration 422, loss = 0.34219318\n",
      "Iteration 423, loss = 0.34210208\n",
      "Iteration 424, loss = 0.34200154\n",
      "Iteration 425, loss = 0.34190354\n",
      "Iteration 426, loss = 0.34181838\n",
      "Iteration 427, loss = 0.34172198\n",
      "Iteration 428, loss = 0.34164741\n",
      "Iteration 429, loss = 0.34156076\n",
      "Iteration 430, loss = 0.34146555\n",
      "Iteration 431, loss = 0.34135576\n",
      "Iteration 432, loss = 0.34126616\n",
      "Iteration 433, loss = 0.34118302\n",
      "Iteration 434, loss = 0.34108924\n",
      "Iteration 435, loss = 0.34102262\n",
      "Iteration 436, loss = 0.34089394\n",
      "Iteration 437, loss = 0.34081840\n",
      "Iteration 438, loss = 0.34074139\n",
      "Iteration 439, loss = 0.34063311\n",
      "Iteration 440, loss = 0.34054043\n",
      "Iteration 441, loss = 0.34046364\n",
      "Iteration 442, loss = 0.34036491\n",
      "Iteration 443, loss = 0.34028357\n",
      "Iteration 444, loss = 0.34021991\n",
      "Iteration 445, loss = 0.34009384\n",
      "Iteration 446, loss = 0.34003540\n",
      "Iteration 447, loss = 0.33994829\n",
      "Iteration 448, loss = 0.33984291\n",
      "Iteration 449, loss = 0.33974478\n",
      "Iteration 450, loss = 0.33964644\n",
      "Iteration 451, loss = 0.33957343\n",
      "Iteration 452, loss = 0.33946852\n",
      "Iteration 453, loss = 0.33938174\n",
      "Iteration 454, loss = 0.33931898\n",
      "Iteration 455, loss = 0.33924887\n",
      "Iteration 456, loss = 0.33914848\n",
      "Iteration 457, loss = 0.33905366\n",
      "Iteration 458, loss = 0.33895832\n",
      "Iteration 459, loss = 0.33886521\n",
      "Iteration 460, loss = 0.33880186\n",
      "Iteration 461, loss = 0.33868641\n",
      "Iteration 462, loss = 0.33860230\n",
      "Iteration 463, loss = 0.33852686\n",
      "Iteration 464, loss = 0.33843857\n",
      "Iteration 465, loss = 0.33835953\n",
      "Iteration 466, loss = 0.33828648\n",
      "Iteration 467, loss = 0.33819074\n",
      "Iteration 468, loss = 0.33810401\n",
      "Iteration 469, loss = 0.33800862\n",
      "Iteration 470, loss = 0.33791974\n",
      "Iteration 471, loss = 0.33784288\n",
      "Iteration 472, loss = 0.33776652\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86181226\n",
      "Iteration 2, loss = 0.85262883\n",
      "Iteration 3, loss = 0.83914469\n",
      "Iteration 4, loss = 0.82432398\n",
      "Iteration 5, loss = 0.80825476\n",
      "Iteration 6, loss = 0.79268977\n",
      "Iteration 7, loss = 0.77847695\n",
      "Iteration 8, loss = 0.76474339\n",
      "Iteration 9, loss = 0.75297349\n",
      "Iteration 10, loss = 0.74249897\n",
      "Iteration 11, loss = 0.73243576\n",
      "Iteration 12, loss = 0.72400218\n",
      "Iteration 13, loss = 0.71601489\n",
      "Iteration 14, loss = 0.70886863\n",
      "Iteration 15, loss = 0.70207644\n",
      "Iteration 16, loss = 0.69593113\n",
      "Iteration 17, loss = 0.68994362\n",
      "Iteration 18, loss = 0.68473522\n",
      "Iteration 19, loss = 0.67949827\n",
      "Iteration 20, loss = 0.67451080\n",
      "Iteration 21, loss = 0.66989938\n",
      "Iteration 22, loss = 0.66510349\n",
      "Iteration 23, loss = 0.66079568\n",
      "Iteration 24, loss = 0.65637609\n",
      "Iteration 25, loss = 0.65201786\n",
      "Iteration 26, loss = 0.64778022\n",
      "Iteration 27, loss = 0.64352136\n",
      "Iteration 28, loss = 0.63949421\n",
      "Iteration 29, loss = 0.63530548\n",
      "Iteration 30, loss = 0.63120733\n",
      "Iteration 31, loss = 0.62717727\n",
      "Iteration 32, loss = 0.62320143\n",
      "Iteration 33, loss = 0.61912171\n",
      "Iteration 34, loss = 0.61509400\n",
      "Iteration 35, loss = 0.61113355\n",
      "Iteration 36, loss = 0.60706580\n",
      "Iteration 37, loss = 0.60320320\n",
      "Iteration 38, loss = 0.59924775\n",
      "Iteration 39, loss = 0.59522935\n",
      "Iteration 40, loss = 0.59140233\n",
      "Iteration 41, loss = 0.58747961\n",
      "Iteration 42, loss = 0.58371271\n",
      "Iteration 43, loss = 0.57983275\n",
      "Iteration 44, loss = 0.57610963\n",
      "Iteration 45, loss = 0.57227622\n",
      "Iteration 46, loss = 0.56851666\n",
      "Iteration 47, loss = 0.56484853\n",
      "Iteration 48, loss = 0.56118056\n",
      "Iteration 49, loss = 0.55748519\n",
      "Iteration 50, loss = 0.55396402\n",
      "Iteration 51, loss = 0.55033053\n",
      "Iteration 52, loss = 0.54677047\n",
      "Iteration 53, loss = 0.54338055\n",
      "Iteration 54, loss = 0.53993841\n",
      "Iteration 55, loss = 0.53658230\n",
      "Iteration 56, loss = 0.53321841\n",
      "Iteration 57, loss = 0.52996118\n",
      "Iteration 58, loss = 0.52674436\n",
      "Iteration 59, loss = 0.52349661\n",
      "Iteration 60, loss = 0.52034840\n",
      "Iteration 61, loss = 0.51729079\n",
      "Iteration 62, loss = 0.51424605\n",
      "Iteration 63, loss = 0.51135775\n",
      "Iteration 64, loss = 0.50840056\n",
      "Iteration 65, loss = 0.50552220\n",
      "Iteration 66, loss = 0.50278026\n",
      "Iteration 67, loss = 0.50003403\n",
      "Iteration 68, loss = 0.49727119\n",
      "Iteration 69, loss = 0.49473916\n",
      "Iteration 70, loss = 0.49218565\n",
      "Iteration 71, loss = 0.48969899\n",
      "Iteration 72, loss = 0.48734409\n",
      "Iteration 73, loss = 0.48475838\n",
      "Iteration 74, loss = 0.48265153\n",
      "Iteration 75, loss = 0.48025108\n",
      "Iteration 76, loss = 0.47819207\n",
      "Iteration 77, loss = 0.47595636\n",
      "Iteration 78, loss = 0.47382012\n",
      "Iteration 79, loss = 0.47181036\n",
      "Iteration 80, loss = 0.46988104\n",
      "Iteration 81, loss = 0.46792775\n",
      "Iteration 82, loss = 0.46603807\n",
      "Iteration 83, loss = 0.46423823\n",
      "Iteration 84, loss = 0.46251487\n",
      "Iteration 85, loss = 0.46079729\n",
      "Iteration 86, loss = 0.45914170\n",
      "Iteration 87, loss = 0.45760337\n",
      "Iteration 88, loss = 0.45599076\n",
      "Iteration 89, loss = 0.45443660\n",
      "Iteration 90, loss = 0.45287098\n",
      "Iteration 91, loss = 0.45147441\n",
      "Iteration 92, loss = 0.45007709\n",
      "Iteration 93, loss = 0.44869001\n",
      "Iteration 94, loss = 0.44728028\n",
      "Iteration 95, loss = 0.44598245\n",
      "Iteration 96, loss = 0.44476182\n",
      "Iteration 97, loss = 0.44355477\n",
      "Iteration 98, loss = 0.44229559\n",
      "Iteration 99, loss = 0.44117390\n",
      "Iteration 100, loss = 0.43996719\n",
      "Iteration 101, loss = 0.43886871\n",
      "Iteration 102, loss = 0.43777034\n",
      "Iteration 103, loss = 0.43674103\n",
      "Iteration 104, loss = 0.43562224\n",
      "Iteration 105, loss = 0.43469969\n",
      "Iteration 106, loss = 0.43374976\n",
      "Iteration 107, loss = 0.43274655\n",
      "Iteration 108, loss = 0.43180929\n",
      "Iteration 109, loss = 0.43100677\n",
      "Iteration 110, loss = 0.43008557\n",
      "Iteration 111, loss = 0.42920721\n",
      "Iteration 112, loss = 0.42829851\n",
      "Iteration 113, loss = 0.42751551\n",
      "Iteration 114, loss = 0.42666280\n",
      "Iteration 115, loss = 0.42590060\n",
      "Iteration 116, loss = 0.42515831\n",
      "Iteration 117, loss = 0.42438314\n",
      "Iteration 118, loss = 0.42369786\n",
      "Iteration 119, loss = 0.42295109\n",
      "Iteration 120, loss = 0.42221104\n",
      "Iteration 121, loss = 0.42153526\n",
      "Iteration 122, loss = 0.42081576\n",
      "Iteration 123, loss = 0.42015316\n",
      "Iteration 124, loss = 0.41950237\n",
      "Iteration 125, loss = 0.41886697\n",
      "Iteration 126, loss = 0.41823803\n",
      "Iteration 127, loss = 0.41757204\n",
      "Iteration 128, loss = 0.41696627\n",
      "Iteration 129, loss = 0.41638549\n",
      "Iteration 130, loss = 0.41580493\n",
      "Iteration 131, loss = 0.41523914\n",
      "Iteration 132, loss = 0.41461731\n",
      "Iteration 133, loss = 0.41407993\n",
      "Iteration 134, loss = 0.41356008\n",
      "Iteration 135, loss = 0.41296075\n",
      "Iteration 136, loss = 0.41244740\n",
      "Iteration 137, loss = 0.41191639\n",
      "Iteration 138, loss = 0.41140288\n",
      "Iteration 139, loss = 0.41089265\n",
      "Iteration 140, loss = 0.41039863\n",
      "Iteration 141, loss = 0.40991180\n",
      "Iteration 142, loss = 0.40946545\n",
      "Iteration 143, loss = 0.40896223\n",
      "Iteration 144, loss = 0.40846024\n",
      "Iteration 145, loss = 0.40798945\n",
      "Iteration 146, loss = 0.40755566\n",
      "Iteration 147, loss = 0.40710480\n",
      "Iteration 148, loss = 0.40664828\n",
      "Iteration 149, loss = 0.40620192\n",
      "Iteration 150, loss = 0.40578447\n",
      "Iteration 151, loss = 0.40544115\n",
      "Iteration 152, loss = 0.40492808\n",
      "Iteration 153, loss = 0.40450108\n",
      "Iteration 154, loss = 0.40408127\n",
      "Iteration 155, loss = 0.40369139\n",
      "Iteration 156, loss = 0.40327551\n",
      "Iteration 157, loss = 0.40288223\n",
      "Iteration 158, loss = 0.40249075\n",
      "Iteration 159, loss = 0.40211411\n",
      "Iteration 160, loss = 0.40171407\n",
      "Iteration 161, loss = 0.40130876\n",
      "Iteration 162, loss = 0.40093478\n",
      "Iteration 163, loss = 0.40054533\n",
      "Iteration 164, loss = 0.40019324\n",
      "Iteration 165, loss = 0.39980959\n",
      "Iteration 166, loss = 0.39953721\n",
      "Iteration 167, loss = 0.39906878\n",
      "Iteration 168, loss = 0.39874552\n",
      "Iteration 169, loss = 0.39837272\n",
      "Iteration 170, loss = 0.39802011\n",
      "Iteration 171, loss = 0.39768745\n",
      "Iteration 172, loss = 0.39732669\n",
      "Iteration 173, loss = 0.39695822\n",
      "Iteration 174, loss = 0.39661479\n",
      "Iteration 175, loss = 0.39627555\n",
      "Iteration 176, loss = 0.39596003\n",
      "Iteration 177, loss = 0.39564374\n",
      "Iteration 178, loss = 0.39528301\n",
      "Iteration 179, loss = 0.39497757\n",
      "Iteration 180, loss = 0.39461377\n",
      "Iteration 181, loss = 0.39426577\n",
      "Iteration 182, loss = 0.39399788\n",
      "Iteration 183, loss = 0.39368010\n",
      "Iteration 184, loss = 0.39333767\n",
      "Iteration 185, loss = 0.39306261\n",
      "Iteration 186, loss = 0.39272802\n",
      "Iteration 187, loss = 0.39240616\n",
      "Iteration 188, loss = 0.39208009\n",
      "Iteration 189, loss = 0.39178019\n",
      "Iteration 190, loss = 0.39147903\n",
      "Iteration 191, loss = 0.39119837\n",
      "Iteration 192, loss = 0.39089591\n",
      "Iteration 193, loss = 0.39057317\n",
      "Iteration 194, loss = 0.39026994\n",
      "Iteration 195, loss = 0.38998642\n",
      "Iteration 196, loss = 0.38970870\n",
      "Iteration 197, loss = 0.38941793\n",
      "Iteration 198, loss = 0.38912154\n",
      "Iteration 199, loss = 0.38892263\n",
      "Iteration 200, loss = 0.38854361\n",
      "Iteration 201, loss = 0.38828838\n",
      "Iteration 202, loss = 0.38801582\n",
      "Iteration 203, loss = 0.38770952\n",
      "Iteration 204, loss = 0.38744759\n",
      "Iteration 205, loss = 0.38716531\n",
      "Iteration 206, loss = 0.38688954\n",
      "Iteration 207, loss = 0.38665893\n",
      "Iteration 208, loss = 0.38634721\n",
      "Iteration 209, loss = 0.38608629\n",
      "Iteration 210, loss = 0.38581289\n",
      "Iteration 211, loss = 0.38556357\n",
      "Iteration 212, loss = 0.38528887\n",
      "Iteration 213, loss = 0.38502035\n",
      "Iteration 214, loss = 0.38477656\n",
      "Iteration 215, loss = 0.38452926\n",
      "Iteration 216, loss = 0.38426125\n",
      "Iteration 217, loss = 0.38402862\n",
      "Iteration 218, loss = 0.38378361\n",
      "Iteration 219, loss = 0.38349958\n",
      "Iteration 220, loss = 0.38324767\n",
      "Iteration 221, loss = 0.38299208\n",
      "Iteration 222, loss = 0.38277695\n",
      "Iteration 223, loss = 0.38248878\n",
      "Iteration 224, loss = 0.38226437\n",
      "Iteration 225, loss = 0.38204404\n",
      "Iteration 226, loss = 0.38176550\n",
      "Iteration 227, loss = 0.38151871\n",
      "Iteration 228, loss = 0.38128760\n",
      "Iteration 229, loss = 0.38106577\n",
      "Iteration 230, loss = 0.38082073\n",
      "Iteration 231, loss = 0.38062727\n",
      "Iteration 232, loss = 0.38036312\n",
      "Iteration 233, loss = 0.38012307\n",
      "Iteration 234, loss = 0.37989308\n",
      "Iteration 235, loss = 0.37967339\n",
      "Iteration 236, loss = 0.37942169\n",
      "Iteration 237, loss = 0.37922183\n",
      "Iteration 238, loss = 0.37898836\n",
      "Iteration 239, loss = 0.37876932\n",
      "Iteration 240, loss = 0.37858611\n",
      "Iteration 241, loss = 0.37831085\n",
      "Iteration 242, loss = 0.37809449\n",
      "Iteration 243, loss = 0.37788673\n",
      "Iteration 244, loss = 0.37769074\n",
      "Iteration 245, loss = 0.37742151\n",
      "Iteration 246, loss = 0.37723546\n",
      "Iteration 247, loss = 0.37700646\n",
      "Iteration 248, loss = 0.37683155\n",
      "Iteration 249, loss = 0.37658363\n",
      "Iteration 250, loss = 0.37637283\n",
      "Iteration 251, loss = 0.37613476\n",
      "Iteration 252, loss = 0.37597268\n",
      "Iteration 253, loss = 0.37573457\n",
      "Iteration 254, loss = 0.37552704\n",
      "Iteration 255, loss = 0.37533483\n",
      "Iteration 256, loss = 0.37513798\n",
      "Iteration 257, loss = 0.37489077\n",
      "Iteration 258, loss = 0.37471945\n",
      "Iteration 259, loss = 0.37451607\n",
      "Iteration 260, loss = 0.37425477\n",
      "Iteration 261, loss = 0.37410612\n",
      "Iteration 262, loss = 0.37386643\n",
      "Iteration 263, loss = 0.37365775\n",
      "Iteration 264, loss = 0.37346268\n",
      "Iteration 265, loss = 0.37327341\n",
      "Iteration 266, loss = 0.37306416\n",
      "Iteration 267, loss = 0.37287475\n",
      "Iteration 268, loss = 0.37269178\n",
      "Iteration 269, loss = 0.37246761\n",
      "Iteration 270, loss = 0.37225910\n",
      "Iteration 271, loss = 0.37207680\n",
      "Iteration 272, loss = 0.37188881\n",
      "Iteration 273, loss = 0.37170105\n",
      "Iteration 274, loss = 0.37149668\n",
      "Iteration 275, loss = 0.37128184\n",
      "Iteration 276, loss = 0.37111219\n",
      "Iteration 277, loss = 0.37090861\n",
      "Iteration 278, loss = 0.37074849\n",
      "Iteration 279, loss = 0.37055872\n",
      "Iteration 280, loss = 0.37034465\n",
      "Iteration 281, loss = 0.37015064\n",
      "Iteration 276, loss = 0.38144852\n",
      "Iteration 277, loss = 0.38121896\n",
      "Iteration 278, loss = 0.38096995\n",
      "Iteration 279, loss = 0.38074966\n",
      "Iteration 280, loss = 0.38044275\n",
      "Iteration 281, loss = 0.38017246\n",
      "Iteration 282, loss = 0.37991786\n",
      "Iteration 283, loss = 0.37974574\n",
      "Iteration 284, loss = 0.37941752\n",
      "Iteration 285, loss = 0.37918738\n",
      "Iteration 286, loss = 0.37895050\n",
      "Iteration 287, loss = 0.37873125\n",
      "Iteration 288, loss = 0.37849180\n",
      "Iteration 289, loss = 0.37820601\n",
      "Iteration 290, loss = 0.37802635\n",
      "Iteration 291, loss = 0.37774811\n",
      "Iteration 292, loss = 0.37750275\n",
      "Iteration 293, loss = 0.37734591\n",
      "Iteration 294, loss = 0.37708166\n",
      "Iteration 295, loss = 0.37700413\n",
      "Iteration 296, loss = 0.37666091\n",
      "Iteration 297, loss = 0.37635116\n",
      "Iteration 298, loss = 0.37614272\n",
      "Iteration 299, loss = 0.37601178\n",
      "Iteration 300, loss = 0.37568396\n",
      "Iteration 301, loss = 0.37545865\n",
      "Iteration 302, loss = 0.37529809\n",
      "Iteration 303, loss = 0.37504018\n",
      "Iteration 304, loss = 0.37487291\n",
      "Iteration 305, loss = 0.37459730\n",
      "Iteration 306, loss = 0.37436553\n",
      "Iteration 307, loss = 0.37416738\n",
      "Iteration 308, loss = 0.37398681\n",
      "Iteration 309, loss = 0.37379066\n",
      "Iteration 310, loss = 0.37364149\n",
      "Iteration 311, loss = 0.37332207\n",
      "Iteration 312, loss = 0.37313903\n",
      "Iteration 313, loss = 0.37291051\n",
      "Iteration 314, loss = 0.37271544\n",
      "Iteration 315, loss = 0.37249827\n",
      "Iteration 316, loss = 0.37229155\n",
      "Iteration 317, loss = 0.37212306\n",
      "Iteration 318, loss = 0.37192051\n",
      "Iteration 319, loss = 0.37171970\n",
      "Iteration 320, loss = 0.37151270\n",
      "Iteration 321, loss = 0.37133326\n",
      "Iteration 322, loss = 0.37113929\n",
      "Iteration 323, loss = 0.37094693\n",
      "Iteration 324, loss = 0.37072990\n",
      "Iteration 325, loss = 0.37052596\n",
      "Iteration 326, loss = 0.37032744\n",
      "Iteration 327, loss = 0.37016218\n",
      "Iteration 328, loss = 0.36995390\n",
      "Iteration 329, loss = 0.36978053\n",
      "Iteration 330, loss = 0.36958926\n",
      "Iteration 331, loss = 0.36947872\n",
      "Iteration 332, loss = 0.36919180\n",
      "Iteration 333, loss = 0.36910619\n",
      "Iteration 334, loss = 0.36883878\n",
      "Iteration 335, loss = 0.36865420\n",
      "Iteration 336, loss = 0.36845096\n",
      "Iteration 337, loss = 0.36829203\n",
      "Iteration 338, loss = 0.36813009\n",
      "Iteration 339, loss = 0.36790216\n",
      "Iteration 340, loss = 0.36780269\n",
      "Iteration 341, loss = 0.36755256\n",
      "Iteration 342, loss = 0.36741172\n",
      "Iteration 343, loss = 0.36720234\n",
      "Iteration 344, loss = 0.36702359\n",
      "Iteration 345, loss = 0.36691791\n",
      "Iteration 346, loss = 0.36671528\n",
      "Iteration 347, loss = 0.36647702\n",
      "Iteration 348, loss = 0.36632491\n",
      "Iteration 349, loss = 0.36620498\n",
      "Iteration 350, loss = 0.36596284\n",
      "Iteration 351, loss = 0.36579964\n",
      "Iteration 352, loss = 0.36565547\n",
      "Iteration 353, loss = 0.36544024\n",
      "Iteration 354, loss = 0.36525492\n",
      "Iteration 355, loss = 0.36511581\n",
      "Iteration 356, loss = 0.36501322\n",
      "Iteration 357, loss = 0.36480718\n",
      "Iteration 358, loss = 0.36456573\n",
      "Iteration 359, loss = 0.36450013\n",
      "Iteration 360, loss = 0.36423286\n",
      "Iteration 361, loss = 0.36408938\n",
      "Iteration 362, loss = 0.36389634\n",
      "Iteration 363, loss = 0.36374177\n",
      "Iteration 364, loss = 0.36354360\n",
      "Iteration 365, loss = 0.36341957\n",
      "Iteration 366, loss = 0.36325585\n",
      "Iteration 367, loss = 0.36305232\n",
      "Iteration 368, loss = 0.36299154\n",
      "Iteration 369, loss = 0.36273760\n",
      "Iteration 370, loss = 0.36261343\n",
      "Iteration 371, loss = 0.36242033\n",
      "Iteration 372, loss = 0.36223236\n",
      "Iteration 373, loss = 0.36206326\n",
      "Iteration 374, loss = 0.36191357\n",
      "Iteration 375, loss = 0.36173071\n",
      "Iteration 376, loss = 0.36157857\n",
      "Iteration 377, loss = 0.36149659\n",
      "Iteration 378, loss = 0.36130583\n",
      "Iteration 379, loss = 0.36117304\n",
      "Iteration 380, loss = 0.36090892\n",
      "Iteration 381, loss = 0.36073887\n",
      "Iteration 382, loss = 0.36058175\n",
      "Iteration 383, loss = 0.36051318\n",
      "Iteration 384, loss = 0.36026660\n",
      "Iteration 385, loss = 0.36009768\n",
      "Iteration 386, loss = 0.35991831\n",
      "Iteration 387, loss = 0.35979046\n",
      "Iteration 388, loss = 0.35965195\n",
      "Iteration 389, loss = 0.35944736\n",
      "Iteration 390, loss = 0.35929584\n",
      "Iteration 391, loss = 0.35912952\n",
      "Iteration 392, loss = 0.35896563\n",
      "Iteration 393, loss = 0.35878637\n",
      "Iteration 394, loss = 0.35868496\n",
      "Iteration 395, loss = 0.35845801\n",
      "Iteration 396, loss = 0.35831228\n",
      "Iteration 397, loss = 0.35815003\n",
      "Iteration 398, loss = 0.35796499\n",
      "Iteration 399, loss = 0.35790809\n",
      "Iteration 400, loss = 0.35763380\n",
      "Iteration 401, loss = 0.35746583\n",
      "Iteration 402, loss = 0.35736422\n",
      "Iteration 403, loss = 0.35715012\n",
      "Iteration 404, loss = 0.35701317\n",
      "Iteration 405, loss = 0.35689968\n",
      "Iteration 406, loss = 0.35666892\n",
      "Iteration 407, loss = 0.35649796\n",
      "Iteration 408, loss = 0.35633213\n",
      "Iteration 409, loss = 0.35630554\n",
      "Iteration 410, loss = 0.35603939\n",
      "Iteration 411, loss = 0.35596252\n",
      "Iteration 412, loss = 0.35572683\n",
      "Iteration 413, loss = 0.35556784\n",
      "Iteration 414, loss = 0.35540827\n",
      "Iteration 415, loss = 0.35519947\n",
      "Iteration 416, loss = 0.35505393\n",
      "Iteration 417, loss = 0.35489810\n",
      "Iteration 418, loss = 0.35472726\n",
      "Iteration 419, loss = 0.35459940\n",
      "Iteration 420, loss = 0.35439657\n",
      "Iteration 421, loss = 0.35429175\n",
      "Iteration 422, loss = 0.35411554\n",
      "Iteration 423, loss = 0.35393936\n",
      "Iteration 424, loss = 0.35379165\n",
      "Iteration 425, loss = 0.35360217\n",
      "Iteration 426, loss = 0.35345404\n",
      "Iteration 427, loss = 0.35331727\n",
      "Iteration 428, loss = 0.35314685\n",
      "Iteration 429, loss = 0.35297860\n",
      "Iteration 430, loss = 0.35287782\n",
      "Iteration 431, loss = 0.35268225\n",
      "Iteration 432, loss = 0.35250766\n",
      "Iteration 433, loss = 0.35235630\n",
      "Iteration 434, loss = 0.35221739\n",
      "Iteration 435, loss = 0.35202910\n",
      "Iteration 436, loss = 0.35188228\n",
      "Iteration 437, loss = 0.35176444\n",
      "Iteration 438, loss = 0.35155322\n",
      "Iteration 439, loss = 0.35143084\n",
      "Iteration 440, loss = 0.35126434\n",
      "Iteration 441, loss = 0.35107688\n",
      "Iteration 442, loss = 0.35091602\n",
      "Iteration 443, loss = 0.35077799\n",
      "Iteration 444, loss = 0.35064265\n",
      "Iteration 445, loss = 0.35044992\n",
      "Iteration 446, loss = 0.35029590\n",
      "Iteration 447, loss = 0.35010829\n",
      "Iteration 448, loss = 0.34996821\n",
      "Iteration 449, loss = 0.34979337\n",
      "Iteration 450, loss = 0.34961473\n",
      "Iteration 451, loss = 0.34948542\n",
      "Iteration 452, loss = 0.34935834\n",
      "Iteration 453, loss = 0.34927824\n",
      "Iteration 454, loss = 0.34905579\n",
      "Iteration 455, loss = 0.34884528\n",
      "Iteration 456, loss = 0.34869823\n",
      "Iteration 457, loss = 0.34857203\n",
      "Iteration 458, loss = 0.34840030\n",
      "Iteration 459, loss = 0.34826932\n",
      "Iteration 460, loss = 0.34814432\n",
      "Iteration 461, loss = 0.34795864\n",
      "Iteration 462, loss = 0.34778598\n",
      "Iteration 463, loss = 0.34764219\n",
      "Iteration 464, loss = 0.34747019\n",
      "Iteration 465, loss = 0.34732945\n",
      "Iteration 466, loss = 0.34714621\n",
      "Iteration 467, loss = 0.34700828\n",
      "Iteration 468, loss = 0.34684640\n",
      "Iteration 469, loss = 0.34678878\n",
      "Iteration 470, loss = 0.34656967\n",
      "Iteration 471, loss = 0.34638189\n",
      "Iteration 472, loss = 0.34624673\n",
      "Iteration 473, loss = 0.34610224\n",
      "Iteration 474, loss = 0.34593338\n",
      "Iteration 475, loss = 0.34586915\n",
      "Iteration 476, loss = 0.34564191\n",
      "Iteration 477, loss = 0.34551438\n",
      "Iteration 478, loss = 0.34538518\n",
      "Iteration 479, loss = 0.34520214\n",
      "Iteration 480, loss = 0.34505453\n",
      "Iteration 481, loss = 0.34493833\n",
      "Iteration 482, loss = 0.34484932\n",
      "Iteration 483, loss = 0.34467900\n",
      "Iteration 484, loss = 0.34451247\n",
      "Iteration 485, loss = 0.34440804\n",
      "Iteration 486, loss = 0.34425559\n",
      "Iteration 487, loss = 0.34406634\n",
      "Iteration 488, loss = 0.34392751\n",
      "Iteration 489, loss = 0.34384433\n",
      "Iteration 490, loss = 0.34364869\n",
      "Iteration 491, loss = 0.34349894\n",
      "Iteration 492, loss = 0.34334776\n",
      "Iteration 493, loss = 0.34320870\n",
      "Iteration 494, loss = 0.34306523\n",
      "Iteration 495, loss = 0.34289576\n",
      "Iteration 496, loss = 0.34277293\n",
      "Iteration 497, loss = 0.34261655\n",
      "Iteration 498, loss = 0.34246790\n",
      "Iteration 499, loss = 0.34232452\n",
      "Iteration 500, loss = 0.34220388\n",
      "Iteration 501, loss = 0.34204122\n",
      "Iteration 502, loss = 0.34189446\n",
      "Iteration 503, loss = 0.34179478\n",
      "Iteration 504, loss = 0.34162979\n",
      "Iteration 505, loss = 0.34148377\n",
      "Iteration 506, loss = 0.34135577\n",
      "Iteration 507, loss = 0.34119545\n",
      "Iteration 508, loss = 0.34103515\n",
      "Iteration 509, loss = 0.34091171\n",
      "Iteration 510, loss = 0.34078847\n",
      "Iteration 511, loss = 0.34066017\n",
      "Iteration 512, loss = 0.34047408\n",
      "Iteration 513, loss = 0.34034657\n",
      "Iteration 514, loss = 0.34019979\n",
      "Iteration 515, loss = 0.34006006\n",
      "Iteration 516, loss = 0.33989464\n",
      "Iteration 517, loss = 0.33980952\n",
      "Iteration 518, loss = 0.33972448\n",
      "Iteration 519, loss = 0.33949522\n",
      "Iteration 520, loss = 0.33936976\n",
      "Iteration 521, loss = 0.33923020\n",
      "Iteration 522, loss = 0.33909731\n",
      "Iteration 523, loss = 0.33896372\n",
      "Iteration 524, loss = 0.33880478\n",
      "Iteration 525, loss = 0.33863657\n",
      "Iteration 526, loss = 0.33853169\n",
      "Iteration 527, loss = 0.33836797\n",
      "Iteration 528, loss = 0.33843934\n",
      "Iteration 529, loss = 0.33809541\n",
      "Iteration 530, loss = 0.33792893\n",
      "Iteration 531, loss = 0.33784935\n",
      "Iteration 532, loss = 0.33769630\n",
      "Iteration 533, loss = 0.33753392\n",
      "Iteration 534, loss = 0.33738090\n",
      "Iteration 535, loss = 0.33724922\n",
      "Iteration 536, loss = 0.33712129\n",
      "Iteration 537, loss = 0.33701848\n",
      "Iteration 538, loss = 0.33682873\n",
      "Iteration 539, loss = 0.33671789\n",
      "Iteration 540, loss = 0.33654621\n",
      "Iteration 541, loss = 0.33637598\n",
      "Iteration 542, loss = 0.33626738\n",
      "Iteration 543, loss = 0.33611994\n",
      "Iteration 544, loss = 0.33604017\n",
      "Iteration 545, loss = 0.33587879\n",
      "Iteration 546, loss = 0.33571143\n",
      "Iteration 547, loss = 0.33557643\n",
      "Iteration 548, loss = 0.33549969\n",
      "Iteration 549, loss = 0.33529672\n",
      "Iteration 550, loss = 0.33524776\n",
      "Iteration 551, loss = 0.33504338\n",
      "Iteration 552, loss = 0.33489247\n",
      "Iteration 553, loss = 0.33472502\n",
      "Iteration 554, loss = 0.33460714\n",
      "Iteration 555, loss = 0.33459958\n",
      "Iteration 556, loss = 0.33434917\n",
      "Iteration 557, loss = 0.33419259\n",
      "Iteration 558, loss = 0.33406109\n",
      "Iteration 559, loss = 0.33390028\n",
      "Iteration 560, loss = 0.33380461\n",
      "Iteration 561, loss = 0.33366670\n",
      "Iteration 562, loss = 0.33350578\n",
      "Iteration 563, loss = 0.33336828\n",
      "Iteration 564, loss = 0.33320873\n",
      "Iteration 565, loss = 0.33308024\n",
      "Iteration 566, loss = 0.33293214\n",
      "Iteration 567, loss = 0.33283768\n",
      "Iteration 568, loss = 0.33264546\n",
      "Iteration 569, loss = 0.33251909\n",
      "Iteration 570, loss = 0.33245436\n",
      "Iteration 571, loss = 0.33226633\n",
      "Iteration 572, loss = 0.33221938\n",
      "Iteration 573, loss = 0.33209314\n",
      "Iteration 574, loss = 0.33181546\n",
      "Iteration 575, loss = 0.33168115\n",
      "Iteration 576, loss = 0.33175857\n",
      "Iteration 577, loss = 0.33144259\n",
      "Iteration 578, loss = 0.33127054\n",
      "Iteration 579, loss = 0.33115342\n",
      "Iteration 580, loss = 0.33109361\n",
      "Iteration 581, loss = 0.33091380\n",
      "Iteration 582, loss = 0.33072157\n",
      "Iteration 583, loss = 0.33063286\n",
      "Iteration 584, loss = 0.33048321\n",
      "Iteration 585, loss = 0.33032304\n",
      "Iteration 586, loss = 0.33017840\n",
      "Iteration 587, loss = 0.33003684\n",
      "Iteration 588, loss = 0.32994616\n",
      "Iteration 589, loss = 0.32979957\n",
      "Iteration 590, loss = 0.32965878\n",
      "Iteration 591, loss = 0.32961904\n",
      "Iteration 592, loss = 0.32935438\n",
      "Iteration 593, loss = 0.32923921\n",
      "Iteration 594, loss = 0.32910371\n",
      "Iteration 595, loss = 0.32899776\n",
      "Iteration 596, loss = 0.32890226\n",
      "Iteration 597, loss = 0.32867831\n",
      "Iteration 598, loss = 0.32856921\n",
      "Iteration 599, loss = 0.32842630\n",
      "Iteration 600, loss = 0.32835319\n",
      "Iteration 601, loss = 0.32814321\n",
      "Iteration 602, loss = 0.32799831\n",
      "Iteration 603, loss = 0.32789040\n",
      "Iteration 604, loss = 0.32776369\n",
      "Iteration 605, loss = 0.32762911\n",
      "Iteration 606, loss = 0.32749236\n",
      "Iteration 607, loss = 0.32734020\n",
      "Iteration 608, loss = 0.32717118\n",
      "Iteration 609, loss = 0.32705612\n",
      "Iteration 610, loss = 0.32691029\n",
      "Iteration 611, loss = 0.32679486\n",
      "Iteration 612, loss = 0.32660492\n",
      "Iteration 613, loss = 0.32646351\n",
      "Iteration 614, loss = 0.32632157\n",
      "Iteration 615, loss = 0.32632456\n",
      "Iteration 616, loss = 0.32603931\n",
      "Iteration 617, loss = 0.32590231\n",
      "Iteration 618, loss = 0.32575630\n",
      "Iteration 619, loss = 0.32566137\n",
      "Iteration 620, loss = 0.32552650\n",
      "Iteration 621, loss = 0.32539515\n",
      "Iteration 622, loss = 0.32518339\n",
      "Iteration 623, loss = 0.32504198\n",
      "Iteration 624, loss = 0.32490094\n",
      "Iteration 625, loss = 0.32478016\n",
      "Iteration 626, loss = 0.32473206\n",
      "Iteration 627, loss = 0.32456337\n",
      "Iteration 628, loss = 0.32433913\n",
      "Iteration 629, loss = 0.32419759\n",
      "Iteration 630, loss = 0.32405325\n",
      "Iteration 631, loss = 0.32396878\n",
      "Iteration 632, loss = 0.32380371\n",
      "Iteration 633, loss = 0.32361791\n",
      "Iteration 634, loss = 0.32349785\n",
      "Iteration 635, loss = 0.32335405\n",
      "Iteration 636, loss = 0.32323602\n",
      "Iteration 637, loss = 0.32310525\n",
      "Iteration 638, loss = 0.32294296\n",
      "Iteration 639, loss = 0.32285633\n",
      "Iteration 640, loss = 0.32274699\n",
      "Iteration 641, loss = 0.32256307\n",
      "Iteration 642, loss = 0.32239631\n",
      "Iteration 643, loss = 0.32223750\n",
      "Iteration 644, loss = 0.32210686\n",
      "Iteration 645, loss = 0.32203486\n",
      "Iteration 646, loss = 0.32183350\n",
      "Iteration 647, loss = 0.32168262\n",
      "Iteration 648, loss = 0.32154737\n",
      "Iteration 649, loss = 0.32140874\n",
      "Iteration 650, loss = 0.32136112\n",
      "Iteration 651, loss = 0.32113107\n",
      "Iteration 652, loss = 0.32099708\n",
      "Iteration 653, loss = 0.32087106\n",
      "Iteration 654, loss = 0.32073453\n",
      "Iteration 655, loss = 0.32057540\n",
      "Iteration 656, loss = 0.32042227\n",
      "Iteration 657, loss = 0.32027069\n",
      "Iteration 658, loss = 0.32027772\n",
      "Iteration 659, loss = 0.32003456\n",
      "Iteration 660, loss = 0.31986023\n",
      "Iteration 661, loss = 0.31975861\n",
      "Iteration 662, loss = 0.31958813\n",
      "Iteration 663, loss = 0.31950245\n",
      "Iteration 664, loss = 0.31946391\n",
      "Iteration 665, loss = 0.31914540\n",
      "Iteration 666, loss = 0.31900846\n",
      "Iteration 667, loss = 0.31885445\n",
      "Iteration 668, loss = 0.31870550\n",
      "Iteration 669, loss = 0.31859760\n",
      "Iteration 670, loss = 0.31845755\n",
      "Iteration 671, loss = 0.31834756\n",
      "Iteration 672, loss = 0.31813033\n",
      "Iteration 673, loss = 0.31800612\n",
      "Iteration 674, loss = 0.31793214\n",
      "Iteration 675, loss = 0.31774340\n",
      "Iteration 676, loss = 0.31757354\n",
      "Iteration 677, loss = 0.31766338\n",
      "Iteration 678, loss = 0.31728962\n",
      "Iteration 679, loss = 0.31720241\n",
      "Iteration 680, loss = 0.31702162\n",
      "Iteration 681, loss = 0.31690219\n",
      "Iteration 682, loss = 0.31675441\n",
      "Iteration 683, loss = 0.31660454\n",
      "Iteration 684, loss = 0.31646165\n",
      "Iteration 685, loss = 0.31630874\n",
      "Iteration 686, loss = 0.31617186\n",
      "Iteration 687, loss = 0.31601778\n",
      "Iteration 688, loss = 0.31604490\n",
      "Iteration 689, loss = 0.31572053\n",
      "Iteration 690, loss = 0.31557575\n",
      "Iteration 691, loss = 0.31544894\n",
      "Iteration 692, loss = 0.31542633\n",
      "Iteration 693, loss = 0.31524666\n",
      "Iteration 694, loss = 0.31502772\n",
      "Iteration 695, loss = 0.31490144\n",
      "Iteration 696, loss = 0.31471431\n",
      "Iteration 697, loss = 0.31460965\n",
      "Iteration 698, loss = 0.31443912\n",
      "Iteration 699, loss = 0.31431715\n",
      "Iteration 700, loss = 0.31414487\n",
      "Iteration 701, loss = 0.31404240\n",
      "Iteration 702, loss = 0.31391628\n",
      "Iteration 703, loss = 0.31376914\n",
      "Iteration 704, loss = 0.31360607\n",
      "Iteration 705, loss = 0.31352269\n",
      "Iteration 706, loss = 0.31338962\n",
      "Iteration 707, loss = 0.31322321\n",
      "Iteration 708, loss = 0.31306308\n",
      "Iteration 709, loss = 0.31290240\n",
      "Iteration 710, loss = 0.31277900\n",
      "Iteration 711, loss = 0.31263838\n",
      "Iteration 712, loss = 0.31252700\n",
      "Iteration 713, loss = 0.31233665\n",
      "Iteration 714, loss = 0.31219951\n",
      "Iteration 715, loss = 0.31208254\n",
      "Iteration 716, loss = 0.31194013\n",
      "Iteration 717, loss = 0.31182484\n",
      "Iteration 718, loss = 0.31170327\n",
      "Iteration 719, loss = 0.31148954\n",
      "Iteration 720, loss = 0.31140515\n",
      "Iteration 721, loss = 0.31126309\n",
      "Iteration 722, loss = 0.31118325\n",
      "Iteration 723, loss = 0.31094842\n",
      "Iteration 724, loss = 0.31088485\n",
      "Iteration 725, loss = 0.31076564\n",
      "Iteration 726, loss = 0.31054728\n",
      "Iteration 727, loss = 0.31040790\n",
      "Iteration 728, loss = 0.31027591\n",
      "Iteration 729, loss = 0.31014658\n",
      "Iteration 730, loss = 0.31000163\n",
      "Iteration 731, loss = 0.30990917\n",
      "Iteration 732, loss = 0.30982730\n",
      "Iteration 733, loss = 0.30961049\n",
      "Iteration 734, loss = 0.30951922\n",
      "Iteration 735, loss = 0.30931242\n",
      "Iteration 736, loss = 0.30916841\n",
      "Iteration 737, loss = 0.30902852\n",
      "Iteration 738, loss = 0.30889652\n",
      "Iteration 739, loss = 0.30873608\n",
      "Iteration 740, loss = 0.30860952\n",
      "Iteration 741, loss = 0.30848416\n",
      "Iteration 742, loss = 0.30830617\n",
      "Iteration 743, loss = 0.30823320\n",
      "Iteration 744, loss = 0.30804559\n",
      "Iteration 745, loss = 0.30790891\n",
      "Iteration 746, loss = 0.30783554\n",
      "Iteration 747, loss = 0.30766786\n",
      "Iteration 748, loss = 0.30748844\n",
      "Iteration 749, loss = 0.30737597\n",
      "Iteration 750, loss = 0.30723797\n",
      "Iteration 751, loss = 0.30729908\n",
      "Iteration 752, loss = 0.30692790\n",
      "Iteration 753, loss = 0.30677608\n",
      "Iteration 754, loss = 0.30673571\n",
      "Iteration 755, loss = 0.30659887\n",
      "Iteration 756, loss = 0.30637471\n",
      "Iteration 757, loss = 0.30621377\n",
      "Iteration 758, loss = 0.30608061\n",
      "Iteration 759, loss = 0.30600970\n",
      "Iteration 760, loss = 0.30580572\n",
      "Iteration 761, loss = 0.30571424\n",
      "Iteration 762, loss = 0.30553664\n",
      "Iteration 763, loss = 0.30537811\n",
      "Iteration 764, loss = 0.30524373\n",
      "Iteration 765, loss = 0.30518092\n",
      "Iteration 766, loss = 0.30498142\n",
      "Iteration 767, loss = 0.30484609\n",
      "Iteration 768, loss = 0.30468068\n",
      "Iteration 769, loss = 0.30451601\n",
      "Iteration 770, loss = 0.30441689\n",
      "Iteration 771, loss = 0.30425342\n",
      "Iteration 47, loss = 0.47765893\n",
      "Iteration 48, loss = 0.47478656\n",
      "Iteration 49, loss = 0.47205448\n",
      "Iteration 50, loss = 0.46946814\n",
      "Iteration 51, loss = 0.46685065\n",
      "Iteration 52, loss = 0.46429124\n",
      "Iteration 53, loss = 0.46190089\n",
      "Iteration 54, loss = 0.45942698\n",
      "Iteration 55, loss = 0.45708850\n",
      "Iteration 56, loss = 0.45468162\n",
      "Iteration 57, loss = 0.45250920\n",
      "Iteration 58, loss = 0.45031806\n",
      "Iteration 59, loss = 0.44816582\n",
      "Iteration 60, loss = 0.44608406\n",
      "Iteration 61, loss = 0.44403341\n",
      "Iteration 62, loss = 0.44205451\n",
      "Iteration 63, loss = 0.44015714\n",
      "Iteration 64, loss = 0.43828689\n",
      "Iteration 65, loss = 0.43632991\n",
      "Iteration 66, loss = 0.43455682\n",
      "Iteration 67, loss = 0.43278557\n",
      "Iteration 68, loss = 0.43103038\n",
      "Iteration 69, loss = 0.42934470\n",
      "Iteration 70, loss = 0.42767891\n",
      "Iteration 71, loss = 0.42602159\n",
      "Iteration 72, loss = 0.42448948\n",
      "Iteration 73, loss = 0.42294693\n",
      "Iteration 74, loss = 0.42134007\n",
      "Iteration 75, loss = 0.41987322\n",
      "Iteration 76, loss = 0.41841875\n",
      "Iteration 77, loss = 0.41697989\n",
      "Iteration 78, loss = 0.41558549\n",
      "Iteration 79, loss = 0.41420484\n",
      "Iteration 80, loss = 0.41291366\n",
      "Iteration 81, loss = 0.41152149\n",
      "Iteration 82, loss = 0.41023300\n",
      "Iteration 83, loss = 0.40899523\n",
      "Iteration 84, loss = 0.40773929\n",
      "Iteration 85, loss = 0.40651655\n",
      "Iteration 86, loss = 0.40530672\n",
      "Iteration 87, loss = 0.40417116\n",
      "Iteration 88, loss = 0.40302066\n",
      "Iteration 89, loss = 0.40193525\n",
      "Iteration 90, loss = 0.40086568\n",
      "Iteration 91, loss = 0.39973521\n",
      "Iteration 92, loss = 0.39867889\n",
      "Iteration 93, loss = 0.39767672\n",
      "Iteration 94, loss = 0.39665059\n",
      "Iteration 95, loss = 0.39565465\n",
      "Iteration 96, loss = 0.39467745\n",
      "Iteration 97, loss = 0.39368615\n",
      "Iteration 98, loss = 0.39278266\n",
      "Iteration 99, loss = 0.39183520\n",
      "Iteration 100, loss = 0.39094747\n",
      "Iteration 101, loss = 0.38999117\n",
      "Iteration 102, loss = 0.38918109\n",
      "Iteration 103, loss = 0.38832030\n",
      "Iteration 104, loss = 0.38747870\n",
      "Iteration 105, loss = 0.38660911\n",
      "Iteration 106, loss = 0.38581907\n",
      "Iteration 107, loss = 0.38499917\n",
      "Iteration 108, loss = 0.38420683\n",
      "Iteration 109, loss = 0.38343350\n",
      "Iteration 110, loss = 0.38267506\n",
      "Iteration 111, loss = 0.38190525\n",
      "Iteration 112, loss = 0.38117136\n",
      "Iteration 113, loss = 0.38048805\n",
      "Iteration 114, loss = 0.37974371\n",
      "Iteration 115, loss = 0.37901792\n",
      "Iteration 116, loss = 0.37837534\n",
      "Iteration 117, loss = 0.37765441\n",
      "Iteration 118, loss = 0.37703513\n",
      "Iteration 119, loss = 0.37635789\n",
      "Iteration 120, loss = 0.37574322\n",
      "Iteration 121, loss = 0.37507954\n",
      "Iteration 122, loss = 0.37443865\n",
      "Iteration 123, loss = 0.37385238\n",
      "Iteration 124, loss = 0.37324443\n",
      "Iteration 125, loss = 0.37265678\n",
      "Iteration 126, loss = 0.37207539\n",
      "Iteration 127, loss = 0.37147976\n",
      "Iteration 128, loss = 0.37091582\n",
      "Iteration 129, loss = 0.37036829\n",
      "Iteration 130, loss = 0.36983085\n",
      "Iteration 131, loss = 0.36927906\n",
      "Iteration 132, loss = 0.36876117\n",
      "Iteration 133, loss = 0.36819427\n",
      "Iteration 134, loss = 0.36770339\n",
      "Iteration 135, loss = 0.36719042\n",
      "Iteration 136, loss = 0.36669812\n",
      "Iteration 137, loss = 0.36617287\n",
      "Iteration 138, loss = 0.36570763\n",
      "Iteration 139, loss = 0.36521730\n",
      "Iteration 140, loss = 0.36473982\n",
      "Iteration 141, loss = 0.36426484\n",
      "Iteration 142, loss = 0.36380408\n",
      "Iteration 143, loss = 0.36334075\n",
      "Iteration 144, loss = 0.36291543\n",
      "Iteration 145, loss = 0.36244798\n",
      "Iteration 146, loss = 0.36203258\n",
      "Iteration 147, loss = 0.36155933\n",
      "Iteration 148, loss = 0.36114133\n",
      "Iteration 149, loss = 0.36072875\n",
      "Iteration 150, loss = 0.36032382\n",
      "Iteration 151, loss = 0.35990915\n",
      "Iteration 152, loss = 0.35948575\n",
      "Iteration 153, loss = 0.35911083\n",
      "Iteration 154, loss = 0.35870097\n",
      "Iteration 155, loss = 0.35832734\n",
      "Iteration 156, loss = 0.35792867\n",
      "Iteration 157, loss = 0.35756416\n",
      "Iteration 158, loss = 0.35717638\n",
      "Iteration 159, loss = 0.35680289\n",
      "Iteration 160, loss = 0.35643544\n",
      "Iteration 161, loss = 0.35610469\n",
      "Iteration 162, loss = 0.35570916\n",
      "Iteration 163, loss = 0.35537110\n",
      "Iteration 164, loss = 0.35503224\n",
      "Iteration 165, loss = 0.35467462\n",
      "Iteration 166, loss = 0.35433255\n",
      "Iteration 167, loss = 0.35401134\n",
      "Iteration 168, loss = 0.35366554\n",
      "Iteration 169, loss = 0.35333275\n",
      "Iteration 170, loss = 0.35302130\n",
      "Iteration 171, loss = 0.35270880\n",
      "Iteration 172, loss = 0.35238414\n",
      "Iteration 173, loss = 0.35207974\n",
      "Iteration 174, loss = 0.35175320\n",
      "Iteration 175, loss = 0.35145529\n",
      "Iteration 176, loss = 0.35115241\n",
      "Iteration 177, loss = 0.35084775\n",
      "Iteration 178, loss = 0.35056105\n",
      "Iteration 179, loss = 0.35024169\n",
      "Iteration 180, loss = 0.34999536\n",
      "Iteration 181, loss = 0.34968325\n",
      "Iteration 182, loss = 0.34938756\n",
      "Iteration 183, loss = 0.34909590\n",
      "Iteration 184, loss = 0.34882754\n",
      "Iteration 185, loss = 0.34855634\n",
      "Iteration 186, loss = 0.34826602\n",
      "Iteration 187, loss = 0.34801902\n",
      "Iteration 188, loss = 0.34775025\n",
      "Iteration 189, loss = 0.34748375\n",
      "Iteration 190, loss = 0.34720427\n",
      "Iteration 191, loss = 0.34695169\n",
      "Iteration 192, loss = 0.34670405\n",
      "Iteration 193, loss = 0.34642530\n",
      "Iteration 194, loss = 0.34618738\n",
      "Iteration 195, loss = 0.34592706\n",
      "Iteration 196, loss = 0.34568714\n",
      "Iteration 197, loss = 0.34544733\n",
      "Iteration 198, loss = 0.34519104\n",
      "Iteration 199, loss = 0.34496684\n",
      "Iteration 200, loss = 0.34471169\n",
      "Iteration 201, loss = 0.34448221\n",
      "Iteration 202, loss = 0.34424215\n",
      "Iteration 203, loss = 0.34401671\n",
      "Iteration 204, loss = 0.34377343\n",
      "Iteration 205, loss = 0.34355613\n",
      "Iteration 206, loss = 0.34332674\n",
      "Iteration 207, loss = 0.34310435\n",
      "Iteration 208, loss = 0.34288070\n",
      "Iteration 209, loss = 0.34266230\n",
      "Iteration 210, loss = 0.34244355\n",
      "Iteration 211, loss = 0.34221219\n",
      "Iteration 212, loss = 0.34201430\n",
      "Iteration 213, loss = 0.34179195\n",
      "Iteration 214, loss = 0.34157999\n",
      "Iteration 215, loss = 0.34136637\n",
      "Iteration 216, loss = 0.34114880\n",
      "Iteration 217, loss = 0.34095665\n",
      "Iteration 218, loss = 0.34074079\n",
      "Iteration 219, loss = 0.34055193\n",
      "Iteration 220, loss = 0.34033592\n",
      "Iteration 221, loss = 0.34015538\n",
      "Iteration 222, loss = 0.33994398\n",
      "Iteration 223, loss = 0.33977016\n",
      "Iteration 224, loss = 0.33955296\n",
      "Iteration 225, loss = 0.33935226\n",
      "Iteration 226, loss = 0.33917597\n",
      "Iteration 227, loss = 0.33897968\n",
      "Iteration 228, loss = 0.33878083\n",
      "Iteration 229, loss = 0.33861667\n",
      "Iteration 230, loss = 0.33842482\n",
      "Iteration 231, loss = 0.33823726\n",
      "Iteration 232, loss = 0.33804799\n",
      "Iteration 233, loss = 0.33786711\n",
      "Iteration 234, loss = 0.33768073\n",
      "Iteration 235, loss = 0.33750610\n",
      "Iteration 236, loss = 0.33735113\n",
      "Iteration 237, loss = 0.33718581\n",
      "Iteration 238, loss = 0.33698157\n",
      "Iteration 239, loss = 0.33680258\n",
      "Iteration 240, loss = 0.33663776\n",
      "Iteration 241, loss = 0.33646831\n",
      "Iteration 242, loss = 0.33629500\n",
      "Iteration 243, loss = 0.33613349\n",
      "Iteration 244, loss = 0.33597978\n",
      "Iteration 245, loss = 0.33581305\n",
      "Iteration 246, loss = 0.33561938\n",
      "Iteration 247, loss = 0.33548330\n",
      "Iteration 248, loss = 0.33531306\n",
      "Iteration 249, loss = 0.33514546\n",
      "Iteration 250, loss = 0.33499185\n",
      "Iteration 251, loss = 0.33483670\n",
      "Iteration 252, loss = 0.33466319\n",
      "Iteration 253, loss = 0.33453001\n",
      "Iteration 254, loss = 0.33435114\n",
      "Iteration 255, loss = 0.33420912\n",
      "Iteration 256, loss = 0.33404835\n",
      "Iteration 257, loss = 0.33389652\n",
      "Iteration 258, loss = 0.33374588\n",
      "Iteration 259, loss = 0.33360201\n",
      "Iteration 260, loss = 0.33344415\n",
      "Iteration 261, loss = 0.33331651\n",
      "Iteration 262, loss = 0.33314705\n",
      "Iteration 263, loss = 0.33300771\n",
      "Iteration 264, loss = 0.33285109\n",
      "Iteration 265, loss = 0.33271160\n",
      "Iteration 266, loss = 0.33255895\n",
      "Iteration 267, loss = 0.33241898\n",
      "Iteration 268, loss = 0.33228648\n",
      "Iteration 269, loss = 0.33214728\n",
      "Iteration 270, loss = 0.33198900\n",
      "Iteration 271, loss = 0.33185558\n",
      "Iteration 272, loss = 0.33172132\n",
      "Iteration 273, loss = 0.33156914\n",
      "Iteration 274, loss = 0.33145712\n",
      "Iteration 275, loss = 0.33131108\n",
      "Iteration 276, loss = 0.33116382\n",
      "Iteration 277, loss = 0.33103096\n",
      "Iteration 278, loss = 0.33089680\n",
      "Iteration 279, loss = 0.33076322\n",
      "Iteration 280, loss = 0.33062977\n",
      "Iteration 281, loss = 0.33049438\n",
      "Iteration 282, loss = 0.33036270\n",
      "Iteration 283, loss = 0.33023845\n",
      "Iteration 284, loss = 0.33009700\n",
      "Iteration 285, loss = 0.32996802\n",
      "Iteration 286, loss = 0.32983309\n",
      "Iteration 287, loss = 0.32972330\n",
      "Iteration 288, loss = 0.32959663\n",
      "Iteration 289, loss = 0.32944878\n",
      "Iteration 290, loss = 0.32932904\n",
      "Iteration 291, loss = 0.32919954\n",
      "Iteration 292, loss = 0.32908566\n",
      "Iteration 293, loss = 0.32895319\n",
      "Iteration 294, loss = 0.32883533\n",
      "Iteration 295, loss = 0.32870820\n",
      "Iteration 296, loss = 0.32857965\n",
      "Iteration 297, loss = 0.32846930\n",
      "Iteration 298, loss = 0.32834900\n",
      "Iteration 299, loss = 0.32822177\n",
      "Iteration 300, loss = 0.32809739\n",
      "Iteration 301, loss = 0.32798415\n",
      "Iteration 302, loss = 0.32786266\n",
      "Iteration 303, loss = 0.32774740\n",
      "Iteration 304, loss = 0.32763479\n",
      "Iteration 305, loss = 0.32752383\n",
      "Iteration 306, loss = 0.32739873\n",
      "Iteration 307, loss = 0.32728924\n",
      "Iteration 308, loss = 0.32716568\n",
      "Iteration 309, loss = 0.32706085\n",
      "Iteration 310, loss = 0.32693685\n",
      "Iteration 311, loss = 0.32682723\n",
      "Iteration 312, loss = 0.32672942\n",
      "Iteration 313, loss = 0.32659829\n",
      "Iteration 314, loss = 0.32649274\n",
      "Iteration 315, loss = 0.32637478\n",
      "Iteration 316, loss = 0.32627424\n",
      "Iteration 317, loss = 0.32616144\n",
      "Iteration 318, loss = 0.32605247\n",
      "Iteration 319, loss = 0.32594208\n",
      "Iteration 320, loss = 0.32582835\n",
      "Iteration 321, loss = 0.32571803\n",
      "Iteration 322, loss = 0.32561515\n",
      "Iteration 323, loss = 0.32551091\n",
      "Iteration 324, loss = 0.32541204\n",
      "Iteration 325, loss = 0.32529344\n",
      "Iteration 326, loss = 0.32518919\n",
      "Iteration 327, loss = 0.32510386\n",
      "Iteration 328, loss = 0.32498375\n",
      "Iteration 329, loss = 0.32488115\n",
      "Iteration 330, loss = 0.32477873\n",
      "Iteration 331, loss = 0.32467831\n",
      "Iteration 332, loss = 0.32456516\n",
      "Iteration 333, loss = 0.32446723\n",
      "Iteration 334, loss = 0.32437035\n",
      "Iteration 335, loss = 0.32427263\n",
      "Iteration 336, loss = 0.32416813\n",
      "Iteration 337, loss = 0.32406797\n",
      "Iteration 338, loss = 0.32396202\n",
      "Iteration 339, loss = 0.32386594\n",
      "Iteration 340, loss = 0.32377206\n",
      "Iteration 341, loss = 0.32368184\n",
      "Iteration 342, loss = 0.32357661\n",
      "Iteration 343, loss = 0.32346667\n",
      "Iteration 344, loss = 0.32338005\n",
      "Iteration 345, loss = 0.32329682\n",
      "Iteration 346, loss = 0.32317777\n",
      "Iteration 347, loss = 0.32308812\n",
      "Iteration 348, loss = 0.32299505\n",
      "Iteration 349, loss = 0.32290565\n",
      "Iteration 350, loss = 0.32280364\n",
      "Iteration 351, loss = 0.32270926\n",
      "Iteration 352, loss = 0.32260769\n",
      "Iteration 353, loss = 0.32252073\n",
      "Iteration 354, loss = 0.32243427\n",
      "Iteration 355, loss = 0.32233410\n",
      "Iteration 356, loss = 0.32223278\n",
      "Iteration 357, loss = 0.32215763\n",
      "Iteration 358, loss = 0.32204685\n",
      "Iteration 359, loss = 0.32196092\n",
      "Iteration 360, loss = 0.32187115\n",
      "Iteration 361, loss = 0.32177736\n",
      "Iteration 362, loss = 0.32168847\n",
      "Iteration 363, loss = 0.32159990\n",
      "Iteration 364, loss = 0.32150542\n",
      "Iteration 365, loss = 0.32141056\n",
      "Iteration 366, loss = 0.32133312\n",
      "Iteration 367, loss = 0.32123977\n",
      "Iteration 368, loss = 0.32115940\n",
      "Iteration 369, loss = 0.32107025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70722671\n",
      "Iteration 2, loss = 0.70438061\n",
      "Iteration 3, loss = 0.69999823\n",
      "Iteration 4, loss = 0.69468223\n",
      "Iteration 5, loss = 0.68857680\n",
      "Iteration 6, loss = 0.68211504\n",
      "Iteration 7, loss = 0.67532052\n",
      "Iteration 8, loss = 0.66848828\n",
      "Iteration 9, loss = 0.66151572\n",
      "Iteration 10, loss = 0.65443160\n",
      "Iteration 11, loss = 0.64778224\n",
      "Iteration 12, loss = 0.64079268\n",
      "Iteration 13, loss = 0.63425884\n",
      "Iteration 14, loss = 0.62762203\n",
      "Iteration 15, loss = 0.62127004\n",
      "Iteration 16, loss = 0.61490879\n",
      "Iteration 17, loss = 0.60882163\n",
      "Iteration 18, loss = 0.60308923\n",
      "Iteration 19, loss = 0.59708190\n",
      "Iteration 20, loss = 0.59131601\n",
      "Iteration 21, loss = 0.58573461\n",
      "Iteration 22, loss = 0.58035479\n",
      "Iteration 23, loss = 0.57503928\n",
      "Iteration 24, loss = 0.56972293\n",
      "Iteration 25, loss = 0.56470626\n",
      "Iteration 26, loss = 0.55955930\n",
      "Iteration 27, loss = 0.55475801\n",
      "Iteration 28, loss = 0.55000503\n",
      "Iteration 29, loss = 0.54540143\n",
      "Iteration 30, loss = 0.54078634\n",
      "Iteration 31, loss = 0.53639321\n",
      "Iteration 32, loss = 0.53207493\n",
      "Iteration 33, loss = 0.52768115\n",
      "Iteration 34, loss = 0.52351848\n",
      "Iteration 35, loss = 0.51957261\n",
      "Iteration 36, loss = 0.51551763\n",
      "Iteration 37, loss = 0.51174629\n",
      "Iteration 38, loss = 0.50786973\n",
      "Iteration 39, loss = 0.50409100\n",
      "Iteration 40, loss = 0.50047605\n",
      "Iteration 41, loss = 0.49696069\n",
      "Iteration 42, loss = 0.49357433\n",
      "Iteration 43, loss = 0.49014092\n",
      "Iteration 44, loss = 0.48686994\n",
      "Iteration 45, loss = 0.48368027\n",
      "Iteration 46, loss = 0.48047682\n",
      "Iteration 47, loss = 0.47746853\n",
      "Iteration 48, loss = 0.47445757\n",
      "Iteration 49, loss = 0.47148053\n",
      "Iteration 50, loss = 0.46870936\n",
      "Iteration 51, loss = 0.46589750\n",
      "Iteration 52, loss = 0.46315630\n",
      "Iteration 53, loss = 0.46057823\n",
      "Iteration 54, loss = 0.45795137\n",
      "Iteration 55, loss = 0.45540605\n",
      "Iteration 56, loss = 0.45290199\n",
      "Iteration 57, loss = 0.45045347\n",
      "Iteration 58, loss = 0.44809543\n",
      "Iteration 59, loss = 0.44589067\n",
      "Iteration 60, loss = 0.44364368\n",
      "Iteration 61, loss = 0.44140423\n",
      "Iteration 62, loss = 0.43923080\n",
      "Iteration 63, loss = 0.43720828\n",
      "Iteration 64, loss = 0.43517831\n",
      "Iteration 65, loss = 0.43309495\n",
      "Iteration 66, loss = 0.43125662\n",
      "Iteration 67, loss = 0.42933757\n",
      "Iteration 68, loss = 0.42757929\n",
      "Iteration 69, loss = 0.42575987\n",
      "Iteration 70, loss = 0.42399716\n",
      "Iteration 71, loss = 0.42237785\n",
      "Iteration 72, loss = 0.42066645\n",
      "Iteration 73, loss = 0.41905405\n",
      "Iteration 74, loss = 0.41747014\n",
      "Iteration 75, loss = 0.41594306\n",
      "Iteration 76, loss = 0.41442853\n",
      "Iteration 77, loss = 0.41299822\n",
      "Iteration 78, loss = 0.41158876\n",
      "Iteration 79, loss = 0.41015686\n",
      "Iteration 80, loss = 0.40885175\n",
      "Iteration 81, loss = 0.40745562\n",
      "Iteration 82, loss = 0.40615832\n",
      "Iteration 83, loss = 0.40492476\n",
      "Iteration 84, loss = 0.40367510\n",
      "Iteration 85, loss = 0.40244823\n",
      "Iteration 86, loss = 0.40127151\n",
      "Iteration 87, loss = 0.40015066\n",
      "Iteration 88, loss = 0.39903134\n",
      "Iteration 89, loss = 0.39784599\n",
      "Iteration 90, loss = 0.39677064\n",
      "Iteration 91, loss = 0.39574248\n",
      "Iteration 92, loss = 0.39467378\n",
      "Iteration 93, loss = 0.39368896\n",
      "Iteration 94, loss = 0.39266852\n",
      "Iteration 95, loss = 0.39173909\n",
      "Iteration 96, loss = 0.39074570\n",
      "Iteration 97, loss = 0.38984073\n",
      "Iteration 98, loss = 0.38895450\n",
      "Iteration 99, loss = 0.38806687\n",
      "Iteration 100, loss = 0.38721294\n",
      "Iteration 101, loss = 0.38635957\n",
      "Iteration 102, loss = 0.38549104\n",
      "Iteration 103, loss = 0.38465061\n",
      "Iteration 104, loss = 0.38387043\n",
      "Iteration 105, loss = 0.38307429\n",
      "Iteration 106, loss = 0.38236886\n",
      "Iteration 107, loss = 0.38156501\n",
      "Iteration 108, loss = 0.38085673\n",
      "Iteration 109, loss = 0.38010168\n",
      "Iteration 110, loss = 0.37941625\n",
      "Iteration 111, loss = 0.37868891\n",
      "Iteration 112, loss = 0.37799449\n",
      "Iteration 113, loss = 0.37733038\n",
      "Iteration 114, loss = 0.37666976\n",
      "Iteration 115, loss = 0.37597527\n",
      "Iteration 116, loss = 0.37533359\n",
      "Iteration 117, loss = 0.37470835\n",
      "Iteration 118, loss = 0.37411351\n",
      "Iteration 119, loss = 0.37346512\n",
      "Iteration 120, loss = 0.37286407\n",
      "Iteration 121, loss = 0.37226676\n",
      "Iteration 122, loss = 0.37168081\n",
      "Iteration 123, loss = 0.37112484\n",
      "Iteration 124, loss = 0.37057579\n",
      "Iteration 125, loss = 0.36998216\n",
      "Iteration 126, loss = 0.36943693\n",
      "Iteration 127, loss = 0.36894878\n",
      "Iteration 128, loss = 0.36837153\n",
      "Iteration 129, loss = 0.36782889\n",
      "Iteration 130, loss = 0.36733966\n",
      "Iteration 131, loss = 0.36680812\n",
      "Iteration 132, loss = 0.36628183\n",
      "Iteration 133, loss = 0.36582816\n",
      "Iteration 134, loss = 0.36530958\n",
      "Iteration 135, loss = 0.36483528\n",
      "Iteration 136, loss = 0.36434450\n",
      "Iteration 137, loss = 0.36387719\n",
      "Iteration 138, loss = 0.36341581\n",
      "Iteration 139, loss = 0.36299490\n",
      "Iteration 140, loss = 0.36251392\n",
      "Iteration 141, loss = 0.36205001\n",
      "Iteration 142, loss = 0.36161416\n",
      "Iteration 143, loss = 0.36118579\n",
      "Iteration 144, loss = 0.36078508\n",
      "Iteration 145, loss = 0.36033478\n",
      "Iteration 146, loss = 0.35993012\n",
      "Iteration 147, loss = 0.35948677\n",
      "Iteration 148, loss = 0.35911006\n",
      "Iteration 149, loss = 0.35869677\n",
      "Iteration 150, loss = 0.35832257\n",
      "Iteration 151, loss = 0.35793598\n",
      "Iteration 152, loss = 0.35752453\n",
      "Iteration 153, loss = 0.35712527\n",
      "Iteration 154, loss = 0.35674628\n",
      "Iteration 155, loss = 0.35637481\n",
      "Iteration 156, loss = 0.35599634\n",
      "Iteration 157, loss = 0.35566458\n",
      "Iteration 158, loss = 0.35527931\n",
      "Iteration 159, loss = 0.35489945\n",
      "Iteration 160, loss = 0.35454574\n",
      "Iteration 161, loss = 0.35418288\n",
      "Iteration 162, loss = 0.35384985\n",
      "Iteration 163, loss = 0.35349230\n",
      "Iteration 164, loss = 0.35314617\n",
      "Iteration 165, loss = 0.35279335\n",
      "Iteration 166, loss = 0.35245542\n",
      "Iteration 167, loss = 0.35209500\n",
      "Iteration 168, loss = 0.35181715\n",
      "Iteration 169, loss = 0.35145637\n",
      "Iteration 170, loss = 0.35112801\n",
      "Iteration 171, loss = 0.35079825\n",
      "Iteration 172, loss = 0.35048101\n",
      "Iteration 173, loss = 0.35014603\n",
      "Iteration 174, loss = 0.34981470\n",
      "Iteration 1158, loss = 0.27145359\n",
      "Iteration 1159, loss = 0.27132927\n",
      "Iteration 1160, loss = 0.27127071\n",
      "Iteration 1161, loss = 0.27114222\n",
      "Iteration 1162, loss = 0.27106874\n",
      "Iteration 1163, loss = 0.27090666\n",
      "Iteration 1164, loss = 0.27084870\n",
      "Iteration 1165, loss = 0.27075202\n",
      "Iteration 1166, loss = 0.27058287\n",
      "Iteration 1167, loss = 0.27048315\n",
      "Iteration 1168, loss = 0.27036881\n",
      "Iteration 1169, loss = 0.27031476\n",
      "Iteration 1170, loss = 0.27016861\n",
      "Iteration 1171, loss = 0.27012386\n",
      "Iteration 1172, loss = 0.26998257\n",
      "Iteration 1173, loss = 0.26988034\n",
      "Iteration 1174, loss = 0.26978230\n",
      "Iteration 1175, loss = 0.26965380\n",
      "Iteration 1176, loss = 0.26963342\n",
      "Iteration 1177, loss = 0.26944930\n",
      "Iteration 1178, loss = 0.26934446\n",
      "Iteration 1179, loss = 0.26922498\n",
      "Iteration 1180, loss = 0.26911787\n",
      "Iteration 1181, loss = 0.26903882\n",
      "Iteration 1182, loss = 0.26891507\n",
      "Iteration 1183, loss = 0.26883769\n",
      "Iteration 1184, loss = 0.26869142\n",
      "Iteration 1185, loss = 0.26864526\n",
      "Iteration 1186, loss = 0.26848083\n",
      "Iteration 1187, loss = 0.26842809\n",
      "Iteration 1188, loss = 0.26830981\n",
      "Iteration 1189, loss = 0.26817884\n",
      "Iteration 1190, loss = 0.26812372\n",
      "Iteration 1191, loss = 0.26797890\n",
      "Iteration 1192, loss = 0.26784473\n",
      "Iteration 1193, loss = 0.26777255\n",
      "Iteration 1194, loss = 0.26765049\n",
      "Iteration 1195, loss = 0.26755047\n",
      "Iteration 1196, loss = 0.26746558\n",
      "Iteration 1197, loss = 0.26741128\n",
      "Iteration 1198, loss = 0.26727058\n",
      "Iteration 1199, loss = 0.26715392\n",
      "Iteration 1200, loss = 0.26702022\n",
      "Iteration 1201, loss = 0.26689983\n",
      "Iteration 1202, loss = 0.26681484\n",
      "Iteration 1203, loss = 0.26670986\n",
      "Iteration 1204, loss = 0.26658793\n",
      "Iteration 1205, loss = 0.26648586\n",
      "Iteration 1206, loss = 0.26640871\n",
      "Iteration 1207, loss = 0.26630465\n",
      "Iteration 1208, loss = 0.26617626\n",
      "Iteration 1209, loss = 0.26608635\n",
      "Iteration 1210, loss = 0.26596982\n",
      "Iteration 1211, loss = 0.26590102\n",
      "Iteration 1212, loss = 0.26581961\n",
      "Iteration 1213, loss = 0.26568784\n",
      "Iteration 1214, loss = 0.26562926\n",
      "Iteration 1215, loss = 0.26545530\n",
      "Iteration 1216, loss = 0.26533507\n",
      "Iteration 1217, loss = 0.26523230\n",
      "Iteration 1218, loss = 0.26519577\n",
      "Iteration 1219, loss = 0.26505552\n",
      "Iteration 1220, loss = 0.26492938\n",
      "Iteration 1221, loss = 0.26482865\n",
      "Iteration 1222, loss = 0.26473813\n",
      "Iteration 1223, loss = 0.26462999\n",
      "Iteration 1224, loss = 0.26452934\n",
      "Iteration 1225, loss = 0.26441089\n",
      "Iteration 1226, loss = 0.26433704\n",
      "Iteration 1227, loss = 0.26422637\n",
      "Iteration 1228, loss = 0.26413106\n",
      "Iteration 1229, loss = 0.26401105\n",
      "Iteration 1230, loss = 0.26394890\n",
      "Iteration 1231, loss = 0.26381167\n",
      "Iteration 1232, loss = 0.26370664\n",
      "Iteration 1233, loss = 0.26362892\n",
      "Iteration 1234, loss = 0.26350508\n",
      "Iteration 1235, loss = 0.26339363\n",
      "Iteration 1236, loss = 0.26330439\n",
      "Iteration 1237, loss = 0.26319060\n",
      "Iteration 1238, loss = 0.26310470\n",
      "Iteration 1239, loss = 0.26297650\n",
      "Iteration 1240, loss = 0.26289824\n",
      "Iteration 1241, loss = 0.26280476\n",
      "Iteration 1242, loss = 0.26276362\n",
      "Iteration 1243, loss = 0.26257203\n",
      "Iteration 1244, loss = 0.26246634\n",
      "Iteration 1245, loss = 0.26240334\n",
      "Iteration 1246, loss = 0.26230294\n",
      "Iteration 1247, loss = 0.26213630\n",
      "Iteration 1248, loss = 0.26203402\n",
      "Iteration 1249, loss = 0.26196435\n",
      "Iteration 1250, loss = 0.26186235\n",
      "Iteration 1251, loss = 0.26174164\n",
      "Iteration 1252, loss = 0.26164916\n",
      "Iteration 1253, loss = 0.26155039\n",
      "Iteration 1254, loss = 0.26143999\n",
      "Iteration 1255, loss = 0.26136748\n",
      "Iteration 1256, loss = 0.26119403\n",
      "Iteration 1257, loss = 0.26116746\n",
      "Iteration 1258, loss = 0.26114812\n",
      "Iteration 1259, loss = 0.26090478\n",
      "Iteration 1260, loss = 0.26080063\n",
      "Iteration 1261, loss = 0.26069958\n",
      "Iteration 1262, loss = 0.26061023\n",
      "Iteration 1263, loss = 0.26055661\n",
      "Iteration 1264, loss = 0.26045280\n",
      "Iteration 1265, loss = 0.26027342\n",
      "Iteration 1266, loss = 0.26017439\n",
      "Iteration 1267, loss = 0.26012597\n",
      "Iteration 1268, loss = 0.26002688\n",
      "Iteration 1269, loss = 0.25990095\n",
      "Iteration 1270, loss = 0.25978998\n",
      "Iteration 1271, loss = 0.25965371\n",
      "Iteration 1272, loss = 0.25956794\n",
      "Iteration 1273, loss = 0.25951848\n",
      "Iteration 1274, loss = 0.25935807\n",
      "Iteration 1275, loss = 0.25924340\n",
      "Iteration 1276, loss = 0.25921417\n",
      "Iteration 1277, loss = 0.25911129\n",
      "Iteration 1278, loss = 0.25891865\n",
      "Iteration 1279, loss = 0.25885777\n",
      "Iteration 1280, loss = 0.25875394\n",
      "Iteration 1281, loss = 0.25863515\n",
      "Iteration 1282, loss = 0.25852225\n",
      "Iteration 1283, loss = 0.25841625\n",
      "Iteration 1284, loss = 0.25836604\n",
      "Iteration 1285, loss = 0.25820113\n",
      "Iteration 1286, loss = 0.25815631\n",
      "Iteration 1287, loss = 0.25801403\n",
      "Iteration 1288, loss = 0.25792470\n",
      "Iteration 1289, loss = 0.25780928\n",
      "Iteration 1290, loss = 0.25766432\n",
      "Iteration 1291, loss = 0.25763574\n",
      "Iteration 1292, loss = 0.25745678\n",
      "Iteration 1293, loss = 0.25742894\n",
      "Iteration 1294, loss = 0.25725277\n",
      "Iteration 1295, loss = 0.25718241\n",
      "Iteration 1296, loss = 0.25709119\n",
      "Iteration 1297, loss = 0.25695077\n",
      "Iteration 1298, loss = 0.25686043\n",
      "Iteration 1299, loss = 0.25673691\n",
      "Iteration 1300, loss = 0.25669105\n",
      "Iteration 1301, loss = 0.25658094\n",
      "Iteration 1302, loss = 0.25645632\n",
      "Iteration 1303, loss = 0.25632050\n",
      "Iteration 1304, loss = 0.25621726\n",
      "Iteration 1305, loss = 0.25612910\n",
      "Iteration 1306, loss = 0.25599678\n",
      "Iteration 1307, loss = 0.25590483\n",
      "Iteration 1308, loss = 0.25589491\n",
      "Iteration 1309, loss = 0.25574046\n",
      "Iteration 1310, loss = 0.25557926\n",
      "Iteration 1311, loss = 0.25548894\n",
      "Iteration 1312, loss = 0.25537715\n",
      "Iteration 1313, loss = 0.25533849\n",
      "Iteration 1314, loss = 0.25519858\n",
      "Iteration 1315, loss = 0.25508170\n",
      "Iteration 1316, loss = 0.25497478\n",
      "Iteration 1317, loss = 0.25492503\n",
      "Iteration 1318, loss = 0.25474728\n",
      "Iteration 1319, loss = 0.25467015\n",
      "Iteration 1320, loss = 0.25453907\n",
      "Iteration 1321, loss = 0.25446867\n",
      "Iteration 1322, loss = 0.25437581\n",
      "Iteration 1323, loss = 0.25424689\n",
      "Iteration 1324, loss = 0.25427167\n",
      "Iteration 1325, loss = 0.25409412\n",
      "Iteration 1326, loss = 0.25393778\n",
      "Iteration 1327, loss = 0.25385447\n",
      "Iteration 1328, loss = 0.25374152\n",
      "Iteration 1329, loss = 0.25365788\n",
      "Iteration 1330, loss = 0.25354029\n",
      "Iteration 1331, loss = 0.25341772\n",
      "Iteration 1332, loss = 0.25334549\n",
      "Iteration 1333, loss = 0.25325054\n",
      "Iteration 1334, loss = 0.25314900\n",
      "Iteration 1335, loss = 0.25300815\n",
      "Iteration 1336, loss = 0.25293048\n",
      "Iteration 1337, loss = 0.25284876\n",
      "Iteration 1338, loss = 0.25272967\n",
      "Iteration 1339, loss = 0.25259796\n",
      "Iteration 1340, loss = 0.25249998\n",
      "Iteration 1341, loss = 0.25240250\n",
      "Iteration 1342, loss = 0.25235526\n",
      "Iteration 1343, loss = 0.25220349\n",
      "Iteration 1344, loss = 0.25211091\n",
      "Iteration 1345, loss = 0.25198984\n",
      "Iteration 1346, loss = 0.25202543\n",
      "Iteration 1347, loss = 0.25180833\n",
      "Iteration 1348, loss = 0.25167523\n",
      "Iteration 1349, loss = 0.25161759\n",
      "Iteration 1350, loss = 0.25149144\n",
      "Iteration 1351, loss = 0.25141001\n",
      "Iteration 1352, loss = 0.25143575\n",
      "Iteration 1353, loss = 0.25118770\n",
      "Iteration 1354, loss = 0.25111692\n",
      "Iteration 1355, loss = 0.25095239\n",
      "Iteration 1356, loss = 0.25085777\n",
      "Iteration 1357, loss = 0.25080253\n",
      "Iteration 1358, loss = 0.25068009\n",
      "Iteration 1359, loss = 0.25055668\n",
      "Iteration 1360, loss = 0.25047733\n",
      "Iteration 1361, loss = 0.25036170\n",
      "Iteration 1362, loss = 0.25028290\n",
      "Iteration 1363, loss = 0.25014208\n",
      "Iteration 1364, loss = 0.25003378\n",
      "Iteration 1365, loss = 0.25002733\n",
      "Iteration 1366, loss = 0.24983279\n",
      "Iteration 1367, loss = 0.24975323\n",
      "Iteration 1368, loss = 0.24964702\n",
      "Iteration 1369, loss = 0.24958572\n",
      "Iteration 1370, loss = 0.24948747\n",
      "Iteration 1371, loss = 0.24934746\n",
      "Iteration 1372, loss = 0.24923407\n",
      "Iteration 1373, loss = 0.24912671\n",
      "Iteration 1374, loss = 0.24900936\n",
      "Iteration 1375, loss = 0.24892415\n",
      "Iteration 1376, loss = 0.24883730\n",
      "Iteration 1377, loss = 0.24874556\n",
      "Iteration 1378, loss = 0.24860329\n",
      "Iteration 1379, loss = 0.24854417\n",
      "Iteration 1380, loss = 0.24840315\n",
      "Iteration 1381, loss = 0.24839199\n",
      "Iteration 1382, loss = 0.24825020\n",
      "Iteration 1383, loss = 0.24813893\n",
      "Iteration 1384, loss = 0.24805557\n",
      "Iteration 1385, loss = 0.24789199\n",
      "Iteration 1386, loss = 0.24779230\n",
      "Iteration 1387, loss = 0.24768265\n",
      "Iteration 1388, loss = 0.24757124\n",
      "Iteration 1389, loss = 0.24748284\n",
      "Iteration 1390, loss = 0.24739419\n",
      "Iteration 1391, loss = 0.24728410\n",
      "Iteration 1392, loss = 0.24721733\n",
      "Iteration 1393, loss = 0.24704804\n",
      "Iteration 1394, loss = 0.24699159\n",
      "Iteration 1395, loss = 0.24699792\n",
      "Iteration 1396, loss = 0.24683059\n",
      "Iteration 1397, loss = 0.24671894\n",
      "Iteration 1398, loss = 0.24656079\n",
      "Iteration 1399, loss = 0.24644290\n",
      "Iteration 1400, loss = 0.24637315\n",
      "Iteration 1401, loss = 0.24624375\n",
      "Iteration 1402, loss = 0.24611922\n",
      "Iteration 1403, loss = 0.24606074\n",
      "Iteration 1404, loss = 0.24592991\n",
      "Iteration 1405, loss = 0.24592485\n",
      "Iteration 1406, loss = 0.24570582\n",
      "Iteration 1407, loss = 0.24563728\n",
      "Iteration 1408, loss = 0.24551166\n",
      "Iteration 1409, loss = 0.24541011\n",
      "Iteration 1410, loss = 0.24529584\n",
      "Iteration 1411, loss = 0.24517690\n",
      "Iteration 1412, loss = 0.24509660\n",
      "Iteration 1413, loss = 0.24498498\n",
      "Iteration 1414, loss = 0.24488408\n",
      "Iteration 1415, loss = 0.24479213\n",
      "Iteration 1416, loss = 0.24470957\n",
      "Iteration 1417, loss = 0.24456691\n",
      "Iteration 1418, loss = 0.24450887\n",
      "Iteration 1419, loss = 0.24441407\n",
      "Iteration 1420, loss = 0.24427786\n",
      "Iteration 1421, loss = 0.24415526\n",
      "Iteration 1422, loss = 0.24407066\n",
      "Iteration 1423, loss = 0.24393924\n",
      "Iteration 1424, loss = 0.24383610\n",
      "Iteration 1425, loss = 0.24379964\n",
      "Iteration 1426, loss = 0.24365962\n",
      "Iteration 1427, loss = 0.24352662\n",
      "Iteration 1428, loss = 0.24344059\n",
      "Iteration 1429, loss = 0.24335274\n",
      "Iteration 1430, loss = 0.24321989\n",
      "Iteration 1431, loss = 0.24314379\n",
      "Iteration 1432, loss = 0.24300541\n",
      "Iteration 1433, loss = 0.24299990\n",
      "Iteration 1434, loss = 0.24279731\n",
      "Iteration 1435, loss = 0.24279133\n",
      "Iteration 1436, loss = 0.24261204\n",
      "Iteration 1437, loss = 0.24259131\n",
      "Iteration 1438, loss = 0.24246399\n",
      "Iteration 1439, loss = 0.24229379\n",
      "Iteration 1440, loss = 0.24219674\n",
      "Iteration 1441, loss = 0.24210695\n",
      "Iteration 1442, loss = 0.24201072\n",
      "Iteration 1443, loss = 0.24190676\n",
      "Iteration 1444, loss = 0.24177756\n",
      "Iteration 1445, loss = 0.24177813\n",
      "Iteration 1446, loss = 0.24159176\n",
      "Iteration 1447, loss = 0.24149422\n",
      "Iteration 1448, loss = 0.24142863\n",
      "Iteration 1449, loss = 0.24129024\n",
      "Iteration 1450, loss = 0.24116838\n",
      "Iteration 1451, loss = 0.24106225\n",
      "Iteration 1452, loss = 0.24095287\n",
      "Iteration 1453, loss = 0.24088219\n",
      "Iteration 1454, loss = 0.24075697\n",
      "Iteration 1455, loss = 0.24066277\n",
      "Iteration 1456, loss = 0.24065135\n",
      "Iteration 1457, loss = 0.24044721\n",
      "Iteration 1458, loss = 0.24036452\n",
      "Iteration 1459, loss = 0.24025824\n",
      "Iteration 1460, loss = 0.24018882\n",
      "Iteration 1461, loss = 0.24008088\n",
      "Iteration 1462, loss = 0.23995686\n",
      "Iteration 1463, loss = 0.23992307\n",
      "Iteration 1464, loss = 0.23974053\n",
      "Iteration 1465, loss = 0.23965427\n",
      "Iteration 1466, loss = 0.23952643\n",
      "Iteration 1467, loss = 0.23942701\n",
      "Iteration 1468, loss = 0.23932521\n",
      "Iteration 1469, loss = 0.23921583\n",
      "Iteration 1470, loss = 0.23916298\n",
      "Iteration 1471, loss = 0.23901990\n",
      "Iteration 1472, loss = 0.23900451\n",
      "Iteration 1473, loss = 0.23881587\n",
      "Iteration 1474, loss = 0.23872073\n",
      "Iteration 1475, loss = 0.23860936\n",
      "Iteration 1476, loss = 0.23855664\n",
      "Iteration 1477, loss = 0.23848415\n",
      "Iteration 1478, loss = 0.23832870\n",
      "Iteration 1479, loss = 0.23821284\n",
      "Iteration 1480, loss = 0.23819592\n",
      "Iteration 1481, loss = 0.23805016\n",
      "Iteration 1482, loss = 0.23788949\n",
      "Iteration 1483, loss = 0.23782271\n",
      "Iteration 1484, loss = 0.23773648\n",
      "Iteration 1485, loss = 0.23760544\n",
      "Iteration 1486, loss = 0.23754441\n",
      "Iteration 1487, loss = 0.23748894\n",
      "Iteration 1488, loss = 0.23734252\n",
      "Iteration 1489, loss = 0.23721430\n",
      "Iteration 1490, loss = 0.23712065\n",
      "Iteration 1491, loss = 0.23700253\n",
      "Iteration 1492, loss = 0.23695988\n",
      "Iteration 1493, loss = 0.23682465\n",
      "Iteration 1494, loss = 0.23674832\n",
      "Iteration 1495, loss = 0.23661722\n",
      "Iteration 1496, loss = 0.23653738\n",
      "Iteration 1497, loss = 0.23647448\n",
      "Iteration 1498, loss = 0.23630795\n",
      "Iteration 1499, loss = 0.23623910\n",
      "Iteration 1500, loss = 0.23613591\n",
      "Iteration 1501, loss = 0.23601597\n",
      "Iteration 1502, loss = 0.23591714\n",
      "Iteration 1503, loss = 0.23582044\n",
      "Iteration 1504, loss = 0.23571252\n",
      "Iteration 1505, loss = 0.23566424\n",
      "Iteration 1506, loss = 0.23553133\n",
      "Iteration 1507, loss = 0.23548831\n",
      "Iteration 1508, loss = 0.23533711\n",
      "Iteration 1509, loss = 0.23521171\n",
      "Iteration 1510, loss = 0.23511140\n",
      "Iteration 1511, loss = 0.23504623\n",
      "Iteration 1512, loss = 0.23501350\n",
      "Iteration 1513, loss = 0.23486320\n",
      "Iteration 1514, loss = 0.23477209\n",
      "Iteration 1515, loss = 0.23462479\n",
      "Iteration 1516, loss = 0.23453536\n",
      "Iteration 1517, loss = 0.23440839\n",
      "Iteration 1518, loss = 0.23436697\n",
      "Iteration 1519, loss = 0.23418716\n",
      "Iteration 1520, loss = 0.23416097\n",
      "Iteration 1521, loss = 0.23416348\n",
      "Iteration 1522, loss = 0.23390570\n",
      "Iteration 1523, loss = 0.23381982\n",
      "Iteration 1524, loss = 0.23369810\n",
      "Iteration 1525, loss = 0.23361186\n",
      "Iteration 1526, loss = 0.23349914\n",
      "Iteration 1527, loss = 0.23339230\n",
      "Iteration 1528, loss = 0.23329030\n",
      "Iteration 1529, loss = 0.23322163\n",
      "Iteration 1530, loss = 0.23307432\n",
      "Iteration 1531, loss = 0.23297511\n",
      "Iteration 1532, loss = 0.23293051\n",
      "Iteration 1533, loss = 0.23277449\n",
      "Iteration 1534, loss = 0.23268267\n",
      "Iteration 1535, loss = 0.23256581\n",
      "Iteration 1536, loss = 0.23250755\n",
      "Iteration 1537, loss = 0.23247205\n",
      "Iteration 1538, loss = 0.23228077\n",
      "Iteration 1539, loss = 0.23221574\n",
      "Iteration 1540, loss = 0.23216041\n",
      "Iteration 1541, loss = 0.23201149\n",
      "Iteration 1542, loss = 0.23188212\n",
      "Iteration 1543, loss = 0.23180693\n",
      "Iteration 1544, loss = 0.23166839\n",
      "Iteration 1545, loss = 0.23156148\n",
      "Iteration 1546, loss = 0.23152774\n",
      "Iteration 1547, loss = 0.23140221\n",
      "Iteration 1548, loss = 0.23126635\n",
      "Iteration 1549, loss = 0.23123141\n",
      "Iteration 1550, loss = 0.23108766\n",
      "Iteration 1551, loss = 0.23094011\n",
      "Iteration 1552, loss = 0.23090095\n",
      "Iteration 1553, loss = 0.23079368\n",
      "Iteration 1554, loss = 0.23070959\n",
      "Iteration 1555, loss = 0.23054169\n",
      "Iteration 1556, loss = 0.23045258\n",
      "Iteration 1557, loss = 0.23037034\n",
      "Iteration 1558, loss = 0.23027140\n",
      "Iteration 1559, loss = 0.23014940\n",
      "Iteration 1560, loss = 0.23015260\n",
      "Iteration 1561, loss = 0.22996527\n",
      "Iteration 1562, loss = 0.22987158\n",
      "Iteration 1563, loss = 0.22976482\n",
      "Iteration 1564, loss = 0.22976681\n",
      "Iteration 1565, loss = 0.22960485\n",
      "Iteration 1566, loss = 0.22949327\n",
      "Iteration 1567, loss = 0.22935644\n",
      "Iteration 1568, loss = 0.22929724\n",
      "Iteration 1569, loss = 0.22918018\n",
      "Iteration 1570, loss = 0.22907477\n",
      "Iteration 1571, loss = 0.22896351\n",
      "Iteration 1572, loss = 0.22885988\n",
      "Iteration 1573, loss = 0.22883818\n",
      "Iteration 1574, loss = 0.22868520\n",
      "Iteration 1575, loss = 0.22872834\n",
      "Iteration 1576, loss = 0.22847278\n",
      "Iteration 1577, loss = 0.22837909\n",
      "Iteration 1578, loss = 0.22826601\n",
      "Iteration 1579, loss = 0.22815462\n",
      "Iteration 1580, loss = 0.22808484\n",
      "Iteration 1581, loss = 0.22805217\n",
      "Iteration 1582, loss = 0.22784803\n",
      "Iteration 1583, loss = 0.22779630\n",
      "Iteration 1584, loss = 0.22769838\n",
      "Iteration 1585, loss = 0.22762432\n",
      "Iteration 1586, loss = 0.22745393\n",
      "Iteration 1587, loss = 0.22746190\n",
      "Iteration 1588, loss = 0.22740179\n",
      "Iteration 1589, loss = 0.22721442\n",
      "Iteration 1590, loss = 0.22709085\n",
      "Iteration 1591, loss = 0.22695999\n",
      "Iteration 1592, loss = 0.22691938\n",
      "Iteration 1593, loss = 0.22681147\n",
      "Iteration 1594, loss = 0.22666315\n",
      "Iteration 1595, loss = 0.22655737\n",
      "Iteration 1596, loss = 0.22647640\n",
      "Iteration 1597, loss = 0.22639262\n",
      "Iteration 1598, loss = 0.22629525\n",
      "Iteration 1599, loss = 0.22617388\n",
      "Iteration 1600, loss = 0.22609123\n",
      "Iteration 1601, loss = 0.22597947\n",
      "Iteration 1602, loss = 0.22586766\n",
      "Iteration 1603, loss = 0.22582430\n",
      "Iteration 1604, loss = 0.22566702\n",
      "Iteration 1605, loss = 0.22560309\n",
      "Iteration 1606, loss = 0.22551785\n",
      "Iteration 1607, loss = 0.22539054\n",
      "Iteration 1608, loss = 0.22529993\n",
      "Iteration 1609, loss = 0.22515636\n",
      "Iteration 1610, loss = 0.22509995\n",
      "Iteration 1611, loss = 0.22501333\n",
      "Iteration 1612, loss = 0.22487777\n",
      "Iteration 1613, loss = 0.22476653\n",
      "Iteration 1614, loss = 0.22478151\n",
      "Iteration 1615, loss = 0.22456549\n",
      "Iteration 1616, loss = 0.22449338\n",
      "Iteration 1617, loss = 0.22442621\n",
      "Iteration 1618, loss = 0.22428435\n",
      "Iteration 1619, loss = 0.22417275\n",
      "Iteration 1620, loss = 0.22405891\n",
      "Iteration 1621, loss = 0.22396404\n",
      "Iteration 1622, loss = 0.22388207\n",
      "Iteration 1623, loss = 0.22382821\n",
      "Iteration 1624, loss = 0.22365143\n",
      "Iteration 1625, loss = 0.22356482\n",
      "Iteration 1626, loss = 0.22347794\n",
      "Iteration 1627, loss = 0.22344080\n",
      "Iteration 1628, loss = 0.22328745\n",
      "Iteration 1629, loss = 0.22323645\n",
      "Iteration 1630, loss = 0.22307095\n",
      "Iteration 1631, loss = 0.22300878\n",
      "Iteration 1632, loss = 0.22295942\n",
      "Iteration 1633, loss = 0.22277902\n",
      "Iteration 1634, loss = 0.22282936\n",
      "Iteration 1635, loss = 0.22258371\n",
      "Iteration 1636, loss = 0.22245438\n",
      "Iteration 1637, loss = 0.22239228\n",
      "Iteration 108, loss = 0.39065671\n",
      "Iteration 109, loss = 0.38974031\n",
      "Iteration 110, loss = 0.38880072\n",
      "Iteration 111, loss = 0.38791168\n",
      "Iteration 112, loss = 0.38704200\n",
      "Iteration 113, loss = 0.38618269\n",
      "Iteration 114, loss = 0.38533903\n",
      "Iteration 115, loss = 0.38452326\n",
      "Iteration 116, loss = 0.38372931\n",
      "Iteration 117, loss = 0.38293473\n",
      "Iteration 118, loss = 0.38213865\n",
      "Iteration 119, loss = 0.38139917\n",
      "Iteration 120, loss = 0.38061352\n",
      "Iteration 121, loss = 0.37989333\n",
      "Iteration 122, loss = 0.37917431\n",
      "Iteration 123, loss = 0.37847263\n",
      "Iteration 124, loss = 0.37780841\n",
      "Iteration 125, loss = 0.37710451\n",
      "Iteration 126, loss = 0.37643043\n",
      "Iteration 127, loss = 0.37584658\n",
      "Iteration 128, loss = 0.37513388\n",
      "Iteration 129, loss = 0.37453497\n",
      "Iteration 130, loss = 0.37390158\n",
      "Iteration 131, loss = 0.37326716\n",
      "Iteration 132, loss = 0.37270289\n",
      "Iteration 133, loss = 0.37210858\n",
      "Iteration 134, loss = 0.37153645\n",
      "Iteration 135, loss = 0.37098385\n",
      "Iteration 136, loss = 0.37041454\n",
      "Iteration 137, loss = 0.36982014\n",
      "Iteration 138, loss = 0.36937643\n",
      "Iteration 139, loss = 0.36878417\n",
      "Iteration 140, loss = 0.36827421\n",
      "Iteration 141, loss = 0.36780760\n",
      "Iteration 142, loss = 0.36727176\n",
      "Iteration 143, loss = 0.36678501\n",
      "Iteration 144, loss = 0.36630561\n",
      "Iteration 145, loss = 0.36580633\n",
      "Iteration 146, loss = 0.36533638\n",
      "Iteration 147, loss = 0.36488789\n",
      "Iteration 148, loss = 0.36440182\n",
      "Iteration 149, loss = 0.36399873\n",
      "Iteration 150, loss = 0.36353620\n",
      "Iteration 151, loss = 0.36313570\n",
      "Iteration 152, loss = 0.36268427\n",
      "Iteration 153, loss = 0.36227348\n",
      "Iteration 154, loss = 0.36182637\n",
      "Iteration 155, loss = 0.36142854\n",
      "Iteration 156, loss = 0.36099911\n",
      "Iteration 157, loss = 0.36063711\n",
      "Iteration 158, loss = 0.36022296\n",
      "Iteration 159, loss = 0.35985595\n",
      "Iteration 160, loss = 0.35943094\n",
      "Iteration 161, loss = 0.35909343\n",
      "Iteration 162, loss = 0.35870688\n",
      "Iteration 163, loss = 0.35833723\n",
      "Iteration 164, loss = 0.35798152\n",
      "Iteration 165, loss = 0.35760714\n",
      "Iteration 166, loss = 0.35724907\n",
      "Iteration 167, loss = 0.35692049\n",
      "Iteration 168, loss = 0.35656318\n",
      "Iteration 169, loss = 0.35626628\n",
      "Iteration 170, loss = 0.35590400\n",
      "Iteration 171, loss = 0.35557891\n",
      "Iteration 172, loss = 0.35526527\n",
      "Iteration 173, loss = 0.35495144\n",
      "Iteration 174, loss = 0.35459971\n",
      "Iteration 175, loss = 0.35430611\n",
      "Iteration 176, loss = 0.35402029\n",
      "Iteration 177, loss = 0.35369005\n",
      "Iteration 178, loss = 0.35341140\n",
      "Iteration 179, loss = 0.35310422\n",
      "Iteration 180, loss = 0.35279418\n",
      "Iteration 181, loss = 0.35250731\n",
      "Iteration 182, loss = 0.35225512\n",
      "Iteration 183, loss = 0.35193218\n",
      "Iteration 184, loss = 0.35165289\n",
      "Iteration 185, loss = 0.35137922\n",
      "Iteration 186, loss = 0.35113514\n",
      "Iteration 187, loss = 0.35081283\n",
      "Iteration 188, loss = 0.35056143\n",
      "Iteration 189, loss = 0.35031836\n",
      "Iteration 190, loss = 0.35003123\n",
      "Iteration 191, loss = 0.34982105\n",
      "Iteration 192, loss = 0.34950818\n",
      "Iteration 193, loss = 0.34925616\n",
      "Iteration 194, loss = 0.34902005\n",
      "Iteration 195, loss = 0.34875254\n",
      "Iteration 196, loss = 0.34852165\n",
      "Iteration 197, loss = 0.34825616\n",
      "Iteration 198, loss = 0.34800440\n",
      "Iteration 199, loss = 0.34778344\n",
      "Iteration 200, loss = 0.34753311\n",
      "Iteration 201, loss = 0.34729203\n",
      "Iteration 202, loss = 0.34705530\n",
      "Iteration 203, loss = 0.34682872\n",
      "Iteration 204, loss = 0.34661139\n",
      "Iteration 205, loss = 0.34639504\n",
      "Iteration 206, loss = 0.34613345\n",
      "Iteration 207, loss = 0.34588995\n",
      "Iteration 208, loss = 0.34568159\n",
      "Iteration 209, loss = 0.34546630\n",
      "Iteration 210, loss = 0.34523816\n",
      "Iteration 211, loss = 0.34501066\n",
      "Iteration 212, loss = 0.34480758\n",
      "Iteration 213, loss = 0.34460128\n",
      "Iteration 214, loss = 0.34436904\n",
      "Iteration 215, loss = 0.34414815\n",
      "Iteration 216, loss = 0.34393854\n",
      "Iteration 217, loss = 0.34372878\n",
      "Iteration 218, loss = 0.34351835\n",
      "Iteration 219, loss = 0.34330669\n",
      "Iteration 220, loss = 0.34310909\n",
      "Iteration 221, loss = 0.34290138\n",
      "Iteration 222, loss = 0.34270465\n",
      "Iteration 223, loss = 0.34249392\n",
      "Iteration 224, loss = 0.34230935\n",
      "Iteration 225, loss = 0.34211438\n",
      "Iteration 226, loss = 0.34189130\n",
      "Iteration 227, loss = 0.34170572\n",
      "Iteration 228, loss = 0.34152322\n",
      "Iteration 229, loss = 0.34132110\n",
      "Iteration 230, loss = 0.34111785\n",
      "Iteration 231, loss = 0.34092459\n",
      "Iteration 232, loss = 0.34075228\n",
      "Iteration 233, loss = 0.34055742\n",
      "Iteration 234, loss = 0.34037480\n",
      "Iteration 235, loss = 0.34018743\n",
      "Iteration 236, loss = 0.34003588\n",
      "Iteration 237, loss = 0.33981679\n",
      "Iteration 238, loss = 0.33965138\n",
      "Iteration 239, loss = 0.33949589\n",
      "Iteration 240, loss = 0.33928677\n",
      "Iteration 241, loss = 0.33910196\n",
      "Iteration 242, loss = 0.33893659\n",
      "Iteration 243, loss = 0.33875101\n",
      "Iteration 244, loss = 0.33856902\n",
      "Iteration 245, loss = 0.33842104\n",
      "Iteration 246, loss = 0.33824373\n",
      "Iteration 247, loss = 0.33807389\n",
      "Iteration 248, loss = 0.33794133\n",
      "Iteration 249, loss = 0.33773163\n",
      "Iteration 250, loss = 0.33756078\n",
      "Iteration 251, loss = 0.33740283\n",
      "Iteration 252, loss = 0.33723373\n",
      "Iteration 253, loss = 0.33707986\n",
      "Iteration 254, loss = 0.33690086\n",
      "Iteration 255, loss = 0.33672877\n",
      "Iteration 256, loss = 0.33656981\n",
      "Iteration 257, loss = 0.33642660\n",
      "Iteration 258, loss = 0.33625977\n",
      "Iteration 259, loss = 0.33609217\n",
      "Iteration 260, loss = 0.33592840\n",
      "Iteration 261, loss = 0.33577585\n",
      "Iteration 262, loss = 0.33561150\n",
      "Iteration 263, loss = 0.33544926\n",
      "Iteration 264, loss = 0.33528527\n",
      "Iteration 265, loss = 0.33514585\n",
      "Iteration 266, loss = 0.33497131\n",
      "Iteration 267, loss = 0.33483061\n",
      "Iteration 268, loss = 0.33467160\n",
      "Iteration 269, loss = 0.33451002\n",
      "Iteration 270, loss = 0.33436002\n",
      "Iteration 271, loss = 0.33418931\n",
      "Iteration 272, loss = 0.33405748\n",
      "Iteration 273, loss = 0.33390090\n",
      "Iteration 274, loss = 0.33374146\n",
      "Iteration 275, loss = 0.33359705\n",
      "Iteration 276, loss = 0.33344301\n",
      "Iteration 277, loss = 0.33329146\n",
      "Iteration 278, loss = 0.33312950\n",
      "Iteration 279, loss = 0.33298119\n",
      "Iteration 280, loss = 0.33283979\n",
      "Iteration 281, loss = 0.33269956\n",
      "Iteration 282, loss = 0.33252406\n",
      "Iteration 283, loss = 0.33240820\n",
      "Iteration 284, loss = 0.33224961\n",
      "Iteration 285, loss = 0.33209215\n",
      "Iteration 286, loss = 0.33197344\n",
      "Iteration 287, loss = 0.33180580\n",
      "Iteration 288, loss = 0.33165827\n",
      "Iteration 289, loss = 0.33150888\n",
      "Iteration 290, loss = 0.33136868\n",
      "Iteration 291, loss = 0.33121152\n",
      "Iteration 292, loss = 0.33107997\n",
      "Iteration 293, loss = 0.33092670\n",
      "Iteration 294, loss = 0.33078024\n",
      "Iteration 295, loss = 0.33064247\n",
      "Iteration 296, loss = 0.33050182\n",
      "Iteration 297, loss = 0.33035542\n",
      "Iteration 298, loss = 0.33021225\n",
      "Iteration 299, loss = 0.33007490\n",
      "Iteration 300, loss = 0.32993587\n",
      "Iteration 301, loss = 0.32980242\n",
      "Iteration 302, loss = 0.32966036\n",
      "Iteration 303, loss = 0.32953143\n",
      "Iteration 304, loss = 0.32939920\n",
      "Iteration 305, loss = 0.32927465\n",
      "Iteration 306, loss = 0.32910517\n",
      "Iteration 307, loss = 0.32899415\n",
      "Iteration 308, loss = 0.32884883\n",
      "Iteration 309, loss = 0.32870674\n",
      "Iteration 310, loss = 0.32859077\n",
      "Iteration 311, loss = 0.32845696\n",
      "Iteration 312, loss = 0.32831383\n",
      "Iteration 313, loss = 0.32817734\n",
      "Iteration 314, loss = 0.32805430\n",
      "Iteration 315, loss = 0.32790993\n",
      "Iteration 316, loss = 0.32776762\n",
      "Iteration 317, loss = 0.32764623\n",
      "Iteration 318, loss = 0.32750845\n",
      "Iteration 319, loss = 0.32736219\n",
      "Iteration 320, loss = 0.32724829\n",
      "Iteration 321, loss = 0.32712360\n",
      "Iteration 322, loss = 0.32698364\n",
      "Iteration 323, loss = 0.32685796\n",
      "Iteration 324, loss = 0.32675855\n",
      "Iteration 325, loss = 0.32659130\n",
      "Iteration 326, loss = 0.32648886\n",
      "Iteration 327, loss = 0.32635919\n",
      "Iteration 328, loss = 0.32622922\n",
      "Iteration 329, loss = 0.32610374\n",
      "Iteration 330, loss = 0.32596941\n",
      "Iteration 331, loss = 0.32586249\n",
      "Iteration 332, loss = 0.32573282\n",
      "Iteration 333, loss = 0.32560099\n",
      "Iteration 334, loss = 0.32548442\n",
      "Iteration 335, loss = 0.32536199\n",
      "Iteration 336, loss = 0.32524166\n",
      "Iteration 337, loss = 0.32512234\n",
      "Iteration 338, loss = 0.32500394\n",
      "Iteration 339, loss = 0.32487066\n",
      "Iteration 340, loss = 0.32477743\n",
      "Iteration 341, loss = 0.32464932\n",
      "Iteration 342, loss = 0.32453447\n",
      "Iteration 343, loss = 0.32440303\n",
      "Iteration 344, loss = 0.32427466\n",
      "Iteration 345, loss = 0.32416790\n",
      "Iteration 346, loss = 0.32404806\n",
      "Iteration 347, loss = 0.32392163\n",
      "Iteration 348, loss = 0.32384106\n",
      "Iteration 349, loss = 0.32369900\n",
      "Iteration 350, loss = 0.32359533\n",
      "Iteration 351, loss = 0.32347613\n",
      "Iteration 352, loss = 0.32335438\n",
      "Iteration 353, loss = 0.32326888\n",
      "Iteration 354, loss = 0.32313056\n",
      "Iteration 355, loss = 0.32303928\n",
      "Iteration 356, loss = 0.32288166\n",
      "Iteration 357, loss = 0.32277852\n",
      "Iteration 358, loss = 0.32265088\n",
      "Iteration 359, loss = 0.32255377\n",
      "Iteration 360, loss = 0.32242474\n",
      "Iteration 361, loss = 0.32230858\n",
      "Iteration 362, loss = 0.32221271\n",
      "Iteration 363, loss = 0.32209714\n",
      "Iteration 364, loss = 0.32198028\n",
      "Iteration 365, loss = 0.32186075\n",
      "Iteration 366, loss = 0.32174679\n",
      "Iteration 367, loss = 0.32162884\n",
      "Iteration 368, loss = 0.32152421\n",
      "Iteration 369, loss = 0.32141245\n",
      "Iteration 370, loss = 0.32130725\n",
      "Iteration 371, loss = 0.32120653\n",
      "Iteration 372, loss = 0.32108830\n",
      "Iteration 373, loss = 0.32099265\n",
      "Iteration 374, loss = 0.32088082\n",
      "Iteration 375, loss = 0.32074553\n",
      "Iteration 376, loss = 0.32065705\n",
      "Iteration 377, loss = 0.32053962\n",
      "Iteration 378, loss = 0.32042616\n",
      "Iteration 379, loss = 0.32033669\n",
      "Iteration 380, loss = 0.32021070\n",
      "Iteration 381, loss = 0.32013920\n",
      "Iteration 382, loss = 0.32001153\n",
      "Iteration 383, loss = 0.31990662\n",
      "Iteration 384, loss = 0.31980895\n",
      "Iteration 385, loss = 0.31969012\n",
      "Iteration 386, loss = 0.31959587\n",
      "Iteration 387, loss = 0.31948345\n",
      "Iteration 388, loss = 0.31938218\n",
      "Iteration 389, loss = 0.31927804\n",
      "Iteration 390, loss = 0.31918189\n",
      "Iteration 391, loss = 0.31907779\n",
      "Iteration 392, loss = 0.31897569\n",
      "Iteration 393, loss = 0.31886863\n",
      "Iteration 394, loss = 0.31876371\n",
      "Iteration 395, loss = 0.31865379\n",
      "Iteration 396, loss = 0.31856202\n",
      "Iteration 397, loss = 0.31844878\n",
      "Iteration 398, loss = 0.31834574\n",
      "Iteration 399, loss = 0.31827251\n",
      "Iteration 400, loss = 0.31815761\n",
      "Iteration 401, loss = 0.31805490\n",
      "Iteration 402, loss = 0.31796349\n",
      "Iteration 403, loss = 0.31786659\n",
      "Iteration 404, loss = 0.31779003\n",
      "Iteration 405, loss = 0.31764657\n",
      "Iteration 406, loss = 0.31755258\n",
      "Iteration 407, loss = 0.31745005\n",
      "Iteration 408, loss = 0.31734158\n",
      "Iteration 409, loss = 0.31724774\n",
      "Iteration 410, loss = 0.31716908\n",
      "Iteration 411, loss = 0.31707207\n",
      "Iteration 412, loss = 0.31695287\n",
      "Iteration 413, loss = 0.31684493\n",
      "Iteration 414, loss = 0.31675241\n",
      "Iteration 415, loss = 0.31665476\n",
      "Iteration 416, loss = 0.31655958\n",
      "Iteration 417, loss = 0.31646918\n",
      "Iteration 418, loss = 0.31636277\n",
      "Iteration 419, loss = 0.31628210\n",
      "Iteration 420, loss = 0.31616841\n",
      "Iteration 421, loss = 0.31608086\n",
      "Iteration 422, loss = 0.31597837\n",
      "Iteration 423, loss = 0.31589382\n",
      "Iteration 424, loss = 0.31577520\n",
      "Iteration 425, loss = 0.31569342\n",
      "Iteration 426, loss = 0.31559976\n",
      "Iteration 427, loss = 0.31550149\n",
      "Iteration 428, loss = 0.31541114\n",
      "Iteration 429, loss = 0.31531388\n",
      "Iteration 430, loss = 0.31522825\n",
      "Iteration 431, loss = 0.31511777\n",
      "Iteration 432, loss = 0.31502520\n",
      "Iteration 433, loss = 0.31493639\n",
      "Iteration 434, loss = 0.31483220\n",
      "Iteration 435, loss = 0.31473917\n",
      "Iteration 436, loss = 0.31463336\n",
      "Iteration 437, loss = 0.31455787\n",
      "Iteration 438, loss = 0.31444341\n",
      "Iteration 439, loss = 0.31435262\n",
      "Iteration 440, loss = 0.31426158\n",
      "Iteration 441, loss = 0.31417927\n",
      "Iteration 442, loss = 0.31408110\n",
      "Iteration 443, loss = 0.31398136\n",
      "Iteration 444, loss = 0.31391202\n",
      "Iteration 445, loss = 0.31381939\n",
      "Iteration 446, loss = 0.31371799\n",
      "Iteration 447, loss = 0.31362262\n",
      "Iteration 448, loss = 0.31353451\n",
      "Iteration 449, loss = 0.31344822\n",
      "Iteration 450, loss = 0.31337505\n",
      "Iteration 451, loss = 0.31326297\n",
      "Iteration 452, loss = 0.31316300\n",
      "Iteration 453, loss = 0.31308261\n",
      "Iteration 454, loss = 0.31299130\n",
      "Iteration 455, loss = 0.31290449\n",
      "Iteration 456, loss = 0.31282642\n",
      "Iteration 457, loss = 0.31271629\n",
      "Iteration 458, loss = 0.31262633\n",
      "Iteration 459, loss = 0.31254721\n",
      "Iteration 460, loss = 0.31245798\n",
      "Iteration 461, loss = 0.31237658\n",
      "Iteration 462, loss = 0.31228080\n",
      "Iteration 463, loss = 0.31218690\n",
      "Iteration 464, loss = 0.31209990\n",
      "Iteration 465, loss = 0.31201877\n",
      "Iteration 466, loss = 0.31195390\n",
      "Iteration 467, loss = 0.31184585\n",
      "Iteration 468, loss = 0.31176142\n",
      "Iteration 469, loss = 0.31166808\n",
      "Iteration 470, loss = 0.31157934\n",
      "Iteration 471, loss = 0.31150551\n",
      "Iteration 472, loss = 0.31142887\n",
      "Iteration 473, loss = 0.31133199\n",
      "Iteration 474, loss = 0.31125206\n",
      "Iteration 475, loss = 0.31114651\n",
      "Iteration 476, loss = 0.31106969\n",
      "Iteration 477, loss = 0.31097322\n",
      "Iteration 478, loss = 0.31089430\n",
      "Iteration 479, loss = 0.31079964\n",
      "Iteration 480, loss = 0.31073312\n",
      "Iteration 481, loss = 0.31063285\n",
      "Iteration 482, loss = 0.31054564\n",
      "Iteration 483, loss = 0.31045918\n",
      "Iteration 484, loss = 0.31037799\n",
      "Iteration 485, loss = 0.31029847\n",
      "Iteration 486, loss = 0.31022082\n",
      "Iteration 487, loss = 0.31011712\n",
      "Iteration 488, loss = 0.31003856\n",
      "Iteration 489, loss = 0.30997682\n",
      "Iteration 490, loss = 0.30988535\n",
      "Iteration 491, loss = 0.30978512\n",
      "Iteration 492, loss = 0.30971092\n",
      "Iteration 493, loss = 0.30963490\n",
      "Iteration 494, loss = 0.30956383\n",
      "Iteration 495, loss = 0.30945852\n",
      "Iteration 496, loss = 0.30937797\n",
      "Iteration 497, loss = 0.30928154\n",
      "Iteration 498, loss = 0.30921390\n",
      "Iteration 499, loss = 0.30911138\n",
      "Iteration 500, loss = 0.30903387\n",
      "Iteration 501, loss = 0.30894961\n",
      "Iteration 502, loss = 0.30886964\n",
      "Iteration 503, loss = 0.30878684\n",
      "Iteration 504, loss = 0.30869866\n",
      "Iteration 505, loss = 0.30861907\n",
      "Iteration 506, loss = 0.30853042\n",
      "Iteration 507, loss = 0.30844778\n",
      "Iteration 508, loss = 0.30837047\n",
      "Iteration 509, loss = 0.30829026\n",
      "Iteration 510, loss = 0.30820784\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77928705\n",
      "Iteration 2, loss = 0.77610297\n",
      "Iteration 3, loss = 0.77124132\n",
      "Iteration 4, loss = 0.76519047\n",
      "Iteration 5, loss = 0.75857509\n",
      "Iteration 6, loss = 0.75123710\n",
      "Iteration 7, loss = 0.74378658\n",
      "Iteration 8, loss = 0.73632363\n",
      "Iteration 9, loss = 0.72853474\n",
      "Iteration 10, loss = 0.72106490\n",
      "Iteration 11, loss = 0.71354880\n",
      "Iteration 12, loss = 0.70627301\n",
      "Iteration 13, loss = 0.69902040\n",
      "Iteration 14, loss = 0.69212604\n",
      "Iteration 15, loss = 0.68533003\n",
      "Iteration 16, loss = 0.67843025\n",
      "Iteration 17, loss = 0.67206883\n",
      "Iteration 18, loss = 0.66556688\n",
      "Iteration 19, loss = 0.65930690\n",
      "Iteration 20, loss = 0.65333876\n",
      "Iteration 21, loss = 0.64734594\n",
      "Iteration 22, loss = 0.64135246\n",
      "Iteration 23, loss = 0.63576173\n",
      "Iteration 24, loss = 0.63031019\n",
      "Iteration 25, loss = 0.62471658\n",
      "Iteration 26, loss = 0.61935107\n",
      "Iteration 27, loss = 0.61408508\n",
      "Iteration 28, loss = 0.60903057\n",
      "Iteration 29, loss = 0.60400391\n",
      "Iteration 30, loss = 0.59911284\n",
      "Iteration 31, loss = 0.59422434\n",
      "Iteration 32, loss = 0.58944049\n",
      "Iteration 33, loss = 0.58481947\n",
      "Iteration 34, loss = 0.58020117\n",
      "Iteration 35, loss = 0.57568398\n",
      "Iteration 36, loss = 0.57132567\n",
      "Iteration 37, loss = 0.56684809\n",
      "Iteration 38, loss = 0.56261396\n",
      "Iteration 39, loss = 0.55847995\n",
      "Iteration 40, loss = 0.55435941\n",
      "Iteration 41, loss = 0.55033974\n",
      "Iteration 42, loss = 0.54636812\n",
      "Iteration 43, loss = 0.54252214\n",
      "Iteration 44, loss = 0.53869836\n",
      "Iteration 45, loss = 0.53492274\n",
      "Iteration 46, loss = 0.53132751\n",
      "Iteration 47, loss = 0.52779260\n",
      "Iteration 48, loss = 0.52425969\n",
      "Iteration 49, loss = 0.52082654\n",
      "Iteration 50, loss = 0.51749792\n",
      "Iteration 51, loss = 0.51414633\n",
      "Iteration 52, loss = 0.51100287\n",
      "Iteration 53, loss = 0.50783158\n",
      "Iteration 54, loss = 0.50471706\n",
      "Iteration 55, loss = 0.50184321\n",
      "Iteration 56, loss = 0.49877973\n",
      "Iteration 57, loss = 0.49586547\n",
      "Iteration 58, loss = 0.49308263\n",
      "Iteration 59, loss = 0.49046609\n",
      "Iteration 60, loss = 0.48773134\n",
      "Iteration 61, loss = 0.48505903\n",
      "Iteration 62, loss = 0.48252812\n",
      "Iteration 63, loss = 0.47996929\n",
      "Iteration 64, loss = 0.47763561\n",
      "Iteration 65, loss = 0.47526284\n",
      "Iteration 66, loss = 0.47290201\n",
      "Iteration 67, loss = 0.47063232\n",
      "Iteration 68, loss = 0.46837950\n",
      "Iteration 69, loss = 0.46628062\n",
      "Iteration 70, loss = 0.46411909\n",
      "Iteration 71, loss = 0.46211713\n",
      "Iteration 72, loss = 0.46009842\n",
      "Iteration 73, loss = 0.45814661\n",
      "Iteration 74, loss = 0.45628939\n",
      "Iteration 75, loss = 0.45434073\n",
      "Iteration 76, loss = 0.45257880\n",
      "Iteration 77, loss = 0.45073755\n",
      "Iteration 78, loss = 0.44901800\n",
      "Iteration 79, loss = 0.44730611\n",
      "Iteration 80, loss = 0.44568647\n",
      "Iteration 81, loss = 0.44409117\n",
      "Iteration 82, loss = 0.44253285\n",
      "Iteration 83, loss = 0.44098731\n",
      "Iteration 84, loss = 0.43953209\n",
      "Iteration 85, loss = 0.43802941\n",
      "Iteration 86, loss = 0.43667174\n",
      "Iteration 87, loss = 0.43528822\n",
      "Iteration 88, loss = 0.43388137\n",
      "Iteration 89, loss = 0.43253585\n",
      "Iteration 90, loss = 0.43129485\n",
      "Iteration 91, loss = 0.43006579\n",
      "Iteration 92, loss = 0.42882136\n",
      "Iteration 93, loss = 0.42758661\n",
      "Iteration 332, loss = 0.36801284\n",
      "Iteration 333, loss = 0.36785677\n",
      "Iteration 334, loss = 0.36766213\n",
      "Iteration 335, loss = 0.36744159\n",
      "Iteration 336, loss = 0.36725063\n",
      "Iteration 337, loss = 0.36706603\n",
      "Iteration 338, loss = 0.36690559\n",
      "Iteration 339, loss = 0.36670603\n",
      "Iteration 340, loss = 0.36649019\n",
      "Iteration 341, loss = 0.36634626\n",
      "Iteration 342, loss = 0.36615150\n",
      "Iteration 343, loss = 0.36593879\n",
      "Iteration 344, loss = 0.36574994\n",
      "Iteration 345, loss = 0.36560069\n",
      "Iteration 346, loss = 0.36535861\n",
      "Iteration 347, loss = 0.36521504\n",
      "Iteration 348, loss = 0.36500312\n",
      "Iteration 349, loss = 0.36485066\n",
      "Iteration 350, loss = 0.36464659\n",
      "Iteration 351, loss = 0.36448079\n",
      "Iteration 352, loss = 0.36426723\n",
      "Iteration 353, loss = 0.36407639\n",
      "Iteration 354, loss = 0.36393011\n",
      "Iteration 355, loss = 0.36373703\n",
      "Iteration 356, loss = 0.36358608\n",
      "Iteration 357, loss = 0.36334746\n",
      "Iteration 358, loss = 0.36315758\n",
      "Iteration 359, loss = 0.36298786\n",
      "Iteration 360, loss = 0.36281125\n",
      "Iteration 361, loss = 0.36260617\n",
      "Iteration 362, loss = 0.36241382\n",
      "Iteration 363, loss = 0.36223396\n",
      "Iteration 364, loss = 0.36205887\n",
      "Iteration 365, loss = 0.36188239\n",
      "Iteration 366, loss = 0.36169733\n",
      "Iteration 367, loss = 0.36153932\n",
      "Iteration 368, loss = 0.36137067\n",
      "Iteration 369, loss = 0.36116031\n",
      "Iteration 370, loss = 0.36102342\n",
      "Iteration 371, loss = 0.36081670\n",
      "Iteration 372, loss = 0.36063677\n",
      "Iteration 373, loss = 0.36046658\n",
      "Iteration 374, loss = 0.36030197\n",
      "Iteration 375, loss = 0.36010799\n",
      "Iteration 376, loss = 0.35993808\n",
      "Iteration 377, loss = 0.35977007\n",
      "Iteration 378, loss = 0.35963705\n",
      "Iteration 379, loss = 0.35944649\n",
      "Iteration 380, loss = 0.35926055\n",
      "Iteration 381, loss = 0.35912861\n",
      "Iteration 382, loss = 0.35891500\n",
      "Iteration 383, loss = 0.35876405\n",
      "Iteration 384, loss = 0.35858606\n",
      "Iteration 385, loss = 0.35840011\n",
      "Iteration 386, loss = 0.35823415\n",
      "Iteration 387, loss = 0.35807135\n",
      "Iteration 388, loss = 0.35790278\n",
      "Iteration 389, loss = 0.35771486\n",
      "Iteration 390, loss = 0.35756023\n",
      "Iteration 391, loss = 0.35741626\n",
      "Iteration 392, loss = 0.35722850\n",
      "Iteration 393, loss = 0.35706682\n",
      "Iteration 394, loss = 0.35691258\n",
      "Iteration 395, loss = 0.35679025\n",
      "Iteration 396, loss = 0.35658580\n",
      "Iteration 397, loss = 0.35641442\n",
      "Iteration 398, loss = 0.35623930\n",
      "Iteration 399, loss = 0.35609150\n",
      "Iteration 400, loss = 0.35599044\n",
      "Iteration 401, loss = 0.35577106\n",
      "Iteration 402, loss = 0.35560654\n",
      "Iteration 403, loss = 0.35544584\n",
      "Iteration 404, loss = 0.35527247\n",
      "Iteration 405, loss = 0.35516757\n",
      "Iteration 406, loss = 0.35496595\n",
      "Iteration 407, loss = 0.35485459\n",
      "Iteration 408, loss = 0.35471234\n",
      "Iteration 409, loss = 0.35451720\n",
      "Iteration 410, loss = 0.35434202\n",
      "Iteration 411, loss = 0.35418606\n",
      "Iteration 412, loss = 0.35406456\n",
      "Iteration 413, loss = 0.35387303\n",
      "Iteration 414, loss = 0.35371243\n",
      "Iteration 415, loss = 0.35354780\n",
      "Iteration 416, loss = 0.35339980\n",
      "Iteration 417, loss = 0.35323449\n",
      "Iteration 418, loss = 0.35311951\n",
      "Iteration 419, loss = 0.35296277\n",
      "Iteration 420, loss = 0.35278098\n",
      "Iteration 421, loss = 0.35260857\n",
      "Iteration 422, loss = 0.35246672\n",
      "Iteration 423, loss = 0.35229095\n",
      "Iteration 424, loss = 0.35213324\n",
      "Iteration 425, loss = 0.35200197\n",
      "Iteration 426, loss = 0.35184261\n",
      "Iteration 427, loss = 0.35169076\n",
      "Iteration 428, loss = 0.35152844\n",
      "Iteration 429, loss = 0.35138630\n",
      "Iteration 430, loss = 0.35122329\n",
      "Iteration 431, loss = 0.35106513\n",
      "Iteration 432, loss = 0.35096514\n",
      "Iteration 433, loss = 0.35078355\n",
      "Iteration 434, loss = 0.35061347\n",
      "Iteration 435, loss = 0.35046025\n",
      "Iteration 436, loss = 0.35030382\n",
      "Iteration 437, loss = 0.35019521\n",
      "Iteration 438, loss = 0.35003563\n",
      "Iteration 439, loss = 0.34987028\n",
      "Iteration 440, loss = 0.34971921\n",
      "Iteration 441, loss = 0.34955609\n",
      "Iteration 442, loss = 0.34945416\n",
      "Iteration 443, loss = 0.34927710\n",
      "Iteration 444, loss = 0.34911765\n",
      "Iteration 445, loss = 0.34895278\n",
      "Iteration 446, loss = 0.34880019\n",
      "Iteration 447, loss = 0.34866206\n",
      "Iteration 448, loss = 0.34852975\n",
      "Iteration 449, loss = 0.34834642\n",
      "Iteration 450, loss = 0.34821617\n",
      "Iteration 451, loss = 0.34804419\n",
      "Iteration 452, loss = 0.34790312\n",
      "Iteration 453, loss = 0.34775055\n",
      "Iteration 454, loss = 0.34761517\n",
      "Iteration 455, loss = 0.34745053\n",
      "Iteration 456, loss = 0.34730287\n",
      "Iteration 457, loss = 0.34713801\n",
      "Iteration 458, loss = 0.34700354\n",
      "Iteration 459, loss = 0.34684057\n",
      "Iteration 460, loss = 0.34673147\n",
      "Iteration 461, loss = 0.34656755\n",
      "Iteration 462, loss = 0.34641166\n",
      "Iteration 463, loss = 0.34629536\n",
      "Iteration 464, loss = 0.34610491\n",
      "Iteration 465, loss = 0.34596328\n",
      "Iteration 466, loss = 0.34584996\n",
      "Iteration 467, loss = 0.34565039\n",
      "Iteration 468, loss = 0.34556439\n",
      "Iteration 469, loss = 0.34542478\n",
      "Iteration 470, loss = 0.34522118\n",
      "Iteration 471, loss = 0.34509091\n",
      "Iteration 472, loss = 0.34492956\n",
      "Iteration 473, loss = 0.34478248\n",
      "Iteration 474, loss = 0.34466530\n",
      "Iteration 475, loss = 0.34448288\n",
      "Iteration 476, loss = 0.34434472\n",
      "Iteration 477, loss = 0.34424588\n",
      "Iteration 478, loss = 0.34406296\n",
      "Iteration 479, loss = 0.34388944\n",
      "Iteration 480, loss = 0.34378764\n",
      "Iteration 481, loss = 0.34365080\n",
      "Iteration 482, loss = 0.34345010\n",
      "Iteration 483, loss = 0.34331646\n",
      "Iteration 484, loss = 0.34319672\n",
      "Iteration 485, loss = 0.34302939\n",
      "Iteration 486, loss = 0.34286812\n",
      "Iteration 487, loss = 0.34280084\n",
      "Iteration 488, loss = 0.34257066\n",
      "Iteration 489, loss = 0.34246032\n",
      "Iteration 490, loss = 0.34230336\n",
      "Iteration 491, loss = 0.34215662\n",
      "Iteration 492, loss = 0.34202042\n",
      "Iteration 493, loss = 0.34187938\n",
      "Iteration 494, loss = 0.34174534\n",
      "Iteration 495, loss = 0.34158168\n",
      "Iteration 496, loss = 0.34143790\n",
      "Iteration 497, loss = 0.34130067\n",
      "Iteration 498, loss = 0.34117764\n",
      "Iteration 499, loss = 0.34104524\n",
      "Iteration 500, loss = 0.34089902\n",
      "Iteration 501, loss = 0.34073453\n",
      "Iteration 502, loss = 0.34060389\n",
      "Iteration 503, loss = 0.34045957\n",
      "Iteration 504, loss = 0.34033652\n",
      "Iteration 505, loss = 0.34029077\n",
      "Iteration 506, loss = 0.34004809\n",
      "Iteration 507, loss = 0.33989596\n",
      "Iteration 508, loss = 0.33975718\n",
      "Iteration 509, loss = 0.33959828\n",
      "Iteration 510, loss = 0.33948311\n",
      "Iteration 511, loss = 0.33933168\n",
      "Iteration 512, loss = 0.33917732\n",
      "Iteration 513, loss = 0.33906385\n",
      "Iteration 514, loss = 0.33892441\n",
      "Iteration 515, loss = 0.33878242\n",
      "Iteration 516, loss = 0.33862730\n",
      "Iteration 517, loss = 0.33850510\n",
      "Iteration 518, loss = 0.33837362\n",
      "Iteration 519, loss = 0.33820980\n",
      "Iteration 520, loss = 0.33810416\n",
      "Iteration 521, loss = 0.33795831\n",
      "Iteration 522, loss = 0.33781017\n",
      "Iteration 523, loss = 0.33767340\n",
      "Iteration 524, loss = 0.33752959\n",
      "Iteration 525, loss = 0.33743947\n",
      "Iteration 526, loss = 0.33728101\n",
      "Iteration 527, loss = 0.33715807\n",
      "Iteration 528, loss = 0.33697996\n",
      "Iteration 529, loss = 0.33685938\n",
      "Iteration 530, loss = 0.33671596\n",
      "Iteration 531, loss = 0.33658707\n",
      "Iteration 532, loss = 0.33644854\n",
      "Iteration 533, loss = 0.33630151\n",
      "Iteration 534, loss = 0.33621737\n",
      "Iteration 535, loss = 0.33603433\n",
      "Iteration 536, loss = 0.33588601\n",
      "Iteration 537, loss = 0.33573514\n",
      "Iteration 538, loss = 0.33560467\n",
      "Iteration 539, loss = 0.33547652\n",
      "Iteration 540, loss = 0.33534259\n",
      "Iteration 541, loss = 0.33520830\n",
      "Iteration 542, loss = 0.33511033\n",
      "Iteration 543, loss = 0.33494420\n",
      "Iteration 544, loss = 0.33477996\n",
      "Iteration 545, loss = 0.33471162\n",
      "Iteration 546, loss = 0.33450944\n",
      "Iteration 547, loss = 0.33439399\n",
      "Iteration 548, loss = 0.33425426\n",
      "Iteration 549, loss = 0.33411703\n",
      "Iteration 550, loss = 0.33398693\n",
      "Iteration 551, loss = 0.33383626\n",
      "Iteration 552, loss = 0.33375427\n",
      "Iteration 553, loss = 0.33356744\n",
      "Iteration 554, loss = 0.33344571\n",
      "Iteration 555, loss = 0.33330214\n",
      "Iteration 556, loss = 0.33315978\n",
      "Iteration 557, loss = 0.33305515\n",
      "Iteration 558, loss = 0.33288334\n",
      "Iteration 559, loss = 0.33276373\n",
      "Iteration 560, loss = 0.33260576\n",
      "Iteration 561, loss = 0.33250738\n",
      "Iteration 562, loss = 0.33234050\n",
      "Iteration 563, loss = 0.33221211\n",
      "Iteration 564, loss = 0.33208150\n",
      "Iteration 565, loss = 0.33194157\n",
      "Iteration 566, loss = 0.33181011\n",
      "Iteration 567, loss = 0.33165805\n",
      "Iteration 568, loss = 0.33154148\n",
      "Iteration 569, loss = 0.33140810\n",
      "Iteration 570, loss = 0.33132842\n",
      "Iteration 571, loss = 0.33115183\n",
      "Iteration 572, loss = 0.33100002\n",
      "Iteration 573, loss = 0.33086896\n",
      "Iteration 574, loss = 0.33074037\n",
      "Iteration 575, loss = 0.33061557\n",
      "Iteration 576, loss = 0.33046513\n",
      "Iteration 577, loss = 0.33034397\n",
      "Iteration 578, loss = 0.33020206\n",
      "Iteration 579, loss = 0.33006313\n",
      "Iteration 580, loss = 0.32994066\n",
      "Iteration 581, loss = 0.32982322\n",
      "Iteration 582, loss = 0.32968900\n",
      "Iteration 583, loss = 0.32960394\n",
      "Iteration 584, loss = 0.32942859\n",
      "Iteration 585, loss = 0.32927462\n",
      "Iteration 586, loss = 0.32918525\n",
      "Iteration 587, loss = 0.32903674\n",
      "Iteration 588, loss = 0.32889278\n",
      "Iteration 589, loss = 0.32873949\n",
      "Iteration 590, loss = 0.32863459\n",
      "Iteration 591, loss = 0.32851817\n",
      "Iteration 592, loss = 0.32841561\n",
      "Iteration 593, loss = 0.32822448\n",
      "Iteration 594, loss = 0.32809631\n",
      "Iteration 595, loss = 0.32794366\n",
      "Iteration 596, loss = 0.32784196\n",
      "Iteration 597, loss = 0.32770714\n",
      "Iteration 598, loss = 0.32755714\n",
      "Iteration 599, loss = 0.32744244\n",
      "Iteration 600, loss = 0.32730684\n",
      "Iteration 601, loss = 0.32720095\n",
      "Iteration 602, loss = 0.32703353\n",
      "Iteration 603, loss = 0.32695952\n",
      "Iteration 604, loss = 0.32676882\n",
      "Iteration 605, loss = 0.32664191\n",
      "Iteration 606, loss = 0.32651305\n",
      "Iteration 607, loss = 0.32644362\n",
      "Iteration 608, loss = 0.32629725\n",
      "Iteration 609, loss = 0.32614648\n",
      "Iteration 610, loss = 0.32601666\n",
      "Iteration 611, loss = 0.32594091\n",
      "Iteration 612, loss = 0.32573265\n",
      "Iteration 613, loss = 0.32561804\n",
      "Iteration 614, loss = 0.32551594\n",
      "Iteration 615, loss = 0.32538514\n",
      "Iteration 616, loss = 0.32524933\n",
      "Iteration 617, loss = 0.32513165\n",
      "Iteration 618, loss = 0.32496953\n",
      "Iteration 619, loss = 0.32485723\n",
      "Iteration 620, loss = 0.32473778\n",
      "Iteration 621, loss = 0.32461211\n",
      "Iteration 622, loss = 0.32447457\n",
      "Iteration 623, loss = 0.32434408\n",
      "Iteration 624, loss = 0.32423573\n",
      "Iteration 625, loss = 0.32412753\n",
      "Iteration 626, loss = 0.32398721\n",
      "Iteration 627, loss = 0.32390689\n",
      "Iteration 628, loss = 0.32371564\n",
      "Iteration 629, loss = 0.32359785\n",
      "Iteration 630, loss = 0.32346247\n",
      "Iteration 631, loss = 0.32339310\n",
      "Iteration 632, loss = 0.32322701\n",
      "Iteration 633, loss = 0.32309267\n",
      "Iteration 634, loss = 0.32300132\n",
      "Iteration 635, loss = 0.32283959\n",
      "Iteration 636, loss = 0.32272598\n",
      "Iteration 637, loss = 0.32260531\n",
      "Iteration 638, loss = 0.32248083\n",
      "Iteration 639, loss = 0.32236220\n",
      "Iteration 640, loss = 0.32222035\n",
      "Iteration 641, loss = 0.32214025\n",
      "Iteration 642, loss = 0.32197048\n",
      "Iteration 643, loss = 0.32192650\n",
      "Iteration 644, loss = 0.32175782\n",
      "Iteration 645, loss = 0.32162004\n",
      "Iteration 646, loss = 0.32146943\n",
      "Iteration 647, loss = 0.32134962\n",
      "Iteration 648, loss = 0.32123914\n",
      "Iteration 649, loss = 0.32109347\n",
      "Iteration 650, loss = 0.32098467\n",
      "Iteration 651, loss = 0.32086409\n",
      "Iteration 652, loss = 0.32076225\n",
      "Iteration 653, loss = 0.32059760\n",
      "Iteration 654, loss = 0.32060130\n",
      "Iteration 655, loss = 0.32040527\n",
      "Iteration 656, loss = 0.32024596\n",
      "Iteration 657, loss = 0.32012569\n",
      "Iteration 658, loss = 0.31998926\n",
      "Iteration 659, loss = 0.31985299\n",
      "Iteration 660, loss = 0.31973349\n",
      "Iteration 661, loss = 0.31963170\n",
      "Iteration 662, loss = 0.31949436\n",
      "Iteration 663, loss = 0.31939698\n",
      "Iteration 664, loss = 0.31924019\n",
      "Iteration 665, loss = 0.31913257\n",
      "Iteration 666, loss = 0.31898665\n",
      "Iteration 667, loss = 0.31887959\n",
      "Iteration 668, loss = 0.31876457\n",
      "Iteration 669, loss = 0.31864270\n",
      "Iteration 670, loss = 0.31848733\n",
      "Iteration 671, loss = 0.31836026\n",
      "Iteration 672, loss = 0.31824241\n",
      "Iteration 673, loss = 0.31812607\n",
      "Iteration 674, loss = 0.31799718\n",
      "Iteration 675, loss = 0.31789596\n",
      "Iteration 676, loss = 0.31777080\n",
      "Iteration 677, loss = 0.31770883\n",
      "Iteration 678, loss = 0.31755701\n",
      "Iteration 679, loss = 0.31743162\n",
      "Iteration 680, loss = 0.31725729\n",
      "Iteration 681, loss = 0.31716201\n",
      "Iteration 682, loss = 0.31705079\n",
      "Iteration 683, loss = 0.31688572\n",
      "Iteration 684, loss = 0.31677378\n",
      "Iteration 685, loss = 0.31664260\n",
      "Iteration 686, loss = 0.31651610\n",
      "Iteration 687, loss = 0.31640661\n",
      "Iteration 688, loss = 0.31630076\n",
      "Iteration 689, loss = 0.31616797\n",
      "Iteration 690, loss = 0.31611847\n",
      "Iteration 691, loss = 0.31591254\n",
      "Iteration 692, loss = 0.31580778\n",
      "Iteration 693, loss = 0.31570997\n",
      "Iteration 694, loss = 0.31554404\n",
      "Iteration 695, loss = 0.31543793\n",
      "Iteration 696, loss = 0.31529788\n",
      "Iteration 697, loss = 0.31523439\n",
      "Iteration 698, loss = 0.31510321\n",
      "Iteration 699, loss = 0.31495132\n",
      "Iteration 700, loss = 0.31483002\n",
      "Iteration 701, loss = 0.31471970\n",
      "Iteration 702, loss = 0.31457064\n",
      "Iteration 703, loss = 0.31445841\n",
      "Iteration 704, loss = 0.31434565\n",
      "Iteration 705, loss = 0.31423992\n",
      "Iteration 706, loss = 0.31412215\n",
      "Iteration 707, loss = 0.31396735\n",
      "Iteration 708, loss = 0.31388050\n",
      "Iteration 709, loss = 0.31375109\n",
      "Iteration 710, loss = 0.31358936\n",
      "Iteration 711, loss = 0.31350829\n",
      "Iteration 712, loss = 0.31340988\n",
      "Iteration 713, loss = 0.31324901\n",
      "Iteration 714, loss = 0.31312081\n",
      "Iteration 715, loss = 0.31307281\n",
      "Iteration 716, loss = 0.31289150\n",
      "Iteration 717, loss = 0.31277381\n",
      "Iteration 718, loss = 0.31262952\n",
      "Iteration 719, loss = 0.31251886\n",
      "Iteration 720, loss = 0.31238864\n",
      "Iteration 721, loss = 0.31226958\n",
      "Iteration 722, loss = 0.31215562\n",
      "Iteration 723, loss = 0.31202701\n",
      "Iteration 724, loss = 0.31190355\n",
      "Iteration 725, loss = 0.31177913\n",
      "Iteration 726, loss = 0.31164563\n",
      "Iteration 727, loss = 0.31155249\n",
      "Iteration 728, loss = 0.31143563\n",
      "Iteration 729, loss = 0.31129001\n",
      "Iteration 730, loss = 0.31116807\n",
      "Iteration 731, loss = 0.31108456\n",
      "Iteration 732, loss = 0.31099300\n",
      "Iteration 733, loss = 0.31081625\n",
      "Iteration 734, loss = 0.31069075\n",
      "Iteration 735, loss = 0.31058370\n",
      "Iteration 736, loss = 0.31046775\n",
      "Iteration 737, loss = 0.31032740\n",
      "Iteration 738, loss = 0.31018815\n",
      "Iteration 739, loss = 0.31008682\n",
      "Iteration 740, loss = 0.30998768\n",
      "Iteration 741, loss = 0.30982962\n",
      "Iteration 742, loss = 0.30970737\n",
      "Iteration 743, loss = 0.30957845\n",
      "Iteration 744, loss = 0.30948201\n",
      "Iteration 745, loss = 0.30934618\n",
      "Iteration 746, loss = 0.30923547\n",
      "Iteration 747, loss = 0.30910943\n",
      "Iteration 748, loss = 0.30897778\n",
      "Iteration 749, loss = 0.30889586\n",
      "Iteration 750, loss = 0.30875404\n",
      "Iteration 751, loss = 0.30862645\n",
      "Iteration 752, loss = 0.30852481\n",
      "Iteration 753, loss = 0.30839208\n",
      "Iteration 754, loss = 0.30825596\n",
      "Iteration 755, loss = 0.30813978\n",
      "Iteration 756, loss = 0.30804547\n",
      "Iteration 757, loss = 0.30795347\n",
      "Iteration 758, loss = 0.30777131\n",
      "Iteration 759, loss = 0.30768025\n",
      "Iteration 760, loss = 0.30756280\n",
      "Iteration 761, loss = 0.30740055\n",
      "Iteration 762, loss = 0.30730261\n",
      "Iteration 763, loss = 0.30716469\n",
      "Iteration 764, loss = 0.30706380\n",
      "Iteration 765, loss = 0.30691023\n",
      "Iteration 766, loss = 0.30681627\n",
      "Iteration 767, loss = 0.30669581\n",
      "Iteration 768, loss = 0.30657412\n",
      "Iteration 769, loss = 0.30643295\n",
      "Iteration 770, loss = 0.30632044\n",
      "Iteration 771, loss = 0.30621110\n",
      "Iteration 772, loss = 0.30609209\n",
      "Iteration 773, loss = 0.30597092\n",
      "Iteration 774, loss = 0.30586236\n",
      "Iteration 775, loss = 0.30570884\n",
      "Iteration 776, loss = 0.30563221\n",
      "Iteration 777, loss = 0.30550031\n",
      "Iteration 778, loss = 0.30534721\n",
      "Iteration 779, loss = 0.30526052\n",
      "Iteration 780, loss = 0.30511119\n",
      "Iteration 781, loss = 0.30500812\n",
      "Iteration 782, loss = 0.30487406\n",
      "Iteration 783, loss = 0.30477259\n",
      "Iteration 784, loss = 0.30462912\n",
      "Iteration 785, loss = 0.30450986\n",
      "Iteration 786, loss = 0.30440416\n",
      "Iteration 787, loss = 0.30430186\n",
      "Iteration 788, loss = 0.30414963\n",
      "Iteration 789, loss = 0.30402669\n",
      "Iteration 790, loss = 0.30390248\n",
      "Iteration 791, loss = 0.30377936\n",
      "Iteration 792, loss = 0.30366897\n",
      "Iteration 793, loss = 0.30355224\n",
      "Iteration 794, loss = 0.30344084\n",
      "Iteration 795, loss = 0.30332359\n",
      "Iteration 796, loss = 0.30319093\n",
      "Iteration 797, loss = 0.30306684\n",
      "Iteration 798, loss = 0.30296590\n",
      "Iteration 799, loss = 0.30283693\n",
      "Iteration 800, loss = 0.30272026\n",
      "Iteration 801, loss = 0.30257761\n",
      "Iteration 802, loss = 0.30253095\n",
      "Iteration 803, loss = 0.30233844\n",
      "Iteration 804, loss = 0.30228511\n",
      "Iteration 805, loss = 0.30219291\n",
      "Iteration 806, loss = 0.30198657\n",
      "Iteration 807, loss = 0.30195492\n",
      "Iteration 808, loss = 0.30176596\n",
      "Iteration 809, loss = 0.30163306\n",
      "Iteration 810, loss = 0.30151388\n",
      "Iteration 811, loss = 0.30139683\n",
      "Iteration 812, loss = 0.30124348\n",
      "Iteration 813, loss = 0.30116093\n",
      "Iteration 814, loss = 0.30103703\n",
      "Iteration 815, loss = 0.30091383\n",
      "Iteration 816, loss = 0.30081840\n",
      "Iteration 817, loss = 0.30066508\n",
      "Iteration 818, loss = 0.30053730\n",
      "Iteration 819, loss = 0.30044894\n",
      "Iteration 820, loss = 0.30032327\n",
      "Iteration 821, loss = 0.30017985\n",
      "Iteration 822, loss = 0.30008162\n",
      "Iteration 823, loss = 0.29995017\n",
      "Iteration 824, loss = 0.29982070\n",
      "Iteration 825, loss = 0.29972117\n",
      "Iteration 826, loss = 0.29956952\n",
      "Iteration 827, loss = 0.29949389\n",
      "Iteration 176, loss = 0.38298553\n",
      "Iteration 177, loss = 0.38265599\n",
      "Iteration 178, loss = 0.38239685\n",
      "Iteration 179, loss = 0.38207761\n",
      "Iteration 180, loss = 0.38177224\n",
      "Iteration 181, loss = 0.38149568\n",
      "Iteration 182, loss = 0.38120843\n",
      "Iteration 183, loss = 0.38093354\n",
      "Iteration 184, loss = 0.38062867\n",
      "Iteration 185, loss = 0.38037614\n",
      "Iteration 186, loss = 0.38009196\n",
      "Iteration 187, loss = 0.37981850\n",
      "Iteration 188, loss = 0.37953210\n",
      "Iteration 189, loss = 0.37927445\n",
      "Iteration 190, loss = 0.37899726\n",
      "Iteration 191, loss = 0.37876447\n",
      "Iteration 192, loss = 0.37849279\n",
      "Iteration 193, loss = 0.37822101\n",
      "Iteration 194, loss = 0.37795395\n",
      "Iteration 195, loss = 0.37769650\n",
      "Iteration 196, loss = 0.37744293\n",
      "Iteration 197, loss = 0.37719479\n",
      "Iteration 198, loss = 0.37695090\n",
      "Iteration 199, loss = 0.37671395\n",
      "Iteration 200, loss = 0.37644848\n",
      "Iteration 201, loss = 0.37620025\n",
      "Iteration 202, loss = 0.37598207\n",
      "Iteration 203, loss = 0.37572076\n",
      "Iteration 204, loss = 0.37547983\n",
      "Iteration 205, loss = 0.37527227\n",
      "Iteration 206, loss = 0.37501007\n",
      "Iteration 207, loss = 0.37478642\n",
      "Iteration 208, loss = 0.37453682\n",
      "Iteration 209, loss = 0.37431567\n",
      "Iteration 210, loss = 0.37407469\n",
      "Iteration 211, loss = 0.37384105\n",
      "Iteration 212, loss = 0.37361564\n",
      "Iteration 213, loss = 0.37340209\n",
      "Iteration 214, loss = 0.37317425\n",
      "Iteration 215, loss = 0.37295852\n",
      "Iteration 216, loss = 0.37273544\n",
      "Iteration 217, loss = 0.37252531\n",
      "Iteration 218, loss = 0.37228267\n",
      "Iteration 219, loss = 0.37207421\n",
      "Iteration 220, loss = 0.37187877\n",
      "Iteration 221, loss = 0.37164927\n",
      "Iteration 222, loss = 0.37144223\n",
      "Iteration 223, loss = 0.37123269\n",
      "Iteration 224, loss = 0.37103951\n",
      "Iteration 225, loss = 0.37081530\n",
      "Iteration 226, loss = 0.37060176\n",
      "Iteration 227, loss = 0.37041769\n",
      "Iteration 228, loss = 0.37020313\n",
      "Iteration 229, loss = 0.37001012\n",
      "Iteration 230, loss = 0.36979839\n",
      "Iteration 231, loss = 0.36960072\n",
      "Iteration 232, loss = 0.36939856\n",
      "Iteration 233, loss = 0.36921143\n",
      "Iteration 234, loss = 0.36900555\n",
      "Iteration 235, loss = 0.36880987\n",
      "Iteration 236, loss = 0.36862548\n",
      "Iteration 237, loss = 0.36843431\n",
      "Iteration 238, loss = 0.36823507\n",
      "Iteration 239, loss = 0.36804677\n",
      "Iteration 240, loss = 0.36787213\n",
      "Iteration 241, loss = 0.36767556\n",
      "Iteration 242, loss = 0.36748581\n",
      "Iteration 243, loss = 0.36730382\n",
      "Iteration 244, loss = 0.36711088\n",
      "Iteration 245, loss = 0.36693496\n",
      "Iteration 246, loss = 0.36676186\n",
      "Iteration 247, loss = 0.36657056\n",
      "Iteration 248, loss = 0.36640249\n",
      "Iteration 249, loss = 0.36620480\n",
      "Iteration 250, loss = 0.36602828\n",
      "Iteration 251, loss = 0.36585069\n",
      "Iteration 252, loss = 0.36567654\n",
      "Iteration 253, loss = 0.36551683\n",
      "Iteration 254, loss = 0.36533348\n",
      "Iteration 255, loss = 0.36514803\n",
      "Iteration 256, loss = 0.36496868\n",
      "Iteration 257, loss = 0.36481423\n",
      "Iteration 258, loss = 0.36462552\n",
      "Iteration 259, loss = 0.36445997\n",
      "Iteration 260, loss = 0.36428380\n",
      "Iteration 261, loss = 0.36411591\n",
      "Iteration 262, loss = 0.36395813\n",
      "Iteration 263, loss = 0.36378476\n",
      "Iteration 264, loss = 0.36361834\n",
      "Iteration 265, loss = 0.36346938\n",
      "Iteration 266, loss = 0.36328350\n",
      "Iteration 267, loss = 0.36312628\n",
      "Iteration 268, loss = 0.36296688\n",
      "Iteration 269, loss = 0.36282303\n",
      "Iteration 270, loss = 0.36264879\n",
      "Iteration 271, loss = 0.36248975\n",
      "Iteration 272, loss = 0.36232443\n",
      "Iteration 273, loss = 0.36218087\n",
      "Iteration 274, loss = 0.36201424\n",
      "Iteration 275, loss = 0.36185315\n",
      "Iteration 276, loss = 0.36169517\n",
      "Iteration 277, loss = 0.36153517\n",
      "Iteration 278, loss = 0.36139883\n",
      "Iteration 279, loss = 0.36122271\n",
      "Iteration 280, loss = 0.36107991\n",
      "Iteration 281, loss = 0.36093430\n",
      "Iteration 282, loss = 0.36077207\n",
      "Iteration 283, loss = 0.36062809\n",
      "Iteration 284, loss = 0.36047156\n",
      "Iteration 285, loss = 0.36032830\n",
      "Iteration 286, loss = 0.36019655\n",
      "Iteration 287, loss = 0.36003212\n",
      "Iteration 288, loss = 0.35988366\n",
      "Iteration 289, loss = 0.35973870\n",
      "Iteration 290, loss = 0.35960368\n",
      "Iteration 291, loss = 0.35946404\n",
      "Iteration 292, loss = 0.35930299\n",
      "Iteration 293, loss = 0.35915753\n",
      "Iteration 294, loss = 0.35903134\n",
      "Iteration 295, loss = 0.35888805\n",
      "Iteration 296, loss = 0.35874652\n",
      "Iteration 297, loss = 0.35860629\n",
      "Iteration 298, loss = 0.35845520\n",
      "Iteration 299, loss = 0.35832825\n",
      "Iteration 300, loss = 0.35818584\n",
      "Iteration 301, loss = 0.35805252\n",
      "Iteration 302, loss = 0.35790599\n",
      "Iteration 303, loss = 0.35777130\n",
      "Iteration 304, loss = 0.35764707\n",
      "Iteration 305, loss = 0.35750652\n",
      "Iteration 306, loss = 0.35737951\n",
      "Iteration 307, loss = 0.35724099\n",
      "Iteration 308, loss = 0.35710919\n",
      "Iteration 309, loss = 0.35697807\n",
      "Iteration 310, loss = 0.35683601\n",
      "Iteration 311, loss = 0.35670448\n",
      "Iteration 312, loss = 0.35657668\n",
      "Iteration 313, loss = 0.35646339\n",
      "Iteration 314, loss = 0.35632971\n",
      "Iteration 315, loss = 0.35618056\n",
      "Iteration 316, loss = 0.35606113\n",
      "Iteration 317, loss = 0.35592344\n",
      "Iteration 318, loss = 0.35580811\n",
      "Iteration 319, loss = 0.35567508\n",
      "Iteration 320, loss = 0.35556161\n",
      "Iteration 321, loss = 0.35542283\n",
      "Iteration 322, loss = 0.35530665\n",
      "Iteration 323, loss = 0.35518308\n",
      "Iteration 324, loss = 0.35506236\n",
      "Iteration 325, loss = 0.35492660\n",
      "Iteration 326, loss = 0.35479810\n",
      "Iteration 327, loss = 0.35467982\n",
      "Iteration 328, loss = 0.35456579\n",
      "Iteration 329, loss = 0.35443644\n",
      "Iteration 330, loss = 0.35431764\n",
      "Iteration 331, loss = 0.35419727\n",
      "Iteration 332, loss = 0.35407409\n",
      "Iteration 333, loss = 0.35394971\n",
      "Iteration 334, loss = 0.35384908\n",
      "Iteration 335, loss = 0.35372449\n",
      "Iteration 336, loss = 0.35359560\n",
      "Iteration 337, loss = 0.35347952\n",
      "Iteration 338, loss = 0.35336269\n",
      "Iteration 339, loss = 0.35324855\n",
      "Iteration 340, loss = 0.35313380\n",
      "Iteration 341, loss = 0.35301823\n",
      "Iteration 342, loss = 0.35290934\n",
      "Iteration 343, loss = 0.35279858\n",
      "Iteration 344, loss = 0.35268940\n",
      "Iteration 345, loss = 0.35258553\n",
      "Iteration 346, loss = 0.35246373\n",
      "Iteration 347, loss = 0.35235280\n",
      "Iteration 348, loss = 0.35223845\n",
      "Iteration 349, loss = 0.35211727\n",
      "Iteration 350, loss = 0.35200322\n",
      "Iteration 351, loss = 0.35189352\n",
      "Iteration 352, loss = 0.35179201\n",
      "Iteration 353, loss = 0.35169018\n",
      "Iteration 354, loss = 0.35157230\n",
      "Iteration 355, loss = 0.35146396\n",
      "Iteration 356, loss = 0.35136002\n",
      "Iteration 357, loss = 0.35127317\n",
      "Iteration 358, loss = 0.35113948\n",
      "Iteration 359, loss = 0.35103235\n",
      "Iteration 360, loss = 0.35092421\n",
      "Iteration 361, loss = 0.35081928\n",
      "Iteration 362, loss = 0.35070908\n",
      "Iteration 363, loss = 0.35059756\n",
      "Iteration 364, loss = 0.35049068\n",
      "Iteration 365, loss = 0.35038984\n",
      "Iteration 366, loss = 0.35029696\n",
      "Iteration 367, loss = 0.35017841\n",
      "Iteration 368, loss = 0.35007696\n",
      "Iteration 369, loss = 0.34997997\n",
      "Iteration 370, loss = 0.34987015\n",
      "Iteration 371, loss = 0.34976047\n",
      "Iteration 372, loss = 0.34966680\n",
      "Iteration 373, loss = 0.34956512\n",
      "Iteration 374, loss = 0.34947476\n",
      "Iteration 375, loss = 0.34935977\n",
      "Iteration 376, loss = 0.34926643\n",
      "Iteration 377, loss = 0.34916136\n",
      "Iteration 378, loss = 0.34905387\n",
      "Iteration 379, loss = 0.34895334\n",
      "Iteration 380, loss = 0.34887434\n",
      "Iteration 381, loss = 0.34877083\n",
      "Iteration 382, loss = 0.34866930\n",
      "Iteration 383, loss = 0.34856832\n",
      "Iteration 384, loss = 0.34847801\n",
      "Iteration 385, loss = 0.34837409\n",
      "Iteration 386, loss = 0.34827923\n",
      "Iteration 387, loss = 0.34817920\n",
      "Iteration 388, loss = 0.34809164\n",
      "Iteration 389, loss = 0.34799330\n",
      "Iteration 390, loss = 0.34789088\n",
      "Iteration 391, loss = 0.34780661\n",
      "Iteration 392, loss = 0.34770266\n",
      "Iteration 393, loss = 0.34760675\n",
      "Iteration 394, loss = 0.34751810\n",
      "Iteration 395, loss = 0.34742370\n",
      "Iteration 396, loss = 0.34733270\n",
      "Iteration 397, loss = 0.34725089\n",
      "Iteration 398, loss = 0.34714617\n",
      "Iteration 399, loss = 0.34706158\n",
      "Iteration 400, loss = 0.34697323\n",
      "Iteration 401, loss = 0.34687155\n",
      "Iteration 402, loss = 0.34678539\n",
      "Iteration 403, loss = 0.34669131\n",
      "Iteration 404, loss = 0.34660340\n",
      "Iteration 405, loss = 0.34650698\n",
      "Iteration 406, loss = 0.34643947\n",
      "Iteration 407, loss = 0.34634799\n",
      "Iteration 408, loss = 0.34623805\n",
      "Iteration 409, loss = 0.34615098\n",
      "Iteration 410, loss = 0.34606407\n",
      "Iteration 411, loss = 0.34597868\n",
      "Iteration 412, loss = 0.34589506\n",
      "Iteration 413, loss = 0.34580798\n",
      "Iteration 414, loss = 0.34570977\n",
      "Iteration 415, loss = 0.34563441\n",
      "Iteration 416, loss = 0.34554730\n",
      "Iteration 417, loss = 0.34546045\n",
      "Iteration 418, loss = 0.34537277\n",
      "Iteration 419, loss = 0.34527737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.84201544\n",
      "Iteration 2, loss = 0.83707078\n",
      "Iteration 3, loss = 0.82935828\n",
      "Iteration 4, loss = 0.82080883\n",
      "Iteration 5, loss = 0.81103196\n",
      "Iteration 6, loss = 0.80168488\n",
      "Iteration 7, loss = 0.79217335\n",
      "Iteration 8, loss = 0.78291911\n",
      "Iteration 9, loss = 0.77462077\n",
      "Iteration 10, loss = 0.76705417\n",
      "Iteration 11, loss = 0.75942212\n",
      "Iteration 12, loss = 0.75281901\n",
      "Iteration 13, loss = 0.74645719\n",
      "Iteration 14, loss = 0.74080766\n",
      "Iteration 15, loss = 0.73523126\n",
      "Iteration 16, loss = 0.73011949\n",
      "Iteration 17, loss = 0.72541016\n",
      "Iteration 18, loss = 0.72075395\n",
      "Iteration 19, loss = 0.71633014\n",
      "Iteration 20, loss = 0.71235964\n",
      "Iteration 21, loss = 0.70821393\n",
      "Iteration 22, loss = 0.70434063\n",
      "Iteration 23, loss = 0.70060430\n",
      "Iteration 24, loss = 0.69690968\n",
      "Iteration 25, loss = 0.69345677\n",
      "Iteration 26, loss = 0.68980332\n",
      "Iteration 27, loss = 0.68628777\n",
      "Iteration 28, loss = 0.68282712\n",
      "Iteration 29, loss = 0.67943810\n",
      "Iteration 30, loss = 0.67620547\n",
      "Iteration 31, loss = 0.67288557\n",
      "Iteration 32, loss = 0.66945073\n",
      "Iteration 33, loss = 0.66611088\n",
      "Iteration 34, loss = 0.66271256\n",
      "Iteration 35, loss = 0.65959542\n",
      "Iteration 36, loss = 0.65617204\n",
      "Iteration 37, loss = 0.65306791\n",
      "Iteration 38, loss = 0.64965990\n",
      "Iteration 39, loss = 0.64637171\n",
      "Iteration 40, loss = 0.64312521\n",
      "Iteration 41, loss = 0.63991250\n",
      "Iteration 42, loss = 0.63664451\n",
      "Iteration 43, loss = 0.63339006\n",
      "Iteration 44, loss = 0.63021165\n",
      "Iteration 45, loss = 0.62699465\n",
      "Iteration 46, loss = 0.62372134\n",
      "Iteration 47, loss = 0.62052438\n",
      "Iteration 48, loss = 0.61733470\n",
      "Iteration 49, loss = 0.61407257\n",
      "Iteration 50, loss = 0.61094895\n",
      "Iteration 51, loss = 0.60780407\n",
      "Iteration 52, loss = 0.60457232\n",
      "Iteration 53, loss = 0.60144676\n",
      "Iteration 54, loss = 0.59830358\n",
      "Iteration 55, loss = 0.59523048\n",
      "Iteration 56, loss = 0.59219812\n",
      "Iteration 57, loss = 0.58908045\n",
      "Iteration 58, loss = 0.58609869\n",
      "Iteration 59, loss = 0.58304459\n",
      "Iteration 60, loss = 0.58009929\n",
      "Iteration 61, loss = 0.57707624\n",
      "Iteration 62, loss = 0.57431143\n",
      "Iteration 63, loss = 0.57129809\n",
      "Iteration 64, loss = 0.56842829\n",
      "Iteration 65, loss = 0.56566140\n",
      "Iteration 66, loss = 0.56286524\n",
      "Iteration 67, loss = 0.56010550\n",
      "Iteration 68, loss = 0.55729202\n",
      "Iteration 69, loss = 0.55470564\n",
      "Iteration 70, loss = 0.55205956\n",
      "Iteration 71, loss = 0.54942030\n",
      "Iteration 72, loss = 0.54689658\n",
      "Iteration 73, loss = 0.54431863\n",
      "Iteration 74, loss = 0.54183274\n",
      "Iteration 75, loss = 0.53941177\n",
      "Iteration 76, loss = 0.53709056\n",
      "Iteration 77, loss = 0.53464871\n",
      "Iteration 78, loss = 0.53239007\n",
      "Iteration 79, loss = 0.53013741\n",
      "Iteration 80, loss = 0.52791660\n",
      "Iteration 81, loss = 0.52569946\n",
      "Iteration 82, loss = 0.52360619\n",
      "Iteration 83, loss = 0.52151322\n",
      "Iteration 84, loss = 0.51940602\n",
      "Iteration 85, loss = 0.51739242\n",
      "Iteration 86, loss = 0.51546241\n",
      "Iteration 87, loss = 0.51347712\n",
      "Iteration 88, loss = 0.51157263\n",
      "Iteration 89, loss = 0.50979449\n",
      "Iteration 90, loss = 0.50799088\n",
      "Iteration 91, loss = 0.50622329\n",
      "Iteration 92, loss = 0.50446137\n",
      "Iteration 93, loss = 0.50278646\n",
      "Iteration 94, loss = 0.50119489\n",
      "Iteration 95, loss = 0.49954112\n",
      "Iteration 96, loss = 0.49802601\n",
      "Iteration 97, loss = 0.49642276\n",
      "Iteration 98, loss = 0.49495852\n",
      "Iteration 99, loss = 0.49350677\n",
      "Iteration 100, loss = 0.49203224\n",
      "Iteration 101, loss = 0.49072366\n",
      "Iteration 102, loss = 0.48934573\n",
      "Iteration 103, loss = 0.48798777\n",
      "Iteration 104, loss = 0.48667191\n",
      "Iteration 105, loss = 0.48547007\n",
      "Iteration 106, loss = 0.48422924\n",
      "Iteration 107, loss = 0.48301728\n",
      "Iteration 108, loss = 0.48191117\n",
      "Iteration 109, loss = 0.48067274\n",
      "Iteration 110, loss = 0.47962005\n",
      "Iteration 111, loss = 0.47849178\n",
      "Iteration 112, loss = 0.47741984\n",
      "Iteration 113, loss = 0.47637728\n",
      "Iteration 114, loss = 0.47535047\n",
      "Iteration 115, loss = 0.47436528\n",
      "Iteration 116, loss = 0.47332180\n",
      "Iteration 117, loss = 0.47240690\n",
      "Iteration 118, loss = 0.47144487\n",
      "Iteration 119, loss = 0.47053757\n",
      "Iteration 120, loss = 0.46962157\n",
      "Iteration 121, loss = 0.46878386\n",
      "Iteration 122, loss = 0.46790304\n",
      "Iteration 123, loss = 0.46712451\n",
      "Iteration 124, loss = 0.46622387\n",
      "Iteration 125, loss = 0.46545692\n",
      "Iteration 126, loss = 0.46466960\n",
      "Iteration 127, loss = 0.46387989\n",
      "Iteration 128, loss = 0.46312774\n",
      "Iteration 129, loss = 0.46234249\n",
      "Iteration 130, loss = 0.46161307\n",
      "Iteration 131, loss = 0.46089282\n",
      "Iteration 132, loss = 0.46019641\n",
      "Iteration 133, loss = 0.45950389\n",
      "Iteration 134, loss = 0.45879486\n",
      "Iteration 135, loss = 0.45811939\n",
      "Iteration 136, loss = 0.45746670\n",
      "Iteration 137, loss = 0.45684453\n",
      "Iteration 138, loss = 0.45612653\n",
      "Iteration 139, loss = 0.45552597\n",
      "Iteration 140, loss = 0.45493713\n",
      "Iteration 141, loss = 0.45427828\n",
      "Iteration 142, loss = 0.45368263\n",
      "Iteration 143, loss = 0.45313672\n",
      "Iteration 144, loss = 0.45250010\n",
      "Iteration 145, loss = 0.45192384\n",
      "Iteration 146, loss = 0.45134906\n",
      "Iteration 147, loss = 0.45084067\n",
      "Iteration 148, loss = 0.45023935\n",
      "Iteration 149, loss = 0.44969538\n",
      "Iteration 150, loss = 0.44915232\n",
      "Iteration 151, loss = 0.44860625\n",
      "Iteration 152, loss = 0.44807185\n",
      "Iteration 153, loss = 0.44754660\n",
      "Iteration 154, loss = 0.44701433\n",
      "Iteration 155, loss = 0.44652026\n",
      "Iteration 156, loss = 0.44600302\n",
      "Iteration 157, loss = 0.44555139\n",
      "Iteration 158, loss = 0.44505054\n",
      "Iteration 159, loss = 0.44451468\n",
      "Iteration 160, loss = 0.44407136\n",
      "Iteration 161, loss = 0.44356255\n",
      "Iteration 162, loss = 0.44306974\n",
      "Iteration 163, loss = 0.44263423\n",
      "Iteration 164, loss = 0.44213968\n",
      "Iteration 165, loss = 0.44167075\n",
      "Iteration 166, loss = 0.44120698\n",
      "Iteration 167, loss = 0.44076061\n",
      "Iteration 168, loss = 0.44033007\n",
      "Iteration 169, loss = 0.43988601\n",
      "Iteration 170, loss = 0.43946449\n",
      "Iteration 171, loss = 0.43900134\n",
      "Iteration 172, loss = 0.43860877\n",
      "Iteration 173, loss = 0.43814776\n",
      "Iteration 174, loss = 0.43771825\n",
      "Iteration 175, loss = 0.43732415\n",
      "Iteration 176, loss = 0.43690911\n",
      "Iteration 177, loss = 0.43648756\n",
      "Iteration 178, loss = 0.43606584\n",
      "Iteration 179, loss = 0.43567084\n",
      "Iteration 180, loss = 0.43527988\n",
      "Iteration 181, loss = 0.43490143\n",
      "Iteration 182, loss = 0.43445449\n",
      "Iteration 183, loss = 0.43408294\n",
      "Iteration 184, loss = 0.43369842\n",
      "Iteration 185, loss = 0.43332434\n",
      "Iteration 186, loss = 0.43291354\n",
      "Iteration 187, loss = 0.43253473\n",
      "Iteration 188, loss = 0.43217426\n",
      "Iteration 189, loss = 0.43177531\n",
      "Iteration 190, loss = 0.43141285\n",
      "Iteration 191, loss = 0.43105752\n",
      "Iteration 192, loss = 0.43072317\n",
      "Iteration 193, loss = 0.43030924\n",
      "Iteration 194, loss = 0.42993824\n",
      "Iteration 195, loss = 0.42960463\n",
      "Iteration 196, loss = 0.42925075\n",
      "Iteration 197, loss = 0.42887823\n",
      "Iteration 198, loss = 0.42851780\n",
      "Iteration 199, loss = 0.42815589\n",
      "Iteration 200, loss = 0.42782950\n",
      "Iteration 201, loss = 0.42745660\n",
      "Iteration 202, loss = 0.42714507\n",
      "Iteration 203, loss = 0.42680254\n",
      "Iteration 204, loss = 0.42645994\n",
      "Iteration 205, loss = 0.42616882\n",
      "Iteration 206, loss = 0.42576632\n",
      "Iteration 207, loss = 0.42542290\n",
      "Iteration 208, loss = 0.42513704\n",
      "Iteration 209, loss = 0.42481513\n",
      "Iteration 210, loss = 0.42446307\n",
      "Iteration 211, loss = 0.42412365\n",
      "Iteration 212, loss = 0.42388641\n",
      "Iteration 213, loss = 0.42351045\n",
      "Iteration 214, loss = 0.42321467\n",
      "Iteration 215, loss = 0.42287206\n",
      "Iteration 216, loss = 0.42256647\n",
      "Iteration 217, loss = 0.42225901\n",
      "Iteration 218, loss = 0.42193242\n",
      "Iteration 219, loss = 0.42162106\n",
      "Iteration 220, loss = 0.42132146\n",
      "Iteration 221, loss = 0.42102386\n",
      "Iteration 222, loss = 0.42070329\n",
      "Iteration 223, loss = 0.42041236\n",
      "Iteration 224, loss = 0.42012351\n",
      "Iteration 225, loss = 0.41982440\n",
      "Iteration 226, loss = 0.41951815\n",
      "Iteration 227, loss = 0.41927022\n",
      "Iteration 228, loss = 0.41894511\n",
      "Iteration 229, loss = 0.41864093\n",
      "Iteration 230, loss = 0.41837153\n",
      "Iteration 231, loss = 0.41806478\n",
      "Iteration 232, loss = 0.41779607\n",
      "Iteration 233, loss = 0.41751175\n",
      "Iteration 234, loss = 0.41723460\n",
      "Iteration 235, loss = 0.41694383\n",
      "Iteration 236, loss = 0.41665723\n",
      "Iteration 237, loss = 0.41637686\n",
      "Iteration 238, loss = 0.41612026\n",
      "Iteration 239, loss = 0.41581494\n",
      "Iteration 240, loss = 0.41556439\n",
      "Iteration 241, loss = 0.41529924\n",
      "Iteration 242, loss = 0.41503294\n",
      "Iteration 243, loss = 0.41474437\n",
      "Iteration 244, loss = 0.41453524\n",
      "Iteration 245, loss = 0.41427368\n",
      "Iteration 246, loss = 0.41394775\n",
      "Iteration 247, loss = 0.41368912\n",
      "Iteration 248, loss = 0.41341977\n",
      "Iteration 249, loss = 0.41314607\n",
      "Iteration 250, loss = 0.41292235\n",
      "Iteration 251, loss = 0.41264998\n",
      "Iteration 252, loss = 0.41239385\n",
      "Iteration 290, loss = 0.38222281\n",
      "Iteration 291, loss = 0.38204813\n",
      "Iteration 292, loss = 0.38185185\n",
      "Iteration 293, loss = 0.38166142\n",
      "Iteration 294, loss = 0.38147361\n",
      "Iteration 295, loss = 0.38131262\n",
      "Iteration 296, loss = 0.38110806\n",
      "Iteration 297, loss = 0.38094745\n",
      "Iteration 298, loss = 0.38076770\n",
      "Iteration 299, loss = 0.38057900\n",
      "Iteration 300, loss = 0.38040244\n",
      "Iteration 301, loss = 0.38023139\n",
      "Iteration 302, loss = 0.38003271\n",
      "Iteration 303, loss = 0.37988199\n",
      "Iteration 304, loss = 0.37969783\n",
      "Iteration 305, loss = 0.37948466\n",
      "Iteration 306, loss = 0.37931855\n",
      "Iteration 307, loss = 0.37915274\n",
      "Iteration 308, loss = 0.37895269\n",
      "Iteration 309, loss = 0.37879398\n",
      "Iteration 310, loss = 0.37861222\n",
      "Iteration 311, loss = 0.37845232\n",
      "Iteration 312, loss = 0.37825574\n",
      "Iteration 313, loss = 0.37809220\n",
      "Iteration 314, loss = 0.37793621\n",
      "Iteration 315, loss = 0.37775990\n",
      "Iteration 316, loss = 0.37759454\n",
      "Iteration 317, loss = 0.37741592\n",
      "Iteration 318, loss = 0.37724180\n",
      "Iteration 319, loss = 0.37706785\n",
      "Iteration 320, loss = 0.37691133\n",
      "Iteration 321, loss = 0.37673116\n",
      "Iteration 322, loss = 0.37657846\n",
      "Iteration 323, loss = 0.37640228\n",
      "Iteration 324, loss = 0.37626962\n",
      "Iteration 325, loss = 0.37608005\n",
      "Iteration 326, loss = 0.37591106\n",
      "Iteration 327, loss = 0.37574997\n",
      "Iteration 328, loss = 0.37559534\n",
      "Iteration 329, loss = 0.37543225\n",
      "Iteration 330, loss = 0.37525787\n",
      "Iteration 331, loss = 0.37509615\n",
      "Iteration 332, loss = 0.37495179\n",
      "Iteration 333, loss = 0.37478179\n",
      "Iteration 334, loss = 0.37461527\n",
      "Iteration 335, loss = 0.37447490\n",
      "Iteration 336, loss = 0.37429812\n",
      "Iteration 337, loss = 0.37415212\n",
      "Iteration 338, loss = 0.37398720\n",
      "Iteration 339, loss = 0.37386010\n",
      "Iteration 340, loss = 0.37368338\n",
      "Iteration 341, loss = 0.37351154\n",
      "Iteration 342, loss = 0.37338007\n",
      "Iteration 343, loss = 0.37320199\n",
      "Iteration 344, loss = 0.37305279\n",
      "Iteration 345, loss = 0.37288022\n",
      "Iteration 346, loss = 0.37278453\n",
      "Iteration 347, loss = 0.37262920\n",
      "Iteration 348, loss = 0.37243818\n",
      "Iteration 349, loss = 0.37230988\n",
      "Iteration 350, loss = 0.37213344\n",
      "Iteration 351, loss = 0.37196936\n",
      "Iteration 352, loss = 0.37187412\n",
      "Iteration 353, loss = 0.37167245\n",
      "Iteration 354, loss = 0.37151027\n",
      "Iteration 355, loss = 0.37137690\n",
      "Iteration 356, loss = 0.37121494\n",
      "Iteration 357, loss = 0.37106852\n",
      "Iteration 358, loss = 0.37091141\n",
      "Iteration 359, loss = 0.37076204\n",
      "Iteration 360, loss = 0.37061704\n",
      "Iteration 361, loss = 0.37047499\n",
      "Iteration 362, loss = 0.37032997\n",
      "Iteration 363, loss = 0.37016534\n",
      "Iteration 364, loss = 0.37002455\n",
      "Iteration 365, loss = 0.36990093\n",
      "Iteration 366, loss = 0.36974787\n",
      "Iteration 367, loss = 0.36961022\n",
      "Iteration 368, loss = 0.36944721\n",
      "Iteration 369, loss = 0.36929127\n",
      "Iteration 370, loss = 0.36918131\n",
      "Iteration 371, loss = 0.36900718\n",
      "Iteration 372, loss = 0.36887528\n",
      "Iteration 373, loss = 0.36871321\n",
      "Iteration 374, loss = 0.36855985\n",
      "Iteration 375, loss = 0.36842866\n",
      "Iteration 376, loss = 0.36827101\n",
      "Iteration 377, loss = 0.36812042\n",
      "Iteration 378, loss = 0.36798898\n",
      "Iteration 379, loss = 0.36784185\n",
      "Iteration 380, loss = 0.36769801\n",
      "Iteration 381, loss = 0.36754987\n",
      "Iteration 382, loss = 0.36739401\n",
      "Iteration 383, loss = 0.36725791\n",
      "Iteration 384, loss = 0.36713350\n",
      "Iteration 385, loss = 0.36696667\n",
      "Iteration 386, loss = 0.36686061\n",
      "Iteration 387, loss = 0.36669625\n",
      "Iteration 388, loss = 0.36653569\n",
      "Iteration 389, loss = 0.36639233\n",
      "Iteration 390, loss = 0.36625957\n",
      "Iteration 391, loss = 0.36610369\n",
      "Iteration 392, loss = 0.36598317\n",
      "Iteration 393, loss = 0.36581320\n",
      "Iteration 394, loss = 0.36568637\n",
      "Iteration 395, loss = 0.36552851\n",
      "Iteration 396, loss = 0.36538801\n",
      "Iteration 397, loss = 0.36524882\n",
      "Iteration 398, loss = 0.36510248\n",
      "Iteration 399, loss = 0.36496771\n",
      "Iteration 400, loss = 0.36481493\n",
      "Iteration 401, loss = 0.36468120\n",
      "Iteration 402, loss = 0.36454705\n",
      "Iteration 403, loss = 0.36442101\n",
      "Iteration 404, loss = 0.36427171\n",
      "Iteration 405, loss = 0.36416994\n",
      "Iteration 406, loss = 0.36396270\n",
      "Iteration 407, loss = 0.36385065\n",
      "Iteration 408, loss = 0.36370033\n",
      "Iteration 409, loss = 0.36355932\n",
      "Iteration 410, loss = 0.36341436\n",
      "Iteration 411, loss = 0.36328156\n",
      "Iteration 412, loss = 0.36312618\n",
      "Iteration 413, loss = 0.36300481\n",
      "Iteration 414, loss = 0.36285382\n",
      "Iteration 415, loss = 0.36272121\n",
      "Iteration 416, loss = 0.36258452\n",
      "Iteration 417, loss = 0.36247666\n",
      "Iteration 418, loss = 0.36232236\n",
      "Iteration 419, loss = 0.36217643\n",
      "Iteration 420, loss = 0.36204980\n",
      "Iteration 421, loss = 0.36190551\n",
      "Iteration 422, loss = 0.36175537\n",
      "Iteration 423, loss = 0.36161119\n",
      "Iteration 424, loss = 0.36148411\n",
      "Iteration 425, loss = 0.36134113\n",
      "Iteration 426, loss = 0.36122017\n",
      "Iteration 427, loss = 0.36108297\n",
      "Iteration 428, loss = 0.36093870\n",
      "Iteration 429, loss = 0.36080860\n",
      "Iteration 430, loss = 0.36068140\n",
      "Iteration 431, loss = 0.36052165\n",
      "Iteration 432, loss = 0.36041922\n",
      "Iteration 433, loss = 0.36026510\n",
      "Iteration 434, loss = 0.36013548\n",
      "Iteration 435, loss = 0.35998610\n",
      "Iteration 436, loss = 0.35987409\n",
      "Iteration 437, loss = 0.35971607\n",
      "Iteration 438, loss = 0.35958061\n",
      "Iteration 439, loss = 0.35944645\n",
      "Iteration 440, loss = 0.35930956\n",
      "Iteration 441, loss = 0.35917145\n",
      "Iteration 442, loss = 0.35905260\n",
      "Iteration 443, loss = 0.35894597\n",
      "Iteration 444, loss = 0.35878208\n",
      "Iteration 445, loss = 0.35870005\n",
      "Iteration 446, loss = 0.35851187\n",
      "Iteration 447, loss = 0.35839322\n",
      "Iteration 448, loss = 0.35822446\n",
      "Iteration 449, loss = 0.35811113\n",
      "Iteration 450, loss = 0.35795547\n",
      "Iteration 451, loss = 0.35785508\n",
      "Iteration 452, loss = 0.35769523\n",
      "Iteration 453, loss = 0.35759303\n",
      "Iteration 454, loss = 0.35744382\n",
      "Iteration 455, loss = 0.35732229\n",
      "Iteration 456, loss = 0.35718195\n",
      "Iteration 457, loss = 0.35704126\n",
      "Iteration 458, loss = 0.35692426\n",
      "Iteration 459, loss = 0.35679518\n",
      "Iteration 460, loss = 0.35664673\n",
      "Iteration 461, loss = 0.35654239\n",
      "Iteration 462, loss = 0.35639668\n",
      "Iteration 463, loss = 0.35629512\n",
      "Iteration 464, loss = 0.35614091\n",
      "Iteration 465, loss = 0.35601213\n",
      "Iteration 466, loss = 0.35587354\n",
      "Iteration 467, loss = 0.35574282\n",
      "Iteration 468, loss = 0.35569414\n",
      "Iteration 469, loss = 0.35550391\n",
      "Iteration 470, loss = 0.35533223\n",
      "Iteration 471, loss = 0.35524281\n",
      "Iteration 472, loss = 0.35507480\n",
      "Iteration 473, loss = 0.35496940\n",
      "Iteration 474, loss = 0.35482323\n",
      "Iteration 475, loss = 0.35469253\n",
      "Iteration 476, loss = 0.35455415\n",
      "Iteration 477, loss = 0.35452380\n",
      "Iteration 478, loss = 0.35431970\n",
      "Iteration 479, loss = 0.35415764\n",
      "Iteration 480, loss = 0.35405384\n",
      "Iteration 481, loss = 0.35391543\n",
      "Iteration 482, loss = 0.35377133\n",
      "Iteration 483, loss = 0.35366243\n",
      "Iteration 484, loss = 0.35352703\n",
      "Iteration 485, loss = 0.35339172\n",
      "Iteration 486, loss = 0.35328716\n",
      "Iteration 487, loss = 0.35313972\n",
      "Iteration 488, loss = 0.35299935\n",
      "Iteration 489, loss = 0.35287141\n",
      "Iteration 490, loss = 0.35280404\n",
      "Iteration 491, loss = 0.35259808\n",
      "Iteration 492, loss = 0.35254383\n",
      "Iteration 493, loss = 0.35240330\n",
      "Iteration 494, loss = 0.35222999\n",
      "Iteration 495, loss = 0.35208360\n",
      "Iteration 496, loss = 0.35198638\n",
      "Iteration 497, loss = 0.35184760\n",
      "Iteration 498, loss = 0.35172826\n",
      "Iteration 499, loss = 0.35158451\n",
      "Iteration 500, loss = 0.35144532\n",
      "Iteration 501, loss = 0.35132220\n",
      "Iteration 502, loss = 0.35122103\n",
      "Iteration 503, loss = 0.35110524\n",
      "Iteration 504, loss = 0.35097222\n",
      "Iteration 505, loss = 0.35084518\n",
      "Iteration 506, loss = 0.35069354\n",
      "Iteration 507, loss = 0.35056487\n",
      "Iteration 508, loss = 0.35046329\n",
      "Iteration 509, loss = 0.35030028\n",
      "Iteration 510, loss = 0.35020723\n",
      "Iteration 511, loss = 0.35006676\n",
      "Iteration 512, loss = 0.34995286\n",
      "Iteration 513, loss = 0.34979401\n",
      "Iteration 514, loss = 0.34966757\n",
      "Iteration 515, loss = 0.34954183\n",
      "Iteration 516, loss = 0.34942722\n",
      "Iteration 517, loss = 0.34930296\n",
      "Iteration 518, loss = 0.34916272\n",
      "Iteration 519, loss = 0.34901027\n",
      "Iteration 520, loss = 0.34894686\n",
      "Iteration 521, loss = 0.34878485\n",
      "Iteration 522, loss = 0.34871889\n",
      "Iteration 523, loss = 0.34852189\n",
      "Iteration 524, loss = 0.34839879\n",
      "Iteration 525, loss = 0.34824880\n",
      "Iteration 526, loss = 0.34814185\n",
      "Iteration 527, loss = 0.34801907\n",
      "Iteration 528, loss = 0.34792605\n",
      "Iteration 529, loss = 0.34775250\n",
      "Iteration 530, loss = 0.34763103\n",
      "Iteration 531, loss = 0.34753620\n",
      "Iteration 532, loss = 0.34740298\n",
      "Iteration 533, loss = 0.34723671\n",
      "Iteration 534, loss = 0.34709945\n",
      "Iteration 535, loss = 0.34700587\n",
      "Iteration 536, loss = 0.34688117\n",
      "Iteration 537, loss = 0.34673136\n",
      "Iteration 538, loss = 0.34660057\n",
      "Iteration 539, loss = 0.34656012\n",
      "Iteration 540, loss = 0.34636254\n",
      "Iteration 541, loss = 0.34622871\n",
      "Iteration 542, loss = 0.34609744\n",
      "Iteration 543, loss = 0.34602287\n",
      "Iteration 544, loss = 0.34583723\n",
      "Iteration 545, loss = 0.34570566\n",
      "Iteration 546, loss = 0.34558611\n",
      "Iteration 547, loss = 0.34546482\n",
      "Iteration 548, loss = 0.34534653\n",
      "Iteration 549, loss = 0.34519975\n",
      "Iteration 550, loss = 0.34508409\n",
      "Iteration 551, loss = 0.34493101\n",
      "Iteration 552, loss = 0.34482406\n",
      "Iteration 553, loss = 0.34469244\n",
      "Iteration 554, loss = 0.34456871\n",
      "Iteration 555, loss = 0.34441822\n",
      "Iteration 556, loss = 0.34435287\n",
      "Iteration 557, loss = 0.34421239\n",
      "Iteration 558, loss = 0.34404184\n",
      "Iteration 559, loss = 0.34393923\n",
      "Iteration 560, loss = 0.34378896\n",
      "Iteration 561, loss = 0.34364593\n",
      "Iteration 562, loss = 0.34353541\n",
      "Iteration 563, loss = 0.34341401\n",
      "Iteration 564, loss = 0.34327340\n",
      "Iteration 565, loss = 0.34315353\n",
      "Iteration 566, loss = 0.34306053\n",
      "Iteration 567, loss = 0.34296007\n",
      "Iteration 568, loss = 0.34279608\n",
      "Iteration 569, loss = 0.34264731\n",
      "Iteration 570, loss = 0.34252576\n",
      "Iteration 571, loss = 0.34237910\n",
      "Iteration 572, loss = 0.34226460\n",
      "Iteration 573, loss = 0.34217930\n",
      "Iteration 574, loss = 0.34203732\n",
      "Iteration 575, loss = 0.34187428\n",
      "Iteration 576, loss = 0.34177549\n",
      "Iteration 577, loss = 0.34163363\n",
      "Iteration 578, loss = 0.34151398\n",
      "Iteration 579, loss = 0.34139373\n",
      "Iteration 580, loss = 0.34141996\n",
      "Iteration 581, loss = 0.34115172\n",
      "Iteration 582, loss = 0.34101122\n",
      "Iteration 583, loss = 0.34089488\n",
      "Iteration 584, loss = 0.34078825\n",
      "Iteration 585, loss = 0.34063044\n",
      "Iteration 586, loss = 0.34051191\n",
      "Iteration 587, loss = 0.34040669\n",
      "Iteration 588, loss = 0.34027537\n",
      "Iteration 589, loss = 0.34016092\n",
      "Iteration 590, loss = 0.34006291\n",
      "Iteration 591, loss = 0.33991128\n",
      "Iteration 592, loss = 0.33980325\n",
      "Iteration 593, loss = 0.33964029\n",
      "Iteration 594, loss = 0.33954767\n",
      "Iteration 595, loss = 0.33939166\n",
      "Iteration 596, loss = 0.33928888\n",
      "Iteration 597, loss = 0.33917490\n",
      "Iteration 598, loss = 0.33905211\n",
      "Iteration 599, loss = 0.33889477\n",
      "Iteration 600, loss = 0.33878889\n",
      "Iteration 601, loss = 0.33866482\n",
      "Iteration 602, loss = 0.33853539\n",
      "Iteration 603, loss = 0.33841227\n",
      "Iteration 604, loss = 0.33830478\n",
      "Iteration 605, loss = 0.33818618\n",
      "Iteration 606, loss = 0.33803903\n",
      "Iteration 607, loss = 0.33792321\n",
      "Iteration 608, loss = 0.33778200\n",
      "Iteration 609, loss = 0.33767141\n",
      "Iteration 610, loss = 0.33753587\n",
      "Iteration 611, loss = 0.33742009\n",
      "Iteration 612, loss = 0.33731269\n",
      "Iteration 613, loss = 0.33716513\n",
      "Iteration 614, loss = 0.33704957\n",
      "Iteration 615, loss = 0.33697442\n",
      "Iteration 616, loss = 0.33682461\n",
      "Iteration 617, loss = 0.33670024\n",
      "Iteration 618, loss = 0.33661062\n",
      "Iteration 619, loss = 0.33645574\n",
      "Iteration 620, loss = 0.33630871\n",
      "Iteration 621, loss = 0.33619414\n",
      "Iteration 622, loss = 0.33604481\n",
      "Iteration 623, loss = 0.33592852\n",
      "Iteration 624, loss = 0.33582547\n",
      "Iteration 625, loss = 0.33572758\n",
      "Iteration 626, loss = 0.33558227\n",
      "Iteration 627, loss = 0.33546169\n",
      "Iteration 628, loss = 0.33530902\n",
      "Iteration 629, loss = 0.33520155\n",
      "Iteration 630, loss = 0.33507130\n",
      "Iteration 631, loss = 0.33499167\n",
      "Iteration 632, loss = 0.33487104\n",
      "Iteration 633, loss = 0.33469143\n",
      "Iteration 634, loss = 0.33462082\n",
      "Iteration 635, loss = 0.33447907\n",
      "Iteration 636, loss = 0.33435064\n",
      "Iteration 637, loss = 0.33422490\n",
      "Iteration 638, loss = 0.33412150\n",
      "Iteration 639, loss = 0.33396643\n",
      "Iteration 640, loss = 0.33388830\n",
      "Iteration 641, loss = 0.33373333\n",
      "Iteration 642, loss = 0.33360432\n",
      "Iteration 643, loss = 0.33351535\n",
      "Iteration 644, loss = 0.33335910\n",
      "Iteration 645, loss = 0.33322177\n",
      "Iteration 646, loss = 0.33311186\n",
      "Iteration 647, loss = 0.33298072\n",
      "Iteration 648, loss = 0.33286006\n",
      "Iteration 649, loss = 0.33273599\n",
      "Iteration 650, loss = 0.33265788\n",
      "Iteration 651, loss = 0.33254145\n",
      "Iteration 652, loss = 0.33236810\n",
      "Iteration 653, loss = 0.33228324\n",
      "Iteration 654, loss = 0.33213996\n",
      "Iteration 655, loss = 0.33199562\n",
      "Iteration 656, loss = 0.33186723\n",
      "Iteration 657, loss = 0.33189224\n",
      "Iteration 658, loss = 0.33162781\n",
      "Iteration 659, loss = 0.33149369\n",
      "Iteration 660, loss = 0.33142118\n",
      "Iteration 661, loss = 0.33126577\n",
      "Iteration 662, loss = 0.33111879\n",
      "Iteration 663, loss = 0.33101662\n",
      "Iteration 664, loss = 0.33091002\n",
      "Iteration 665, loss = 0.33076689\n",
      "Iteration 666, loss = 0.33068249\n",
      "Iteration 667, loss = 0.33051398\n",
      "Iteration 668, loss = 0.33041952\n",
      "Iteration 669, loss = 0.33026603\n",
      "Iteration 670, loss = 0.33012117\n",
      "Iteration 671, loss = 0.33003779\n",
      "Iteration 672, loss = 0.32996461\n",
      "Iteration 673, loss = 0.32980148\n",
      "Iteration 674, loss = 0.32963577\n",
      "Iteration 675, loss = 0.32951935\n",
      "Iteration 676, loss = 0.32945319\n",
      "Iteration 677, loss = 0.32927120\n",
      "Iteration 678, loss = 0.32919272\n",
      "Iteration 679, loss = 0.32901921\n",
      "Iteration 680, loss = 0.32893316\n",
      "Iteration 681, loss = 0.32876663\n",
      "Iteration 682, loss = 0.32865191\n",
      "Iteration 683, loss = 0.32851392\n",
      "Iteration 684, loss = 0.32839444\n",
      "Iteration 685, loss = 0.32832687\n",
      "Iteration 686, loss = 0.32816061\n",
      "Iteration 687, loss = 0.32804391\n",
      "Iteration 688, loss = 0.32790206\n",
      "Iteration 689, loss = 0.32778282\n",
      "Iteration 690, loss = 0.32767142\n",
      "Iteration 691, loss = 0.32751774\n",
      "Iteration 692, loss = 0.32742228\n",
      "Iteration 693, loss = 0.32727443\n",
      "Iteration 694, loss = 0.32715419\n",
      "Iteration 695, loss = 0.32706028\n",
      "Iteration 696, loss = 0.32694185\n",
      "Iteration 697, loss = 0.32680720\n",
      "Iteration 698, loss = 0.32668718\n",
      "Iteration 699, loss = 0.32653841\n",
      "Iteration 700, loss = 0.32643518\n",
      "Iteration 701, loss = 0.32631637\n",
      "Iteration 702, loss = 0.32618080\n",
      "Iteration 703, loss = 0.32605823\n",
      "Iteration 704, loss = 0.32595881\n",
      "Iteration 705, loss = 0.32581511\n",
      "Iteration 706, loss = 0.32570143\n",
      "Iteration 707, loss = 0.32558060\n",
      "Iteration 708, loss = 0.32543765\n",
      "Iteration 709, loss = 0.32536536\n",
      "Iteration 710, loss = 0.32524007\n",
      "Iteration 711, loss = 0.32507028\n",
      "Iteration 712, loss = 0.32498195\n",
      "Iteration 713, loss = 0.32486803\n",
      "Iteration 714, loss = 0.32470364\n",
      "Iteration 715, loss = 0.32460889\n",
      "Iteration 716, loss = 0.32450406\n",
      "Iteration 717, loss = 0.32441262\n",
      "Iteration 718, loss = 0.32421974\n",
      "Iteration 719, loss = 0.32413039\n",
      "Iteration 720, loss = 0.32397691\n",
      "Iteration 721, loss = 0.32387208\n",
      "Iteration 722, loss = 0.32374936\n",
      "Iteration 723, loss = 0.32364349\n",
      "Iteration 724, loss = 0.32350734\n",
      "Iteration 725, loss = 0.32339966\n",
      "Iteration 726, loss = 0.32327403\n",
      "Iteration 727, loss = 0.32315747\n",
      "Iteration 728, loss = 0.32304727\n",
      "Iteration 729, loss = 0.32288664\n",
      "Iteration 730, loss = 0.32276761\n",
      "Iteration 731, loss = 0.32266762\n",
      "Iteration 732, loss = 0.32255727\n",
      "Iteration 733, loss = 0.32242115\n",
      "Iteration 734, loss = 0.32228042\n",
      "Iteration 735, loss = 0.32219614\n",
      "Iteration 736, loss = 0.32205668\n",
      "Iteration 737, loss = 0.32199979\n",
      "Iteration 738, loss = 0.32182101\n",
      "Iteration 739, loss = 0.32169646\n",
      "Iteration 740, loss = 0.32158219\n",
      "Iteration 741, loss = 0.32146477\n",
      "Iteration 742, loss = 0.32136739\n",
      "Iteration 743, loss = 0.32122840\n",
      "Iteration 744, loss = 0.32108646\n",
      "Iteration 745, loss = 0.32098944\n",
      "Iteration 746, loss = 0.32085312\n",
      "Iteration 747, loss = 0.32073273\n",
      "Iteration 748, loss = 0.32062512\n",
      "Iteration 749, loss = 0.32051643\n",
      "Iteration 750, loss = 0.32039246\n",
      "Iteration 751, loss = 0.32025571\n",
      "Iteration 752, loss = 0.32015460\n",
      "Iteration 753, loss = 0.32002672\n",
      "Iteration 754, loss = 0.31989486\n",
      "Iteration 755, loss = 0.31976890\n",
      "Iteration 756, loss = 0.31967730\n",
      "Iteration 757, loss = 0.31954672\n",
      "Iteration 758, loss = 0.31940165\n",
      "Iteration 759, loss = 0.31929067\n",
      "Iteration 760, loss = 0.31918713\n",
      "Iteration 761, loss = 0.31907049\n",
      "Iteration 762, loss = 0.31896467\n",
      "Iteration 763, loss = 0.31882687\n",
      "Iteration 764, loss = 0.31882801\n",
      "Iteration 765, loss = 0.31857867\n",
      "Iteration 766, loss = 0.31846214\n",
      "Iteration 767, loss = 0.31836189\n",
      "Iteration 768, loss = 0.31822630\n",
      "Iteration 769, loss = 0.31817930\n",
      "Iteration 770, loss = 0.31800815\n",
      "Iteration 771, loss = 0.31786998\n",
      "Iteration 772, loss = 0.31776992\n",
      "Iteration 773, loss = 0.31765034\n",
      "Iteration 774, loss = 0.31751281\n",
      "Iteration 775, loss = 0.31753392\n",
      "Iteration 776, loss = 0.31729786\n",
      "Iteration 777, loss = 0.31719833\n",
      "Iteration 778, loss = 0.31720328\n",
      "Iteration 779, loss = 0.31694145\n",
      "Iteration 780, loss = 0.31688248\n",
      "Iteration 781, loss = 0.31673221\n",
      "Iteration 782, loss = 0.31663859\n",
      "Iteration 783, loss = 0.31648290\n",
      "Iteration 784, loss = 0.31633968\n",
      "Iteration 785, loss = 0.31624464\n",
      "Iteration 772, loss = 0.30415868\n",
      "Iteration 773, loss = 0.30396579\n",
      "Iteration 774, loss = 0.30384821\n",
      "Iteration 775, loss = 0.30367109\n",
      "Iteration 776, loss = 0.30354710\n",
      "Iteration 777, loss = 0.30338575\n",
      "Iteration 778, loss = 0.30324425\n",
      "Iteration 779, loss = 0.30311130\n",
      "Iteration 780, loss = 0.30306918\n",
      "Iteration 781, loss = 0.30286031\n",
      "Iteration 782, loss = 0.30275208\n",
      "Iteration 783, loss = 0.30254353\n",
      "Iteration 784, loss = 0.30240295\n",
      "Iteration 785, loss = 0.30230290\n",
      "Iteration 786, loss = 0.30216353\n",
      "Iteration 787, loss = 0.30199600\n",
      "Iteration 788, loss = 0.30182871\n",
      "Iteration 789, loss = 0.30169334\n",
      "Iteration 790, loss = 0.30150763\n",
      "Iteration 791, loss = 0.30138419\n",
      "Iteration 792, loss = 0.30124098\n",
      "Iteration 793, loss = 0.30108927\n",
      "Iteration 794, loss = 0.30092807\n",
      "Iteration 795, loss = 0.30081123\n",
      "Iteration 796, loss = 0.30065073\n",
      "Iteration 797, loss = 0.30057201\n",
      "Iteration 798, loss = 0.30035124\n",
      "Iteration 799, loss = 0.30026339\n",
      "Iteration 800, loss = 0.30005126\n",
      "Iteration 801, loss = 0.29991778\n",
      "Iteration 802, loss = 0.29982435\n",
      "Iteration 803, loss = 0.29962038\n",
      "Iteration 804, loss = 0.29947744\n",
      "Iteration 805, loss = 0.29935144\n",
      "Iteration 806, loss = 0.29920936\n",
      "Iteration 807, loss = 0.29903694\n",
      "Iteration 808, loss = 0.29885278\n",
      "Iteration 809, loss = 0.29874247\n",
      "Iteration 810, loss = 0.29863717\n",
      "Iteration 811, loss = 0.29858297\n",
      "Iteration 812, loss = 0.29826266\n",
      "Iteration 813, loss = 0.29812888\n",
      "Iteration 814, loss = 0.29796864\n",
      "Iteration 815, loss = 0.29782087\n",
      "Iteration 816, loss = 0.29770322\n",
      "Iteration 817, loss = 0.29747802\n",
      "Iteration 818, loss = 0.29736429\n",
      "Iteration 819, loss = 0.29724236\n",
      "Iteration 820, loss = 0.29704404\n",
      "Iteration 821, loss = 0.29702502\n",
      "Iteration 822, loss = 0.29676344\n",
      "Iteration 823, loss = 0.29669747\n",
      "Iteration 824, loss = 0.29651629\n",
      "Iteration 825, loss = 0.29635553\n",
      "Iteration 826, loss = 0.29623923\n",
      "Iteration 827, loss = 0.29608338\n",
      "Iteration 828, loss = 0.29590238\n",
      "Iteration 829, loss = 0.29572678\n",
      "Iteration 830, loss = 0.29563726\n",
      "Iteration 831, loss = 0.29547405\n",
      "Iteration 832, loss = 0.29530549\n",
      "Iteration 833, loss = 0.29513714\n",
      "Iteration 834, loss = 0.29499537\n",
      "Iteration 835, loss = 0.29478559\n",
      "Iteration 836, loss = 0.29465946\n",
      "Iteration 837, loss = 0.29450791\n",
      "Iteration 838, loss = 0.29439418\n",
      "Iteration 839, loss = 0.29420789\n",
      "Iteration 840, loss = 0.29405728\n",
      "Iteration 841, loss = 0.29389335\n",
      "Iteration 842, loss = 0.29376522\n",
      "Iteration 843, loss = 0.29368009\n",
      "Iteration 844, loss = 0.29346661\n",
      "Iteration 845, loss = 0.29330830\n",
      "Iteration 846, loss = 0.29318201\n",
      "Iteration 847, loss = 0.29303049\n",
      "Iteration 848, loss = 0.29303601\n",
      "Iteration 849, loss = 0.29273187\n",
      "Iteration 850, loss = 0.29258312\n",
      "Iteration 851, loss = 0.29247647\n",
      "Iteration 852, loss = 0.29227717\n",
      "Iteration 853, loss = 0.29218769\n",
      "Iteration 854, loss = 0.29205609\n",
      "Iteration 855, loss = 0.29186452\n",
      "Iteration 856, loss = 0.29170668\n",
      "Iteration 857, loss = 0.29151842\n",
      "Iteration 858, loss = 0.29162362\n",
      "Iteration 859, loss = 0.29127733\n",
      "Iteration 860, loss = 0.29111001\n",
      "Iteration 861, loss = 0.29103974\n",
      "Iteration 862, loss = 0.29086041\n",
      "Iteration 863, loss = 0.29069590\n",
      "Iteration 864, loss = 0.29058783\n",
      "Iteration 865, loss = 0.29042221\n",
      "Iteration 866, loss = 0.29036638\n",
      "Iteration 867, loss = 0.29019993\n",
      "Iteration 868, loss = 0.28994232\n",
      "Iteration 869, loss = 0.28982986\n",
      "Iteration 870, loss = 0.28970981\n",
      "Iteration 871, loss = 0.28966476\n",
      "Iteration 872, loss = 0.28939694\n",
      "Iteration 873, loss = 0.28923466\n",
      "Iteration 874, loss = 0.28912396\n",
      "Iteration 875, loss = 0.28899747\n",
      "Iteration 876, loss = 0.28893357\n",
      "Iteration 877, loss = 0.28873635\n",
      "Iteration 878, loss = 0.28853699\n",
      "Iteration 879, loss = 0.28846621\n",
      "Iteration 880, loss = 0.28835014\n",
      "Iteration 881, loss = 0.28813142\n",
      "Iteration 882, loss = 0.28799893\n",
      "Iteration 883, loss = 0.28781588\n",
      "Iteration 884, loss = 0.28786433\n",
      "Iteration 885, loss = 0.28757353\n",
      "Iteration 886, loss = 0.28742611\n",
      "Iteration 887, loss = 0.28729593\n",
      "Iteration 888, loss = 0.28710834\n",
      "Iteration 889, loss = 0.28697247\n",
      "Iteration 890, loss = 0.28685481\n",
      "Iteration 891, loss = 0.28675356\n",
      "Iteration 892, loss = 0.28657511\n",
      "Iteration 893, loss = 0.28648997\n",
      "Iteration 894, loss = 0.28629524\n",
      "Iteration 895, loss = 0.28612571\n",
      "Iteration 896, loss = 0.28595865\n",
      "Iteration 897, loss = 0.28581867\n",
      "Iteration 898, loss = 0.28570594\n",
      "Iteration 899, loss = 0.28560653\n",
      "Iteration 900, loss = 0.28535402\n",
      "Iteration 901, loss = 0.28521823\n",
      "Iteration 902, loss = 0.28520403\n",
      "Iteration 903, loss = 0.28490317\n",
      "Iteration 904, loss = 0.28476120\n",
      "Iteration 905, loss = 0.28462221\n",
      "Iteration 906, loss = 0.28456652\n",
      "Iteration 907, loss = 0.28432625\n",
      "Iteration 908, loss = 0.28419291\n",
      "Iteration 909, loss = 0.28409177\n",
      "Iteration 910, loss = 0.28395149\n",
      "Iteration 911, loss = 0.28376139\n",
      "Iteration 912, loss = 0.28366346\n",
      "Iteration 913, loss = 0.28363421\n",
      "Iteration 914, loss = 0.28348327\n",
      "Iteration 915, loss = 0.28316340\n",
      "Iteration 916, loss = 0.28320367\n",
      "Iteration 917, loss = 0.28290708\n",
      "Iteration 918, loss = 0.28275098\n",
      "Iteration 919, loss = 0.28261255\n",
      "Iteration 920, loss = 0.28255832\n",
      "Iteration 921, loss = 0.28232343\n",
      "Iteration 922, loss = 0.28216703\n",
      "Iteration 923, loss = 0.28210326\n",
      "Iteration 924, loss = 0.28192896\n",
      "Iteration 925, loss = 0.28174015\n",
      "Iteration 926, loss = 0.28161981\n",
      "Iteration 927, loss = 0.28145659\n",
      "Iteration 928, loss = 0.28138851\n",
      "Iteration 929, loss = 0.28111672\n",
      "Iteration 930, loss = 0.28101958\n",
      "Iteration 931, loss = 0.28087502\n",
      "Iteration 932, loss = 0.28072096\n",
      "Iteration 933, loss = 0.28063806\n",
      "Iteration 934, loss = 0.28048933\n",
      "Iteration 935, loss = 0.28031449\n",
      "Iteration 936, loss = 0.28015033\n",
      "Iteration 937, loss = 0.28013552\n",
      "Iteration 938, loss = 0.27989261\n",
      "Iteration 939, loss = 0.27969635\n",
      "Iteration 940, loss = 0.27953025\n",
      "Iteration 941, loss = 0.27940256\n",
      "Iteration 942, loss = 0.27930019\n",
      "Iteration 943, loss = 0.27931125\n",
      "Iteration 944, loss = 0.27900089\n",
      "Iteration 945, loss = 0.27890886\n",
      "Iteration 946, loss = 0.27869770\n",
      "Iteration 947, loss = 0.27856770\n",
      "Iteration 948, loss = 0.27858501\n",
      "Iteration 949, loss = 0.27835551\n",
      "Iteration 950, loss = 0.27813832\n",
      "Iteration 951, loss = 0.27797963\n",
      "Iteration 952, loss = 0.27784754\n",
      "Iteration 953, loss = 0.27772713\n",
      "Iteration 954, loss = 0.27753967\n",
      "Iteration 955, loss = 0.27738619\n",
      "Iteration 956, loss = 0.27727288\n",
      "Iteration 957, loss = 0.27717775\n",
      "Iteration 958, loss = 0.27695435\n",
      "Iteration 959, loss = 0.27685257\n",
      "Iteration 960, loss = 0.27663744\n",
      "Iteration 961, loss = 0.27649978\n",
      "Iteration 962, loss = 0.27675560\n",
      "Iteration 963, loss = 0.27634963\n",
      "Iteration 964, loss = 0.27607347\n",
      "Iteration 965, loss = 0.27602687\n",
      "Iteration 966, loss = 0.27581686\n",
      "Iteration 967, loss = 0.27564402\n",
      "Iteration 968, loss = 0.27549448\n",
      "Iteration 969, loss = 0.27542542\n",
      "Iteration 970, loss = 0.27523011\n",
      "Iteration 971, loss = 0.27501868\n",
      "Iteration 972, loss = 0.27491214\n",
      "Iteration 973, loss = 0.27474654\n",
      "Iteration 974, loss = 0.27458720\n",
      "Iteration 975, loss = 0.27441814\n",
      "Iteration 976, loss = 0.27440292\n",
      "Iteration 977, loss = 0.27420486\n",
      "Iteration 978, loss = 0.27408823\n",
      "Iteration 979, loss = 0.27384952\n",
      "Iteration 980, loss = 0.27375599\n",
      "Iteration 981, loss = 0.27358083\n",
      "Iteration 982, loss = 0.27348782\n",
      "Iteration 983, loss = 0.27327726\n",
      "Iteration 984, loss = 0.27312860\n",
      "Iteration 985, loss = 0.27300006\n",
      "Iteration 986, loss = 0.27284565\n",
      "Iteration 987, loss = 0.27268706\n",
      "Iteration 988, loss = 0.27249984\n",
      "Iteration 989, loss = 0.27247014\n",
      "Iteration 990, loss = 0.27223381\n",
      "Iteration 991, loss = 0.27209710\n",
      "Iteration 992, loss = 0.27197861\n",
      "Iteration 993, loss = 0.27188165\n",
      "Iteration 994, loss = 0.27163131\n",
      "Iteration 995, loss = 0.27150385\n",
      "Iteration 996, loss = 0.27157977\n",
      "Iteration 997, loss = 0.27121259\n",
      "Iteration 998, loss = 0.27112521\n",
      "Iteration 999, loss = 0.27092847\n",
      "Iteration 1000, loss = 0.27076345\n",
      "Iteration 1001, loss = 0.27059721\n",
      "Iteration 1002, loss = 0.27041318\n",
      "Iteration 1003, loss = 0.27035933\n",
      "Iteration 1004, loss = 0.27015291\n",
      "Iteration 1005, loss = 0.27018634\n",
      "Iteration 1006, loss = 0.26983795\n",
      "Iteration 1007, loss = 0.26970393\n",
      "Iteration 1008, loss = 0.26952784\n",
      "Iteration 1009, loss = 0.26946149\n",
      "Iteration 1010, loss = 0.26924881\n",
      "Iteration 1011, loss = 0.26908605\n",
      "Iteration 1012, loss = 0.26894943\n",
      "Iteration 1013, loss = 0.26896258\n",
      "Iteration 1014, loss = 0.26866885\n",
      "Iteration 1015, loss = 0.26844805\n",
      "Iteration 1016, loss = 0.26829003\n",
      "Iteration 1017, loss = 0.26812937\n",
      "Iteration 1018, loss = 0.26799171\n",
      "Iteration 1019, loss = 0.26795055\n",
      "Iteration 1020, loss = 0.26766320\n",
      "Iteration 1021, loss = 0.26765423\n",
      "Iteration 1022, loss = 0.26738143\n",
      "Iteration 1023, loss = 0.26727508\n",
      "Iteration 1024, loss = 0.26700368\n",
      "Iteration 1025, loss = 0.26695943\n",
      "Iteration 1026, loss = 0.26690429\n",
      "Iteration 1027, loss = 0.26659786\n",
      "Iteration 1028, loss = 0.26637757\n",
      "Iteration 1029, loss = 0.26621346\n",
      "Iteration 1030, loss = 0.26622198\n",
      "Iteration 1031, loss = 0.26599185\n",
      "Iteration 1032, loss = 0.26576432\n",
      "Iteration 1033, loss = 0.26560867\n",
      "Iteration 1034, loss = 0.26543663\n",
      "Iteration 1035, loss = 0.26527609\n",
      "Iteration 1036, loss = 0.26510596\n",
      "Iteration 1037, loss = 0.26498029\n",
      "Iteration 1038, loss = 0.26480242\n",
      "Iteration 1039, loss = 0.26465654\n",
      "Iteration 1040, loss = 0.26453245\n",
      "Iteration 1041, loss = 0.26433713\n",
      "Iteration 1042, loss = 0.26415621\n",
      "Iteration 1043, loss = 0.26402306\n",
      "Iteration 1044, loss = 0.26385627\n",
      "Iteration 1045, loss = 0.26372457\n",
      "Iteration 1046, loss = 0.26349783\n",
      "Iteration 1047, loss = 0.26341035\n",
      "Iteration 1048, loss = 0.26321530\n",
      "Iteration 1049, loss = 0.26313789\n",
      "Iteration 1050, loss = 0.26290645\n",
      "Iteration 1051, loss = 0.26278542\n",
      "Iteration 1052, loss = 0.26258019\n",
      "Iteration 1053, loss = 0.26242681\n",
      "Iteration 1054, loss = 0.26226345\n",
      "Iteration 1055, loss = 0.26207240\n",
      "Iteration 1056, loss = 0.26194998\n",
      "Iteration 1057, loss = 0.26186625\n",
      "Iteration 1058, loss = 0.26161733\n",
      "Iteration 1059, loss = 0.26145560\n",
      "Iteration 1060, loss = 0.26137476\n",
      "Iteration 1061, loss = 0.26110821\n",
      "Iteration 1062, loss = 0.26103097\n",
      "Iteration 1063, loss = 0.26083163\n",
      "Iteration 1064, loss = 0.26067314\n",
      "Iteration 1065, loss = 0.26062384\n",
      "Iteration 1066, loss = 0.26043581\n",
      "Iteration 1067, loss = 0.26019083\n",
      "Iteration 1068, loss = 0.26006113\n",
      "Iteration 1069, loss = 0.26002822\n",
      "Iteration 1070, loss = 0.25978038\n",
      "Iteration 1071, loss = 0.25958179\n",
      "Iteration 1072, loss = 0.25945075\n",
      "Iteration 1073, loss = 0.25928465\n",
      "Iteration 1074, loss = 0.25917387\n",
      "Iteration 1075, loss = 0.25892356\n",
      "Iteration 1076, loss = 0.25877349\n",
      "Iteration 1077, loss = 0.25856790\n",
      "Iteration 1078, loss = 0.25841189\n",
      "Iteration 1079, loss = 0.25827075\n",
      "Iteration 1080, loss = 0.25807869\n",
      "Iteration 1081, loss = 0.25796246\n",
      "Iteration 1082, loss = 0.25778352\n",
      "Iteration 1083, loss = 0.25762608\n",
      "Iteration 1084, loss = 0.25751154\n",
      "Iteration 1085, loss = 0.25729633\n",
      "Iteration 1086, loss = 0.25716661\n",
      "Iteration 1087, loss = 0.25700415\n",
      "Iteration 1088, loss = 0.25690239\n",
      "Iteration 1089, loss = 0.25664227\n",
      "Iteration 1090, loss = 0.25648472\n",
      "Iteration 1091, loss = 0.25629232\n",
      "Iteration 1092, loss = 0.25611563\n",
      "Iteration 1093, loss = 0.25600564\n",
      "Iteration 1094, loss = 0.25584401\n",
      "Iteration 1095, loss = 0.25561517\n",
      "Iteration 1096, loss = 0.25554649\n",
      "Iteration 1097, loss = 0.25534532\n",
      "Iteration 1098, loss = 0.25518625\n",
      "Iteration 1099, loss = 0.25499124\n",
      "Iteration 1100, loss = 0.25489552\n",
      "Iteration 1101, loss = 0.25468367\n",
      "Iteration 1102, loss = 0.25456943\n",
      "Iteration 1103, loss = 0.25427971\n",
      "Iteration 1104, loss = 0.25413854\n",
      "Iteration 1105, loss = 0.25401092\n",
      "Iteration 1106, loss = 0.25407666\n",
      "Iteration 1107, loss = 0.25367107\n",
      "Iteration 1108, loss = 0.25357246\n",
      "Iteration 1109, loss = 0.25345559\n",
      "Iteration 1110, loss = 0.25316141\n",
      "Iteration 1111, loss = 0.25321738\n",
      "Iteration 1112, loss = 0.25290711\n",
      "Iteration 1113, loss = 0.25269884\n",
      "Iteration 1114, loss = 0.25252872\n",
      "Iteration 1115, loss = 0.25246096\n",
      "Iteration 1116, loss = 0.25217110\n",
      "Iteration 1117, loss = 0.25207652\n",
      "Iteration 1118, loss = 0.25192561\n",
      "Iteration 1119, loss = 0.25177923\n",
      "Iteration 1120, loss = 0.25170328\n",
      "Iteration 1121, loss = 0.25142378\n",
      "Iteration 1122, loss = 0.25137480\n",
      "Iteration 1123, loss = 0.25105622\n",
      "Iteration 1124, loss = 0.25089646\n",
      "Iteration 1125, loss = 0.25070292\n",
      "Iteration 1126, loss = 0.25058035\n",
      "Iteration 1127, loss = 0.25038786\n",
      "Iteration 1128, loss = 0.25024346\n",
      "Iteration 1129, loss = 0.25007266\n",
      "Iteration 1130, loss = 0.24989324\n",
      "Iteration 1131, loss = 0.24973035\n",
      "Iteration 1132, loss = 0.24959935\n",
      "Iteration 1133, loss = 0.24956783\n",
      "Iteration 1134, loss = 0.24926460\n",
      "Iteration 1135, loss = 0.24906983\n",
      "Iteration 1136, loss = 0.24894680\n",
      "Iteration 1137, loss = 0.24875968\n",
      "Iteration 1138, loss = 0.24865152\n",
      "Iteration 1139, loss = 0.24844828\n",
      "Iteration 1140, loss = 0.24834197\n",
      "Iteration 1141, loss = 0.24803987\n",
      "Iteration 1142, loss = 0.24793143\n",
      "Iteration 1143, loss = 0.24776279\n",
      "Iteration 1144, loss = 0.24774469\n",
      "Iteration 1145, loss = 0.24744062\n",
      "Iteration 1146, loss = 0.24729211\n",
      "Iteration 1147, loss = 0.24722552\n",
      "Iteration 1148, loss = 0.24692605\n",
      "Iteration 1149, loss = 0.24720784\n",
      "Iteration 1150, loss = 0.24663662\n",
      "Iteration 1151, loss = 0.24634894\n",
      "Iteration 1152, loss = 0.24632265\n",
      "Iteration 1153, loss = 0.24604395\n",
      "Iteration 1154, loss = 0.24591913\n",
      "Iteration 1155, loss = 0.24575665\n",
      "Iteration 1156, loss = 0.24555521\n",
      "Iteration 1157, loss = 0.24534321\n",
      "Iteration 1158, loss = 0.24527366\n",
      "Iteration 1159, loss = 0.24509233\n",
      "Iteration 1160, loss = 0.24482806\n",
      "Iteration 1161, loss = 0.24465689\n",
      "Iteration 1162, loss = 0.24450184\n",
      "Iteration 1163, loss = 0.24443945\n",
      "Iteration 1164, loss = 0.24423079\n",
      "Iteration 1165, loss = 0.24397751\n",
      "Iteration 1166, loss = 0.24377286\n",
      "Iteration 1167, loss = 0.24372416\n",
      "Iteration 1168, loss = 0.24367045\n",
      "Iteration 1169, loss = 0.24330613\n",
      "Iteration 1170, loss = 0.24320556\n",
      "Iteration 1171, loss = 0.24304943\n",
      "Iteration 1172, loss = 0.24275448\n",
      "Iteration 1173, loss = 0.24272057\n",
      "Iteration 1174, loss = 0.24253721\n",
      "Iteration 1175, loss = 0.24230073\n",
      "Iteration 1176, loss = 0.24206681\n",
      "Iteration 1177, loss = 0.24189723\n",
      "Iteration 1178, loss = 0.24174954\n",
      "Iteration 1179, loss = 0.24167323\n",
      "Iteration 1180, loss = 0.24139834\n",
      "Iteration 1181, loss = 0.24138309\n",
      "Iteration 1182, loss = 0.24133620\n",
      "Iteration 1183, loss = 0.24082097\n",
      "Iteration 1184, loss = 0.24068091\n",
      "Iteration 1185, loss = 0.24048206\n",
      "Iteration 1186, loss = 0.24029580\n",
      "Iteration 1187, loss = 0.24028603\n",
      "Iteration 1188, loss = 0.23995000\n",
      "Iteration 1189, loss = 0.23982914\n",
      "Iteration 1190, loss = 0.23961639\n",
      "Iteration 1191, loss = 0.23945222\n",
      "Iteration 1192, loss = 0.23935520\n",
      "Iteration 1193, loss = 0.23907392\n",
      "Iteration 1194, loss = 0.23897641\n",
      "Iteration 1195, loss = 0.23881890\n",
      "Iteration 1196, loss = 0.23857895\n",
      "Iteration 1197, loss = 0.23844925\n",
      "Iteration 1198, loss = 0.23817012\n",
      "Iteration 1199, loss = 0.23806824\n",
      "Iteration 1200, loss = 0.23788987\n",
      "Iteration 1201, loss = 0.23769792\n",
      "Iteration 1202, loss = 0.23750369\n",
      "Iteration 1203, loss = 0.23731591\n",
      "Iteration 1204, loss = 0.23710009\n",
      "Iteration 1205, loss = 0.23694756\n",
      "Iteration 1206, loss = 0.23672071\n",
      "Iteration 1207, loss = 0.23657900\n",
      "Iteration 1208, loss = 0.23683330\n",
      "Iteration 1209, loss = 0.23620042\n",
      "Iteration 1210, loss = 0.23607419\n",
      "Iteration 1211, loss = 0.23587869\n",
      "Iteration 1212, loss = 0.23567823\n",
      "Iteration 1213, loss = 0.23548449\n",
      "Iteration 1214, loss = 0.23531249\n",
      "Iteration 1215, loss = 0.23504551\n",
      "Iteration 1216, loss = 0.23488972\n",
      "Iteration 1217, loss = 0.23472187\n",
      "Iteration 1218, loss = 0.23454729\n",
      "Iteration 1219, loss = 0.23427517\n",
      "Iteration 1220, loss = 0.23411410\n",
      "Iteration 1221, loss = 0.23405214\n",
      "Iteration 1222, loss = 0.23373400\n",
      "Iteration 1223, loss = 0.23351948\n",
      "Iteration 1224, loss = 0.23337157\n",
      "Iteration 1225, loss = 0.23320920\n",
      "Iteration 1226, loss = 0.23297675\n",
      "Iteration 1227, loss = 0.23282158\n",
      "Iteration 1228, loss = 0.23261216\n",
      "Iteration 1229, loss = 0.23247170\n",
      "Iteration 1230, loss = 0.23228484\n",
      "Iteration 1231, loss = 0.23204382\n",
      "Iteration 1232, loss = 0.23185928\n",
      "Iteration 1233, loss = 0.23170360\n",
      "Iteration 1234, loss = 0.23146788\n",
      "Iteration 1235, loss = 0.23151906\n",
      "Iteration 1236, loss = 0.23117307\n",
      "Iteration 1237, loss = 0.23097090\n",
      "Iteration 1238, loss = 0.23088292\n",
      "Iteration 1239, loss = 0.23061985\n",
      "Iteration 1240, loss = 0.23048796\n",
      "Iteration 1241, loss = 0.23057840\n",
      "Iteration 1242, loss = 0.23022199\n",
      "Iteration 1243, loss = 0.22999730\n",
      "Iteration 1244, loss = 0.22977049\n",
      "Iteration 1245, loss = 0.22951142\n",
      "Iteration 1246, loss = 0.22950587\n",
      "Iteration 1247, loss = 0.22922695\n",
      "Iteration 1248, loss = 0.22903368\n",
      "Iteration 1249, loss = 0.22883893\n",
      "Iteration 1250, loss = 0.22860189\n",
      "Iteration 1251, loss = 0.22839183\n",
      "Iteration 1252, loss = 0.22825702\n",
      "Iteration 1253, loss = 0.22822455\n",
      "Iteration 1254, loss = 0.22794425\n",
      "Iteration 1255, loss = 0.22776788\n",
      "Iteration 1256, loss = 0.22763721\n",
      "Iteration 1257, loss = 0.22737292\n",
      "Iteration 1258, loss = 0.22722537\n",
      "Iteration 1638, loss = 0.22229068\n",
      "Iteration 1639, loss = 0.22224721\n",
      "Iteration 1640, loss = 0.22209709\n",
      "Iteration 1641, loss = 0.22194443\n",
      "Iteration 1642, loss = 0.22193796\n",
      "Iteration 1643, loss = 0.22173945\n",
      "Iteration 1644, loss = 0.22167380\n",
      "Iteration 1645, loss = 0.22158388\n",
      "Iteration 1646, loss = 0.22144001\n",
      "Iteration 1647, loss = 0.22135272\n",
      "Iteration 1648, loss = 0.22122592\n",
      "Iteration 1649, loss = 0.22114445\n",
      "Iteration 1650, loss = 0.22103704\n",
      "Iteration 1651, loss = 0.22094683\n",
      "Iteration 1652, loss = 0.22092223\n",
      "Iteration 1653, loss = 0.22074551\n",
      "Iteration 1654, loss = 0.22063232\n",
      "Iteration 1655, loss = 0.22054472\n",
      "Iteration 1656, loss = 0.22040625\n",
      "Iteration 1657, loss = 0.22031988\n",
      "Iteration 1658, loss = 0.22024190\n",
      "Iteration 1659, loss = 0.22015604\n",
      "Iteration 1660, loss = 0.22003173\n",
      "Iteration 1661, loss = 0.21991602\n",
      "Iteration 1662, loss = 0.21984404\n",
      "Iteration 1663, loss = 0.21976956\n",
      "Iteration 1664, loss = 0.21961390\n",
      "Iteration 1665, loss = 0.21950456\n",
      "Iteration 1666, loss = 0.21941325\n",
      "Iteration 1667, loss = 0.21942390\n",
      "Iteration 1668, loss = 0.21919798\n",
      "Iteration 1669, loss = 0.21916773\n",
      "Iteration 1670, loss = 0.21898745\n",
      "Iteration 1671, loss = 0.21892142\n",
      "Iteration 1672, loss = 0.21879250\n",
      "Iteration 1673, loss = 0.21871318\n",
      "Iteration 1674, loss = 0.21855941\n",
      "Iteration 1675, loss = 0.21848685\n",
      "Iteration 1676, loss = 0.21836132\n",
      "Iteration 1677, loss = 0.21826056\n",
      "Iteration 1678, loss = 0.21816265\n",
      "Iteration 1679, loss = 0.21805502\n",
      "Iteration 1680, loss = 0.21803014\n",
      "Iteration 1681, loss = 0.21788676\n",
      "Iteration 1682, loss = 0.21774584\n",
      "Iteration 1683, loss = 0.21768399\n",
      "Iteration 1684, loss = 0.21756025\n",
      "Iteration 1685, loss = 0.21749920\n",
      "Iteration 1686, loss = 0.21737374\n",
      "Iteration 1687, loss = 0.21723980\n",
      "Iteration 1688, loss = 0.21714720\n",
      "Iteration 1689, loss = 0.21708354\n",
      "Iteration 1690, loss = 0.21693877\n",
      "Iteration 1691, loss = 0.21686878\n",
      "Iteration 1692, loss = 0.21685066\n",
      "Iteration 1693, loss = 0.21668564\n",
      "Iteration 1694, loss = 0.21656709\n",
      "Iteration 1695, loss = 0.21647313\n",
      "Iteration 1696, loss = 0.21634936\n",
      "Iteration 1697, loss = 0.21630114\n",
      "Iteration 1698, loss = 0.21618871\n",
      "Iteration 1699, loss = 0.21604870\n",
      "Iteration 1700, loss = 0.21595271\n",
      "Iteration 1701, loss = 0.21589558\n",
      "Iteration 1702, loss = 0.21576864\n",
      "Iteration 1703, loss = 0.21563067\n",
      "Iteration 1704, loss = 0.21556885\n",
      "Iteration 1705, loss = 0.21545373\n",
      "Iteration 1706, loss = 0.21545962\n",
      "Iteration 1707, loss = 0.21522358\n",
      "Iteration 1708, loss = 0.21517907\n",
      "Iteration 1709, loss = 0.21504769\n",
      "Iteration 1710, loss = 0.21495683\n",
      "Iteration 1711, loss = 0.21488602\n",
      "Iteration 1712, loss = 0.21480690\n",
      "Iteration 1713, loss = 0.21466626\n",
      "Iteration 1714, loss = 0.21455886\n",
      "Iteration 1715, loss = 0.21453872\n",
      "Iteration 1716, loss = 0.21435756\n",
      "Iteration 1717, loss = 0.21438940\n",
      "Iteration 1718, loss = 0.21421885\n",
      "Iteration 1719, loss = 0.21406536\n",
      "Iteration 1720, loss = 0.21395270\n",
      "Iteration 1721, loss = 0.21397957\n",
      "Iteration 1722, loss = 0.21373667\n",
      "Iteration 1723, loss = 0.21364582\n",
      "Iteration 1724, loss = 0.21354407\n",
      "Iteration 1725, loss = 0.21349795\n",
      "Iteration 1726, loss = 0.21335162\n",
      "Iteration 1727, loss = 0.21325975\n",
      "Iteration 1728, loss = 0.21321289\n",
      "Iteration 1729, loss = 0.21303751\n",
      "Iteration 1730, loss = 0.21306269\n",
      "Iteration 1731, loss = 0.21289025\n",
      "Iteration 1732, loss = 0.21275320\n",
      "Iteration 1733, loss = 0.21266832\n",
      "Iteration 1734, loss = 0.21259045\n",
      "Iteration 1735, loss = 0.21247109\n",
      "Iteration 1736, loss = 0.21239379\n",
      "Iteration 1737, loss = 0.21223937\n",
      "Iteration 1738, loss = 0.21216712\n",
      "Iteration 1739, loss = 0.21208413\n",
      "Iteration 1740, loss = 0.21199330\n",
      "Iteration 1741, loss = 0.21193795\n",
      "Iteration 1742, loss = 0.21182964\n",
      "Iteration 1743, loss = 0.21166029\n",
      "Iteration 1744, loss = 0.21160705\n",
      "Iteration 1745, loss = 0.21150086\n",
      "Iteration 1746, loss = 0.21148365\n",
      "Iteration 1747, loss = 0.21126029\n",
      "Iteration 1748, loss = 0.21114751\n",
      "Iteration 1749, loss = 0.21105911\n",
      "Iteration 1750, loss = 0.21118015\n",
      "Iteration 1751, loss = 0.21096567\n",
      "Iteration 1752, loss = 0.21074121\n",
      "Iteration 1753, loss = 0.21066769\n",
      "Iteration 1754, loss = 0.21054588\n",
      "Iteration 1755, loss = 0.21046225\n",
      "Iteration 1756, loss = 0.21035910\n",
      "Iteration 1757, loss = 0.21029216\n",
      "Iteration 1758, loss = 0.21020947\n",
      "Iteration 1759, loss = 0.21003207\n",
      "Iteration 1760, loss = 0.20993490\n",
      "Iteration 1761, loss = 0.20991566\n",
      "Iteration 1762, loss = 0.20975894\n",
      "Iteration 1763, loss = 0.20964373\n",
      "Iteration 1764, loss = 0.20956317\n",
      "Iteration 1765, loss = 0.20942645\n",
      "Iteration 1766, loss = 0.20932361\n",
      "Iteration 1767, loss = 0.20920260\n",
      "Iteration 1768, loss = 0.20911959\n",
      "Iteration 1769, loss = 0.20903231\n",
      "Iteration 1770, loss = 0.20889959\n",
      "Iteration 1771, loss = 0.20881351\n",
      "Iteration 1772, loss = 0.20871401\n",
      "Iteration 1773, loss = 0.20868959\n",
      "Iteration 1774, loss = 0.20847056\n",
      "Iteration 1775, loss = 0.20839399\n",
      "Iteration 1776, loss = 0.20833758\n",
      "Iteration 1777, loss = 0.20821340\n",
      "Iteration 1778, loss = 0.20805442\n",
      "Iteration 1779, loss = 0.20803087\n",
      "Iteration 1780, loss = 0.20787754\n",
      "Iteration 1781, loss = 0.20782452\n",
      "Iteration 1782, loss = 0.20765136\n",
      "Iteration 1783, loss = 0.20756553\n",
      "Iteration 1784, loss = 0.20745356\n",
      "Iteration 1785, loss = 0.20733713\n",
      "Iteration 1786, loss = 0.20724965\n",
      "Iteration 1787, loss = 0.20726102\n",
      "Iteration 1788, loss = 0.20704876\n",
      "Iteration 1789, loss = 0.20702761\n",
      "Iteration 1790, loss = 0.20681371\n",
      "Iteration 1791, loss = 0.20679851\n",
      "Iteration 1792, loss = 0.20661097\n",
      "Iteration 1793, loss = 0.20653376\n",
      "Iteration 1794, loss = 0.20636750\n",
      "Iteration 1795, loss = 0.20627770\n",
      "Iteration 1796, loss = 0.20622950\n",
      "Iteration 1797, loss = 0.20612415\n",
      "Iteration 1798, loss = 0.20600369\n",
      "Iteration 1799, loss = 0.20583495\n",
      "Iteration 1800, loss = 0.20577623\n",
      "Iteration 1801, loss = 0.20571974\n",
      "Iteration 1802, loss = 0.20556108\n",
      "Iteration 1803, loss = 0.20543348\n",
      "Iteration 1804, loss = 0.20531503\n",
      "Iteration 1805, loss = 0.20523714\n",
      "Iteration 1806, loss = 0.20527110\n",
      "Iteration 1807, loss = 0.20501844\n",
      "Iteration 1808, loss = 0.20490352\n",
      "Iteration 1809, loss = 0.20489145\n",
      "Iteration 1810, loss = 0.20478997\n",
      "Iteration 1811, loss = 0.20459148\n",
      "Iteration 1812, loss = 0.20450503\n",
      "Iteration 1813, loss = 0.20441398\n",
      "Iteration 1814, loss = 0.20431115\n",
      "Iteration 1815, loss = 0.20422206\n",
      "Iteration 1816, loss = 0.20421415\n",
      "Iteration 1817, loss = 0.20403613\n",
      "Iteration 1818, loss = 0.20385326\n",
      "Iteration 1819, loss = 0.20377569\n",
      "Iteration 1820, loss = 0.20368430\n",
      "Iteration 1821, loss = 0.20356861\n",
      "Iteration 1822, loss = 0.20343403\n",
      "Iteration 1823, loss = 0.20333236\n",
      "Iteration 1824, loss = 0.20322922\n",
      "Iteration 1825, loss = 0.20313398\n",
      "Iteration 1826, loss = 0.20310385\n",
      "Iteration 1827, loss = 0.20293996\n",
      "Iteration 1828, loss = 0.20284940\n",
      "Iteration 1829, loss = 0.20279291\n",
      "Iteration 1830, loss = 0.20264047\n",
      "Iteration 1831, loss = 0.20252240\n",
      "Iteration 1832, loss = 0.20255378\n",
      "Iteration 1833, loss = 0.20232996\n",
      "Iteration 1834, loss = 0.20220354\n",
      "Iteration 1835, loss = 0.20209841\n",
      "Iteration 1836, loss = 0.20214692\n",
      "Iteration 1837, loss = 0.20190377\n",
      "Iteration 1838, loss = 0.20181675\n",
      "Iteration 1839, loss = 0.20174878\n",
      "Iteration 1840, loss = 0.20164845\n",
      "Iteration 1841, loss = 0.20164527\n",
      "Iteration 1842, loss = 0.20146522\n",
      "Iteration 1843, loss = 0.20135325\n",
      "Iteration 1844, loss = 0.20123029\n",
      "Iteration 1845, loss = 0.20114263\n",
      "Iteration 1846, loss = 0.20099712\n",
      "Iteration 1847, loss = 0.20093612\n",
      "Iteration 1848, loss = 0.20079517\n",
      "Iteration 1849, loss = 0.20080777\n",
      "Iteration 1850, loss = 0.20057500\n",
      "Iteration 1851, loss = 0.20048374\n",
      "Iteration 1852, loss = 0.20038151\n",
      "Iteration 1853, loss = 0.20033235\n",
      "Iteration 1854, loss = 0.20017283\n",
      "Iteration 1855, loss = 0.20008871\n",
      "Iteration 1856, loss = 0.19996204\n",
      "Iteration 1857, loss = 0.19989730\n",
      "Iteration 1858, loss = 0.19984413\n",
      "Iteration 1859, loss = 0.19965882\n",
      "Iteration 1860, loss = 0.19958434\n",
      "Iteration 1861, loss = 0.19949122\n",
      "Iteration 1862, loss = 0.19936229\n",
      "Iteration 1863, loss = 0.19936072\n",
      "Iteration 1864, loss = 0.19922067\n",
      "Iteration 1865, loss = 0.19912611\n",
      "Iteration 1866, loss = 0.19900203\n",
      "Iteration 1867, loss = 0.19885451\n",
      "Iteration 1868, loss = 0.19878203\n",
      "Iteration 1869, loss = 0.19863719\n",
      "Iteration 1870, loss = 0.19852806\n",
      "Iteration 1871, loss = 0.19844847\n",
      "Iteration 1872, loss = 0.19836155\n",
      "Iteration 1873, loss = 0.19830879\n",
      "Iteration 1874, loss = 0.19818030\n",
      "Iteration 1875, loss = 0.19812869\n",
      "Iteration 1876, loss = 0.19791343\n",
      "Iteration 1877, loss = 0.19786784\n",
      "Iteration 1878, loss = 0.19772971\n",
      "Iteration 1879, loss = 0.19761961\n",
      "Iteration 1880, loss = 0.19757505\n",
      "Iteration 1881, loss = 0.19743910\n",
      "Iteration 1882, loss = 0.19733483\n",
      "Iteration 1883, loss = 0.19730973\n",
      "Iteration 1884, loss = 0.19713818\n",
      "Iteration 1885, loss = 0.19706636\n",
      "Iteration 1886, loss = 0.19698199\n",
      "Iteration 1887, loss = 0.19681372\n",
      "Iteration 1888, loss = 0.19669259\n",
      "Iteration 1889, loss = 0.19665377\n",
      "Iteration 1890, loss = 0.19649620\n",
      "Iteration 1891, loss = 0.19642796\n",
      "Iteration 1892, loss = 0.19631218\n",
      "Iteration 1893, loss = 0.19627848\n",
      "Iteration 1894, loss = 0.19614773\n",
      "Iteration 1895, loss = 0.19602571\n",
      "Iteration 1896, loss = 0.19589779\n",
      "Iteration 1897, loss = 0.19603239\n",
      "Iteration 1898, loss = 0.19569612\n",
      "Iteration 1899, loss = 0.19567203\n",
      "Iteration 1900, loss = 0.19551806\n",
      "Iteration 1901, loss = 0.19540844\n",
      "Iteration 1902, loss = 0.19528963\n",
      "Iteration 1903, loss = 0.19520619\n",
      "Iteration 1904, loss = 0.19507002\n",
      "Iteration 1905, loss = 0.19495992\n",
      "Iteration 1906, loss = 0.19487608\n",
      "Iteration 1907, loss = 0.19483012\n",
      "Iteration 1908, loss = 0.19469074\n",
      "Iteration 1909, loss = 0.19457613\n",
      "Iteration 1910, loss = 0.19449375\n",
      "Iteration 1911, loss = 0.19435272\n",
      "Iteration 1912, loss = 0.19425776\n",
      "Iteration 1913, loss = 0.19414974\n",
      "Iteration 1914, loss = 0.19421501\n",
      "Iteration 1915, loss = 0.19398010\n",
      "Iteration 1916, loss = 0.19387077\n",
      "Iteration 1917, loss = 0.19372178\n",
      "Iteration 1918, loss = 0.19362429\n",
      "Iteration 1919, loss = 0.19354424\n",
      "Iteration 1920, loss = 0.19344616\n",
      "Iteration 1921, loss = 0.19333628\n",
      "Iteration 1922, loss = 0.19323883\n",
      "Iteration 1923, loss = 0.19311653\n",
      "Iteration 1924, loss = 0.19313560\n",
      "Iteration 1925, loss = 0.19290906\n",
      "Iteration 1926, loss = 0.19281342\n",
      "Iteration 1927, loss = 0.19272093\n",
      "Iteration 1928, loss = 0.19258727\n",
      "Iteration 1929, loss = 0.19247611\n",
      "Iteration 1930, loss = 0.19248550\n",
      "Iteration 1931, loss = 0.19227238\n",
      "Iteration 1932, loss = 0.19219671\n",
      "Iteration 1933, loss = 0.19216169\n",
      "Iteration 1934, loss = 0.19200289\n",
      "Iteration 1935, loss = 0.19191485\n",
      "Iteration 1936, loss = 0.19177328\n",
      "Iteration 1937, loss = 0.19166381\n",
      "Iteration 1938, loss = 0.19163015\n",
      "Iteration 1939, loss = 0.19147160\n",
      "Iteration 1940, loss = 0.19135979\n",
      "Iteration 1941, loss = 0.19127505\n",
      "Iteration 1942, loss = 0.19125329\n",
      "Iteration 1943, loss = 0.19109913\n",
      "Iteration 1944, loss = 0.19098353\n",
      "Iteration 1945, loss = 0.19084671\n",
      "Iteration 1946, loss = 0.19073005\n",
      "Iteration 1947, loss = 0.19074501\n",
      "Iteration 1948, loss = 0.19055791\n",
      "Iteration 1949, loss = 0.19042347\n",
      "Iteration 1950, loss = 0.19032016\n",
      "Iteration 1951, loss = 0.19028854\n",
      "Iteration 1952, loss = 0.19015907\n",
      "Iteration 1953, loss = 0.19001967\n",
      "Iteration 1954, loss = 0.18998495\n",
      "Iteration 1955, loss = 0.18985982\n",
      "Iteration 1956, loss = 0.18980193\n",
      "Iteration 1957, loss = 0.18963463\n",
      "Iteration 1958, loss = 0.18952708\n",
      "Iteration 1959, loss = 0.18945432\n",
      "Iteration 1960, loss = 0.18929651\n",
      "Iteration 1961, loss = 0.18927065\n",
      "Iteration 1962, loss = 0.18910201\n",
      "Iteration 1963, loss = 0.18904924\n",
      "Iteration 1964, loss = 0.18896858\n",
      "Iteration 1965, loss = 0.18877505\n",
      "Iteration 1966, loss = 0.18870262\n",
      "Iteration 1967, loss = 0.18861582\n",
      "Iteration 1968, loss = 0.18848888\n",
      "Iteration 1969, loss = 0.18836221\n",
      "Iteration 1970, loss = 0.18827556\n",
      "Iteration 1971, loss = 0.18824846\n",
      "Iteration 1972, loss = 0.18810259\n",
      "Iteration 1973, loss = 0.18797432\n",
      "Iteration 1974, loss = 0.18790258\n",
      "Iteration 1975, loss = 0.18781899\n",
      "Iteration 1976, loss = 0.18763433\n",
      "Iteration 1977, loss = 0.18759561\n",
      "Iteration 1978, loss = 0.18748117\n",
      "Iteration 1979, loss = 0.18735455\n",
      "Iteration 1980, loss = 0.18729448\n",
      "Iteration 1981, loss = 0.18715232\n",
      "Iteration 1982, loss = 0.18707144\n",
      "Iteration 1983, loss = 0.18694298\n",
      "Iteration 1984, loss = 0.18685414\n",
      "Iteration 1985, loss = 0.18671690\n",
      "Iteration 1986, loss = 0.18660159\n",
      "Iteration 1987, loss = 0.18653215\n",
      "Iteration 1988, loss = 0.18641153\n",
      "Iteration 1989, loss = 0.18630933\n",
      "Iteration 1990, loss = 0.18621506\n",
      "Iteration 1991, loss = 0.18612127\n",
      "Iteration 1992, loss = 0.18611352\n",
      "Iteration 1993, loss = 0.18597881\n",
      "Iteration 1994, loss = 0.18576296\n",
      "Iteration 1995, loss = 0.18581149\n",
      "Iteration 1996, loss = 0.18557740\n",
      "Iteration 1997, loss = 0.18546178\n",
      "Iteration 1998, loss = 0.18543022\n",
      "Iteration 1999, loss = 0.18527320\n",
      "Iteration 2000, loss = 0.18522952\n",
      "Iteration 2001, loss = 0.18506627\n",
      "Iteration 2002, loss = 0.18501272\n",
      "Iteration 2003, loss = 0.18503835\n",
      "Iteration 2004, loss = 0.18479855\n",
      "Iteration 2005, loss = 0.18467636\n",
      "Iteration 2006, loss = 0.18460293\n",
      "Iteration 2007, loss = 0.18451891\n",
      "Iteration 2008, loss = 0.18442373\n",
      "Iteration 2009, loss = 0.18425933\n",
      "Iteration 2010, loss = 0.18436079\n",
      "Iteration 2011, loss = 0.18405626\n",
      "Iteration 2012, loss = 0.18398985\n",
      "Iteration 2013, loss = 0.18396630\n",
      "Iteration 2014, loss = 0.18377320\n",
      "Iteration 2015, loss = 0.18363503\n",
      "Iteration 2016, loss = 0.18362811\n",
      "Iteration 2017, loss = 0.18342369\n",
      "Iteration 2018, loss = 0.18339685\n",
      "Iteration 2019, loss = 0.18326878\n",
      "Iteration 2020, loss = 0.18316504\n",
      "Iteration 2021, loss = 0.18301604\n",
      "Iteration 2022, loss = 0.18291249\n",
      "Iteration 2023, loss = 0.18285292\n",
      "Iteration 2024, loss = 0.18269020\n",
      "Iteration 2025, loss = 0.18260073\n",
      "Iteration 2026, loss = 0.18250788\n",
      "Iteration 2027, loss = 0.18242621\n",
      "Iteration 2028, loss = 0.18237534\n",
      "Iteration 2029, loss = 0.18221375\n",
      "Iteration 2030, loss = 0.18220049\n",
      "Iteration 2031, loss = 0.18218336\n",
      "Iteration 2032, loss = 0.18193192\n",
      "Iteration 2033, loss = 0.18187577\n",
      "Iteration 2034, loss = 0.18167489\n",
      "Iteration 2035, loss = 0.18169237\n",
      "Iteration 2036, loss = 0.18151734\n",
      "Iteration 2037, loss = 0.18139546\n",
      "Iteration 2038, loss = 0.18130769\n",
      "Iteration 2039, loss = 0.18116157\n",
      "Iteration 2040, loss = 0.18109033\n",
      "Iteration 2041, loss = 0.18109692\n",
      "Iteration 2042, loss = 0.18088043\n",
      "Iteration 2043, loss = 0.18076286\n",
      "Iteration 2044, loss = 0.18071262\n",
      "Iteration 2045, loss = 0.18053970\n",
      "Iteration 2046, loss = 0.18042344\n",
      "Iteration 2047, loss = 0.18036037\n",
      "Iteration 2048, loss = 0.18022651\n",
      "Iteration 2049, loss = 0.18014186\n",
      "Iteration 2050, loss = 0.18007485\n",
      "Iteration 2051, loss = 0.18004768\n",
      "Iteration 2052, loss = 0.17985314\n",
      "Iteration 2053, loss = 0.17975941\n",
      "Iteration 2054, loss = 0.17960312\n",
      "Iteration 2055, loss = 0.17953197\n",
      "Iteration 2056, loss = 0.17941598\n",
      "Iteration 2057, loss = 0.17935189\n",
      "Iteration 2058, loss = 0.17923977\n",
      "Iteration 2059, loss = 0.17914670\n",
      "Iteration 2060, loss = 0.17904737\n",
      "Iteration 2061, loss = 0.17896188\n",
      "Iteration 2062, loss = 0.17889266\n",
      "Iteration 2063, loss = 0.17871411\n",
      "Iteration 2064, loss = 0.17859821\n",
      "Iteration 2065, loss = 0.17867003\n",
      "Iteration 2066, loss = 0.17836526\n",
      "Iteration 2067, loss = 0.17826923\n",
      "Iteration 2068, loss = 0.17845598\n",
      "Iteration 2069, loss = 0.17809768\n",
      "Iteration 2070, loss = 0.17803608\n",
      "Iteration 2071, loss = 0.17785389\n",
      "Iteration 2072, loss = 0.17778025\n",
      "Iteration 2073, loss = 0.17766959\n",
      "Iteration 2074, loss = 0.17769032\n",
      "Iteration 2075, loss = 0.17750329\n",
      "Iteration 2076, loss = 0.17742238\n",
      "Iteration 2077, loss = 0.17732387\n",
      "Iteration 2078, loss = 0.17718645\n",
      "Iteration 2079, loss = 0.17706757\n",
      "Iteration 2080, loss = 0.17695624\n",
      "Iteration 2081, loss = 0.17687170\n",
      "Iteration 2082, loss = 0.17693468\n",
      "Iteration 2083, loss = 0.17674807\n",
      "Iteration 2084, loss = 0.17653595\n",
      "Iteration 2085, loss = 0.17642452\n",
      "Iteration 2086, loss = 0.17636302\n",
      "Iteration 2087, loss = 0.17629870\n",
      "Iteration 2088, loss = 0.17620668\n",
      "Iteration 2089, loss = 0.17614679\n",
      "Iteration 2090, loss = 0.17593000\n",
      "Iteration 2091, loss = 0.17596905\n",
      "Iteration 2092, loss = 0.17576229\n",
      "Iteration 2093, loss = 0.17568557\n",
      "Iteration 2094, loss = 0.17554915\n",
      "Iteration 2095, loss = 0.17545500\n",
      "Iteration 2096, loss = 0.17532773\n",
      "Iteration 2097, loss = 0.17528948\n",
      "Iteration 2098, loss = 0.17513064\n",
      "Iteration 2099, loss = 0.17499868\n",
      "Iteration 2100, loss = 0.17494116\n",
      "Iteration 2101, loss = 0.17477677\n",
      "Iteration 2102, loss = 0.17466436\n",
      "Iteration 2103, loss = 0.17468637\n",
      "Iteration 2104, loss = 0.17450382\n",
      "Iteration 2105, loss = 0.17435634\n",
      "Iteration 2106, loss = 0.17428658\n",
      "Iteration 2107, loss = 0.17416840\n",
      "Iteration 2108, loss = 0.17405903\n",
      "Iteration 2109, loss = 0.17409122\n",
      "Iteration 2110, loss = 0.17392193\n",
      "Iteration 2111, loss = 0.17378981\n",
      "Iteration 2112, loss = 0.17365066\n",
      "Iteration 2113, loss = 0.17353919\n",
      "Iteration 2114, loss = 0.17356137\n",
      "Iteration 2115, loss = 0.17339699\n",
      "Iteration 2116, loss = 0.17325703\n",
      "Iteration 2117, loss = 0.17321132\n",
      "Iteration 175, loss = 0.34951507\n",
      "Iteration 176, loss = 0.34920565\n",
      "Iteration 177, loss = 0.34890650\n",
      "Iteration 178, loss = 0.34860720\n",
      "Iteration 179, loss = 0.34829328\n",
      "Iteration 180, loss = 0.34797252\n",
      "Iteration 181, loss = 0.34771810\n",
      "Iteration 182, loss = 0.34739009\n",
      "Iteration 183, loss = 0.34712428\n",
      "Iteration 184, loss = 0.34682836\n",
      "Iteration 185, loss = 0.34655142\n",
      "Iteration 186, loss = 0.34627124\n",
      "Iteration 187, loss = 0.34598905\n",
      "Iteration 188, loss = 0.34570238\n",
      "Iteration 189, loss = 0.34543069\n",
      "Iteration 190, loss = 0.34515357\n",
      "Iteration 191, loss = 0.34488587\n",
      "Iteration 192, loss = 0.34462101\n",
      "Iteration 193, loss = 0.34435072\n",
      "Iteration 194, loss = 0.34408590\n",
      "Iteration 195, loss = 0.34382341\n",
      "Iteration 196, loss = 0.34360769\n",
      "Iteration 197, loss = 0.34333022\n",
      "Iteration 198, loss = 0.34303086\n",
      "Iteration 199, loss = 0.34280618\n",
      "Iteration 200, loss = 0.34254961\n",
      "Iteration 201, loss = 0.34231206\n",
      "Iteration 202, loss = 0.34204029\n",
      "Iteration 203, loss = 0.34181294\n",
      "Iteration 204, loss = 0.34153832\n",
      "Iteration 205, loss = 0.34132099\n",
      "Iteration 206, loss = 0.34108415\n",
      "Iteration 207, loss = 0.34085029\n",
      "Iteration 208, loss = 0.34059792\n",
      "Iteration 209, loss = 0.34038014\n",
      "Iteration 210, loss = 0.34012241\n",
      "Iteration 211, loss = 0.33990395\n",
      "Iteration 212, loss = 0.33967231\n",
      "Iteration 213, loss = 0.33942620\n",
      "Iteration 214, loss = 0.33921142\n",
      "Iteration 215, loss = 0.33898039\n",
      "Iteration 216, loss = 0.33873826\n",
      "Iteration 217, loss = 0.33851840\n",
      "Iteration 218, loss = 0.33830388\n",
      "Iteration 219, loss = 0.33808077\n",
      "Iteration 220, loss = 0.33786147\n",
      "Iteration 221, loss = 0.33763762\n",
      "Iteration 222, loss = 0.33739129\n",
      "Iteration 223, loss = 0.33718096\n",
      "Iteration 224, loss = 0.33697495\n",
      "Iteration 225, loss = 0.33676283\n",
      "Iteration 226, loss = 0.33654726\n",
      "Iteration 227, loss = 0.33634129\n",
      "Iteration 228, loss = 0.33613828\n",
      "Iteration 229, loss = 0.33592875\n",
      "Iteration 230, loss = 0.33572471\n",
      "Iteration 231, loss = 0.33551025\n",
      "Iteration 232, loss = 0.33531716\n",
      "Iteration 233, loss = 0.33510598\n",
      "Iteration 234, loss = 0.33493165\n",
      "Iteration 235, loss = 0.33471139\n",
      "Iteration 236, loss = 0.33450303\n",
      "Iteration 237, loss = 0.33430737\n",
      "Iteration 238, loss = 0.33411328\n",
      "Iteration 239, loss = 0.33393818\n",
      "Iteration 240, loss = 0.33372013\n",
      "Iteration 241, loss = 0.33353645\n",
      "Iteration 242, loss = 0.33335169\n",
      "Iteration 243, loss = 0.33314773\n",
      "Iteration 244, loss = 0.33296755\n",
      "Iteration 245, loss = 0.33276369\n",
      "Iteration 246, loss = 0.33260364\n",
      "Iteration 247, loss = 0.33240128\n",
      "Iteration 248, loss = 0.33222398\n",
      "Iteration 249, loss = 0.33204463\n",
      "Iteration 250, loss = 0.33185456\n",
      "Iteration 251, loss = 0.33167670\n",
      "Iteration 252, loss = 0.33147113\n",
      "Iteration 253, loss = 0.33129841\n",
      "Iteration 254, loss = 0.33113991\n",
      "Iteration 255, loss = 0.33095970\n",
      "Iteration 256, loss = 0.33075912\n",
      "Iteration 257, loss = 0.33058880\n",
      "Iteration 258, loss = 0.33044306\n",
      "Iteration 259, loss = 0.33024614\n",
      "Iteration 260, loss = 0.33005471\n",
      "Iteration 261, loss = 0.32988357\n",
      "Iteration 262, loss = 0.32970618\n",
      "Iteration 263, loss = 0.32954690\n",
      "Iteration 264, loss = 0.32936985\n",
      "Iteration 265, loss = 0.32921449\n",
      "Iteration 266, loss = 0.32905030\n",
      "Iteration 267, loss = 0.32887718\n",
      "Iteration 268, loss = 0.32871103\n",
      "Iteration 269, loss = 0.32856415\n",
      "Iteration 270, loss = 0.32839537\n",
      "Iteration 271, loss = 0.32822364\n",
      "Iteration 272, loss = 0.32804392\n",
      "Iteration 273, loss = 0.32788573\n",
      "Iteration 274, loss = 0.32773998\n",
      "Iteration 275, loss = 0.32756936\n",
      "Iteration 276, loss = 0.32740479\n",
      "Iteration 277, loss = 0.32724560\n",
      "Iteration 278, loss = 0.32711647\n",
      "Iteration 279, loss = 0.32694636\n",
      "Iteration 280, loss = 0.32679150\n",
      "Iteration 281, loss = 0.32662675\n",
      "Iteration 282, loss = 0.32648402\n",
      "Iteration 283, loss = 0.32631385\n",
      "Iteration 284, loss = 0.32616946\n",
      "Iteration 285, loss = 0.32604228\n",
      "Iteration 286, loss = 0.32587206\n",
      "Iteration 287, loss = 0.32571247\n",
      "Iteration 288, loss = 0.32556383\n",
      "Iteration 289, loss = 0.32543036\n",
      "Iteration 290, loss = 0.32528144\n",
      "Iteration 291, loss = 0.32512252\n",
      "Iteration 292, loss = 0.32498809\n",
      "Iteration 293, loss = 0.32483099\n",
      "Iteration 294, loss = 0.32471936\n",
      "Iteration 295, loss = 0.32454998\n",
      "Iteration 296, loss = 0.32438430\n",
      "Iteration 297, loss = 0.32424312\n",
      "Iteration 298, loss = 0.32410101\n",
      "Iteration 299, loss = 0.32396515\n",
      "Iteration 300, loss = 0.32383316\n",
      "Iteration 301, loss = 0.32366171\n",
      "Iteration 302, loss = 0.32351455\n",
      "Iteration 303, loss = 0.32338191\n",
      "Iteration 304, loss = 0.32324089\n",
      "Iteration 305, loss = 0.32310484\n",
      "Iteration 306, loss = 0.32297121\n",
      "Iteration 307, loss = 0.32282335\n",
      "Iteration 308, loss = 0.32267997\n",
      "Iteration 309, loss = 0.32253541\n",
      "Iteration 310, loss = 0.32240841\n",
      "Iteration 311, loss = 0.32227852\n",
      "Iteration 312, loss = 0.32213986\n",
      "Iteration 313, loss = 0.32199508\n",
      "Iteration 314, loss = 0.32186591\n",
      "Iteration 315, loss = 0.32172334\n",
      "Iteration 316, loss = 0.32158607\n",
      "Iteration 317, loss = 0.32147760\n",
      "Iteration 318, loss = 0.32132417\n",
      "Iteration 319, loss = 0.32122173\n",
      "Iteration 320, loss = 0.32106512\n",
      "Iteration 321, loss = 0.32093853\n",
      "Iteration 322, loss = 0.32080187\n",
      "Iteration 323, loss = 0.32067251\n",
      "Iteration 324, loss = 0.32055238\n",
      "Iteration 325, loss = 0.32042411\n",
      "Iteration 326, loss = 0.32030039\n",
      "Iteration 327, loss = 0.32015898\n",
      "Iteration 328, loss = 0.32003016\n",
      "Iteration 329, loss = 0.31990744\n",
      "Iteration 330, loss = 0.31978481\n",
      "Iteration 331, loss = 0.31966905\n",
      "Iteration 332, loss = 0.31955123\n",
      "Iteration 333, loss = 0.31941733\n",
      "Iteration 334, loss = 0.31928371\n",
      "Iteration 335, loss = 0.31916333\n",
      "Iteration 336, loss = 0.31903499\n",
      "Iteration 337, loss = 0.31891916\n",
      "Iteration 338, loss = 0.31879150\n",
      "Iteration 339, loss = 0.31868117\n",
      "Iteration 340, loss = 0.31854787\n",
      "Iteration 341, loss = 0.31845086\n",
      "Iteration 342, loss = 0.31833291\n",
      "Iteration 343, loss = 0.31818161\n",
      "Iteration 344, loss = 0.31806711\n",
      "Iteration 345, loss = 0.31794842\n",
      "Iteration 346, loss = 0.31783320\n",
      "Iteration 347, loss = 0.31773519\n",
      "Iteration 348, loss = 0.31758492\n",
      "Iteration 349, loss = 0.31748839\n",
      "Iteration 350, loss = 0.31735233\n",
      "Iteration 351, loss = 0.31723917\n",
      "Iteration 352, loss = 0.31711474\n",
      "Iteration 353, loss = 0.31699712\n",
      "Iteration 354, loss = 0.31691685\n",
      "Iteration 355, loss = 0.31677419\n",
      "Iteration 356, loss = 0.31667386\n",
      "Iteration 357, loss = 0.31654085\n",
      "Iteration 358, loss = 0.31642505\n",
      "Iteration 359, loss = 0.31631027\n",
      "Iteration 360, loss = 0.31620323\n",
      "Iteration 361, loss = 0.31607581\n",
      "Iteration 362, loss = 0.31596714\n",
      "Iteration 363, loss = 0.31585731\n",
      "Iteration 364, loss = 0.31573690\n",
      "Iteration 365, loss = 0.31569314\n",
      "Iteration 366, loss = 0.31550873\n",
      "Iteration 367, loss = 0.31539811\n",
      "Iteration 368, loss = 0.31528633\n",
      "Iteration 369, loss = 0.31516291\n",
      "Iteration 370, loss = 0.31505545\n",
      "Iteration 371, loss = 0.31496269\n",
      "Iteration 372, loss = 0.31483747\n",
      "Iteration 373, loss = 0.31475152\n",
      "Iteration 374, loss = 0.31461138\n",
      "Iteration 375, loss = 0.31450806\n",
      "Iteration 376, loss = 0.31439744\n",
      "Iteration 377, loss = 0.31430688\n",
      "Iteration 378, loss = 0.31416803\n",
      "Iteration 379, loss = 0.31407745\n",
      "Iteration 380, loss = 0.31395236\n",
      "Iteration 381, loss = 0.31384738\n",
      "Iteration 382, loss = 0.31372750\n",
      "Iteration 383, loss = 0.31363484\n",
      "Iteration 384, loss = 0.31351790\n",
      "Iteration 385, loss = 0.31340906\n",
      "Iteration 386, loss = 0.31329494\n",
      "Iteration 387, loss = 0.31324445\n",
      "Iteration 388, loss = 0.31309180\n",
      "Iteration 389, loss = 0.31299904\n",
      "Iteration 390, loss = 0.31289916\n",
      "Iteration 391, loss = 0.31276534\n",
      "Iteration 392, loss = 0.31267586\n",
      "Iteration 393, loss = 0.31255975\n",
      "Iteration 394, loss = 0.31247362\n",
      "Iteration 395, loss = 0.31235130\n",
      "Iteration 396, loss = 0.31225801\n",
      "Iteration 397, loss = 0.31213693\n",
      "Iteration 398, loss = 0.31205829\n",
      "Iteration 399, loss = 0.31193188\n",
      "Iteration 400, loss = 0.31184668\n",
      "Iteration 401, loss = 0.31173869\n",
      "Iteration 402, loss = 0.31164701\n",
      "Iteration 403, loss = 0.31152393\n",
      "Iteration 404, loss = 0.31142848\n",
      "Iteration 405, loss = 0.31132310\n",
      "Iteration 406, loss = 0.31124320\n",
      "Iteration 407, loss = 0.31113366\n",
      "Iteration 408, loss = 0.31102447\n",
      "Iteration 409, loss = 0.31091915\n",
      "Iteration 410, loss = 0.31082371\n",
      "Iteration 411, loss = 0.31071457\n",
      "Iteration 412, loss = 0.31062064\n",
      "Iteration 413, loss = 0.31054046\n",
      "Iteration 414, loss = 0.31041293\n",
      "Iteration 415, loss = 0.31032631\n",
      "Iteration 416, loss = 0.31022208\n",
      "Iteration 417, loss = 0.31010883\n",
      "Iteration 418, loss = 0.31002538\n",
      "Iteration 419, loss = 0.30992211\n",
      "Iteration 420, loss = 0.30982127\n",
      "Iteration 421, loss = 0.30971552\n",
      "Iteration 422, loss = 0.30961374\n",
      "Iteration 423, loss = 0.30956129\n",
      "Iteration 424, loss = 0.30942735\n",
      "Iteration 425, loss = 0.30933893\n",
      "Iteration 426, loss = 0.30922922\n",
      "Iteration 427, loss = 0.30911624\n",
      "Iteration 428, loss = 0.30902469\n",
      "Iteration 429, loss = 0.30892679\n",
      "Iteration 430, loss = 0.30883640\n",
      "Iteration 431, loss = 0.30871736\n",
      "Iteration 432, loss = 0.30862376\n",
      "Iteration 433, loss = 0.30852402\n",
      "Iteration 434, loss = 0.30842269\n",
      "Iteration 435, loss = 0.30832658\n",
      "Iteration 436, loss = 0.30821139\n",
      "Iteration 437, loss = 0.30811473\n",
      "Iteration 438, loss = 0.30803475\n",
      "Iteration 439, loss = 0.30791389\n",
      "Iteration 440, loss = 0.30783239\n",
      "Iteration 441, loss = 0.30773722\n",
      "Iteration 442, loss = 0.30761387\n",
      "Iteration 443, loss = 0.30752275\n",
      "Iteration 444, loss = 0.30741052\n",
      "Iteration 445, loss = 0.30731873\n",
      "Iteration 446, loss = 0.30723797\n",
      "Iteration 447, loss = 0.30712271\n",
      "Iteration 448, loss = 0.30702328\n",
      "Iteration 449, loss = 0.30693664\n",
      "Iteration 450, loss = 0.30682954\n",
      "Iteration 451, loss = 0.30674143\n",
      "Iteration 452, loss = 0.30667358\n",
      "Iteration 453, loss = 0.30654709\n",
      "Iteration 454, loss = 0.30644365\n",
      "Iteration 455, loss = 0.30634320\n",
      "Iteration 456, loss = 0.30625536\n",
      "Iteration 457, loss = 0.30615555\n",
      "Iteration 458, loss = 0.30606183\n",
      "Iteration 459, loss = 0.30596410\n",
      "Iteration 460, loss = 0.30585931\n",
      "Iteration 461, loss = 0.30576259\n",
      "Iteration 462, loss = 0.30568596\n",
      "Iteration 463, loss = 0.30560648\n",
      "Iteration 464, loss = 0.30549061\n",
      "Iteration 465, loss = 0.30539133\n",
      "Iteration 466, loss = 0.30529416\n",
      "Iteration 467, loss = 0.30520011\n",
      "Iteration 468, loss = 0.30510521\n",
      "Iteration 469, loss = 0.30499849\n",
      "Iteration 470, loss = 0.30490730\n",
      "Iteration 471, loss = 0.30482199\n",
      "Iteration 472, loss = 0.30472010\n",
      "Iteration 473, loss = 0.30461742\n",
      "Iteration 474, loss = 0.30452902\n",
      "Iteration 475, loss = 0.30444800\n",
      "Iteration 476, loss = 0.30434642\n",
      "Iteration 477, loss = 0.30424230\n",
      "Iteration 478, loss = 0.30415014\n",
      "Iteration 479, loss = 0.30405943\n",
      "Iteration 480, loss = 0.30397387\n",
      "Iteration 481, loss = 0.30386685\n",
      "Iteration 482, loss = 0.30378639\n",
      "Iteration 483, loss = 0.30370651\n",
      "Iteration 484, loss = 0.30359798\n",
      "Iteration 485, loss = 0.30350752\n",
      "Iteration 486, loss = 0.30341527\n",
      "Iteration 487, loss = 0.30333587\n",
      "Iteration 488, loss = 0.30322964\n",
      "Iteration 489, loss = 0.30313093\n",
      "Iteration 490, loss = 0.30304644\n",
      "Iteration 491, loss = 0.30296777\n",
      "Iteration 492, loss = 0.30286254\n",
      "Iteration 493, loss = 0.30280222\n",
      "Iteration 494, loss = 0.30267863\n",
      "Iteration 495, loss = 0.30260357\n",
      "Iteration 496, loss = 0.30249246\n",
      "Iteration 497, loss = 0.30243658\n",
      "Iteration 498, loss = 0.30231967\n",
      "Iteration 499, loss = 0.30223256\n",
      "Iteration 500, loss = 0.30213238\n",
      "Iteration 501, loss = 0.30204376\n",
      "Iteration 502, loss = 0.30195886\n",
      "Iteration 503, loss = 0.30187051\n",
      "Iteration 504, loss = 0.30177790\n",
      "Iteration 505, loss = 0.30168278\n",
      "Iteration 506, loss = 0.30158865\n",
      "Iteration 507, loss = 0.30152619\n",
      "Iteration 508, loss = 0.30141358\n",
      "Iteration 509, loss = 0.30132767\n",
      "Iteration 510, loss = 0.30123261\n",
      "Iteration 511, loss = 0.30114821\n",
      "Iteration 512, loss = 0.30107451\n",
      "Iteration 513, loss = 0.30096863\n",
      "Iteration 514, loss = 0.30087551\n",
      "Iteration 515, loss = 0.30080746\n",
      "Iteration 516, loss = 0.30070452\n",
      "Iteration 517, loss = 0.30061115\n",
      "Iteration 518, loss = 0.30052985\n",
      "Iteration 519, loss = 0.30043805\n",
      "Iteration 520, loss = 0.30034970\n",
      "Iteration 521, loss = 0.30025722\n",
      "Iteration 522, loss = 0.30016579\n",
      "Iteration 523, loss = 0.30008936\n",
      "Iteration 524, loss = 0.29999877\n",
      "Iteration 525, loss = 0.29991158\n",
      "Iteration 526, loss = 0.29981425\n",
      "Iteration 527, loss = 0.29972961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73065418\n",
      "Iteration 2, loss = 0.72825233\n",
      "Iteration 3, loss = 0.72469370\n",
      "Iteration 4, loss = 0.72024617\n",
      "Iteration 5, loss = 0.71513752\n",
      "Iteration 6, loss = 0.70981379\n",
      "Iteration 7, loss = 0.70417272\n",
      "Iteration 8, loss = 0.69856380\n",
      "Iteration 9, loss = 0.69282253\n",
      "Iteration 10, loss = 0.68719549\n",
      "Iteration 11, loss = 0.68152281\n",
      "Iteration 12, loss = 0.67594812\n",
      "Iteration 13, loss = 0.67045055\n",
      "Iteration 14, loss = 0.66503802\n",
      "Iteration 15, loss = 0.65977091\n",
      "Iteration 16, loss = 0.65460402\n",
      "Iteration 17, loss = 0.64943525\n",
      "Iteration 18, loss = 0.64454548\n",
      "Iteration 19, loss = 0.63952724\n",
      "Iteration 20, loss = 0.63468995\n",
      "Iteration 21, loss = 0.62993049\n",
      "Iteration 22, loss = 0.62520940\n",
      "Iteration 23, loss = 0.62068638\n",
      "Iteration 24, loss = 0.61609274\n",
      "Iteration 25, loss = 0.61163959\n",
      "Iteration 26, loss = 0.60717669\n",
      "Iteration 27, loss = 0.60293355\n",
      "Iteration 28, loss = 0.59852796\n",
      "Iteration 29, loss = 0.59429916\n",
      "Iteration 30, loss = 0.59008647\n",
      "Iteration 31, loss = 0.58600365\n",
      "Iteration 32, loss = 0.58189108\n",
      "Iteration 33, loss = 0.57788252\n",
      "Iteration 34, loss = 0.57383720\n",
      "Iteration 35, loss = 0.57004758\n",
      "Iteration 36, loss = 0.56618773\n",
      "Iteration 37, loss = 0.56246398\n",
      "Iteration 38, loss = 0.55862609\n",
      "Iteration 39, loss = 0.55488892\n",
      "Iteration 40, loss = 0.55125663\n",
      "Iteration 41, loss = 0.54766289\n",
      "Iteration 42, loss = 0.54410871\n",
      "Iteration 43, loss = 0.54061096\n",
      "Iteration 44, loss = 0.53716438\n",
      "Iteration 45, loss = 0.53371857\n",
      "Iteration 46, loss = 0.53041344\n",
      "Iteration 47, loss = 0.52704588\n",
      "Iteration 48, loss = 0.52381142\n",
      "Iteration 49, loss = 0.52052296\n",
      "Iteration 50, loss = 0.51746867\n",
      "Iteration 51, loss = 0.51434986\n",
      "Iteration 52, loss = 0.51121693\n",
      "Iteration 53, loss = 0.50821639\n",
      "Iteration 54, loss = 0.50529428\n",
      "Iteration 55, loss = 0.50231672\n",
      "Iteration 56, loss = 0.49938826\n",
      "Iteration 57, loss = 0.49657416\n",
      "Iteration 58, loss = 0.49383995\n",
      "Iteration 59, loss = 0.49109283\n",
      "Iteration 60, loss = 0.48833462\n",
      "Iteration 61, loss = 0.48564915\n",
      "Iteration 62, loss = 0.48309182\n",
      "Iteration 63, loss = 0.48051640\n",
      "Iteration 64, loss = 0.47797080\n",
      "Iteration 65, loss = 0.47553086\n",
      "Iteration 66, loss = 0.47310477\n",
      "Iteration 67, loss = 0.47075463\n",
      "Iteration 68, loss = 0.46840416\n",
      "Iteration 69, loss = 0.46608307\n",
      "Iteration 70, loss = 0.46380138\n",
      "Iteration 71, loss = 0.46165603\n",
      "Iteration 72, loss = 0.45950112\n",
      "Iteration 73, loss = 0.45733138\n",
      "Iteration 74, loss = 0.45532189\n",
      "Iteration 75, loss = 0.45323167\n",
      "Iteration 76, loss = 0.45124626\n",
      "Iteration 77, loss = 0.44929852\n",
      "Iteration 78, loss = 0.44740084\n",
      "Iteration 79, loss = 0.44553194\n",
      "Iteration 80, loss = 0.44366906\n",
      "Iteration 81, loss = 0.44191685\n",
      "Iteration 82, loss = 0.44007426\n",
      "Iteration 83, loss = 0.43837968\n",
      "Iteration 84, loss = 0.43669779\n",
      "Iteration 85, loss = 0.43503321\n",
      "Iteration 86, loss = 0.43344781\n",
      "Iteration 87, loss = 0.43180368\n",
      "Iteration 88, loss = 0.43029105\n",
      "Iteration 89, loss = 0.42881872\n",
      "Iteration 90, loss = 0.42732937\n",
      "Iteration 91, loss = 0.42578458\n",
      "Iteration 92, loss = 0.42438631\n",
      "Iteration 93, loss = 0.42297190\n",
      "Iteration 94, loss = 0.42164459\n",
      "Iteration 95, loss = 0.42029130\n",
      "Iteration 96, loss = 0.41905596\n",
      "Iteration 97, loss = 0.41779197\n",
      "Iteration 98, loss = 0.41653557\n",
      "Iteration 99, loss = 0.41523521\n",
      "Iteration 100, loss = 0.41410162\n",
      "Iteration 101, loss = 0.41292040\n",
      "Iteration 102, loss = 0.41176117\n",
      "Iteration 103, loss = 0.41068680\n",
      "Iteration 104, loss = 0.40956433\n",
      "Iteration 105, loss = 0.40849554\n",
      "Iteration 106, loss = 0.40750930\n",
      "Iteration 107, loss = 0.40646540\n",
      "Iteration 108, loss = 0.40543898\n",
      "Iteration 109, loss = 0.40445019\n",
      "Iteration 110, loss = 0.40352722\n",
      "Iteration 111, loss = 0.40253976\n",
      "Iteration 112, loss = 0.40165101\n",
      "Iteration 113, loss = 0.40076667\n",
      "Iteration 114, loss = 0.39986662\n",
      "Iteration 115, loss = 0.39896619\n",
      "Iteration 116, loss = 0.39819316\n",
      "Iteration 117, loss = 0.39736466\n",
      "Iteration 118, loss = 0.39648881\n",
      "Iteration 119, loss = 0.39573062\n",
      "Iteration 120, loss = 0.39492848\n",
      "Iteration 121, loss = 0.39416548\n",
      "Iteration 122, loss = 0.39342864\n",
      "Iteration 123, loss = 0.39268027\n",
      "Iteration 124, loss = 0.39198716\n",
      "Iteration 125, loss = 0.39128747\n",
      "Iteration 126, loss = 0.39055999\n",
      "Iteration 127, loss = 0.38990521\n",
      "Iteration 128, loss = 0.38924814\n",
      "Iteration 129, loss = 0.38856491\n",
      "Iteration 130, loss = 0.38793294\n",
      "Iteration 131, loss = 0.38730090\n",
      "Iteration 132, loss = 0.38666675\n",
      "Iteration 133, loss = 0.38607269\n",
      "Iteration 134, loss = 0.38549661\n",
      "Iteration 135, loss = 0.38489434\n",
      "Iteration 136, loss = 0.38429425\n",
      "Iteration 137, loss = 0.38373820\n",
      "Iteration 138, loss = 0.38317692\n",
      "Iteration 139, loss = 0.38262170\n",
      "Iteration 140, loss = 0.38209570\n",
      "Iteration 141, loss = 0.38158472\n",
      "Iteration 142, loss = 0.38106071\n",
      "Iteration 143, loss = 0.38051884\n",
      "Iteration 94, loss = 0.42650601\n",
      "Iteration 95, loss = 0.42532546\n",
      "Iteration 96, loss = 0.42418201\n",
      "Iteration 97, loss = 0.42312424\n",
      "Iteration 98, loss = 0.42205179\n",
      "Iteration 99, loss = 0.42098947\n",
      "Iteration 100, loss = 0.41995137\n",
      "Iteration 101, loss = 0.41898257\n",
      "Iteration 102, loss = 0.41796662\n",
      "Iteration 103, loss = 0.41702025\n",
      "Iteration 104, loss = 0.41606278\n",
      "Iteration 105, loss = 0.41518681\n",
      "Iteration 106, loss = 0.41419684\n",
      "Iteration 107, loss = 0.41334886\n",
      "Iteration 108, loss = 0.41251584\n",
      "Iteration 109, loss = 0.41160256\n",
      "Iteration 110, loss = 0.41076120\n",
      "Iteration 111, loss = 0.40996168\n",
      "Iteration 112, loss = 0.40914173\n",
      "Iteration 113, loss = 0.40831094\n",
      "Iteration 114, loss = 0.40755846\n",
      "Iteration 115, loss = 0.40680554\n",
      "Iteration 116, loss = 0.40606255\n",
      "Iteration 117, loss = 0.40534948\n",
      "Iteration 118, loss = 0.40460511\n",
      "Iteration 119, loss = 0.40384202\n",
      "Iteration 120, loss = 0.40318158\n",
      "Iteration 121, loss = 0.40245023\n",
      "Iteration 122, loss = 0.40177150\n",
      "Iteration 123, loss = 0.40112237\n",
      "Iteration 124, loss = 0.40048357\n",
      "Iteration 125, loss = 0.39984611\n",
      "Iteration 126, loss = 0.39920715\n",
      "Iteration 127, loss = 0.39854484\n",
      "Iteration 128, loss = 0.39796265\n",
      "Iteration 129, loss = 0.39732993\n",
      "Iteration 130, loss = 0.39673737\n",
      "Iteration 131, loss = 0.39616585\n",
      "Iteration 132, loss = 0.39558080\n",
      "Iteration 133, loss = 0.39499350\n",
      "Iteration 134, loss = 0.39444837\n",
      "Iteration 135, loss = 0.39393678\n",
      "Iteration 136, loss = 0.39334327\n",
      "Iteration 137, loss = 0.39283040\n",
      "Iteration 138, loss = 0.39225744\n",
      "Iteration 139, loss = 0.39173844\n",
      "Iteration 140, loss = 0.39123028\n",
      "Iteration 141, loss = 0.39071632\n",
      "Iteration 142, loss = 0.39023820\n",
      "Iteration 143, loss = 0.38970519\n",
      "Iteration 144, loss = 0.38925149\n",
      "Iteration 145, loss = 0.38876325\n",
      "Iteration 146, loss = 0.38827867\n",
      "Iteration 147, loss = 0.38779490\n",
      "Iteration 148, loss = 0.38734057\n",
      "Iteration 149, loss = 0.38685801\n",
      "Iteration 150, loss = 0.38640912\n",
      "Iteration 151, loss = 0.38594998\n",
      "Iteration 152, loss = 0.38547658\n",
      "Iteration 153, loss = 0.38506374\n",
      "Iteration 154, loss = 0.38463109\n",
      "Iteration 155, loss = 0.38419846\n",
      "Iteration 156, loss = 0.38377707\n",
      "Iteration 157, loss = 0.38335047\n",
      "Iteration 158, loss = 0.38295049\n",
      "Iteration 159, loss = 0.38253687\n",
      "Iteration 160, loss = 0.38211588\n",
      "Iteration 161, loss = 0.38173144\n",
      "Iteration 162, loss = 0.38131355\n",
      "Iteration 163, loss = 0.38092514\n",
      "Iteration 164, loss = 0.38054035\n",
      "Iteration 165, loss = 0.38011398\n",
      "Iteration 166, loss = 0.37980564\n",
      "Iteration 167, loss = 0.37941645\n",
      "Iteration 168, loss = 0.37903330\n",
      "Iteration 169, loss = 0.37864815\n",
      "Iteration 170, loss = 0.37825499\n",
      "Iteration 171, loss = 0.37790338\n",
      "Iteration 172, loss = 0.37752919\n",
      "Iteration 173, loss = 0.37717716\n",
      "Iteration 174, loss = 0.37685511\n",
      "Iteration 175, loss = 0.37647083\n",
      "Iteration 176, loss = 0.37612726\n",
      "Iteration 177, loss = 0.37577460\n",
      "Iteration 178, loss = 0.37543923\n",
      "Iteration 179, loss = 0.37510939\n",
      "Iteration 180, loss = 0.37477113\n",
      "Iteration 181, loss = 0.37442234\n",
      "Iteration 182, loss = 0.37406377\n",
      "Iteration 183, loss = 0.37375227\n",
      "Iteration 184, loss = 0.37343690\n",
      "Iteration 185, loss = 0.37309175\n",
      "Iteration 186, loss = 0.37279958\n",
      "Iteration 187, loss = 0.37245003\n",
      "Iteration 188, loss = 0.37216824\n",
      "Iteration 189, loss = 0.37182954\n",
      "Iteration 190, loss = 0.37151707\n",
      "Iteration 191, loss = 0.37118922\n",
      "Iteration 192, loss = 0.37091098\n",
      "Iteration 193, loss = 0.37059027\n",
      "Iteration 194, loss = 0.37033458\n",
      "Iteration 195, loss = 0.36997010\n",
      "Iteration 196, loss = 0.36968304\n",
      "Iteration 197, loss = 0.36938337\n",
      "Iteration 198, loss = 0.36908844\n",
      "Iteration 199, loss = 0.36880385\n",
      "Iteration 200, loss = 0.36850002\n",
      "Iteration 201, loss = 0.36822364\n",
      "Iteration 202, loss = 0.36795091\n",
      "Iteration 203, loss = 0.36765784\n",
      "Iteration 204, loss = 0.36736955\n",
      "Iteration 205, loss = 0.36708067\n",
      "Iteration 206, loss = 0.36681715\n",
      "Iteration 207, loss = 0.36654126\n",
      "Iteration 208, loss = 0.36625928\n",
      "Iteration 209, loss = 0.36597496\n",
      "Iteration 210, loss = 0.36570384\n",
      "Iteration 211, loss = 0.36547179\n",
      "Iteration 212, loss = 0.36516665\n",
      "Iteration 213, loss = 0.36491757\n",
      "Iteration 214, loss = 0.36467726\n",
      "Iteration 215, loss = 0.36437096\n",
      "Iteration 216, loss = 0.36415334\n",
      "Iteration 217, loss = 0.36388393\n",
      "Iteration 218, loss = 0.36361345\n",
      "Iteration 219, loss = 0.36335581\n",
      "Iteration 220, loss = 0.36311176\n",
      "Iteration 221, loss = 0.36285009\n",
      "Iteration 222, loss = 0.36260669\n",
      "Iteration 223, loss = 0.36235614\n",
      "Iteration 224, loss = 0.36209879\n",
      "Iteration 225, loss = 0.36187127\n",
      "Iteration 226, loss = 0.36161597\n",
      "Iteration 227, loss = 0.36136649\n",
      "Iteration 228, loss = 0.36114460\n",
      "Iteration 229, loss = 0.36089538\n",
      "Iteration 230, loss = 0.36065588\n",
      "Iteration 231, loss = 0.36041572\n",
      "Iteration 232, loss = 0.36019476\n",
      "Iteration 233, loss = 0.35995823\n",
      "Iteration 234, loss = 0.35971401\n",
      "Iteration 235, loss = 0.35949762\n",
      "Iteration 236, loss = 0.35927170\n",
      "Iteration 237, loss = 0.35903752\n",
      "Iteration 238, loss = 0.35880676\n",
      "Iteration 239, loss = 0.35856917\n",
      "Iteration 240, loss = 0.35835111\n",
      "Iteration 241, loss = 0.35811577\n",
      "Iteration 242, loss = 0.35790082\n",
      "Iteration 243, loss = 0.35769277\n",
      "Iteration 244, loss = 0.35747127\n",
      "Iteration 245, loss = 0.35727780\n",
      "Iteration 246, loss = 0.35704485\n",
      "Iteration 247, loss = 0.35681657\n",
      "Iteration 248, loss = 0.35660543\n",
      "Iteration 249, loss = 0.35637565\n",
      "Iteration 250, loss = 0.35616786\n",
      "Iteration 251, loss = 0.35597311\n",
      "Iteration 252, loss = 0.35574193\n",
      "Iteration 253, loss = 0.35553325\n",
      "Iteration 254, loss = 0.35530047\n",
      "Iteration 255, loss = 0.35510276\n",
      "Iteration 256, loss = 0.35490723\n",
      "Iteration 257, loss = 0.35469814\n",
      "Iteration 258, loss = 0.35448844\n",
      "Iteration 259, loss = 0.35428991\n",
      "Iteration 260, loss = 0.35407213\n",
      "Iteration 261, loss = 0.35386751\n",
      "Iteration 262, loss = 0.35370586\n",
      "Iteration 263, loss = 0.35346538\n",
      "Iteration 264, loss = 0.35326254\n",
      "Iteration 265, loss = 0.35306748\n",
      "Iteration 266, loss = 0.35287888\n",
      "Iteration 267, loss = 0.35269220\n",
      "Iteration 268, loss = 0.35249210\n",
      "Iteration 269, loss = 0.35230997\n",
      "Iteration 270, loss = 0.35208415\n",
      "Iteration 271, loss = 0.35190563\n",
      "Iteration 272, loss = 0.35169687\n",
      "Iteration 273, loss = 0.35152757\n",
      "Iteration 274, loss = 0.35135579\n",
      "Iteration 275, loss = 0.35114041\n",
      "Iteration 276, loss = 0.35097043\n",
      "Iteration 277, loss = 0.35076885\n",
      "Iteration 278, loss = 0.35056566\n",
      "Iteration 279, loss = 0.35040634\n",
      "Iteration 280, loss = 0.35023434\n",
      "Iteration 281, loss = 0.35003333\n",
      "Iteration 282, loss = 0.34986629\n",
      "Iteration 283, loss = 0.34967113\n",
      "Iteration 284, loss = 0.34948223\n",
      "Iteration 285, loss = 0.34931203\n",
      "Iteration 286, loss = 0.34913360\n",
      "Iteration 287, loss = 0.34895908\n",
      "Iteration 288, loss = 0.34878129\n",
      "Iteration 289, loss = 0.34860902\n",
      "Iteration 290, loss = 0.34843945\n",
      "Iteration 291, loss = 0.34826800\n",
      "Iteration 292, loss = 0.34809441\n",
      "Iteration 293, loss = 0.34790020\n",
      "Iteration 294, loss = 0.34774125\n",
      "Iteration 295, loss = 0.34759732\n",
      "Iteration 296, loss = 0.34740107\n",
      "Iteration 297, loss = 0.34721023\n",
      "Iteration 298, loss = 0.34707404\n",
      "Iteration 299, loss = 0.34688008\n",
      "Iteration 300, loss = 0.34670937\n",
      "Iteration 301, loss = 0.34653969\n",
      "Iteration 302, loss = 0.34638009\n",
      "Iteration 303, loss = 0.34622769\n",
      "Iteration 304, loss = 0.34605068\n",
      "Iteration 305, loss = 0.34587929\n",
      "Iteration 306, loss = 0.34571712\n",
      "Iteration 307, loss = 0.34556894\n",
      "Iteration 308, loss = 0.34539849\n",
      "Iteration 309, loss = 0.34523436\n",
      "Iteration 310, loss = 0.34506667\n",
      "Iteration 311, loss = 0.34491474\n",
      "Iteration 312, loss = 0.34475773\n",
      "Iteration 313, loss = 0.34458025\n",
      "Iteration 314, loss = 0.34443115\n",
      "Iteration 315, loss = 0.34428109\n",
      "Iteration 316, loss = 0.34411692\n",
      "Iteration 317, loss = 0.34397279\n",
      "Iteration 318, loss = 0.34380491\n",
      "Iteration 319, loss = 0.34364278\n",
      "Iteration 320, loss = 0.34349482\n",
      "Iteration 321, loss = 0.34332763\n",
      "Iteration 322, loss = 0.34319360\n",
      "Iteration 323, loss = 0.34302051\n",
      "Iteration 324, loss = 0.34286534\n",
      "Iteration 325, loss = 0.34270850\n",
      "Iteration 326, loss = 0.34256037\n",
      "Iteration 327, loss = 0.34241800\n",
      "Iteration 328, loss = 0.34225316\n",
      "Iteration 329, loss = 0.34211352\n",
      "Iteration 330, loss = 0.34195711\n",
      "Iteration 331, loss = 0.34182819\n",
      "Iteration 332, loss = 0.34166423\n",
      "Iteration 333, loss = 0.34150646\n",
      "Iteration 334, loss = 0.34137296\n",
      "Iteration 335, loss = 0.34121575\n",
      "Iteration 336, loss = 0.34106076\n",
      "Iteration 337, loss = 0.34092986\n",
      "Iteration 338, loss = 0.34077233\n",
      "Iteration 339, loss = 0.34062943\n",
      "Iteration 340, loss = 0.34048209\n",
      "Iteration 341, loss = 0.34037112\n",
      "Iteration 342, loss = 0.34022050\n",
      "Iteration 343, loss = 0.34006328\n",
      "Iteration 344, loss = 0.33992879\n",
      "Iteration 345, loss = 0.33976872\n",
      "Iteration 346, loss = 0.33962138\n",
      "Iteration 347, loss = 0.33950406\n",
      "Iteration 348, loss = 0.33933932\n",
      "Iteration 349, loss = 0.33921936\n",
      "Iteration 350, loss = 0.33905882\n",
      "Iteration 351, loss = 0.33894311\n",
      "Iteration 352, loss = 0.33877829\n",
      "Iteration 353, loss = 0.33864489\n",
      "Iteration 354, loss = 0.33850367\n",
      "Iteration 355, loss = 0.33837836\n",
      "Iteration 356, loss = 0.33822659\n",
      "Iteration 357, loss = 0.33808988\n",
      "Iteration 358, loss = 0.33796279\n",
      "Iteration 359, loss = 0.33782634\n",
      "Iteration 360, loss = 0.33768304\n",
      "Iteration 361, loss = 0.33754884\n",
      "Iteration 362, loss = 0.33740445\n",
      "Iteration 363, loss = 0.33729827\n",
      "Iteration 364, loss = 0.33715486\n",
      "Iteration 365, loss = 0.33701751\n",
      "Iteration 366, loss = 0.33687137\n",
      "Iteration 367, loss = 0.33673980\n",
      "Iteration 368, loss = 0.33661034\n",
      "Iteration 369, loss = 0.33647199\n",
      "Iteration 370, loss = 0.33635948\n",
      "Iteration 371, loss = 0.33622996\n",
      "Iteration 372, loss = 0.33608162\n",
      "Iteration 373, loss = 0.33596410\n",
      "Iteration 374, loss = 0.33582884\n",
      "Iteration 375, loss = 0.33575873\n",
      "Iteration 376, loss = 0.33557218\n",
      "Iteration 377, loss = 0.33543900\n",
      "Iteration 378, loss = 0.33529900\n",
      "Iteration 379, loss = 0.33517272\n",
      "Iteration 380, loss = 0.33507584\n",
      "Iteration 381, loss = 0.33491314\n",
      "Iteration 382, loss = 0.33478713\n",
      "Iteration 383, loss = 0.33465702\n",
      "Iteration 384, loss = 0.33451369\n",
      "Iteration 385, loss = 0.33439468\n",
      "Iteration 386, loss = 0.33428812\n",
      "Iteration 387, loss = 0.33416794\n",
      "Iteration 388, loss = 0.33401097\n",
      "Iteration 389, loss = 0.33389252\n",
      "Iteration 390, loss = 0.33377278\n",
      "Iteration 391, loss = 0.33364082\n",
      "Iteration 392, loss = 0.33351120\n",
      "Iteration 393, loss = 0.33337815\n",
      "Iteration 394, loss = 0.33325016\n",
      "Iteration 395, loss = 0.33313551\n",
      "Iteration 396, loss = 0.33301362\n",
      "Iteration 397, loss = 0.33287631\n",
      "Iteration 398, loss = 0.33278525\n",
      "Iteration 399, loss = 0.33264283\n",
      "Iteration 400, loss = 0.33251842\n",
      "Iteration 401, loss = 0.33237845\n",
      "Iteration 402, loss = 0.33225760\n",
      "Iteration 403, loss = 0.33214764\n",
      "Iteration 404, loss = 0.33201538\n",
      "Iteration 405, loss = 0.33190392\n",
      "Iteration 406, loss = 0.33178232\n",
      "Iteration 407, loss = 0.33165151\n",
      "Iteration 408, loss = 0.33154007\n",
      "Iteration 409, loss = 0.33143019\n",
      "Iteration 410, loss = 0.33129698\n",
      "Iteration 411, loss = 0.33117370\n",
      "Iteration 412, loss = 0.33104336\n",
      "Iteration 413, loss = 0.33092755\n",
      "Iteration 414, loss = 0.33085193\n",
      "Iteration 415, loss = 0.33068933\n",
      "Iteration 416, loss = 0.33057546\n",
      "Iteration 417, loss = 0.33046129\n",
      "Iteration 418, loss = 0.33033579\n",
      "Iteration 419, loss = 0.33020793\n",
      "Iteration 420, loss = 0.33009131\n",
      "Iteration 421, loss = 0.32997816\n",
      "Iteration 422, loss = 0.32985988\n",
      "Iteration 423, loss = 0.32973343\n",
      "Iteration 424, loss = 0.32962062\n",
      "Iteration 425, loss = 0.32951258\n",
      "Iteration 426, loss = 0.32939195\n",
      "Iteration 427, loss = 0.32926370\n",
      "Iteration 428, loss = 0.32919086\n",
      "Iteration 429, loss = 0.32905072\n",
      "Iteration 430, loss = 0.32891454\n",
      "Iteration 431, loss = 0.32880305\n",
      "Iteration 432, loss = 0.32870002\n",
      "Iteration 433, loss = 0.32857376\n",
      "Iteration 434, loss = 0.32849424\n",
      "Iteration 435, loss = 0.32834765\n",
      "Iteration 436, loss = 0.32823834\n",
      "Iteration 437, loss = 0.32812752\n",
      "Iteration 438, loss = 0.32801010\n",
      "Iteration 439, loss = 0.32789795\n",
      "Iteration 440, loss = 0.32777995\n",
      "Iteration 441, loss = 0.32766663\n",
      "Iteration 442, loss = 0.32755220\n",
      "Iteration 443, loss = 0.32745223\n",
      "Iteration 444, loss = 0.32732780\n",
      "Iteration 445, loss = 0.32722260\n",
      "Iteration 446, loss = 0.32710840\n",
      "Iteration 447, loss = 0.32699160\n",
      "Iteration 448, loss = 0.32688223\n",
      "Iteration 449, loss = 0.32677111\n",
      "Iteration 450, loss = 0.32667153\n",
      "Iteration 451, loss = 0.32660642\n",
      "Iteration 452, loss = 0.32646655\n",
      "Iteration 453, loss = 0.32633400\n",
      "Iteration 454, loss = 0.32621953\n",
      "Iteration 455, loss = 0.32611565\n",
      "Iteration 456, loss = 0.32602246\n",
      "Iteration 457, loss = 0.32590236\n",
      "Iteration 458, loss = 0.32579311\n",
      "Iteration 459, loss = 0.32568224\n",
      "Iteration 460, loss = 0.32557643\n",
      "Iteration 461, loss = 0.32545582\n",
      "Iteration 462, loss = 0.32537175\n",
      "Iteration 463, loss = 0.32524852\n",
      "Iteration 464, loss = 0.32514167\n",
      "Iteration 465, loss = 0.32502957\n",
      "Iteration 466, loss = 0.32492830\n",
      "Iteration 467, loss = 0.32481935\n",
      "Iteration 468, loss = 0.32470893\n",
      "Iteration 469, loss = 0.32462246\n",
      "Iteration 470, loss = 0.32451334\n",
      "Iteration 471, loss = 0.32440745\n",
      "Iteration 472, loss = 0.32428965\n",
      "Iteration 473, loss = 0.32419671\n",
      "Iteration 474, loss = 0.32409217\n",
      "Iteration 475, loss = 0.32399720\n",
      "Iteration 476, loss = 0.32386536\n",
      "Iteration 477, loss = 0.32377234\n",
      "Iteration 478, loss = 0.32366961\n",
      "Iteration 479, loss = 0.32356225\n",
      "Iteration 480, loss = 0.32347416\n",
      "Iteration 481, loss = 0.32337722\n",
      "Iteration 482, loss = 0.32325970\n",
      "Iteration 483, loss = 0.32316740\n",
      "Iteration 484, loss = 0.32305665\n",
      "Iteration 485, loss = 0.32294801\n",
      "Iteration 486, loss = 0.32285101\n",
      "Iteration 487, loss = 0.32276638\n",
      "Iteration 488, loss = 0.32267390\n",
      "Iteration 489, loss = 0.32256002\n",
      "Iteration 490, loss = 0.32245148\n",
      "Iteration 491, loss = 0.32236784\n",
      "Iteration 492, loss = 0.32225681\n",
      "Iteration 493, loss = 0.32215233\n",
      "Iteration 494, loss = 0.32206361\n",
      "Iteration 495, loss = 0.32195581\n",
      "Iteration 496, loss = 0.32186914\n",
      "Iteration 497, loss = 0.32177777\n",
      "Iteration 498, loss = 0.32166525\n",
      "Iteration 499, loss = 0.32156927\n",
      "Iteration 500, loss = 0.32146710\n",
      "Iteration 501, loss = 0.32138932\n",
      "Iteration 502, loss = 0.32127442\n",
      "Iteration 503, loss = 0.32118945\n",
      "Iteration 504, loss = 0.32106875\n",
      "Iteration 505, loss = 0.32098571\n",
      "Iteration 506, loss = 0.32090348\n",
      "Iteration 507, loss = 0.32077920\n",
      "Iteration 508, loss = 0.32069068\n",
      "Iteration 509, loss = 0.32059500\n",
      "Iteration 510, loss = 0.32049474\n",
      "Iteration 511, loss = 0.32039077\n",
      "Iteration 512, loss = 0.32031136\n",
      "Iteration 513, loss = 0.32020482\n",
      "Iteration 514, loss = 0.32011074\n",
      "Iteration 515, loss = 0.32000722\n",
      "Iteration 516, loss = 0.31991866\n",
      "Iteration 517, loss = 0.31981342\n",
      "Iteration 518, loss = 0.31972447\n",
      "Iteration 519, loss = 0.31962726\n",
      "Iteration 520, loss = 0.31953838\n",
      "Iteration 521, loss = 0.31942992\n",
      "Iteration 522, loss = 0.31934619\n",
      "Iteration 523, loss = 0.31924311\n",
      "Iteration 524, loss = 0.31916405\n",
      "Iteration 525, loss = 0.31906344\n",
      "Iteration 526, loss = 0.31896205\n",
      "Iteration 527, loss = 0.31886161\n",
      "Iteration 528, loss = 0.31877682\n",
      "Iteration 529, loss = 0.31868981\n",
      "Iteration 530, loss = 0.31859044\n",
      "Iteration 531, loss = 0.31849498\n",
      "Iteration 532, loss = 0.31840082\n",
      "Iteration 533, loss = 0.31829775\n",
      "Iteration 534, loss = 0.31824306\n",
      "Iteration 535, loss = 0.31813938\n",
      "Iteration 536, loss = 0.31803441\n",
      "Iteration 537, loss = 0.31794489\n",
      "Iteration 538, loss = 0.31783895\n",
      "Iteration 539, loss = 0.31776877\n",
      "Iteration 540, loss = 0.31766526\n",
      "Iteration 541, loss = 0.31757219\n",
      "Iteration 542, loss = 0.31747897\n",
      "Iteration 543, loss = 0.31738152\n",
      "Iteration 544, loss = 0.31730800\n",
      "Iteration 545, loss = 0.31722119\n",
      "Iteration 546, loss = 0.31710839\n",
      "Iteration 547, loss = 0.31702669\n",
      "Iteration 548, loss = 0.31695650\n",
      "Iteration 549, loss = 0.31684055\n",
      "Iteration 550, loss = 0.31675494\n",
      "Iteration 551, loss = 0.31666789\n",
      "Iteration 552, loss = 0.31659737\n",
      "Iteration 553, loss = 0.31649213\n",
      "Iteration 554, loss = 0.31639487\n",
      "Iteration 555, loss = 0.31630308\n",
      "Iteration 556, loss = 0.31622589\n",
      "Iteration 557, loss = 0.31611537\n",
      "Iteration 558, loss = 0.31604727\n",
      "Iteration 559, loss = 0.31594399\n",
      "Iteration 560, loss = 0.31586158\n",
      "Iteration 561, loss = 0.31578912\n",
      "Iteration 562, loss = 0.31569749\n",
      "Iteration 563, loss = 0.31560562\n",
      "Iteration 564, loss = 0.31549553\n",
      "Iteration 565, loss = 0.31540782\n",
      "Iteration 566, loss = 0.31532681\n",
      "Iteration 567, loss = 0.31523530\n",
      "Iteration 568, loss = 0.31514847\n",
      "Iteration 569, loss = 0.31507877\n",
      "Iteration 570, loss = 0.31498718\n",
      "Iteration 571, loss = 0.31490313\n",
      "Iteration 572, loss = 0.31479401\n",
      "Iteration 573, loss = 0.31471572\n",
      "Iteration 574, loss = 0.31462476\n",
      "Iteration 575, loss = 0.31452505\n",
      "Iteration 576, loss = 0.31444525\n",
      "Iteration 577, loss = 0.31438989\n",
      "Iteration 578, loss = 0.31426308\n",
      "Iteration 579, loss = 0.31418742\n",
      "Iteration 580, loss = 0.31409413\n",
      "Iteration 581, loss = 0.31402992\n",
      "Iteration 582, loss = 0.31392539\n",
      "Iteration 583, loss = 0.31383613\n",
      "Iteration 584, loss = 0.31377032\n",
      "Iteration 585, loss = 0.31367013\n",
      "Iteration 586, loss = 0.31357260\n",
      "Iteration 587, loss = 0.31349853\n",
      "Iteration 588, loss = 0.31340363\n",
      "Iteration 589, loss = 0.31334395\n",
      "Iteration 282, loss = 0.36998838\n",
      "Iteration 283, loss = 0.36979619\n",
      "Iteration 284, loss = 0.36963787\n",
      "Iteration 285, loss = 0.36940533\n",
      "Iteration 286, loss = 0.36922154\n",
      "Iteration 287, loss = 0.36903505\n",
      "Iteration 288, loss = 0.36886412\n",
      "Iteration 289, loss = 0.36867705\n",
      "Iteration 290, loss = 0.36848646\n",
      "Iteration 291, loss = 0.36829895\n",
      "Iteration 292, loss = 0.36812841\n",
      "Iteration 293, loss = 0.36795230\n",
      "Iteration 294, loss = 0.36779446\n",
      "Iteration 295, loss = 0.36760363\n",
      "Iteration 296, loss = 0.36741017\n",
      "Iteration 297, loss = 0.36724168\n",
      "Iteration 298, loss = 0.36707026\n",
      "Iteration 299, loss = 0.36692391\n",
      "Iteration 300, loss = 0.36671443\n",
      "Iteration 301, loss = 0.36652900\n",
      "Iteration 302, loss = 0.36637480\n",
      "Iteration 303, loss = 0.36619446\n",
      "Iteration 304, loss = 0.36601982\n",
      "Iteration 305, loss = 0.36585949\n",
      "Iteration 306, loss = 0.36567179\n",
      "Iteration 307, loss = 0.36549753\n",
      "Iteration 308, loss = 0.36544533\n",
      "Iteration 309, loss = 0.36515907\n",
      "Iteration 310, loss = 0.36500647\n",
      "Iteration 311, loss = 0.36482012\n",
      "Iteration 312, loss = 0.36464195\n",
      "Iteration 313, loss = 0.36449846\n",
      "Iteration 314, loss = 0.36431654\n",
      "Iteration 315, loss = 0.36416518\n",
      "Iteration 316, loss = 0.36400990\n",
      "Iteration 317, loss = 0.36381714\n",
      "Iteration 318, loss = 0.36365858\n",
      "Iteration 319, loss = 0.36350431\n",
      "Iteration 320, loss = 0.36331877\n",
      "Iteration 321, loss = 0.36316072\n",
      "Iteration 322, loss = 0.36304464\n",
      "Iteration 323, loss = 0.36281485\n",
      "Iteration 324, loss = 0.36266063\n",
      "Iteration 325, loss = 0.36252529\n",
      "Iteration 326, loss = 0.36234838\n",
      "Iteration 327, loss = 0.36218546\n",
      "Iteration 328, loss = 0.36199812\n",
      "Iteration 329, loss = 0.36184009\n",
      "Iteration 330, loss = 0.36167951\n",
      "Iteration 331, loss = 0.36158469\n",
      "Iteration 332, loss = 0.36137415\n",
      "Iteration 333, loss = 0.36123064\n",
      "Iteration 334, loss = 0.36104906\n",
      "Iteration 335, loss = 0.36087804\n",
      "Iteration 336, loss = 0.36072181\n",
      "Iteration 337, loss = 0.36055498\n",
      "Iteration 338, loss = 0.36043244\n",
      "Iteration 339, loss = 0.36028380\n",
      "Iteration 340, loss = 0.36010343\n",
      "Iteration 341, loss = 0.35994794\n",
      "Iteration 342, loss = 0.35977586\n",
      "Iteration 343, loss = 0.35964909\n",
      "Iteration 344, loss = 0.35946822\n",
      "Iteration 345, loss = 0.35933184\n",
      "Iteration 346, loss = 0.35914945\n",
      "Iteration 347, loss = 0.35900703\n",
      "Iteration 348, loss = 0.35885058\n",
      "Iteration 349, loss = 0.35869421\n",
      "Iteration 350, loss = 0.35855089\n",
      "Iteration 351, loss = 0.35837964\n",
      "Iteration 352, loss = 0.35824250\n",
      "Iteration 353, loss = 0.35806690\n",
      "Iteration 354, loss = 0.35793265\n",
      "Iteration 355, loss = 0.35778433\n",
      "Iteration 356, loss = 0.35762266\n",
      "Iteration 357, loss = 0.35747095\n",
      "Iteration 358, loss = 0.35729559\n",
      "Iteration 359, loss = 0.35715879\n",
      "Iteration 360, loss = 0.35702850\n",
      "Iteration 361, loss = 0.35686190\n",
      "Iteration 362, loss = 0.35669363\n",
      "Iteration 363, loss = 0.35658188\n",
      "Iteration 364, loss = 0.35639689\n",
      "Iteration 365, loss = 0.35623723\n",
      "Iteration 366, loss = 0.35612007\n",
      "Iteration 367, loss = 0.35593503\n",
      "Iteration 368, loss = 0.35581996\n",
      "Iteration 369, loss = 0.35565501\n",
      "Iteration 370, loss = 0.35548290\n",
      "Iteration 371, loss = 0.35533045\n",
      "Iteration 372, loss = 0.35518132\n",
      "Iteration 373, loss = 0.35503671\n",
      "Iteration 374, loss = 0.35489203\n",
      "Iteration 375, loss = 0.35479404\n",
      "Iteration 376, loss = 0.35457843\n",
      "Iteration 377, loss = 0.35440869\n",
      "Iteration 378, loss = 0.35426765\n",
      "Iteration 379, loss = 0.35416865\n",
      "Iteration 380, loss = 0.35397995\n",
      "Iteration 381, loss = 0.35382505\n",
      "Iteration 382, loss = 0.35367100\n",
      "Iteration 383, loss = 0.35359440\n",
      "Iteration 384, loss = 0.35336816\n",
      "Iteration 385, loss = 0.35321395\n",
      "Iteration 386, loss = 0.35307551\n",
      "Iteration 387, loss = 0.35293048\n",
      "Iteration 388, loss = 0.35278172\n",
      "Iteration 389, loss = 0.35265351\n",
      "Iteration 390, loss = 0.35252289\n",
      "Iteration 391, loss = 0.35235075\n",
      "Iteration 392, loss = 0.35219217\n",
      "Iteration 393, loss = 0.35204364\n",
      "Iteration 394, loss = 0.35191535\n",
      "Iteration 395, loss = 0.35178778\n",
      "Iteration 396, loss = 0.35162391\n",
      "Iteration 397, loss = 0.35147703\n",
      "Iteration 398, loss = 0.35134198\n",
      "Iteration 399, loss = 0.35117038\n",
      "Iteration 400, loss = 0.35109298\n",
      "Iteration 401, loss = 0.35091292\n",
      "Iteration 402, loss = 0.35075157\n",
      "Iteration 403, loss = 0.35061288\n",
      "Iteration 404, loss = 0.35050918\n",
      "Iteration 405, loss = 0.35032003\n",
      "Iteration 406, loss = 0.35018038\n",
      "Iteration 407, loss = 0.35003463\n",
      "Iteration 408, loss = 0.34990515\n",
      "Iteration 409, loss = 0.34973522\n",
      "Iteration 410, loss = 0.34960771\n",
      "Iteration 411, loss = 0.34952358\n",
      "Iteration 412, loss = 0.34933441\n",
      "Iteration 413, loss = 0.34918213\n",
      "Iteration 414, loss = 0.34910711\n",
      "Iteration 415, loss = 0.34890343\n",
      "Iteration 416, loss = 0.34874942\n",
      "Iteration 417, loss = 0.34861401\n",
      "Iteration 418, loss = 0.34846561\n",
      "Iteration 419, loss = 0.34832360\n",
      "Iteration 420, loss = 0.34818790\n",
      "Iteration 421, loss = 0.34808828\n",
      "Iteration 422, loss = 0.34793019\n",
      "Iteration 423, loss = 0.34775980\n",
      "Iteration 424, loss = 0.34762753\n",
      "Iteration 425, loss = 0.34751051\n",
      "Iteration 426, loss = 0.34734315\n",
      "Iteration 427, loss = 0.34719498\n",
      "Iteration 428, loss = 0.34706057\n",
      "Iteration 429, loss = 0.34693767\n",
      "Iteration 430, loss = 0.34681195\n",
      "Iteration 431, loss = 0.34663511\n",
      "Iteration 432, loss = 0.34652595\n",
      "Iteration 433, loss = 0.34635926\n",
      "Iteration 434, loss = 0.34620457\n",
      "Iteration 435, loss = 0.34610411\n",
      "Iteration 436, loss = 0.34590717\n",
      "Iteration 437, loss = 0.34580044\n",
      "Iteration 438, loss = 0.34563477\n",
      "Iteration 439, loss = 0.34550226\n",
      "Iteration 440, loss = 0.34536120\n",
      "Iteration 441, loss = 0.34521852\n",
      "Iteration 442, loss = 0.34510648\n",
      "Iteration 443, loss = 0.34493410\n",
      "Iteration 444, loss = 0.34480086\n",
      "Iteration 445, loss = 0.34466080\n",
      "Iteration 446, loss = 0.34456342\n",
      "Iteration 447, loss = 0.34439416\n",
      "Iteration 448, loss = 0.34424128\n",
      "Iteration 449, loss = 0.34409817\n",
      "Iteration 450, loss = 0.34398482\n",
      "Iteration 451, loss = 0.34382914\n",
      "Iteration 452, loss = 0.34369133\n",
      "Iteration 453, loss = 0.34358163\n",
      "Iteration 454, loss = 0.34341577\n",
      "Iteration 455, loss = 0.34327829\n",
      "Iteration 456, loss = 0.34312348\n",
      "Iteration 457, loss = 0.34300026\n",
      "Iteration 458, loss = 0.34290924\n",
      "Iteration 459, loss = 0.34271987\n",
      "Iteration 460, loss = 0.34257161\n",
      "Iteration 461, loss = 0.34244918\n",
      "Iteration 462, loss = 0.34230205\n",
      "Iteration 463, loss = 0.34216727\n",
      "Iteration 464, loss = 0.34203738\n",
      "Iteration 465, loss = 0.34188060\n",
      "Iteration 466, loss = 0.34175741\n",
      "Iteration 467, loss = 0.34161851\n",
      "Iteration 468, loss = 0.34147563\n",
      "Iteration 469, loss = 0.34138594\n",
      "Iteration 470, loss = 0.34121432\n",
      "Iteration 471, loss = 0.34107313\n",
      "Iteration 472, loss = 0.34092533\n",
      "Iteration 473, loss = 0.34083535\n",
      "Iteration 474, loss = 0.34064543\n",
      "Iteration 475, loss = 0.34053621\n",
      "Iteration 476, loss = 0.34040161\n",
      "Iteration 477, loss = 0.34023636\n",
      "Iteration 478, loss = 0.34013351\n",
      "Iteration 479, loss = 0.33999886\n",
      "Iteration 480, loss = 0.33984491\n",
      "Iteration 481, loss = 0.33970717\n",
      "Iteration 482, loss = 0.33955458\n",
      "Iteration 483, loss = 0.33942531\n",
      "Iteration 484, loss = 0.33933392\n",
      "Iteration 485, loss = 0.33920105\n",
      "Iteration 486, loss = 0.33911188\n",
      "Iteration 487, loss = 0.33889542\n",
      "Iteration 488, loss = 0.33890048\n",
      "Iteration 489, loss = 0.33866590\n",
      "Iteration 490, loss = 0.33848755\n",
      "Iteration 491, loss = 0.33836370\n",
      "Iteration 492, loss = 0.33822054\n",
      "Iteration 493, loss = 0.33808235\n",
      "Iteration 494, loss = 0.33794350\n",
      "Iteration 495, loss = 0.33782030\n",
      "Iteration 496, loss = 0.33766349\n",
      "Iteration 497, loss = 0.33754023\n",
      "Iteration 498, loss = 0.33751393\n",
      "Iteration 499, loss = 0.33728367\n",
      "Iteration 500, loss = 0.33714491\n",
      "Iteration 501, loss = 0.33700255\n",
      "Iteration 502, loss = 0.33690006\n",
      "Iteration 503, loss = 0.33676965\n",
      "Iteration 504, loss = 0.33660994\n",
      "Iteration 505, loss = 0.33647560\n",
      "Iteration 506, loss = 0.33636061\n",
      "Iteration 507, loss = 0.33623541\n",
      "Iteration 508, loss = 0.33608740\n",
      "Iteration 509, loss = 0.33594575\n",
      "Iteration 510, loss = 0.33582140\n",
      "Iteration 511, loss = 0.33571112\n",
      "Iteration 512, loss = 0.33558602\n",
      "Iteration 513, loss = 0.33543877\n",
      "Iteration 514, loss = 0.33530763\n",
      "Iteration 515, loss = 0.33515660\n",
      "Iteration 516, loss = 0.33503656\n",
      "Iteration 517, loss = 0.33489445\n",
      "Iteration 518, loss = 0.33478302\n",
      "Iteration 519, loss = 0.33465777\n",
      "Iteration 520, loss = 0.33453222\n",
      "Iteration 521, loss = 0.33438299\n",
      "Iteration 522, loss = 0.33430446\n",
      "Iteration 523, loss = 0.33410788\n",
      "Iteration 524, loss = 0.33398105\n",
      "Iteration 525, loss = 0.33385172\n",
      "Iteration 526, loss = 0.33375356\n",
      "Iteration 527, loss = 0.33358351\n",
      "Iteration 528, loss = 0.33347391\n",
      "Iteration 529, loss = 0.33332381\n",
      "Iteration 530, loss = 0.33320225\n",
      "Iteration 531, loss = 0.33307488\n",
      "Iteration 532, loss = 0.33293113\n",
      "Iteration 533, loss = 0.33284055\n",
      "Iteration 534, loss = 0.33268064\n",
      "Iteration 535, loss = 0.33254875\n",
      "Iteration 536, loss = 0.33241410\n",
      "Iteration 537, loss = 0.33228783\n",
      "Iteration 538, loss = 0.33216533\n",
      "Iteration 539, loss = 0.33204710\n",
      "Iteration 540, loss = 0.33193444\n",
      "Iteration 541, loss = 0.33186978\n",
      "Iteration 542, loss = 0.33168300\n",
      "Iteration 543, loss = 0.33153203\n",
      "Iteration 544, loss = 0.33138791\n",
      "Iteration 545, loss = 0.33125601\n",
      "Iteration 546, loss = 0.33113642\n",
      "Iteration 547, loss = 0.33100561\n",
      "Iteration 548, loss = 0.33092310\n",
      "Iteration 549, loss = 0.33077828\n",
      "Iteration 550, loss = 0.33061130\n",
      "Iteration 551, loss = 0.33055375\n",
      "Iteration 552, loss = 0.33035363\n",
      "Iteration 553, loss = 0.33024137\n",
      "Iteration 554, loss = 0.33008787\n",
      "Iteration 555, loss = 0.32998702\n",
      "Iteration 556, loss = 0.32983630\n",
      "Iteration 557, loss = 0.32972906\n",
      "Iteration 558, loss = 0.32957768\n",
      "Iteration 559, loss = 0.32942340\n",
      "Iteration 560, loss = 0.32929057\n",
      "Iteration 561, loss = 0.32917985\n",
      "Iteration 562, loss = 0.32903635\n",
      "Iteration 563, loss = 0.32890600\n",
      "Iteration 564, loss = 0.32881188\n",
      "Iteration 565, loss = 0.32865780\n",
      "Iteration 566, loss = 0.32850531\n",
      "Iteration 567, loss = 0.32837319\n",
      "Iteration 568, loss = 0.32826338\n",
      "Iteration 569, loss = 0.32811891\n",
      "Iteration 570, loss = 0.32802008\n",
      "Iteration 571, loss = 0.32788158\n",
      "Iteration 572, loss = 0.32774596\n",
      "Iteration 573, loss = 0.32759053\n",
      "Iteration 574, loss = 0.32747977\n",
      "Iteration 575, loss = 0.32736883\n",
      "Iteration 576, loss = 0.32720151\n",
      "Iteration 577, loss = 0.32708819\n",
      "Iteration 578, loss = 0.32696034\n",
      "Iteration 579, loss = 0.32687675\n",
      "Iteration 580, loss = 0.32669283\n",
      "Iteration 581, loss = 0.32657227\n",
      "Iteration 582, loss = 0.32641960\n",
      "Iteration 583, loss = 0.32631945\n",
      "Iteration 584, loss = 0.32617501\n",
      "Iteration 585, loss = 0.32602348\n",
      "Iteration 586, loss = 0.32595602\n",
      "Iteration 587, loss = 0.32578771\n",
      "Iteration 588, loss = 0.32564101\n",
      "Iteration 589, loss = 0.32552311\n",
      "Iteration 590, loss = 0.32540083\n",
      "Iteration 591, loss = 0.32528328\n",
      "Iteration 592, loss = 0.32514784\n",
      "Iteration 593, loss = 0.32504286\n",
      "Iteration 594, loss = 0.32487967\n",
      "Iteration 595, loss = 0.32475027\n",
      "Iteration 596, loss = 0.32475345\n",
      "Iteration 597, loss = 0.32450221\n",
      "Iteration 598, loss = 0.32436050\n",
      "Iteration 599, loss = 0.32427250\n",
      "Iteration 600, loss = 0.32412465\n",
      "Iteration 601, loss = 0.32399324\n",
      "Iteration 602, loss = 0.32390515\n",
      "Iteration 603, loss = 0.32372301\n",
      "Iteration 604, loss = 0.32361798\n",
      "Iteration 605, loss = 0.32347209\n",
      "Iteration 606, loss = 0.32335043\n",
      "Iteration 607, loss = 0.32324346\n",
      "Iteration 608, loss = 0.32308602\n",
      "Iteration 609, loss = 0.32295705\n",
      "Iteration 610, loss = 0.32285585\n",
      "Iteration 611, loss = 0.32273170\n",
      "Iteration 612, loss = 0.32259559\n",
      "Iteration 613, loss = 0.32247047\n",
      "Iteration 614, loss = 0.32234231\n",
      "Iteration 615, loss = 0.32221389\n",
      "Iteration 616, loss = 0.32207805\n",
      "Iteration 617, loss = 0.32195067\n",
      "Iteration 618, loss = 0.32184480\n",
      "Iteration 619, loss = 0.32171361\n",
      "Iteration 620, loss = 0.32157703\n",
      "Iteration 621, loss = 0.32150727\n",
      "Iteration 622, loss = 0.32133460\n",
      "Iteration 623, loss = 0.32119239\n",
      "Iteration 624, loss = 0.32107617\n",
      "Iteration 625, loss = 0.32095208\n",
      "Iteration 626, loss = 0.32080349\n",
      "Iteration 627, loss = 0.32070195\n",
      "Iteration 628, loss = 0.32058005\n",
      "Iteration 629, loss = 0.32043900\n",
      "Iteration 630, loss = 0.32035109\n",
      "Iteration 631, loss = 0.32017578\n",
      "Iteration 632, loss = 0.32005280\n",
      "Iteration 633, loss = 0.31995606\n",
      "Iteration 634, loss = 0.31977976\n",
      "Iteration 635, loss = 0.31970517\n",
      "Iteration 636, loss = 0.31955831\n",
      "Iteration 637, loss = 0.31940261\n",
      "Iteration 638, loss = 0.31930214\n",
      "Iteration 639, loss = 0.31918505\n",
      "Iteration 640, loss = 0.31906047\n",
      "Iteration 641, loss = 0.31891624\n",
      "Iteration 642, loss = 0.31878183\n",
      "Iteration 643, loss = 0.31865470\n",
      "Iteration 644, loss = 0.31856134\n",
      "Iteration 645, loss = 0.31847097\n",
      "Iteration 646, loss = 0.31828259\n",
      "Iteration 647, loss = 0.31815474\n",
      "Iteration 648, loss = 0.31801927\n",
      "Iteration 649, loss = 0.31791116\n",
      "Iteration 650, loss = 0.31776256\n",
      "Iteration 651, loss = 0.31764172\n",
      "Iteration 652, loss = 0.31752217\n",
      "Iteration 653, loss = 0.31736760\n",
      "Iteration 654, loss = 0.31724779\n",
      "Iteration 655, loss = 0.31716814\n",
      "Iteration 656, loss = 0.31701341\n",
      "Iteration 657, loss = 0.31687012\n",
      "Iteration 658, loss = 0.31674532\n",
      "Iteration 659, loss = 0.31667217\n",
      "Iteration 660, loss = 0.31651696\n",
      "Iteration 661, loss = 0.31634577\n",
      "Iteration 662, loss = 0.31629125\n",
      "Iteration 663, loss = 0.31613275\n",
      "Iteration 664, loss = 0.31598980\n",
      "Iteration 665, loss = 0.31587391\n",
      "Iteration 666, loss = 0.31573348\n",
      "Iteration 667, loss = 0.31562129\n",
      "Iteration 668, loss = 0.31545159\n",
      "Iteration 669, loss = 0.31536688\n",
      "Iteration 670, loss = 0.31521445\n",
      "Iteration 671, loss = 0.31505891\n",
      "Iteration 672, loss = 0.31497633\n",
      "Iteration 673, loss = 0.31486599\n",
      "Iteration 674, loss = 0.31468240\n",
      "Iteration 675, loss = 0.31458710\n",
      "Iteration 676, loss = 0.31442182\n",
      "Iteration 677, loss = 0.31438288\n",
      "Iteration 678, loss = 0.31418102\n",
      "Iteration 679, loss = 0.31416173\n",
      "Iteration 680, loss = 0.31394931\n",
      "Iteration 681, loss = 0.31379169\n",
      "Iteration 682, loss = 0.31365416\n",
      "Iteration 683, loss = 0.31353404\n",
      "Iteration 684, loss = 0.31343213\n",
      "Iteration 685, loss = 0.31326596\n",
      "Iteration 686, loss = 0.31317394\n",
      "Iteration 687, loss = 0.31302727\n",
      "Iteration 688, loss = 0.31288291\n",
      "Iteration 689, loss = 0.31276347\n",
      "Iteration 690, loss = 0.31264902\n",
      "Iteration 691, loss = 0.31252879\n",
      "Iteration 692, loss = 0.31239032\n",
      "Iteration 693, loss = 0.31223365\n",
      "Iteration 694, loss = 0.31215564\n",
      "Iteration 695, loss = 0.31201604\n",
      "Iteration 696, loss = 0.31186010\n",
      "Iteration 697, loss = 0.31172891\n",
      "Iteration 698, loss = 0.31162910\n",
      "Iteration 699, loss = 0.31145557\n",
      "Iteration 700, loss = 0.31133882\n",
      "Iteration 701, loss = 0.31120694\n",
      "Iteration 702, loss = 0.31106612\n",
      "Iteration 703, loss = 0.31093452\n",
      "Iteration 704, loss = 0.31083583\n",
      "Iteration 705, loss = 0.31068628\n",
      "Iteration 706, loss = 0.31056157\n",
      "Iteration 707, loss = 0.31042278\n",
      "Iteration 708, loss = 0.31032824\n",
      "Iteration 709, loss = 0.31017968\n",
      "Iteration 710, loss = 0.31007795\n",
      "Iteration 711, loss = 0.30991366\n",
      "Iteration 712, loss = 0.30981478\n",
      "Iteration 713, loss = 0.30974379\n",
      "Iteration 714, loss = 0.30954220\n",
      "Iteration 715, loss = 0.30942168\n",
      "Iteration 716, loss = 0.30930092\n",
      "Iteration 717, loss = 0.30912417\n",
      "Iteration 718, loss = 0.30901171\n",
      "Iteration 719, loss = 0.30886827\n",
      "Iteration 720, loss = 0.30877160\n",
      "Iteration 721, loss = 0.30862153\n",
      "Iteration 722, loss = 0.30850255\n",
      "Iteration 723, loss = 0.30838414\n",
      "Iteration 724, loss = 0.30822948\n",
      "Iteration 725, loss = 0.30809396\n",
      "Iteration 726, loss = 0.30798998\n",
      "Iteration 727, loss = 0.30785430\n",
      "Iteration 728, loss = 0.30773485\n",
      "Iteration 729, loss = 0.30768132\n",
      "Iteration 730, loss = 0.30745197\n",
      "Iteration 731, loss = 0.30735496\n",
      "Iteration 732, loss = 0.30722414\n",
      "Iteration 733, loss = 0.30703800\n",
      "Iteration 734, loss = 0.30691103\n",
      "Iteration 735, loss = 0.30678069\n",
      "Iteration 736, loss = 0.30664097\n",
      "Iteration 737, loss = 0.30657454\n",
      "Iteration 738, loss = 0.30638144\n",
      "Iteration 739, loss = 0.30633886\n",
      "Iteration 740, loss = 0.30614034\n",
      "Iteration 741, loss = 0.30601267\n",
      "Iteration 742, loss = 0.30584000\n",
      "Iteration 743, loss = 0.30571596\n",
      "Iteration 744, loss = 0.30556764\n",
      "Iteration 745, loss = 0.30545824\n",
      "Iteration 746, loss = 0.30532809\n",
      "Iteration 747, loss = 0.30520357\n",
      "Iteration 748, loss = 0.30505572\n",
      "Iteration 749, loss = 0.30491810\n",
      "Iteration 750, loss = 0.30478815\n",
      "Iteration 751, loss = 0.30464400\n",
      "Iteration 752, loss = 0.30456293\n",
      "Iteration 753, loss = 0.30437343\n",
      "Iteration 754, loss = 0.30426499\n",
      "Iteration 755, loss = 0.30411762\n",
      "Iteration 756, loss = 0.30399536\n",
      "Iteration 757, loss = 0.30384674\n",
      "Iteration 758, loss = 0.30372601\n",
      "Iteration 759, loss = 0.30360033\n",
      "Iteration 760, loss = 0.30347115\n",
      "Iteration 761, loss = 0.30330258\n",
      "Iteration 762, loss = 0.30319599\n",
      "Iteration 763, loss = 0.30304795\n",
      "Iteration 764, loss = 0.30297702\n",
      "Iteration 765, loss = 0.30278562\n",
      "Iteration 766, loss = 0.30271254\n",
      "Iteration 767, loss = 0.30254304\n",
      "Iteration 768, loss = 0.30237085\n",
      "Iteration 769, loss = 0.30226903\n",
      "Iteration 770, loss = 0.30213034\n",
      "Iteration 771, loss = 0.30199411\n",
      "Iteration 772, loss = 0.30184510\n",
      "Iteration 773, loss = 0.30173032\n",
      "Iteration 774, loss = 0.30159888\n",
      "Iteration 775, loss = 0.30148767\n",
      "Iteration 776, loss = 0.30130897\n",
      "Iteration 777, loss = 0.30117975\n",
      "Iteration 1259, loss = 0.22697224\n",
      "Iteration 1260, loss = 0.22684504\n",
      "Iteration 1261, loss = 0.22668446\n",
      "Iteration 1262, loss = 0.22651165\n",
      "Iteration 1263, loss = 0.22632849\n",
      "Iteration 1264, loss = 0.22623436\n",
      "Iteration 1265, loss = 0.22599074\n",
      "Iteration 1266, loss = 0.22587727\n",
      "Iteration 1267, loss = 0.22564140\n",
      "Iteration 1268, loss = 0.22540797\n",
      "Iteration 1269, loss = 0.22520339\n",
      "Iteration 1270, loss = 0.22518629\n",
      "Iteration 1271, loss = 0.22483563\n",
      "Iteration 1272, loss = 0.22490843\n",
      "Iteration 1273, loss = 0.22459389\n",
      "Iteration 1274, loss = 0.22431958\n",
      "Iteration 1275, loss = 0.22413329\n",
      "Iteration 1276, loss = 0.22395630\n",
      "Iteration 1277, loss = 0.22391000\n",
      "Iteration 1278, loss = 0.22384706\n",
      "Iteration 1279, loss = 0.22344158\n",
      "Iteration 1280, loss = 0.22336526\n",
      "Iteration 1281, loss = 0.22321800\n",
      "Iteration 1282, loss = 0.22299104\n",
      "Iteration 1283, loss = 0.22285152\n",
      "Iteration 1284, loss = 0.22251959\n",
      "Iteration 1285, loss = 0.22235284\n",
      "Iteration 1286, loss = 0.22220139\n",
      "Iteration 1287, loss = 0.22210656\n",
      "Iteration 1288, loss = 0.22186245\n",
      "Iteration 1289, loss = 0.22189192\n",
      "Iteration 1290, loss = 0.22148467\n",
      "Iteration 1291, loss = 0.22127824\n",
      "Iteration 1292, loss = 0.22109068\n",
      "Iteration 1293, loss = 0.22089543\n",
      "Iteration 1294, loss = 0.22082802\n",
      "Iteration 1295, loss = 0.22071051\n",
      "Iteration 1296, loss = 0.22062248\n",
      "Iteration 1297, loss = 0.22018217\n",
      "Iteration 1298, loss = 0.22017174\n",
      "Iteration 1299, loss = 0.21986999\n",
      "Iteration 1300, loss = 0.21991978\n",
      "Iteration 1301, loss = 0.21952763\n",
      "Iteration 1302, loss = 0.21936918\n",
      "Iteration 1303, loss = 0.21908829\n",
      "Iteration 1304, loss = 0.21887408\n",
      "Iteration 1305, loss = 0.21882210\n",
      "Iteration 1306, loss = 0.21852917\n",
      "Iteration 1307, loss = 0.21861148\n",
      "Iteration 1308, loss = 0.21818852\n",
      "Iteration 1309, loss = 0.21813508\n",
      "Iteration 1310, loss = 0.21807278\n",
      "Iteration 1311, loss = 0.21773528\n",
      "Iteration 1312, loss = 0.21742858\n",
      "Iteration 1313, loss = 0.21728109\n",
      "Iteration 1314, loss = 0.21719770\n",
      "Iteration 1315, loss = 0.21693187\n",
      "Iteration 1316, loss = 0.21675193\n",
      "Iteration 1317, loss = 0.21664426\n",
      "Iteration 1318, loss = 0.21645159\n",
      "Iteration 1319, loss = 0.21629796\n",
      "Iteration 1320, loss = 0.21601868\n",
      "Iteration 1321, loss = 0.21586478\n",
      "Iteration 1322, loss = 0.21567735\n",
      "Iteration 1323, loss = 0.21550983\n",
      "Iteration 1324, loss = 0.21531625\n",
      "Iteration 1325, loss = 0.21514831\n",
      "Iteration 1326, loss = 0.21491807\n",
      "Iteration 1327, loss = 0.21483942\n",
      "Iteration 1328, loss = 0.21455226\n",
      "Iteration 1329, loss = 0.21442769\n",
      "Iteration 1330, loss = 0.21428326\n",
      "Iteration 1331, loss = 0.21417025\n",
      "Iteration 1332, loss = 0.21398413\n",
      "Iteration 1333, loss = 0.21381835\n",
      "Iteration 1334, loss = 0.21355036\n",
      "Iteration 1335, loss = 0.21338227\n",
      "Iteration 1336, loss = 0.21325098\n",
      "Iteration 1337, loss = 0.21303517\n",
      "Iteration 1338, loss = 0.21291169\n",
      "Iteration 1339, loss = 0.21288191\n",
      "Iteration 1340, loss = 0.21260052\n",
      "Iteration 1341, loss = 0.21226609\n",
      "Iteration 1342, loss = 0.21228140\n",
      "Iteration 1343, loss = 0.21189598\n",
      "Iteration 1344, loss = 0.21175979\n",
      "Iteration 1345, loss = 0.21162301\n",
      "Iteration 1346, loss = 0.21138276\n",
      "Iteration 1347, loss = 0.21120524\n",
      "Iteration 1348, loss = 0.21101235\n",
      "Iteration 1349, loss = 0.21088916\n",
      "Iteration 1350, loss = 0.21073329\n",
      "Iteration 1351, loss = 0.21047059\n",
      "Iteration 1352, loss = 0.21036746\n",
      "Iteration 1353, loss = 0.21053967\n",
      "Iteration 1354, loss = 0.20998234\n",
      "Iteration 1355, loss = 0.20997953\n",
      "Iteration 1356, loss = 0.20976614\n",
      "Iteration 1357, loss = 0.20950971\n",
      "Iteration 1358, loss = 0.20936915\n",
      "Iteration 1359, loss = 0.20921335\n",
      "Iteration 1360, loss = 0.20898995\n",
      "Iteration 1361, loss = 0.20887228\n",
      "Iteration 1362, loss = 0.20859884\n",
      "Iteration 1363, loss = 0.20859478\n",
      "Iteration 1364, loss = 0.20830875\n",
      "Iteration 1365, loss = 0.20814048\n",
      "Iteration 1366, loss = 0.20793922\n",
      "Iteration 1367, loss = 0.20784272\n",
      "Iteration 1368, loss = 0.20759031\n",
      "Iteration 1369, loss = 0.20745034\n",
      "Iteration 1370, loss = 0.20734999\n",
      "Iteration 1371, loss = 0.20712510\n",
      "Iteration 1372, loss = 0.20709569\n",
      "Iteration 1373, loss = 0.20680333\n",
      "Iteration 1374, loss = 0.20656980\n",
      "Iteration 1375, loss = 0.20641244\n",
      "Iteration 1376, loss = 0.20623939\n",
      "Iteration 1377, loss = 0.20606128\n",
      "Iteration 1378, loss = 0.20591779\n",
      "Iteration 1379, loss = 0.20576774\n",
      "Iteration 1380, loss = 0.20566103\n",
      "Iteration 1381, loss = 0.20550852\n",
      "Iteration 1382, loss = 0.20530186\n",
      "Iteration 1383, loss = 0.20512857\n",
      "Iteration 1384, loss = 0.20486807\n",
      "Iteration 1385, loss = 0.20473070\n",
      "Iteration 1386, loss = 0.20472247\n",
      "Iteration 1387, loss = 0.20441000\n",
      "Iteration 1388, loss = 0.20423380\n",
      "Iteration 1389, loss = 0.20401424\n",
      "Iteration 1390, loss = 0.20393466\n",
      "Iteration 1391, loss = 0.20365702\n",
      "Iteration 1392, loss = 0.20348184\n",
      "Iteration 1393, loss = 0.20336562\n",
      "Iteration 1394, loss = 0.20330372\n",
      "Iteration 1395, loss = 0.20295649\n",
      "Iteration 1396, loss = 0.20287642\n",
      "Iteration 1397, loss = 0.20266202\n",
      "Iteration 1398, loss = 0.20250720\n",
      "Iteration 1399, loss = 0.20257320\n",
      "Iteration 1400, loss = 0.20215461\n",
      "Iteration 1401, loss = 0.20190669\n",
      "Iteration 1402, loss = 0.20185898\n",
      "Iteration 1403, loss = 0.20160040\n",
      "Iteration 1404, loss = 0.20151221\n",
      "Iteration 1405, loss = 0.20128821\n",
      "Iteration 1406, loss = 0.20108460\n",
      "Iteration 1407, loss = 0.20102017\n",
      "Iteration 1408, loss = 0.20086032\n",
      "Iteration 1409, loss = 0.20050960\n",
      "Iteration 1410, loss = 0.20032303\n",
      "Iteration 1411, loss = 0.20033958\n",
      "Iteration 1412, loss = 0.19997529\n",
      "Iteration 1413, loss = 0.19993343\n",
      "Iteration 1414, loss = 0.19979170\n",
      "Iteration 1415, loss = 0.19964143\n",
      "Iteration 1416, loss = 0.19962644\n",
      "Iteration 1417, loss = 0.19946336\n",
      "Iteration 1418, loss = 0.19906451\n",
      "Iteration 1419, loss = 0.19884680\n",
      "Iteration 1420, loss = 0.19863689\n",
      "Iteration 1421, loss = 0.19844317\n",
      "Iteration 1422, loss = 0.19832934\n",
      "Iteration 1423, loss = 0.19807767\n",
      "Iteration 1424, loss = 0.19794406\n",
      "Iteration 1425, loss = 0.19774161\n",
      "Iteration 1426, loss = 0.19759695\n",
      "Iteration 1427, loss = 0.19758346\n",
      "Iteration 1428, loss = 0.19722674\n",
      "Iteration 1429, loss = 0.19715333\n",
      "Iteration 1430, loss = 0.19685347\n",
      "Iteration 1431, loss = 0.19680902\n",
      "Iteration 1432, loss = 0.19648757\n",
      "Iteration 1433, loss = 0.19636156\n",
      "Iteration 1434, loss = 0.19638711\n",
      "Iteration 1435, loss = 0.19595726\n",
      "Iteration 1436, loss = 0.19581458\n",
      "Iteration 1437, loss = 0.19567808\n",
      "Iteration 1438, loss = 0.19567178\n",
      "Iteration 1439, loss = 0.19526880\n",
      "Iteration 1440, loss = 0.19556125\n",
      "Iteration 1441, loss = 0.19496817\n",
      "Iteration 1442, loss = 0.19480887\n",
      "Iteration 1443, loss = 0.19499724\n",
      "Iteration 1444, loss = 0.19437995\n",
      "Iteration 1445, loss = 0.19422777\n",
      "Iteration 1446, loss = 0.19421703\n",
      "Iteration 1447, loss = 0.19406051\n",
      "Iteration 1448, loss = 0.19369709\n",
      "Iteration 1449, loss = 0.19356059\n",
      "Iteration 1450, loss = 0.19336502\n",
      "Iteration 1451, loss = 0.19324764\n",
      "Iteration 1452, loss = 0.19303649\n",
      "Iteration 1453, loss = 0.19282299\n",
      "Iteration 1454, loss = 0.19266765\n",
      "Iteration 1455, loss = 0.19257438\n",
      "Iteration 1456, loss = 0.19233898\n",
      "Iteration 1457, loss = 0.19216702\n",
      "Iteration 1458, loss = 0.19195053\n",
      "Iteration 1459, loss = 0.19176690\n",
      "Iteration 1460, loss = 0.19165442\n",
      "Iteration 1461, loss = 0.19150724\n",
      "Iteration 1462, loss = 0.19122499\n",
      "Iteration 1463, loss = 0.19113317\n",
      "Iteration 1464, loss = 0.19104456\n",
      "Iteration 1465, loss = 0.19099674\n",
      "Iteration 1466, loss = 0.19062532\n",
      "Iteration 1467, loss = 0.19051153\n",
      "Iteration 1468, loss = 0.19032778\n",
      "Iteration 1469, loss = 0.19007662\n",
      "Iteration 1470, loss = 0.18992653\n",
      "Iteration 1471, loss = 0.18995439\n",
      "Iteration 1472, loss = 0.18954666\n",
      "Iteration 1473, loss = 0.18945196\n",
      "Iteration 1474, loss = 0.18916572\n",
      "Iteration 1475, loss = 0.18906650\n",
      "Iteration 1476, loss = 0.18882418\n",
      "Iteration 1477, loss = 0.18862474\n",
      "Iteration 1478, loss = 0.18857190\n",
      "Iteration 1479, loss = 0.18828983\n",
      "Iteration 1480, loss = 0.18820037\n",
      "Iteration 1481, loss = 0.18788741\n",
      "Iteration 1482, loss = 0.18801833\n",
      "Iteration 1483, loss = 0.18758986\n",
      "Iteration 1484, loss = 0.18741991\n",
      "Iteration 1485, loss = 0.18733900\n",
      "Iteration 1486, loss = 0.18706212\n",
      "Iteration 1487, loss = 0.18695643\n",
      "Iteration 1488, loss = 0.18670352\n",
      "Iteration 1489, loss = 0.18647253\n",
      "Iteration 1490, loss = 0.18632740\n",
      "Iteration 1491, loss = 0.18617681\n",
      "Iteration 1492, loss = 0.18618267\n",
      "Iteration 1493, loss = 0.18612554\n",
      "Iteration 1494, loss = 0.18578170\n",
      "Iteration 1495, loss = 0.18546592\n",
      "Iteration 1496, loss = 0.18532385\n",
      "Iteration 1497, loss = 0.18511561\n",
      "Iteration 1498, loss = 0.18491080\n",
      "Iteration 1499, loss = 0.18470788\n",
      "Iteration 1500, loss = 0.18456194\n",
      "Iteration 1501, loss = 0.18438133\n",
      "Iteration 1502, loss = 0.18417158\n",
      "Iteration 1503, loss = 0.18400295\n",
      "Iteration 1504, loss = 0.18387176\n",
      "Iteration 1505, loss = 0.18370406\n",
      "Iteration 1506, loss = 0.18352879\n",
      "Iteration 1507, loss = 0.18334590\n",
      "Iteration 1508, loss = 0.18314805\n",
      "Iteration 1509, loss = 0.18295191\n",
      "Iteration 1510, loss = 0.18292446\n",
      "Iteration 1511, loss = 0.18258309\n",
      "Iteration 1512, loss = 0.18243400\n",
      "Iteration 1513, loss = 0.18229107\n",
      "Iteration 1514, loss = 0.18223867\n",
      "Iteration 1515, loss = 0.18198561\n",
      "Iteration 1516, loss = 0.18177570\n",
      "Iteration 1517, loss = 0.18160352\n",
      "Iteration 1518, loss = 0.18153065\n",
      "Iteration 1519, loss = 0.18118656\n",
      "Iteration 1520, loss = 0.18107615\n",
      "Iteration 1521, loss = 0.18093300\n",
      "Iteration 1522, loss = 0.18094749\n",
      "Iteration 1523, loss = 0.18052165\n",
      "Iteration 1524, loss = 0.18043006\n",
      "Iteration 1525, loss = 0.18021833\n",
      "Iteration 1526, loss = 0.17995348\n",
      "Iteration 1527, loss = 0.17976293\n",
      "Iteration 1528, loss = 0.17964066\n",
      "Iteration 1529, loss = 0.17953474\n",
      "Iteration 1530, loss = 0.17934513\n",
      "Iteration 1531, loss = 0.17908092\n",
      "Iteration 1532, loss = 0.17907669\n",
      "Iteration 1533, loss = 0.17880280\n",
      "Iteration 1534, loss = 0.17856176\n",
      "Iteration 1535, loss = 0.17846172\n",
      "Iteration 1536, loss = 0.17821116\n",
      "Iteration 1537, loss = 0.17810394\n",
      "Iteration 1538, loss = 0.17786433\n",
      "Iteration 1539, loss = 0.17775596\n",
      "Iteration 1540, loss = 0.17754051\n",
      "Iteration 1541, loss = 0.17753326\n",
      "Iteration 1542, loss = 0.17722260\n",
      "Iteration 1543, loss = 0.17700993\n",
      "Iteration 1544, loss = 0.17686579\n",
      "Iteration 1545, loss = 0.17667604\n",
      "Iteration 1546, loss = 0.17652948\n",
      "Iteration 1547, loss = 0.17635670\n",
      "Iteration 1548, loss = 0.17610414\n",
      "Iteration 1549, loss = 0.17604432\n",
      "Iteration 1550, loss = 0.17580995\n",
      "Iteration 1551, loss = 0.17560721\n",
      "Iteration 1552, loss = 0.17600321\n",
      "Iteration 1553, loss = 0.17549060\n",
      "Iteration 1554, loss = 0.17509009\n",
      "Iteration 1555, loss = 0.17498325\n",
      "Iteration 1556, loss = 0.17477588\n",
      "Iteration 1557, loss = 0.17473523\n",
      "Iteration 1558, loss = 0.17444540\n",
      "Iteration 1559, loss = 0.17431341\n",
      "Iteration 1560, loss = 0.17415829\n",
      "Iteration 1561, loss = 0.17387257\n",
      "Iteration 1562, loss = 0.17385357\n",
      "Iteration 1563, loss = 0.17362264\n",
      "Iteration 1564, loss = 0.17334876\n",
      "Iteration 1565, loss = 0.17328357\n",
      "Iteration 1566, loss = 0.17299849\n",
      "Iteration 1567, loss = 0.17300475\n",
      "Iteration 1568, loss = 0.17274035\n",
      "Iteration 1569, loss = 0.17266956\n",
      "Iteration 1570, loss = 0.17247151\n",
      "Iteration 1571, loss = 0.17218904\n",
      "Iteration 1572, loss = 0.17206639\n",
      "Iteration 1573, loss = 0.17185896\n",
      "Iteration 1574, loss = 0.17185966\n",
      "Iteration 1575, loss = 0.17153579\n",
      "Iteration 1576, loss = 0.17134635\n",
      "Iteration 1577, loss = 0.17117652\n",
      "Iteration 1578, loss = 0.17100438\n",
      "Iteration 1579, loss = 0.17086567\n",
      "Iteration 1580, loss = 0.17068766\n",
      "Iteration 1581, loss = 0.17049021\n",
      "Iteration 1582, loss = 0.17032892\n",
      "Iteration 1583, loss = 0.17015442\n",
      "Iteration 1584, loss = 0.16989931\n",
      "Iteration 1585, loss = 0.16983945\n",
      "Iteration 1586, loss = 0.16983898\n",
      "Iteration 1587, loss = 0.16957812\n",
      "Iteration 1588, loss = 0.16956868\n",
      "Iteration 1589, loss = 0.16924793\n",
      "Iteration 1590, loss = 0.16896815\n",
      "Iteration 1591, loss = 0.16889191\n",
      "Iteration 1592, loss = 0.16866890\n",
      "Iteration 1593, loss = 0.16849676\n",
      "Iteration 1594, loss = 0.16835009\n",
      "Iteration 1595, loss = 0.16802883\n",
      "Iteration 1596, loss = 0.16809005\n",
      "Iteration 1597, loss = 0.16779541\n",
      "Iteration 1598, loss = 0.16767611\n",
      "Iteration 1599, loss = 0.16750833\n",
      "Iteration 1600, loss = 0.16726415\n",
      "Iteration 1601, loss = 0.16709559\n",
      "Iteration 1602, loss = 0.16711386\n",
      "Iteration 1603, loss = 0.16706221\n",
      "Iteration 1604, loss = 0.16654684\n",
      "Iteration 1605, loss = 0.16632216\n",
      "Iteration 1606, loss = 0.16622156\n",
      "Iteration 1607, loss = 0.16618626\n",
      "Iteration 1608, loss = 0.16592248\n",
      "Iteration 1609, loss = 0.16563604\n",
      "Iteration 1610, loss = 0.16550037\n",
      "Iteration 1611, loss = 0.16549985\n",
      "Iteration 1612, loss = 0.16518691\n",
      "Iteration 1613, loss = 0.16506971\n",
      "Iteration 1614, loss = 0.16482098\n",
      "Iteration 1615, loss = 0.16484290\n",
      "Iteration 1616, loss = 0.16467739\n",
      "Iteration 1617, loss = 0.16434982\n",
      "Iteration 1618, loss = 0.16423964\n",
      "Iteration 1619, loss = 0.16402589\n",
      "Iteration 1620, loss = 0.16379759\n",
      "Iteration 1621, loss = 0.16371971\n",
      "Iteration 1622, loss = 0.16345882\n",
      "Iteration 1623, loss = 0.16331869\n",
      "Iteration 1624, loss = 0.16319938\n",
      "Iteration 1625, loss = 0.16317247\n",
      "Iteration 1626, loss = 0.16287092\n",
      "Iteration 1627, loss = 0.16269989\n",
      "Iteration 1628, loss = 0.16249726\n",
      "Iteration 1629, loss = 0.16242643\n",
      "Iteration 1630, loss = 0.16218065\n",
      "Iteration 1631, loss = 0.16200163\n",
      "Iteration 1632, loss = 0.16181883\n",
      "Iteration 1633, loss = 0.16168047\n",
      "Iteration 1634, loss = 0.16153683\n",
      "Iteration 1635, loss = 0.16127218\n",
      "Iteration 1636, loss = 0.16142286\n",
      "Iteration 1637, loss = 0.16107744\n",
      "Iteration 1638, loss = 0.16078706\n",
      "Iteration 1639, loss = 0.16073128\n",
      "Iteration 1640, loss = 0.16048622\n",
      "Iteration 1641, loss = 0.16043859\n",
      "Iteration 1642, loss = 0.16019142\n",
      "Iteration 1643, loss = 0.16005477\n",
      "Iteration 1644, loss = 0.15984220\n",
      "Iteration 1645, loss = 0.15964204\n",
      "Iteration 1646, loss = 0.15946396\n",
      "Iteration 1647, loss = 0.15946646\n",
      "Iteration 1648, loss = 0.15927069\n",
      "Iteration 1649, loss = 0.15906710\n",
      "Iteration 1650, loss = 0.15884140\n",
      "Iteration 1651, loss = 0.15872090\n",
      "Iteration 1652, loss = 0.15874929\n",
      "Iteration 1653, loss = 0.15844304\n",
      "Iteration 1654, loss = 0.15816681\n",
      "Iteration 1655, loss = 0.15803816\n",
      "Iteration 1656, loss = 0.15781745\n",
      "Iteration 1657, loss = 0.15780277\n",
      "Iteration 1658, loss = 0.15748842\n",
      "Iteration 1659, loss = 0.15738256\n",
      "Iteration 1660, loss = 0.15725706\n",
      "Iteration 1661, loss = 0.15706120\n",
      "Iteration 1662, loss = 0.15688066\n",
      "Iteration 1663, loss = 0.15700526\n",
      "Iteration 1664, loss = 0.15651408\n",
      "Iteration 1665, loss = 0.15637348\n",
      "Iteration 1666, loss = 0.15624270\n",
      "Iteration 1667, loss = 0.15612214\n",
      "Iteration 1668, loss = 0.15596202\n",
      "Iteration 1669, loss = 0.15583494\n",
      "Iteration 1670, loss = 0.15565234\n",
      "Iteration 1671, loss = 0.15539022\n",
      "Iteration 1672, loss = 0.15527840\n",
      "Iteration 1673, loss = 0.15512023\n",
      "Iteration 1674, loss = 0.15512760\n",
      "Iteration 1675, loss = 0.15488834\n",
      "Iteration 1676, loss = 0.15476748\n",
      "Iteration 1677, loss = 0.15444741\n",
      "Iteration 1678, loss = 0.15428231\n",
      "Iteration 1679, loss = 0.15415971\n",
      "Iteration 1680, loss = 0.15393519\n",
      "Iteration 1681, loss = 0.15404576\n",
      "Iteration 1682, loss = 0.15382575\n",
      "Iteration 1683, loss = 0.15352437\n",
      "Iteration 1684, loss = 0.15345822\n",
      "Iteration 1685, loss = 0.15309277\n",
      "Iteration 1686, loss = 0.15302635\n",
      "Iteration 1687, loss = 0.15277347\n",
      "Iteration 1688, loss = 0.15259356\n",
      "Iteration 1689, loss = 0.15254797\n",
      "Iteration 1690, loss = 0.15279467\n",
      "Iteration 1691, loss = 0.15223802\n",
      "Iteration 1692, loss = 0.15217031\n",
      "Iteration 1693, loss = 0.15191457\n",
      "Iteration 1694, loss = 0.15164565\n",
      "Iteration 1695, loss = 0.15167166\n",
      "Iteration 1696, loss = 0.15143577\n",
      "Iteration 1697, loss = 0.15128927\n",
      "Iteration 1698, loss = 0.15098721\n",
      "Iteration 1699, loss = 0.15087272\n",
      "Iteration 1700, loss = 0.15072404\n",
      "Iteration 1701, loss = 0.15046860\n",
      "Iteration 1702, loss = 0.15038511\n",
      "Iteration 1703, loss = 0.15023835\n",
      "Iteration 1704, loss = 0.15006779\n",
      "Iteration 1705, loss = 0.15020605\n",
      "Iteration 1706, loss = 0.14979672\n",
      "Iteration 1707, loss = 0.14973220\n",
      "Iteration 1708, loss = 0.14952616\n",
      "Iteration 1709, loss = 0.14925421\n",
      "Iteration 1710, loss = 0.14921976\n",
      "Iteration 1711, loss = 0.14893589\n",
      "Iteration 1712, loss = 0.14890384\n",
      "Iteration 1713, loss = 0.14859309\n",
      "Iteration 1714, loss = 0.14857208\n",
      "Iteration 1715, loss = 0.14834967\n",
      "Iteration 1716, loss = 0.14833256\n",
      "Iteration 1717, loss = 0.14797715\n",
      "Iteration 1718, loss = 0.14781760\n",
      "Iteration 1719, loss = 0.14773394\n",
      "Iteration 1720, loss = 0.14755503\n",
      "Iteration 1721, loss = 0.14759428\n",
      "Iteration 1722, loss = 0.14741179\n",
      "Iteration 1723, loss = 0.14710612\n",
      "Iteration 1724, loss = 0.14700733\n",
      "Iteration 1725, loss = 0.14673760\n",
      "Iteration 1726, loss = 0.14651051\n",
      "Iteration 1727, loss = 0.14644604\n",
      "Iteration 1728, loss = 0.14633052\n",
      "Iteration 1729, loss = 0.14609209\n",
      "Iteration 1730, loss = 0.14599903\n",
      "Iteration 1731, loss = 0.14576961\n",
      "Iteration 1732, loss = 0.14572493\n",
      "Iteration 1733, loss = 0.14544357\n",
      "Iteration 1734, loss = 0.14541072\n",
      "Iteration 1735, loss = 0.14512883\n",
      "Iteration 1736, loss = 0.14505934\n",
      "Iteration 1737, loss = 0.14484067\n",
      "Iteration 1738, loss = 0.14463789\n",
      "Iteration 2118, loss = 0.17306578\n",
      "Iteration 2119, loss = 0.17292719\n",
      "Iteration 2120, loss = 0.17277513\n",
      "Iteration 2121, loss = 0.17276930\n",
      "Iteration 2122, loss = 0.17260594\n",
      "Iteration 2123, loss = 0.17247299\n",
      "Iteration 2124, loss = 0.17254104\n",
      "Iteration 2125, loss = 0.17229353\n",
      "Iteration 2126, loss = 0.17215301\n",
      "Iteration 2127, loss = 0.17211928\n",
      "Iteration 2128, loss = 0.17219520\n",
      "Iteration 2129, loss = 0.17183206\n",
      "Iteration 2130, loss = 0.17174178\n",
      "Iteration 2131, loss = 0.17168573\n",
      "Iteration 2132, loss = 0.17150856\n",
      "Iteration 2133, loss = 0.17167946\n",
      "Iteration 2134, loss = 0.17144550\n",
      "Iteration 2135, loss = 0.17130036\n",
      "Iteration 2136, loss = 0.17110374\n",
      "Iteration 2137, loss = 0.17099882\n",
      "Iteration 2138, loss = 0.17089789\n",
      "Iteration 2139, loss = 0.17083486\n",
      "Iteration 2140, loss = 0.17070272\n",
      "Iteration 2141, loss = 0.17058346\n",
      "Iteration 2142, loss = 0.17047998\n",
      "Iteration 2143, loss = 0.17053404\n",
      "Iteration 2144, loss = 0.17024994\n",
      "Iteration 2145, loss = 0.17020309\n",
      "Iteration 2146, loss = 0.17011896\n",
      "Iteration 2147, loss = 0.17012506\n",
      "Iteration 2148, loss = 0.16998390\n",
      "Iteration 2149, loss = 0.16976167\n",
      "Iteration 2150, loss = 0.16968654\n",
      "Iteration 2151, loss = 0.16952806\n",
      "Iteration 2152, loss = 0.16944456\n",
      "Iteration 2153, loss = 0.16939585\n",
      "Iteration 2154, loss = 0.16927014\n",
      "Iteration 2155, loss = 0.16913436\n",
      "Iteration 2156, loss = 0.16908637\n",
      "Iteration 2157, loss = 0.16902000\n",
      "Iteration 2158, loss = 0.16884616\n",
      "Iteration 2159, loss = 0.16871498\n",
      "Iteration 2160, loss = 0.16864311\n",
      "Iteration 2161, loss = 0.16855256\n",
      "Iteration 2162, loss = 0.16849445\n",
      "Iteration 2163, loss = 0.16851510\n",
      "Iteration 2164, loss = 0.16821837\n",
      "Iteration 2165, loss = 0.16821025\n",
      "Iteration 2166, loss = 0.16801444\n",
      "Iteration 2167, loss = 0.16789887\n",
      "Iteration 2168, loss = 0.16782393\n",
      "Iteration 2169, loss = 0.16772564\n",
      "Iteration 2170, loss = 0.16762644\n",
      "Iteration 2171, loss = 0.16756214\n",
      "Iteration 2172, loss = 0.16760943\n",
      "Iteration 2173, loss = 0.16727719\n",
      "Iteration 2174, loss = 0.16722322\n",
      "Iteration 2175, loss = 0.16711308\n",
      "Iteration 2176, loss = 0.16702707\n",
      "Iteration 2177, loss = 0.16688376\n",
      "Iteration 2178, loss = 0.16677946\n",
      "Iteration 2179, loss = 0.16667805\n",
      "Iteration 2180, loss = 0.16657354\n",
      "Iteration 2181, loss = 0.16648659\n",
      "Iteration 2182, loss = 0.16635904\n",
      "Iteration 2183, loss = 0.16629554\n",
      "Iteration 2184, loss = 0.16621746\n",
      "Iteration 2185, loss = 0.16606435\n",
      "Iteration 2186, loss = 0.16605852\n",
      "Iteration 2187, loss = 0.16588962\n",
      "Iteration 2188, loss = 0.16592914\n",
      "Iteration 2189, loss = 0.16569153\n",
      "Iteration 2190, loss = 0.16554826\n",
      "Iteration 2191, loss = 0.16551755\n",
      "Iteration 2192, loss = 0.16539330\n",
      "Iteration 2193, loss = 0.16534431\n",
      "Iteration 2194, loss = 0.16516204\n",
      "Iteration 2195, loss = 0.16510724\n",
      "Iteration 2196, loss = 0.16497572\n",
      "Iteration 2197, loss = 0.16488544\n",
      "Iteration 2198, loss = 0.16475960\n",
      "Iteration 2199, loss = 0.16468278\n",
      "Iteration 2200, loss = 0.16459058\n",
      "Iteration 2201, loss = 0.16456001\n",
      "Iteration 2202, loss = 0.16450324\n",
      "Iteration 2203, loss = 0.16435494\n",
      "Iteration 2204, loss = 0.16414626\n",
      "Iteration 2205, loss = 0.16438479\n",
      "Iteration 2206, loss = 0.16396818\n",
      "Iteration 2207, loss = 0.16392247\n",
      "Iteration 2208, loss = 0.16375752\n",
      "Iteration 2209, loss = 0.16368995\n",
      "Iteration 2210, loss = 0.16357024\n",
      "Iteration 2211, loss = 0.16353632\n",
      "Iteration 2212, loss = 0.16337173\n",
      "Iteration 2213, loss = 0.16329664\n",
      "Iteration 2214, loss = 0.16324023\n",
      "Iteration 2215, loss = 0.16303060\n",
      "Iteration 2216, loss = 0.16296352\n",
      "Iteration 2217, loss = 0.16284212\n",
      "Iteration 2218, loss = 0.16281363\n",
      "Iteration 2219, loss = 0.16265758\n",
      "Iteration 2220, loss = 0.16257314\n",
      "Iteration 2221, loss = 0.16245758\n",
      "Iteration 2222, loss = 0.16238701\n",
      "Iteration 2223, loss = 0.16225210\n",
      "Iteration 2224, loss = 0.16217588\n",
      "Iteration 2225, loss = 0.16203116\n",
      "Iteration 2226, loss = 0.16202030\n",
      "Iteration 2227, loss = 0.16183865\n",
      "Iteration 2228, loss = 0.16171200\n",
      "Iteration 2229, loss = 0.16166713\n",
      "Iteration 2230, loss = 0.16159075\n",
      "Iteration 2231, loss = 0.16146041\n",
      "Iteration 2232, loss = 0.16133300\n",
      "Iteration 2233, loss = 0.16121516\n",
      "Iteration 2234, loss = 0.16113373\n",
      "Iteration 2235, loss = 0.16105895\n",
      "Iteration 2236, loss = 0.16100137\n",
      "Iteration 2237, loss = 0.16120590\n",
      "Iteration 2238, loss = 0.16077029\n",
      "Iteration 2239, loss = 0.16065069\n",
      "Iteration 2240, loss = 0.16057176\n",
      "Iteration 2241, loss = 0.16040740\n",
      "Iteration 2242, loss = 0.16032816\n",
      "Iteration 2243, loss = 0.16029729\n",
      "Iteration 2244, loss = 0.16013164\n",
      "Iteration 2245, loss = 0.16002040\n",
      "Iteration 2246, loss = 0.15992501\n",
      "Iteration 2247, loss = 0.15984411\n",
      "Iteration 2248, loss = 0.15979055\n",
      "Iteration 2249, loss = 0.15958937\n",
      "Iteration 2250, loss = 0.15953541\n",
      "Iteration 2251, loss = 0.15942488\n",
      "Iteration 2252, loss = 0.15933369\n",
      "Iteration 2253, loss = 0.15920391\n",
      "Iteration 2254, loss = 0.15908104\n",
      "Iteration 2255, loss = 0.15903080\n",
      "Iteration 2256, loss = 0.15908582\n",
      "Iteration 2257, loss = 0.15889679\n",
      "Iteration 2258, loss = 0.15872157\n",
      "Iteration 2259, loss = 0.15869656\n",
      "Iteration 2260, loss = 0.15859170\n",
      "Iteration 2261, loss = 0.15838559\n",
      "Iteration 2262, loss = 0.15833328\n",
      "Iteration 2263, loss = 0.15823332\n",
      "Iteration 2264, loss = 0.15813473\n",
      "Iteration 2265, loss = 0.15801251\n",
      "Iteration 2266, loss = 0.15793646\n",
      "Iteration 2267, loss = 0.15784745\n",
      "Iteration 2268, loss = 0.15790273\n",
      "Iteration 2269, loss = 0.15767454\n",
      "Iteration 2270, loss = 0.15754651\n",
      "Iteration 2271, loss = 0.15741323\n",
      "Iteration 2272, loss = 0.15740274\n",
      "Iteration 2273, loss = 0.15719511\n",
      "Iteration 2274, loss = 0.15716330\n",
      "Iteration 2275, loss = 0.15703691\n",
      "Iteration 2276, loss = 0.15690506\n",
      "Iteration 2277, loss = 0.15684379\n",
      "Iteration 2278, loss = 0.15677649\n",
      "Iteration 2279, loss = 0.15663949\n",
      "Iteration 2280, loss = 0.15661510\n",
      "Iteration 2281, loss = 0.15647633\n",
      "Iteration 2282, loss = 0.15633305\n",
      "Iteration 2283, loss = 0.15624057\n",
      "Iteration 2284, loss = 0.15618124\n",
      "Iteration 2285, loss = 0.15601167\n",
      "Iteration 2286, loss = 0.15592999\n",
      "Iteration 2287, loss = 0.15591921\n",
      "Iteration 2288, loss = 0.15584205\n",
      "Iteration 2289, loss = 0.15563646\n",
      "Iteration 2290, loss = 0.15560297\n",
      "Iteration 2291, loss = 0.15553838\n",
      "Iteration 2292, loss = 0.15547739\n",
      "Iteration 2293, loss = 0.15526268\n",
      "Iteration 2294, loss = 0.15522252\n",
      "Iteration 2295, loss = 0.15516431\n",
      "Iteration 2296, loss = 0.15519398\n",
      "Iteration 2297, loss = 0.15490697\n",
      "Iteration 2298, loss = 0.15475641\n",
      "Iteration 2299, loss = 0.15474841\n",
      "Iteration 2300, loss = 0.15462133\n",
      "Iteration 2301, loss = 0.15448172\n",
      "Iteration 2302, loss = 0.15438273\n",
      "Iteration 2303, loss = 0.15444941\n",
      "Iteration 2304, loss = 0.15419502\n",
      "Iteration 2305, loss = 0.15420649\n",
      "Iteration 2306, loss = 0.15400032\n",
      "Iteration 2307, loss = 0.15404930\n",
      "Iteration 2308, loss = 0.15383386\n",
      "Iteration 2309, loss = 0.15370319\n",
      "Iteration 2310, loss = 0.15360929\n",
      "Iteration 2311, loss = 0.15356840\n",
      "Iteration 2312, loss = 0.15340558\n",
      "Iteration 2313, loss = 0.15331087\n",
      "Iteration 2314, loss = 0.15328557\n",
      "Iteration 2315, loss = 0.15315903\n",
      "Iteration 2316, loss = 0.15307284\n",
      "Iteration 2317, loss = 0.15316369\n",
      "Iteration 2318, loss = 0.15290718\n",
      "Iteration 2319, loss = 0.15300954\n",
      "Iteration 2320, loss = 0.15266003\n",
      "Iteration 2321, loss = 0.15256900\n",
      "Iteration 2322, loss = 0.15245058\n",
      "Iteration 2323, loss = 0.15258990\n",
      "Iteration 2324, loss = 0.15232258\n",
      "Iteration 2325, loss = 0.15221556\n",
      "Iteration 2326, loss = 0.15209170\n",
      "Iteration 2327, loss = 0.15200106\n",
      "Iteration 2328, loss = 0.15191619\n",
      "Iteration 2329, loss = 0.15186747\n",
      "Iteration 2330, loss = 0.15175336\n",
      "Iteration 2331, loss = 0.15162317\n",
      "Iteration 2332, loss = 0.15162676\n",
      "Iteration 2333, loss = 0.15143967\n",
      "Iteration 2334, loss = 0.15137105\n",
      "Iteration 2335, loss = 0.15129116\n",
      "Iteration 2336, loss = 0.15136746\n",
      "Iteration 2337, loss = 0.15104892\n",
      "Iteration 2338, loss = 0.15097055\n",
      "Iteration 2339, loss = 0.15082734\n",
      "Iteration 2340, loss = 0.15083301\n",
      "Iteration 2341, loss = 0.15070761\n",
      "Iteration 2342, loss = 0.15065654\n",
      "Iteration 2343, loss = 0.15049359\n",
      "Iteration 2344, loss = 0.15041737\n",
      "Iteration 2345, loss = 0.15038237\n",
      "Iteration 2346, loss = 0.15025590\n",
      "Iteration 2347, loss = 0.15014415\n",
      "Iteration 2348, loss = 0.14997589\n",
      "Iteration 2349, loss = 0.14998913\n",
      "Iteration 2350, loss = 0.14983343\n",
      "Iteration 2351, loss = 0.14976468\n",
      "Iteration 2352, loss = 0.14970853\n",
      "Iteration 2353, loss = 0.14950651\n",
      "Iteration 2354, loss = 0.14948251\n",
      "Iteration 2355, loss = 0.14936100\n",
      "Iteration 2356, loss = 0.14924600\n",
      "Iteration 2357, loss = 0.14919763\n",
      "Iteration 2358, loss = 0.14906411\n",
      "Iteration 2359, loss = 0.14899646\n",
      "Iteration 2360, loss = 0.14891938\n",
      "Iteration 2361, loss = 0.14880092\n",
      "Iteration 2362, loss = 0.14870618\n",
      "Iteration 2363, loss = 0.14861441\n",
      "Iteration 2364, loss = 0.14848179\n",
      "Iteration 2365, loss = 0.14847349\n",
      "Iteration 2366, loss = 0.14830369\n",
      "Iteration 2367, loss = 0.14821336\n",
      "Iteration 2368, loss = 0.14813514\n",
      "Iteration 2369, loss = 0.14811403\n",
      "Iteration 2370, loss = 0.14796663\n",
      "Iteration 2371, loss = 0.14793026\n",
      "Iteration 2372, loss = 0.14771620\n",
      "Iteration 2373, loss = 0.14767018\n",
      "Iteration 2374, loss = 0.14756860\n",
      "Iteration 2375, loss = 0.14754810\n",
      "Iteration 2376, loss = 0.14737013\n",
      "Iteration 2377, loss = 0.14726569\n",
      "Iteration 2378, loss = 0.14721654\n",
      "Iteration 2379, loss = 0.14705803\n",
      "Iteration 2380, loss = 0.14700244\n",
      "Iteration 2381, loss = 0.14692544\n",
      "Iteration 2382, loss = 0.14685099\n",
      "Iteration 2383, loss = 0.14669480\n",
      "Iteration 2384, loss = 0.14661562\n",
      "Iteration 2385, loss = 0.14657455\n",
      "Iteration 2386, loss = 0.14642099\n",
      "Iteration 2387, loss = 0.14634560\n",
      "Iteration 2388, loss = 0.14623480\n",
      "Iteration 2389, loss = 0.14618244\n",
      "Iteration 2390, loss = 0.14608759\n",
      "Iteration 2391, loss = 0.14596531\n",
      "Iteration 2392, loss = 0.14596458\n",
      "Iteration 2393, loss = 0.14576653\n",
      "Iteration 2394, loss = 0.14565438\n",
      "Iteration 2395, loss = 0.14561438\n",
      "Iteration 2396, loss = 0.14547853\n",
      "Iteration 2397, loss = 0.14541548\n",
      "Iteration 2398, loss = 0.14534048\n",
      "Iteration 2399, loss = 0.14530626\n",
      "Iteration 2400, loss = 0.14512573\n",
      "Iteration 2401, loss = 0.14508929\n",
      "Iteration 2402, loss = 0.14498031\n",
      "Iteration 2403, loss = 0.14489253\n",
      "Iteration 2404, loss = 0.14478223\n",
      "Iteration 2405, loss = 0.14470528\n",
      "Iteration 2406, loss = 0.14457359\n",
      "Iteration 2407, loss = 0.14448758\n",
      "Iteration 2408, loss = 0.14437098\n",
      "Iteration 2409, loss = 0.14433760\n",
      "Iteration 2410, loss = 0.14426731\n",
      "Iteration 2411, loss = 0.14416043\n",
      "Iteration 2412, loss = 0.14403954\n",
      "Iteration 2413, loss = 0.14403332\n",
      "Iteration 2414, loss = 0.14403456\n",
      "Iteration 2415, loss = 0.14379271\n",
      "Iteration 2416, loss = 0.14364844\n",
      "Iteration 2417, loss = 0.14364827\n",
      "Iteration 2418, loss = 0.14345734\n",
      "Iteration 2419, loss = 0.14341278\n",
      "Iteration 2420, loss = 0.14342003\n",
      "Iteration 2421, loss = 0.14320627\n",
      "Iteration 2422, loss = 0.14312011\n",
      "Iteration 2423, loss = 0.14303686\n",
      "Iteration 2424, loss = 0.14307427\n",
      "Iteration 2425, loss = 0.14286950\n",
      "Iteration 2426, loss = 0.14279669\n",
      "Iteration 2427, loss = 0.14269127\n",
      "Iteration 2428, loss = 0.14257673\n",
      "Iteration 2429, loss = 0.14247640\n",
      "Iteration 2430, loss = 0.14235732\n",
      "Iteration 2431, loss = 0.14242108\n",
      "Iteration 2432, loss = 0.14232571\n",
      "Iteration 2433, loss = 0.14218084\n",
      "Iteration 2434, loss = 0.14206334\n",
      "Iteration 2435, loss = 0.14194263\n",
      "Iteration 2436, loss = 0.14183802\n",
      "Iteration 2437, loss = 0.14184038\n",
      "Iteration 2438, loss = 0.14185873\n",
      "Iteration 2439, loss = 0.14184051\n",
      "Iteration 2440, loss = 0.14159659\n",
      "Iteration 2441, loss = 0.14144378\n",
      "Iteration 2442, loss = 0.14132826\n",
      "Iteration 2443, loss = 0.14124904\n",
      "Iteration 2444, loss = 0.14115556\n",
      "Iteration 2445, loss = 0.14103442\n",
      "Iteration 2446, loss = 0.14095209\n",
      "Iteration 2447, loss = 0.14094960\n",
      "Iteration 2448, loss = 0.14101216\n",
      "Iteration 2449, loss = 0.14075921\n",
      "Iteration 2450, loss = 0.14059782\n",
      "Iteration 2451, loss = 0.14051078\n",
      "Iteration 2452, loss = 0.14053003\n",
      "Iteration 2453, loss = 0.14033970\n",
      "Iteration 2454, loss = 0.14028366\n",
      "Iteration 2455, loss = 0.14017954\n",
      "Iteration 2456, loss = 0.14006544\n",
      "Iteration 2457, loss = 0.13998286\n",
      "Iteration 2458, loss = 0.13989059\n",
      "Iteration 2459, loss = 0.13986191\n",
      "Iteration 2460, loss = 0.13969336\n",
      "Iteration 2461, loss = 0.13967215\n",
      "Iteration 2462, loss = 0.13968943\n",
      "Iteration 2463, loss = 0.13947409\n",
      "Iteration 2464, loss = 0.13942245\n",
      "Iteration 2465, loss = 0.13928851\n",
      "Iteration 2466, loss = 0.13917854\n",
      "Iteration 2467, loss = 0.13918141\n",
      "Iteration 2468, loss = 0.13907431\n",
      "Iteration 2469, loss = 0.13899841\n",
      "Iteration 2470, loss = 0.13891241\n",
      "Iteration 2471, loss = 0.13875025\n",
      "Iteration 2472, loss = 0.13865221\n",
      "Iteration 2473, loss = 0.13859014\n",
      "Iteration 2474, loss = 0.13845393\n",
      "Iteration 2475, loss = 0.13839643\n",
      "Iteration 2476, loss = 0.13833500\n",
      "Iteration 2477, loss = 0.13824038\n",
      "Iteration 2478, loss = 0.13814513\n",
      "Iteration 2479, loss = 0.13815655\n",
      "Iteration 2480, loss = 0.13811163\n",
      "Iteration 2481, loss = 0.13789069\n",
      "Iteration 2482, loss = 0.13780870\n",
      "Iteration 2483, loss = 0.13775032\n",
      "Iteration 2484, loss = 0.13762328\n",
      "Iteration 2485, loss = 0.13752512\n",
      "Iteration 2486, loss = 0.13740895\n",
      "Iteration 2487, loss = 0.13733931\n",
      "Iteration 2488, loss = 0.13724980\n",
      "Iteration 2489, loss = 0.13723317\n",
      "Iteration 2490, loss = 0.13707287\n",
      "Iteration 2491, loss = 0.13716761\n",
      "Iteration 2492, loss = 0.13688683\n",
      "Iteration 2493, loss = 0.13681700\n",
      "Iteration 2494, loss = 0.13694822\n",
      "Iteration 2495, loss = 0.13670658\n",
      "Iteration 2496, loss = 0.13663030\n",
      "Iteration 2497, loss = 0.13646050\n",
      "Iteration 2498, loss = 0.13645460\n",
      "Iteration 2499, loss = 0.13644043\n",
      "Iteration 2500, loss = 0.13622825\n",
      "Iteration 2501, loss = 0.13626928\n",
      "Iteration 2502, loss = 0.13611521\n",
      "Iteration 2503, loss = 0.13603087\n",
      "Iteration 2504, loss = 0.13588133\n",
      "Iteration 2505, loss = 0.13580561\n",
      "Iteration 2506, loss = 0.13571427\n",
      "Iteration 2507, loss = 0.13560772\n",
      "Iteration 2508, loss = 0.13552241\n",
      "Iteration 2509, loss = 0.13541111\n",
      "Iteration 2510, loss = 0.13556710\n",
      "Iteration 2511, loss = 0.13540689\n",
      "Iteration 2512, loss = 0.13531857\n",
      "Iteration 2513, loss = 0.13508829\n",
      "Iteration 2514, loss = 0.13515461\n",
      "Iteration 2515, loss = 0.13492868\n",
      "Iteration 2516, loss = 0.13485115\n",
      "Iteration 2517, loss = 0.13476133\n",
      "Iteration 2518, loss = 0.13470872\n",
      "Iteration 2519, loss = 0.13459986\n",
      "Iteration 2520, loss = 0.13453665\n",
      "Iteration 2521, loss = 0.13441985\n",
      "Iteration 2522, loss = 0.13436252\n",
      "Iteration 2523, loss = 0.13424920\n",
      "Iteration 2524, loss = 0.13420495\n",
      "Iteration 2525, loss = 0.13406648\n",
      "Iteration 2526, loss = 0.13402724\n",
      "Iteration 2527, loss = 0.13391487\n",
      "Iteration 2528, loss = 0.13382909\n",
      "Iteration 2529, loss = 0.13378749\n",
      "Iteration 2530, loss = 0.13366764\n",
      "Iteration 2531, loss = 0.13372589\n",
      "Iteration 2532, loss = 0.13352839\n",
      "Iteration 2533, loss = 0.13345779\n",
      "Iteration 2534, loss = 0.13339190\n",
      "Iteration 2535, loss = 0.13327855\n",
      "Iteration 2536, loss = 0.13322482\n",
      "Iteration 2537, loss = 0.13318831\n",
      "Iteration 2538, loss = 0.13303918\n",
      "Iteration 2539, loss = 0.13302628\n",
      "Iteration 2540, loss = 0.13286381\n",
      "Iteration 2541, loss = 0.13280896\n",
      "Iteration 2542, loss = 0.13287708\n",
      "Iteration 2543, loss = 0.13257286\n",
      "Iteration 2544, loss = 0.13250489\n",
      "Iteration 2545, loss = 0.13245352\n",
      "Iteration 2546, loss = 0.13239743\n",
      "Iteration 2547, loss = 0.13254607\n",
      "Iteration 2548, loss = 0.13215698\n",
      "Iteration 2549, loss = 0.13212826\n",
      "Iteration 2550, loss = 0.13215627\n",
      "Iteration 2551, loss = 0.13194432\n",
      "Iteration 2552, loss = 0.13198859\n",
      "Iteration 2553, loss = 0.13186048\n",
      "Iteration 2554, loss = 0.13192887\n",
      "Iteration 2555, loss = 0.13167857\n",
      "Iteration 2556, loss = 0.13153485\n",
      "Iteration 2557, loss = 0.13156371\n",
      "Iteration 2558, loss = 0.13137066\n",
      "Iteration 2559, loss = 0.13139625\n",
      "Iteration 2560, loss = 0.13121681\n",
      "Iteration 2561, loss = 0.13125540\n",
      "Iteration 2562, loss = 0.13108034\n",
      "Iteration 2563, loss = 0.13101151\n",
      "Iteration 2564, loss = 0.13099076\n",
      "Iteration 2565, loss = 0.13088799\n",
      "Iteration 2566, loss = 0.13077118\n",
      "Iteration 2567, loss = 0.13071007\n",
      "Iteration 2568, loss = 0.13058015\n",
      "Iteration 2569, loss = 0.13049009\n",
      "Iteration 2570, loss = 0.13039872\n",
      "Iteration 2571, loss = 0.13031550\n",
      "Iteration 2572, loss = 0.13026548\n",
      "Iteration 2573, loss = 0.13019516\n",
      "Iteration 2574, loss = 0.13006020\n",
      "Iteration 2575, loss = 0.12999804\n",
      "Iteration 2576, loss = 0.13000081\n",
      "Iteration 2577, loss = 0.12987190\n",
      "Iteration 2578, loss = 0.12976090\n",
      "Iteration 2579, loss = 0.12966691\n",
      "Iteration 2580, loss = 0.12966522\n",
      "Iteration 2581, loss = 0.12972093\n",
      "Iteration 2582, loss = 0.12956728\n",
      "Iteration 2583, loss = 0.12944452\n",
      "Iteration 2584, loss = 0.12933758\n",
      "Iteration 2585, loss = 0.12926458\n",
      "Iteration 2586, loss = 0.12917398\n",
      "Iteration 2587, loss = 0.12910089\n",
      "Iteration 2588, loss = 0.12896431\n",
      "Iteration 2589, loss = 0.12888386\n",
      "Iteration 2590, loss = 0.12886454\n",
      "Iteration 2591, loss = 0.12872207\n",
      "Iteration 2592, loss = 0.12863600\n",
      "Iteration 2593, loss = 0.12857378\n",
      "Iteration 2594, loss = 0.12846119\n",
      "Iteration 2595, loss = 0.12847143\n",
      "Iteration 2596, loss = 0.12830587\n",
      "Iteration 2597, loss = 0.12827383\n",
      "Iteration 828, loss = 0.29936493\n",
      "Iteration 829, loss = 0.29922014\n",
      "Iteration 830, loss = 0.29909737\n",
      "Iteration 831, loss = 0.29898913\n",
      "Iteration 832, loss = 0.29885057\n",
      "Iteration 833, loss = 0.29875841\n",
      "Iteration 834, loss = 0.29859286\n",
      "Iteration 835, loss = 0.29851261\n",
      "Iteration 836, loss = 0.29837021\n",
      "Iteration 837, loss = 0.29828012\n",
      "Iteration 838, loss = 0.29817373\n",
      "Iteration 839, loss = 0.29806804\n",
      "Iteration 840, loss = 0.29789906\n",
      "Iteration 841, loss = 0.29775812\n",
      "Iteration 842, loss = 0.29763970\n",
      "Iteration 843, loss = 0.29756106\n",
      "Iteration 844, loss = 0.29744581\n",
      "Iteration 845, loss = 0.29728623\n",
      "Iteration 846, loss = 0.29716824\n",
      "Iteration 847, loss = 0.29704676\n",
      "Iteration 848, loss = 0.29693580\n",
      "Iteration 849, loss = 0.29686463\n",
      "Iteration 850, loss = 0.29669275\n",
      "Iteration 851, loss = 0.29657187\n",
      "Iteration 852, loss = 0.29644940\n",
      "Iteration 853, loss = 0.29630604\n",
      "Iteration 854, loss = 0.29618606\n",
      "Iteration 855, loss = 0.29608217\n",
      "Iteration 856, loss = 0.29595577\n",
      "Iteration 857, loss = 0.29582246\n",
      "Iteration 858, loss = 0.29571641\n",
      "Iteration 859, loss = 0.29557843\n",
      "Iteration 860, loss = 0.29547355\n",
      "Iteration 861, loss = 0.29540515\n",
      "Iteration 862, loss = 0.29525709\n",
      "Iteration 863, loss = 0.29512405\n",
      "Iteration 864, loss = 0.29501007\n",
      "Iteration 865, loss = 0.29486480\n",
      "Iteration 866, loss = 0.29475738\n",
      "Iteration 867, loss = 0.29462855\n",
      "Iteration 868, loss = 0.29451202\n",
      "Iteration 869, loss = 0.29437444\n",
      "Iteration 870, loss = 0.29427046\n",
      "Iteration 871, loss = 0.29413343\n",
      "Iteration 872, loss = 0.29402085\n",
      "Iteration 873, loss = 0.29390380\n",
      "Iteration 874, loss = 0.29378644\n",
      "Iteration 875, loss = 0.29365802\n",
      "Iteration 876, loss = 0.29353724\n",
      "Iteration 877, loss = 0.29345826\n",
      "Iteration 878, loss = 0.29328609\n",
      "Iteration 879, loss = 0.29321573\n",
      "Iteration 880, loss = 0.29306049\n",
      "Iteration 881, loss = 0.29292380\n",
      "Iteration 882, loss = 0.29280994\n",
      "Iteration 883, loss = 0.29268964\n",
      "Iteration 884, loss = 0.29257830\n",
      "Iteration 885, loss = 0.29245734\n",
      "Iteration 886, loss = 0.29234247\n",
      "Iteration 887, loss = 0.29225569\n",
      "Iteration 888, loss = 0.29208395\n",
      "Iteration 889, loss = 0.29194933\n",
      "Iteration 890, loss = 0.29184259\n",
      "Iteration 891, loss = 0.29171762\n",
      "Iteration 892, loss = 0.29160298\n",
      "Iteration 893, loss = 0.29146855\n",
      "Iteration 894, loss = 0.29134458\n",
      "Iteration 895, loss = 0.29131831\n",
      "Iteration 896, loss = 0.29114985\n",
      "Iteration 897, loss = 0.29100371\n",
      "Iteration 898, loss = 0.29091431\n",
      "Iteration 899, loss = 0.29075294\n",
      "Iteration 900, loss = 0.29063191\n",
      "Iteration 901, loss = 0.29054448\n",
      "Iteration 902, loss = 0.29049576\n",
      "Iteration 903, loss = 0.29028882\n",
      "Iteration 904, loss = 0.29013766\n",
      "Iteration 905, loss = 0.29012917\n",
      "Iteration 906, loss = 0.28991662\n",
      "Iteration 907, loss = 0.28981692\n",
      "Iteration 908, loss = 0.28969793\n",
      "Iteration 909, loss = 0.28955445\n",
      "Iteration 910, loss = 0.28942224\n",
      "Iteration 911, loss = 0.28934772\n",
      "Iteration 912, loss = 0.28919191\n",
      "Iteration 913, loss = 0.28911653\n",
      "Iteration 914, loss = 0.28895301\n",
      "Iteration 915, loss = 0.28887489\n",
      "Iteration 916, loss = 0.28869983\n",
      "Iteration 917, loss = 0.28857778\n",
      "Iteration 918, loss = 0.28849222\n",
      "Iteration 919, loss = 0.28837977\n",
      "Iteration 920, loss = 0.28821776\n",
      "Iteration 921, loss = 0.28813012\n",
      "Iteration 922, loss = 0.28797673\n",
      "Iteration 923, loss = 0.28789189\n",
      "Iteration 924, loss = 0.28778939\n",
      "Iteration 925, loss = 0.28764552\n",
      "Iteration 926, loss = 0.28753636\n",
      "Iteration 927, loss = 0.28740200\n",
      "Iteration 928, loss = 0.28735870\n",
      "Iteration 929, loss = 0.28717522\n",
      "Iteration 930, loss = 0.28707906\n",
      "Iteration 931, loss = 0.28693909\n",
      "Iteration 932, loss = 0.28677918\n",
      "Iteration 933, loss = 0.28668724\n",
      "Iteration 934, loss = 0.28658769\n",
      "Iteration 935, loss = 0.28642763\n",
      "Iteration 936, loss = 0.28634922\n",
      "Iteration 937, loss = 0.28624345\n",
      "Iteration 938, loss = 0.28609902\n",
      "Iteration 939, loss = 0.28597181\n",
      "Iteration 940, loss = 0.28587046\n",
      "Iteration 941, loss = 0.28576154\n",
      "Iteration 942, loss = 0.28560538\n",
      "Iteration 943, loss = 0.28548135\n",
      "Iteration 944, loss = 0.28537451\n",
      "Iteration 945, loss = 0.28523909\n",
      "Iteration 946, loss = 0.28512479\n",
      "Iteration 947, loss = 0.28500812\n",
      "Iteration 948, loss = 0.28489541\n",
      "Iteration 949, loss = 0.28475973\n",
      "Iteration 950, loss = 0.28465497\n",
      "Iteration 951, loss = 0.28453391\n",
      "Iteration 952, loss = 0.28442259\n",
      "Iteration 953, loss = 0.28427004\n",
      "Iteration 954, loss = 0.28417943\n",
      "Iteration 955, loss = 0.28404295\n",
      "Iteration 956, loss = 0.28391208\n",
      "Iteration 957, loss = 0.28383728\n",
      "Iteration 958, loss = 0.28371432\n",
      "Iteration 959, loss = 0.28358223\n",
      "Iteration 960, loss = 0.28345223\n",
      "Iteration 961, loss = 0.28334350\n",
      "Iteration 962, loss = 0.28321285\n",
      "Iteration 963, loss = 0.28307653\n",
      "Iteration 964, loss = 0.28300994\n",
      "Iteration 965, loss = 0.28284197\n",
      "Iteration 966, loss = 0.28272543\n",
      "Iteration 967, loss = 0.28262851\n",
      "Iteration 968, loss = 0.28246569\n",
      "Iteration 969, loss = 0.28240099\n",
      "Iteration 970, loss = 0.28224199\n",
      "Iteration 971, loss = 0.28213388\n",
      "Iteration 972, loss = 0.28206719\n",
      "Iteration 973, loss = 0.28189077\n",
      "Iteration 974, loss = 0.28185977\n",
      "Iteration 975, loss = 0.28165910\n",
      "Iteration 976, loss = 0.28153320\n",
      "Iteration 977, loss = 0.28140274\n",
      "Iteration 978, loss = 0.28134728\n",
      "Iteration 979, loss = 0.28117628\n",
      "Iteration 980, loss = 0.28104870\n",
      "Iteration 981, loss = 0.28092373\n",
      "Iteration 982, loss = 0.28080075\n",
      "Iteration 983, loss = 0.28067949\n",
      "Iteration 984, loss = 0.28055751\n",
      "Iteration 985, loss = 0.28046834\n",
      "Iteration 986, loss = 0.28032954\n",
      "Iteration 987, loss = 0.28021143\n",
      "Iteration 988, loss = 0.28007489\n",
      "Iteration 989, loss = 0.28002458\n",
      "Iteration 990, loss = 0.27985751\n",
      "Iteration 991, loss = 0.27971461\n",
      "Iteration 992, loss = 0.27965277\n",
      "Iteration 993, loss = 0.27951481\n",
      "Iteration 994, loss = 0.27935535\n",
      "Iteration 995, loss = 0.27930048\n",
      "Iteration 996, loss = 0.27916348\n",
      "Iteration 997, loss = 0.27903217\n",
      "Iteration 998, loss = 0.27888888\n",
      "Iteration 999, loss = 0.27879134\n",
      "Iteration 1000, loss = 0.27863892\n",
      "Iteration 1001, loss = 0.27854108\n",
      "Iteration 1002, loss = 0.27843261\n",
      "Iteration 1003, loss = 0.27829906\n",
      "Iteration 1004, loss = 0.27820662\n",
      "Iteration 1005, loss = 0.27804993\n",
      "Iteration 1006, loss = 0.27794956\n",
      "Iteration 1007, loss = 0.27781362\n",
      "Iteration 1008, loss = 0.27770831\n",
      "Iteration 1009, loss = 0.27758214\n",
      "Iteration 1010, loss = 0.27757848\n",
      "Iteration 1011, loss = 0.27737656\n",
      "Iteration 1012, loss = 0.27725462\n",
      "Iteration 1013, loss = 0.27718675\n",
      "Iteration 1014, loss = 0.27700763\n",
      "Iteration 1015, loss = 0.27689170\n",
      "Iteration 1016, loss = 0.27681369\n",
      "Iteration 1017, loss = 0.27666315\n",
      "Iteration 1018, loss = 0.27664195\n",
      "Iteration 1019, loss = 0.27648328\n",
      "Iteration 1020, loss = 0.27629935\n",
      "Iteration 1021, loss = 0.27625227\n",
      "Iteration 1022, loss = 0.27609834\n",
      "Iteration 1023, loss = 0.27597169\n",
      "Iteration 1024, loss = 0.27586730\n",
      "Iteration 1025, loss = 0.27578693\n",
      "Iteration 1026, loss = 0.27570488\n",
      "Iteration 1027, loss = 0.27551697\n",
      "Iteration 1028, loss = 0.27544382\n",
      "Iteration 1029, loss = 0.27530762\n",
      "Iteration 1030, loss = 0.27518555\n",
      "Iteration 1031, loss = 0.27504424\n",
      "Iteration 1032, loss = 0.27497835\n",
      "Iteration 1033, loss = 0.27484972\n",
      "Iteration 1034, loss = 0.27478474\n",
      "Iteration 1035, loss = 0.27458595\n",
      "Iteration 1036, loss = 0.27447910\n",
      "Iteration 1037, loss = 0.27436841\n",
      "Iteration 1038, loss = 0.27434918\n",
      "Iteration 1039, loss = 0.27420802\n",
      "Iteration 1040, loss = 0.27402413\n",
      "Iteration 1041, loss = 0.27392160\n",
      "Iteration 1042, loss = 0.27387532\n",
      "Iteration 1043, loss = 0.27367768\n",
      "Iteration 1044, loss = 0.27357324\n",
      "Iteration 1045, loss = 0.27349653\n",
      "Iteration 1046, loss = 0.27334649\n",
      "Iteration 1047, loss = 0.27322768\n",
      "Iteration 1048, loss = 0.27308999\n",
      "Iteration 1049, loss = 0.27301049\n",
      "Iteration 1050, loss = 0.27288090\n",
      "Iteration 1051, loss = 0.27278438\n",
      "Iteration 1052, loss = 0.27262379\n",
      "Iteration 1053, loss = 0.27256295\n",
      "Iteration 1054, loss = 0.27246186\n",
      "Iteration 1055, loss = 0.27229352\n",
      "Iteration 1056, loss = 0.27226696\n",
      "Iteration 1057, loss = 0.27213892\n",
      "Iteration 1058, loss = 0.27197263\n",
      "Iteration 1059, loss = 0.27182280\n",
      "Iteration 1060, loss = 0.27175972\n",
      "Iteration 1061, loss = 0.27164633\n",
      "Iteration 1062, loss = 0.27148588\n",
      "Iteration 1063, loss = 0.27139123\n",
      "Iteration 1064, loss = 0.27128310\n",
      "Iteration 1065, loss = 0.27116198\n",
      "Iteration 1066, loss = 0.27103412\n",
      "Iteration 1067, loss = 0.27094521\n",
      "Iteration 1068, loss = 0.27087958\n",
      "Iteration 1069, loss = 0.27070916\n",
      "Iteration 1070, loss = 0.27058520\n",
      "Iteration 1071, loss = 0.27047495\n",
      "Iteration 1072, loss = 0.27036722\n",
      "Iteration 1073, loss = 0.27025447\n",
      "Iteration 1074, loss = 0.27012867\n",
      "Iteration 1075, loss = 0.27004263\n",
      "Iteration 1076, loss = 0.26989295\n",
      "Iteration 1077, loss = 0.26979135\n",
      "Iteration 1078, loss = 0.26968077\n",
      "Iteration 1079, loss = 0.26960944\n",
      "Iteration 1080, loss = 0.26942732\n",
      "Iteration 1081, loss = 0.26934313\n",
      "Iteration 1082, loss = 0.26924905\n",
      "Iteration 1083, loss = 0.26908824\n",
      "Iteration 1084, loss = 0.26897795\n",
      "Iteration 1085, loss = 0.26888514\n",
      "Iteration 1086, loss = 0.26874017\n",
      "Iteration 1087, loss = 0.26867403\n",
      "Iteration 1088, loss = 0.26855481\n",
      "Iteration 1089, loss = 0.26841534\n",
      "Iteration 1090, loss = 0.26827776\n",
      "Iteration 1091, loss = 0.26820865\n",
      "Iteration 1092, loss = 0.26814528\n",
      "Iteration 1093, loss = 0.26796442\n",
      "Iteration 1094, loss = 0.26786686\n",
      "Iteration 1095, loss = 0.26778283\n",
      "Iteration 1096, loss = 0.26757242\n",
      "Iteration 1097, loss = 0.26745951\n",
      "Iteration 1098, loss = 0.26739077\n",
      "Iteration 1099, loss = 0.26724503\n",
      "Iteration 1100, loss = 0.26717915\n",
      "Iteration 1101, loss = 0.26704044\n",
      "Iteration 1102, loss = 0.26691762\n",
      "Iteration 1103, loss = 0.26679408\n",
      "Iteration 1104, loss = 0.26666695\n",
      "Iteration 1105, loss = 0.26656717\n",
      "Iteration 1106, loss = 0.26646145\n",
      "Iteration 1107, loss = 0.26636896\n",
      "Iteration 1108, loss = 0.26622251\n",
      "Iteration 1109, loss = 0.26613062\n",
      "Iteration 1110, loss = 0.26600278\n",
      "Iteration 1111, loss = 0.26588829\n",
      "Iteration 1112, loss = 0.26577400\n",
      "Iteration 1113, loss = 0.26578230\n",
      "Iteration 1114, loss = 0.26552678\n",
      "Iteration 1115, loss = 0.26542731\n",
      "Iteration 1116, loss = 0.26545439\n",
      "Iteration 1117, loss = 0.26532754\n",
      "Iteration 1118, loss = 0.26509767\n",
      "Iteration 1119, loss = 0.26497131\n",
      "Iteration 1120, loss = 0.26484535\n",
      "Iteration 1121, loss = 0.26474128\n",
      "Iteration 1122, loss = 0.26464316\n",
      "Iteration 1123, loss = 0.26449739\n",
      "Iteration 1124, loss = 0.26440241\n",
      "Iteration 1125, loss = 0.26428419\n",
      "Iteration 1126, loss = 0.26413955\n",
      "Iteration 1127, loss = 0.26415128\n",
      "Iteration 1128, loss = 0.26393577\n",
      "Iteration 1129, loss = 0.26388814\n",
      "Iteration 1130, loss = 0.26372961\n",
      "Iteration 1131, loss = 0.26358945\n",
      "Iteration 1132, loss = 0.26349028\n",
      "Iteration 1133, loss = 0.26336053\n",
      "Iteration 1134, loss = 0.26323909\n",
      "Iteration 1135, loss = 0.26313126\n",
      "Iteration 1136, loss = 0.26301633\n",
      "Iteration 1137, loss = 0.26292699\n",
      "Iteration 1138, loss = 0.26279285\n",
      "Iteration 1139, loss = 0.26272702\n",
      "Iteration 1140, loss = 0.26258789\n",
      "Iteration 1141, loss = 0.26245630\n",
      "Iteration 1142, loss = 0.26231716\n",
      "Iteration 1143, loss = 0.26225145\n",
      "Iteration 1144, loss = 0.26210610\n",
      "Iteration 1145, loss = 0.26199074\n",
      "Iteration 1146, loss = 0.26187417\n",
      "Iteration 1147, loss = 0.26174125\n",
      "Iteration 1148, loss = 0.26165868\n",
      "Iteration 1149, loss = 0.26150733\n",
      "Iteration 1150, loss = 0.26141502\n",
      "Iteration 1151, loss = 0.26131527\n",
      "Iteration 1152, loss = 0.26119246\n",
      "Iteration 1153, loss = 0.26106441\n",
      "Iteration 1154, loss = 0.26094140\n",
      "Iteration 1155, loss = 0.26081945\n",
      "Iteration 1156, loss = 0.26072993\n",
      "Iteration 1157, loss = 0.26060675\n",
      "Iteration 1158, loss = 0.26051337\n",
      "Iteration 1159, loss = 0.26042963\n",
      "Iteration 1160, loss = 0.26031501\n",
      "Iteration 1161, loss = 0.26016522\n",
      "Iteration 1162, loss = 0.26003753\n",
      "Iteration 1163, loss = 0.25996467\n",
      "Iteration 1164, loss = 0.25979558\n",
      "Iteration 1165, loss = 0.25967644\n",
      "Iteration 1166, loss = 0.25959220\n",
      "Iteration 1167, loss = 0.25946773\n",
      "Iteration 1168, loss = 0.25946263\n",
      "Iteration 1169, loss = 0.25924364\n",
      "Iteration 1170, loss = 0.25913547\n",
      "Iteration 1171, loss = 0.25905010\n",
      "Iteration 1172, loss = 0.25891082\n",
      "Iteration 1173, loss = 0.25883711\n",
      "Iteration 1174, loss = 0.25866025\n",
      "Iteration 1175, loss = 0.25856458\n",
      "Iteration 1176, loss = 0.25843827\n",
      "Iteration 1177, loss = 0.25832885\n",
      "Iteration 1178, loss = 0.25822906\n",
      "Iteration 1179, loss = 0.25812181\n",
      "Iteration 1180, loss = 0.25798838\n",
      "Iteration 1181, loss = 0.25788909\n",
      "Iteration 1182, loss = 0.25779761\n",
      "Iteration 1183, loss = 0.25764387\n",
      "Iteration 1184, loss = 0.25756964\n",
      "Iteration 1185, loss = 0.25741388\n",
      "Iteration 1186, loss = 0.25732238\n",
      "Iteration 1187, loss = 0.25723283\n",
      "Iteration 1188, loss = 0.25707364\n",
      "Iteration 1189, loss = 0.25696282\n",
      "Iteration 1190, loss = 0.25683944\n",
      "Iteration 1191, loss = 0.25675607\n",
      "Iteration 1192, loss = 0.25667886\n",
      "Iteration 1193, loss = 0.25649906\n",
      "Iteration 1194, loss = 0.25638475\n",
      "Iteration 1195, loss = 0.25627597\n",
      "Iteration 1196, loss = 0.25623747\n",
      "Iteration 1197, loss = 0.25606799\n",
      "Iteration 1198, loss = 0.25595507\n",
      "Iteration 1199, loss = 0.25582652\n",
      "Iteration 1200, loss = 0.25578832\n",
      "Iteration 1201, loss = 0.25561605\n",
      "Iteration 1202, loss = 0.25546979\n",
      "Iteration 1203, loss = 0.25541656\n",
      "Iteration 1204, loss = 0.25529945\n",
      "Iteration 1205, loss = 0.25515811\n",
      "Iteration 1206, loss = 0.25506270\n",
      "Iteration 1207, loss = 0.25490546\n",
      "Iteration 1208, loss = 0.25486910\n",
      "Iteration 1209, loss = 0.25471198\n",
      "Iteration 1210, loss = 0.25459373\n",
      "Iteration 1211, loss = 0.25446971\n",
      "Iteration 1212, loss = 0.25434179\n",
      "Iteration 1213, loss = 0.25421587\n",
      "Iteration 1214, loss = 0.25410354\n",
      "Iteration 1215, loss = 0.25404669\n",
      "Iteration 1216, loss = 0.25391645\n",
      "Iteration 1217, loss = 0.25379188\n",
      "Iteration 1218, loss = 0.25371000\n",
      "Iteration 1219, loss = 0.25356299\n",
      "Iteration 1220, loss = 0.25344349\n",
      "Iteration 1221, loss = 0.25335113\n",
      "Iteration 1222, loss = 0.25324963\n",
      "Iteration 1223, loss = 0.25311469\n",
      "Iteration 1224, loss = 0.25298329\n",
      "Iteration 1225, loss = 0.25288655\n",
      "Iteration 1226, loss = 0.25276541\n",
      "Iteration 1227, loss = 0.25263435\n",
      "Iteration 1228, loss = 0.25261313\n",
      "Iteration 1229, loss = 0.25244577\n",
      "Iteration 1230, loss = 0.25232475\n",
      "Iteration 1231, loss = 0.25218463\n",
      "Iteration 1232, loss = 0.25207523\n",
      "Iteration 1233, loss = 0.25199291\n",
      "Iteration 1234, loss = 0.25187081\n",
      "Iteration 1235, loss = 0.25173669\n",
      "Iteration 1236, loss = 0.25165731\n",
      "Iteration 1237, loss = 0.25155150\n",
      "Iteration 1238, loss = 0.25138311\n",
      "Iteration 1239, loss = 0.25130919\n",
      "Iteration 1240, loss = 0.25119236\n",
      "Iteration 1241, loss = 0.25106505\n",
      "Iteration 1242, loss = 0.25094927\n",
      "Iteration 1243, loss = 0.25083587\n",
      "Iteration 1244, loss = 0.25075395\n",
      "Iteration 1245, loss = 0.25062053\n",
      "Iteration 1246, loss = 0.25051662\n",
      "Iteration 1247, loss = 0.25046666\n",
      "Iteration 1248, loss = 0.25026600\n",
      "Iteration 1249, loss = 0.25019337\n",
      "Iteration 1250, loss = 0.25005639\n",
      "Iteration 1251, loss = 0.24992273\n",
      "Iteration 1252, loss = 0.24982783\n",
      "Iteration 1253, loss = 0.24968645\n",
      "Iteration 1254, loss = 0.24961149\n",
      "Iteration 1255, loss = 0.24950831\n",
      "Iteration 1256, loss = 0.24937512\n",
      "Iteration 1257, loss = 0.24922622\n",
      "Iteration 1258, loss = 0.24913566\n",
      "Iteration 1259, loss = 0.24905579\n",
      "Iteration 1260, loss = 0.24887444\n",
      "Iteration 1261, loss = 0.24882662\n",
      "Iteration 1262, loss = 0.24868598\n",
      "Iteration 1263, loss = 0.24855878\n",
      "Iteration 1264, loss = 0.24845629\n",
      "Iteration 1265, loss = 0.24833645\n",
      "Iteration 1266, loss = 0.24829035\n",
      "Iteration 1267, loss = 0.24813748\n",
      "Iteration 1268, loss = 0.24799580\n",
      "Iteration 1269, loss = 0.24792774\n",
      "Iteration 1270, loss = 0.24775363\n",
      "Iteration 1271, loss = 0.24761308\n",
      "Iteration 1272, loss = 0.24750405\n",
      "Iteration 1273, loss = 0.24745927\n",
      "Iteration 1274, loss = 0.24727409\n",
      "Iteration 1275, loss = 0.24715834\n",
      "Iteration 1276, loss = 0.24710513\n",
      "Iteration 1277, loss = 0.24695043\n",
      "Iteration 1278, loss = 0.24683713\n",
      "Iteration 1279, loss = 0.24674868\n",
      "Iteration 1280, loss = 0.24664129\n",
      "Iteration 1281, loss = 0.24649720\n",
      "Iteration 1282, loss = 0.24636034\n",
      "Iteration 1283, loss = 0.24624846\n",
      "Iteration 1284, loss = 0.24617922\n",
      "Iteration 1285, loss = 0.24606363\n",
      "Iteration 1286, loss = 0.24590088\n",
      "Iteration 1287, loss = 0.24577965\n",
      "Iteration 1288, loss = 0.24568875\n",
      "Iteration 1289, loss = 0.24558974\n",
      "Iteration 1290, loss = 0.24543839\n",
      "Iteration 1291, loss = 0.24534094\n",
      "Iteration 1292, loss = 0.24524450\n",
      "Iteration 1293, loss = 0.24510606\n",
      "Iteration 1294, loss = 0.24504942\n",
      "Iteration 1295, loss = 0.24487147\n",
      "Iteration 1296, loss = 0.24475973\n",
      "Iteration 1297, loss = 0.24464709\n",
      "Iteration 1298, loss = 0.24456476\n",
      "Iteration 1299, loss = 0.24444301\n",
      "Iteration 1300, loss = 0.24434930\n",
      "Iteration 1301, loss = 0.24425183\n",
      "Iteration 1302, loss = 0.24407755\n",
      "Iteration 1303, loss = 0.24396114\n",
      "Iteration 1304, loss = 0.24388992\n",
      "Iteration 1305, loss = 0.24377466\n",
      "Iteration 1306, loss = 0.24363766\n",
      "Iteration 1307, loss = 0.24354014\n",
      "Iteration 1308, loss = 0.24342332\n",
      "Iteration 1309, loss = 0.24328637\n",
      "Iteration 1310, loss = 0.24320268\n",
      "Iteration 1311, loss = 0.24304782\n",
      "Iteration 1312, loss = 0.24295938\n",
      "Iteration 1313, loss = 0.24300698\n",
      "Iteration 590, loss = 0.31323393\n",
      "Iteration 591, loss = 0.31314947\n",
      "Iteration 592, loss = 0.31306952\n",
      "Iteration 593, loss = 0.31298249\n",
      "Iteration 594, loss = 0.31290688\n",
      "Iteration 595, loss = 0.31280718\n",
      "Iteration 596, loss = 0.31273132\n",
      "Iteration 597, loss = 0.31265539\n",
      "Iteration 598, loss = 0.31256826\n",
      "Iteration 599, loss = 0.31247958\n",
      "Iteration 600, loss = 0.31237992\n",
      "Iteration 601, loss = 0.31230609\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74915961\n",
      "Iteration 2, loss = 0.74568824\n",
      "Iteration 3, loss = 0.74015328\n",
      "Iteration 4, loss = 0.73375431\n",
      "Iteration 5, loss = 0.72739950\n",
      "Iteration 6, loss = 0.72071080\n",
      "Iteration 7, loss = 0.71444406\n",
      "Iteration 8, loss = 0.70822096\n",
      "Iteration 9, loss = 0.70257385\n",
      "Iteration 10, loss = 0.69748345\n",
      "Iteration 11, loss = 0.69248421\n",
      "Iteration 12, loss = 0.68805334\n",
      "Iteration 13, loss = 0.68359357\n",
      "Iteration 14, loss = 0.67961141\n",
      "Iteration 15, loss = 0.67581943\n",
      "Iteration 16, loss = 0.67226954\n",
      "Iteration 17, loss = 0.66865410\n",
      "Iteration 18, loss = 0.66518079\n",
      "Iteration 19, loss = 0.66184147\n",
      "Iteration 20, loss = 0.65864835\n",
      "Iteration 21, loss = 0.65538883\n",
      "Iteration 22, loss = 0.65216836\n",
      "Iteration 23, loss = 0.64906574\n",
      "Iteration 24, loss = 0.64589166\n",
      "Iteration 25, loss = 0.64278083\n",
      "Iteration 26, loss = 0.63962933\n",
      "Iteration 27, loss = 0.63652000\n",
      "Iteration 28, loss = 0.63332882\n",
      "Iteration 29, loss = 0.63018144\n",
      "Iteration 30, loss = 0.62716709\n",
      "Iteration 31, loss = 0.62394919\n",
      "Iteration 32, loss = 0.62080922\n",
      "Iteration 33, loss = 0.61765054\n",
      "Iteration 34, loss = 0.61448312\n",
      "Iteration 35, loss = 0.61133274\n",
      "Iteration 36, loss = 0.60809259\n",
      "Iteration 37, loss = 0.60486928\n",
      "Iteration 38, loss = 0.60169341\n",
      "Iteration 39, loss = 0.59839834\n",
      "Iteration 40, loss = 0.59520324\n",
      "Iteration 41, loss = 0.59189260\n",
      "Iteration 42, loss = 0.58877620\n",
      "Iteration 43, loss = 0.58549702\n",
      "Iteration 44, loss = 0.58230102\n",
      "Iteration 45, loss = 0.57900853\n",
      "Iteration 46, loss = 0.57577076\n",
      "Iteration 47, loss = 0.57257792\n",
      "Iteration 48, loss = 0.56928413\n",
      "Iteration 49, loss = 0.56604165\n",
      "Iteration 50, loss = 0.56288697\n",
      "Iteration 51, loss = 0.55959866\n",
      "Iteration 52, loss = 0.55627619\n",
      "Iteration 53, loss = 0.55311184\n",
      "Iteration 54, loss = 0.54980235\n",
      "Iteration 55, loss = 0.54648891\n",
      "Iteration 56, loss = 0.54328408\n",
      "Iteration 57, loss = 0.54002275\n",
      "Iteration 58, loss = 0.53674784\n",
      "Iteration 59, loss = 0.53348216\n",
      "Iteration 60, loss = 0.53029406\n",
      "Iteration 61, loss = 0.52708245\n",
      "Iteration 62, loss = 0.52388833\n",
      "Iteration 63, loss = 0.52067605\n",
      "Iteration 64, loss = 0.51740248\n",
      "Iteration 65, loss = 0.51428939\n",
      "Iteration 66, loss = 0.51110306\n",
      "Iteration 67, loss = 0.50786694\n",
      "Iteration 68, loss = 0.50473124\n",
      "Iteration 69, loss = 0.50158477\n",
      "Iteration 70, loss = 0.49854588\n",
      "Iteration 71, loss = 0.49557600\n",
      "Iteration 72, loss = 0.49235668\n",
      "Iteration 73, loss = 0.48940945\n",
      "Iteration 74, loss = 0.48635759\n",
      "Iteration 75, loss = 0.48348961\n",
      "Iteration 76, loss = 0.48057041\n",
      "Iteration 77, loss = 0.47765600\n",
      "Iteration 78, loss = 0.47486404\n",
      "Iteration 79, loss = 0.47203237\n",
      "Iteration 80, loss = 0.46938115\n",
      "Iteration 81, loss = 0.46658900\n",
      "Iteration 82, loss = 0.46388670\n",
      "Iteration 83, loss = 0.46121821\n",
      "Iteration 84, loss = 0.45866631\n",
      "Iteration 85, loss = 0.45617007\n",
      "Iteration 86, loss = 0.45366039\n",
      "Iteration 87, loss = 0.45114898\n",
      "Iteration 88, loss = 0.44879314\n",
      "Iteration 89, loss = 0.44647700\n",
      "Iteration 90, loss = 0.44408909\n",
      "Iteration 91, loss = 0.44185077\n",
      "Iteration 92, loss = 0.43972799\n",
      "Iteration 93, loss = 0.43746004\n",
      "Iteration 94, loss = 0.43543502\n",
      "Iteration 95, loss = 0.43334995\n",
      "Iteration 96, loss = 0.43134776\n",
      "Iteration 97, loss = 0.42945841\n",
      "Iteration 98, loss = 0.42745209\n",
      "Iteration 99, loss = 0.42560683\n",
      "Iteration 100, loss = 0.42370602\n",
      "Iteration 101, loss = 0.42201583\n",
      "Iteration 102, loss = 0.42020631\n",
      "Iteration 103, loss = 0.41848880\n",
      "Iteration 104, loss = 0.41690050\n",
      "Iteration 105, loss = 0.41538226\n",
      "Iteration 106, loss = 0.41373723\n",
      "Iteration 107, loss = 0.41218218\n",
      "Iteration 108, loss = 0.41064112\n",
      "Iteration 109, loss = 0.40922507\n",
      "Iteration 110, loss = 0.40782468\n",
      "Iteration 111, loss = 0.40651728\n",
      "Iteration 112, loss = 0.40512029\n",
      "Iteration 113, loss = 0.40386599\n",
      "Iteration 114, loss = 0.40261200\n",
      "Iteration 115, loss = 0.40133932\n",
      "Iteration 116, loss = 0.40016516\n",
      "Iteration 117, loss = 0.39903394\n",
      "Iteration 118, loss = 0.39783494\n",
      "Iteration 119, loss = 0.39675125\n",
      "Iteration 120, loss = 0.39570421\n",
      "Iteration 121, loss = 0.39458261\n",
      "Iteration 122, loss = 0.39358018\n",
      "Iteration 123, loss = 0.39257614\n",
      "Iteration 124, loss = 0.39162371\n",
      "Iteration 125, loss = 0.39067170\n",
      "Iteration 126, loss = 0.38973911\n",
      "Iteration 127, loss = 0.38886508\n",
      "Iteration 128, loss = 0.38798088\n",
      "Iteration 129, loss = 0.38710423\n",
      "Iteration 130, loss = 0.38630946\n",
      "Iteration 131, loss = 0.38548050\n",
      "Iteration 132, loss = 0.38464521\n",
      "Iteration 133, loss = 0.38391720\n",
      "Iteration 134, loss = 0.38315095\n",
      "Iteration 135, loss = 0.38236579\n",
      "Iteration 136, loss = 0.38161518\n",
      "Iteration 137, loss = 0.38095049\n",
      "Iteration 138, loss = 0.38022838\n",
      "Iteration 139, loss = 0.37960554\n",
      "Iteration 140, loss = 0.37889457\n",
      "Iteration 141, loss = 0.37823161\n",
      "Iteration 142, loss = 0.37765054\n",
      "Iteration 143, loss = 0.37700094\n",
      "Iteration 144, loss = 0.37640270\n",
      "Iteration 145, loss = 0.37584241\n",
      "Iteration 146, loss = 0.37524052\n",
      "Iteration 147, loss = 0.37464441\n",
      "Iteration 148, loss = 0.37416286\n",
      "Iteration 149, loss = 0.37355196\n",
      "Iteration 150, loss = 0.37302783\n",
      "Iteration 151, loss = 0.37250199\n",
      "Iteration 152, loss = 0.37200421\n",
      "Iteration 153, loss = 0.37150388\n",
      "Iteration 154, loss = 0.37103202\n",
      "Iteration 155, loss = 0.37050947\n",
      "Iteration 156, loss = 0.37005969\n",
      "Iteration 157, loss = 0.36961642\n",
      "Iteration 158, loss = 0.36910491\n",
      "Iteration 159, loss = 0.36869162\n",
      "Iteration 160, loss = 0.36825106\n",
      "Iteration 161, loss = 0.36778899\n",
      "Iteration 162, loss = 0.36736461\n",
      "Iteration 163, loss = 0.36692138\n",
      "Iteration 164, loss = 0.36650805\n",
      "Iteration 165, loss = 0.36613191\n",
      "Iteration 166, loss = 0.36572367\n",
      "Iteration 167, loss = 0.36533903\n",
      "Iteration 168, loss = 0.36489063\n",
      "Iteration 169, loss = 0.36451156\n",
      "Iteration 170, loss = 0.36415728\n",
      "Iteration 171, loss = 0.36376167\n",
      "Iteration 172, loss = 0.36339097\n",
      "Iteration 173, loss = 0.36307317\n",
      "Iteration 174, loss = 0.36267462\n",
      "Iteration 175, loss = 0.36230644\n",
      "Iteration 176, loss = 0.36193233\n",
      "Iteration 177, loss = 0.36159333\n",
      "Iteration 178, loss = 0.36123490\n",
      "Iteration 179, loss = 0.36091315\n",
      "Iteration 180, loss = 0.36060264\n",
      "Iteration 181, loss = 0.36023829\n",
      "Iteration 182, loss = 0.35988355\n",
      "Iteration 183, loss = 0.35956073\n",
      "Iteration 184, loss = 0.35923639\n",
      "Iteration 185, loss = 0.35893116\n",
      "Iteration 186, loss = 0.35859419\n",
      "Iteration 187, loss = 0.35829169\n",
      "Iteration 188, loss = 0.35793794\n",
      "Iteration 189, loss = 0.35767196\n",
      "Iteration 190, loss = 0.35739573\n",
      "Iteration 191, loss = 0.35704647\n",
      "Iteration 192, loss = 0.35675921\n",
      "Iteration 193, loss = 0.35647524\n",
      "Iteration 194, loss = 0.35616654\n",
      "Iteration 195, loss = 0.35586097\n",
      "Iteration 196, loss = 0.35557323\n",
      "Iteration 197, loss = 0.35531620\n",
      "Iteration 198, loss = 0.35500506\n",
      "Iteration 199, loss = 0.35472097\n",
      "Iteration 200, loss = 0.35446586\n",
      "Iteration 201, loss = 0.35416850\n",
      "Iteration 202, loss = 0.35389946\n",
      "Iteration 203, loss = 0.35361718\n",
      "Iteration 204, loss = 0.35335187\n",
      "Iteration 205, loss = 0.35308912\n",
      "Iteration 206, loss = 0.35284308\n",
      "Iteration 207, loss = 0.35255931\n",
      "Iteration 208, loss = 0.35232764\n",
      "Iteration 209, loss = 0.35200998\n",
      "Iteration 210, loss = 0.35178115\n",
      "Iteration 211, loss = 0.35153321\n",
      "Iteration 212, loss = 0.35127435\n",
      "Iteration 213, loss = 0.35102136\n",
      "Iteration 214, loss = 0.35076995\n",
      "Iteration 215, loss = 0.35049302\n",
      "Iteration 216, loss = 0.35026895\n",
      "Iteration 217, loss = 0.35004071\n",
      "Iteration 218, loss = 0.34978295\n",
      "Iteration 219, loss = 0.34954854\n",
      "Iteration 220, loss = 0.34929996\n",
      "Iteration 221, loss = 0.34905782\n",
      "Iteration 222, loss = 0.34881264\n",
      "Iteration 223, loss = 0.34858617\n",
      "Iteration 224, loss = 0.34839231\n",
      "Iteration 225, loss = 0.34812710\n",
      "Iteration 226, loss = 0.34788432\n",
      "Iteration 227, loss = 0.34766505\n",
      "Iteration 228, loss = 0.34744421\n",
      "Iteration 229, loss = 0.34721391\n",
      "Iteration 230, loss = 0.34700594\n",
      "Iteration 231, loss = 0.34677571\n",
      "Iteration 232, loss = 0.34655108\n",
      "Iteration 233, loss = 0.34632748\n",
      "Iteration 234, loss = 0.34614484\n",
      "Iteration 235, loss = 0.34591221\n",
      "Iteration 236, loss = 0.34569223\n",
      "Iteration 237, loss = 0.34548663\n",
      "Iteration 238, loss = 0.34527076\n",
      "Iteration 239, loss = 0.34507580\n",
      "Iteration 240, loss = 0.34487802\n",
      "Iteration 241, loss = 0.34464873\n",
      "Iteration 242, loss = 0.34446124\n",
      "Iteration 243, loss = 0.34430293\n",
      "Iteration 244, loss = 0.34405519\n",
      "Iteration 245, loss = 0.34387155\n",
      "Iteration 246, loss = 0.34367563\n",
      "Iteration 247, loss = 0.34348837\n",
      "Iteration 248, loss = 0.34331198\n",
      "Iteration 249, loss = 0.34310611\n",
      "Iteration 250, loss = 0.34290373\n",
      "Iteration 251, loss = 0.34271317\n",
      "Iteration 252, loss = 0.34252756\n",
      "Iteration 253, loss = 0.34239157\n",
      "Iteration 254, loss = 0.34217857\n",
      "Iteration 255, loss = 0.34194272\n",
      "Iteration 256, loss = 0.34178007\n",
      "Iteration 257, loss = 0.34157634\n",
      "Iteration 258, loss = 0.34139626\n",
      "Iteration 259, loss = 0.34121370\n",
      "Iteration 260, loss = 0.34103553\n",
      "Iteration 261, loss = 0.34084939\n",
      "Iteration 262, loss = 0.34067302\n",
      "Iteration 263, loss = 0.34052753\n",
      "Iteration 264, loss = 0.34029732\n",
      "Iteration 265, loss = 0.34012424\n",
      "Iteration 266, loss = 0.33994347\n",
      "Iteration 267, loss = 0.33975873\n",
      "Iteration 268, loss = 0.33965239\n",
      "Iteration 269, loss = 0.33943541\n",
      "Iteration 270, loss = 0.33925531\n",
      "Iteration 271, loss = 0.33906531\n",
      "Iteration 272, loss = 0.33888023\n",
      "Iteration 273, loss = 0.33869955\n",
      "Iteration 274, loss = 0.33853011\n",
      "Iteration 275, loss = 0.33838488\n",
      "Iteration 276, loss = 0.33818338\n",
      "Iteration 277, loss = 0.33806324\n",
      "Iteration 278, loss = 0.33783806\n",
      "Iteration 279, loss = 0.33770543\n",
      "Iteration 280, loss = 0.33750102\n",
      "Iteration 281, loss = 0.33734291\n",
      "Iteration 282, loss = 0.33716112\n",
      "Iteration 283, loss = 0.33700942\n",
      "Iteration 284, loss = 0.33684183\n",
      "Iteration 285, loss = 0.33666435\n",
      "Iteration 286, loss = 0.33648814\n",
      "Iteration 287, loss = 0.33632766\n",
      "Iteration 288, loss = 0.33617874\n",
      "Iteration 289, loss = 0.33601663\n",
      "Iteration 290, loss = 0.33593026\n",
      "Iteration 291, loss = 0.33567297\n",
      "Iteration 292, loss = 0.33553774\n",
      "Iteration 293, loss = 0.33538928\n",
      "Iteration 294, loss = 0.33518631\n",
      "Iteration 295, loss = 0.33501199\n",
      "Iteration 296, loss = 0.33496100\n",
      "Iteration 297, loss = 0.33470592\n",
      "Iteration 298, loss = 0.33455227\n",
      "Iteration 299, loss = 0.33443346\n",
      "Iteration 300, loss = 0.33426950\n",
      "Iteration 301, loss = 0.33409023\n",
      "Iteration 302, loss = 0.33396631\n",
      "Iteration 303, loss = 0.33379946\n",
      "Iteration 304, loss = 0.33369693\n",
      "Iteration 305, loss = 0.33351019\n",
      "Iteration 306, loss = 0.33336004\n",
      "Iteration 307, loss = 0.33319444\n",
      "Iteration 308, loss = 0.33305075\n",
      "Iteration 309, loss = 0.33288897\n",
      "Iteration 310, loss = 0.33276037\n",
      "Iteration 311, loss = 0.33261813\n",
      "Iteration 312, loss = 0.33250002\n",
      "Iteration 313, loss = 0.33232940\n",
      "Iteration 314, loss = 0.33218770\n",
      "Iteration 315, loss = 0.33205115\n",
      "Iteration 316, loss = 0.33193080\n",
      "Iteration 317, loss = 0.33174451\n",
      "Iteration 318, loss = 0.33161236\n",
      "Iteration 319, loss = 0.33146232\n",
      "Iteration 320, loss = 0.33132299\n",
      "Iteration 321, loss = 0.33119367\n",
      "Iteration 322, loss = 0.33108784\n",
      "Iteration 323, loss = 0.33095789\n",
      "Iteration 324, loss = 0.33078008\n",
      "Iteration 325, loss = 0.33062220\n",
      "Iteration 326, loss = 0.33050213\n",
      "Iteration 327, loss = 0.33038053\n",
      "Iteration 328, loss = 0.33020728\n",
      "Iteration 329, loss = 0.33008707\n",
      "Iteration 330, loss = 0.32995136\n",
      "Iteration 331, loss = 0.32979342\n",
      "Iteration 332, loss = 0.32966482\n",
      "Iteration 333, loss = 0.32952156\n",
      "Iteration 334, loss = 0.32942841\n",
      "Iteration 335, loss = 0.32926567\n",
      "Iteration 336, loss = 0.32912740\n",
      "Iteration 337, loss = 0.32897810\n",
      "Iteration 338, loss = 0.32886255\n",
      "Iteration 339, loss = 0.32873764\n",
      "Iteration 340, loss = 0.32857675\n",
      "Iteration 341, loss = 0.32845677\n",
      "Iteration 342, loss = 0.32830463\n",
      "Iteration 343, loss = 0.32817677\n",
      "Iteration 344, loss = 0.32802703\n",
      "Iteration 345, loss = 0.32792062\n",
      "Iteration 346, loss = 0.32779776\n",
      "Iteration 347, loss = 0.32764633\n",
      "Iteration 348, loss = 0.32751902\n",
      "Iteration 349, loss = 0.32738770\n",
      "Iteration 350, loss = 0.32725857\n",
      "Iteration 351, loss = 0.32710777\n",
      "Iteration 352, loss = 0.32697880\n",
      "Iteration 353, loss = 0.32687040\n",
      "Iteration 354, loss = 0.32672390\n",
      "Iteration 355, loss = 0.32658798\n",
      "Iteration 356, loss = 0.32645844\n",
      "Iteration 357, loss = 0.32633722\n",
      "Iteration 358, loss = 0.32621253\n",
      "Iteration 359, loss = 0.32607154\n",
      "Iteration 360, loss = 0.32595962\n",
      "Iteration 361, loss = 0.32582596\n",
      "Iteration 362, loss = 0.32571284\n",
      "Iteration 363, loss = 0.32558373\n",
      "Iteration 364, loss = 0.32546396\n",
      "Iteration 365, loss = 0.32531525\n",
      "Iteration 366, loss = 0.32519192\n",
      "Iteration 367, loss = 0.32507715\n",
      "Iteration 368, loss = 0.32494808\n",
      "Iteration 369, loss = 0.32483658\n",
      "Iteration 370, loss = 0.32471165\n",
      "Iteration 371, loss = 0.32456629\n",
      "Iteration 372, loss = 0.32443798\n",
      "Iteration 373, loss = 0.32432835\n",
      "Iteration 374, loss = 0.32421449\n",
      "Iteration 375, loss = 0.32408129\n",
      "Iteration 376, loss = 0.32396133\n",
      "Iteration 377, loss = 0.32389666\n",
      "Iteration 378, loss = 0.32372576\n",
      "Iteration 379, loss = 0.32359882\n",
      "Iteration 380, loss = 0.32351287\n",
      "Iteration 381, loss = 0.32340562\n",
      "Iteration 382, loss = 0.32323844\n",
      "Iteration 383, loss = 0.32311029\n",
      "Iteration 384, loss = 0.32299157\n",
      "Iteration 385, loss = 0.32291530\n",
      "Iteration 386, loss = 0.32276039\n",
      "Iteration 387, loss = 0.32262740\n",
      "Iteration 388, loss = 0.32251641\n",
      "Iteration 389, loss = 0.32239374\n",
      "Iteration 390, loss = 0.32229777\n",
      "Iteration 391, loss = 0.32215580\n",
      "Iteration 392, loss = 0.32205350\n",
      "Iteration 393, loss = 0.32192834\n",
      "Iteration 394, loss = 0.32179727\n",
      "Iteration 395, loss = 0.32169915\n",
      "Iteration 396, loss = 0.32158183\n",
      "Iteration 397, loss = 0.32147232\n",
      "Iteration 398, loss = 0.32133616\n",
      "Iteration 399, loss = 0.32122214\n",
      "Iteration 400, loss = 0.32115216\n",
      "Iteration 401, loss = 0.32099990\n",
      "Iteration 402, loss = 0.32086775\n",
      "Iteration 403, loss = 0.32076521\n",
      "Iteration 404, loss = 0.32067559\n",
      "Iteration 405, loss = 0.32056429\n",
      "Iteration 406, loss = 0.32042784\n",
      "Iteration 407, loss = 0.32029113\n",
      "Iteration 408, loss = 0.32019552\n",
      "Iteration 409, loss = 0.32006832\n",
      "Iteration 410, loss = 0.31997181\n",
      "Iteration 411, loss = 0.31983706\n",
      "Iteration 412, loss = 0.31970858\n",
      "Iteration 413, loss = 0.31959479\n",
      "Iteration 414, loss = 0.31950186\n",
      "Iteration 415, loss = 0.31937014\n",
      "Iteration 416, loss = 0.31928085\n",
      "Iteration 417, loss = 0.31918254\n",
      "Iteration 418, loss = 0.31905952\n",
      "Iteration 419, loss = 0.31892924\n",
      "Iteration 420, loss = 0.31880820\n",
      "Iteration 421, loss = 0.31869058\n",
      "Iteration 422, loss = 0.31860475\n",
      "Iteration 423, loss = 0.31848155\n",
      "Iteration 424, loss = 0.31836048\n",
      "Iteration 425, loss = 0.31823858\n",
      "Iteration 426, loss = 0.31815269\n",
      "Iteration 427, loss = 0.31802440\n",
      "Iteration 428, loss = 0.31792628\n",
      "Iteration 429, loss = 0.31780787\n",
      "Iteration 430, loss = 0.31769472\n",
      "Iteration 431, loss = 0.31759732\n",
      "Iteration 432, loss = 0.31748334\n",
      "Iteration 433, loss = 0.31737766\n",
      "Iteration 434, loss = 0.31728837\n",
      "Iteration 435, loss = 0.31716894\n",
      "Iteration 436, loss = 0.31704281\n",
      "Iteration 437, loss = 0.31691764\n",
      "Iteration 438, loss = 0.31685787\n",
      "Iteration 439, loss = 0.31670375\n",
      "Iteration 440, loss = 0.31660771\n",
      "Iteration 441, loss = 0.31647891\n",
      "Iteration 442, loss = 0.31636175\n",
      "Iteration 443, loss = 0.31627638\n",
      "Iteration 444, loss = 0.31619166\n",
      "Iteration 445, loss = 0.31610856\n",
      "Iteration 446, loss = 0.31601086\n",
      "Iteration 447, loss = 0.31587781\n",
      "Iteration 448, loss = 0.31576479\n",
      "Iteration 449, loss = 0.31561555\n",
      "Iteration 450, loss = 0.31551947\n",
      "Iteration 451, loss = 0.31544824\n",
      "Iteration 452, loss = 0.31531874\n",
      "Iteration 453, loss = 0.31523490\n",
      "Iteration 454, loss = 0.31509295\n",
      "Iteration 455, loss = 0.31498979\n",
      "Iteration 456, loss = 0.31488754\n",
      "Iteration 457, loss = 0.31477739\n",
      "Iteration 458, loss = 0.31467995\n",
      "Iteration 459, loss = 0.31457696\n",
      "Iteration 460, loss = 0.31447384\n",
      "Iteration 461, loss = 0.31438615\n",
      "Iteration 462, loss = 0.31428439\n",
      "Iteration 463, loss = 0.31420441\n",
      "Iteration 464, loss = 0.31409601\n",
      "Iteration 465, loss = 0.31395729\n",
      "Iteration 466, loss = 0.31385024\n",
      "Iteration 467, loss = 0.31374429\n",
      "Iteration 468, loss = 0.31363804\n",
      "Iteration 469, loss = 0.31355684\n",
      "Iteration 470, loss = 0.31348320\n",
      "Iteration 471, loss = 0.31334438\n",
      "Iteration 472, loss = 0.31325670\n",
      "Iteration 473, loss = 0.31313786\n",
      "Iteration 474, loss = 0.31305938\n",
      "Iteration 475, loss = 0.31292505\n",
      "Iteration 476, loss = 0.31281488\n",
      "Iteration 477, loss = 0.31271343\n",
      "Iteration 478, loss = 0.31262356\n",
      "Iteration 479, loss = 0.31251413\n",
      "Iteration 480, loss = 0.31242615\n",
      "Iteration 481, loss = 0.31231641\n",
      "Iteration 482, loss = 0.31221007\n",
      "Iteration 483, loss = 0.31211591\n",
      "Iteration 484, loss = 0.31203681\n",
      "Iteration 786, loss = 0.31613939\n",
      "Iteration 787, loss = 0.31604029\n",
      "Iteration 788, loss = 0.31586528\n",
      "Iteration 789, loss = 0.31584091\n",
      "Iteration 790, loss = 0.31562823\n",
      "Iteration 791, loss = 0.31553395\n",
      "Iteration 792, loss = 0.31541267\n",
      "Iteration 793, loss = 0.31527739\n",
      "Iteration 794, loss = 0.31516821\n",
      "Iteration 795, loss = 0.31508243\n",
      "Iteration 796, loss = 0.31497785\n",
      "Iteration 797, loss = 0.31480278\n",
      "Iteration 798, loss = 0.31473353\n",
      "Iteration 799, loss = 0.31457604\n",
      "Iteration 800, loss = 0.31446656\n",
      "Iteration 801, loss = 0.31434280\n",
      "Iteration 802, loss = 0.31424078\n",
      "Iteration 803, loss = 0.31411287\n",
      "Iteration 804, loss = 0.31402249\n",
      "Iteration 805, loss = 0.31398785\n",
      "Iteration 806, loss = 0.31376669\n",
      "Iteration 807, loss = 0.31369582\n",
      "Iteration 808, loss = 0.31349951\n",
      "Iteration 809, loss = 0.31341774\n",
      "Iteration 810, loss = 0.31329324\n",
      "Iteration 811, loss = 0.31319166\n",
      "Iteration 812, loss = 0.31304073\n",
      "Iteration 813, loss = 0.31294278\n",
      "Iteration 814, loss = 0.31280013\n",
      "Iteration 815, loss = 0.31269812\n",
      "Iteration 816, loss = 0.31258107\n",
      "Iteration 817, loss = 0.31245088\n",
      "Iteration 818, loss = 0.31234996\n",
      "Iteration 819, loss = 0.31222974\n",
      "Iteration 820, loss = 0.31210451\n",
      "Iteration 821, loss = 0.31200525\n",
      "Iteration 822, loss = 0.31191354\n",
      "Iteration 823, loss = 0.31173221\n",
      "Iteration 824, loss = 0.31162352\n",
      "Iteration 825, loss = 0.31151569\n",
      "Iteration 826, loss = 0.31136843\n",
      "Iteration 827, loss = 0.31128732\n",
      "Iteration 828, loss = 0.31114817\n",
      "Iteration 829, loss = 0.31103906\n",
      "Iteration 830, loss = 0.31091294\n",
      "Iteration 831, loss = 0.31090258\n",
      "Iteration 832, loss = 0.31069600\n",
      "Iteration 833, loss = 0.31056350\n",
      "Iteration 834, loss = 0.31052764\n",
      "Iteration 835, loss = 0.31038965\n",
      "Iteration 836, loss = 0.31020701\n",
      "Iteration 837, loss = 0.31013267\n",
      "Iteration 838, loss = 0.30998916\n",
      "Iteration 839, loss = 0.30988818\n",
      "Iteration 840, loss = 0.30982672\n",
      "Iteration 841, loss = 0.30964315\n",
      "Iteration 842, loss = 0.30956235\n",
      "Iteration 843, loss = 0.30940154\n",
      "Iteration 844, loss = 0.30928224\n",
      "Iteration 845, loss = 0.30923440\n",
      "Iteration 846, loss = 0.30908319\n",
      "Iteration 847, loss = 0.30894842\n",
      "Iteration 848, loss = 0.30884017\n",
      "Iteration 849, loss = 0.30870251\n",
      "Iteration 850, loss = 0.30863472\n",
      "Iteration 851, loss = 0.30846414\n",
      "Iteration 852, loss = 0.30836074\n",
      "Iteration 853, loss = 0.30824832\n",
      "Iteration 854, loss = 0.30813685\n",
      "Iteration 855, loss = 0.30807017\n",
      "Iteration 856, loss = 0.30791970\n",
      "Iteration 857, loss = 0.30780389\n",
      "Iteration 858, loss = 0.30766139\n",
      "Iteration 859, loss = 0.30757963\n",
      "Iteration 860, loss = 0.30746541\n",
      "Iteration 861, loss = 0.30733573\n",
      "Iteration 862, loss = 0.30720744\n",
      "Iteration 863, loss = 0.30712202\n",
      "Iteration 864, loss = 0.30705032\n",
      "Iteration 865, loss = 0.30689686\n",
      "Iteration 866, loss = 0.30674306\n",
      "Iteration 867, loss = 0.30663179\n",
      "Iteration 868, loss = 0.30654564\n",
      "Iteration 869, loss = 0.30642421\n",
      "Iteration 870, loss = 0.30629259\n",
      "Iteration 871, loss = 0.30617813\n",
      "Iteration 872, loss = 0.30609269\n",
      "Iteration 873, loss = 0.30593835\n",
      "Iteration 874, loss = 0.30581974\n",
      "Iteration 875, loss = 0.30572618\n",
      "Iteration 876, loss = 0.30561870\n",
      "Iteration 877, loss = 0.30548232\n",
      "Iteration 878, loss = 0.30538713\n",
      "Iteration 879, loss = 0.30525921\n",
      "Iteration 880, loss = 0.30518761\n",
      "Iteration 881, loss = 0.30505094\n",
      "Iteration 882, loss = 0.30490835\n",
      "Iteration 883, loss = 0.30479694\n",
      "Iteration 884, loss = 0.30467644\n",
      "Iteration 885, loss = 0.30456346\n",
      "Iteration 886, loss = 0.30448504\n",
      "Iteration 887, loss = 0.30438278\n",
      "Iteration 888, loss = 0.30422677\n",
      "Iteration 889, loss = 0.30410085\n",
      "Iteration 890, loss = 0.30400170\n",
      "Iteration 891, loss = 0.30389928\n",
      "Iteration 892, loss = 0.30379044\n",
      "Iteration 893, loss = 0.30364295\n",
      "Iteration 894, loss = 0.30352910\n",
      "Iteration 895, loss = 0.30342418\n",
      "Iteration 896, loss = 0.30330501\n",
      "Iteration 897, loss = 0.30317223\n",
      "Iteration 898, loss = 0.30307611\n",
      "Iteration 899, loss = 0.30296508\n",
      "Iteration 900, loss = 0.30287619\n",
      "Iteration 901, loss = 0.30274110\n",
      "Iteration 902, loss = 0.30262971\n",
      "Iteration 903, loss = 0.30256604\n",
      "Iteration 904, loss = 0.30236570\n",
      "Iteration 905, loss = 0.30225176\n",
      "Iteration 906, loss = 0.30213861\n",
      "Iteration 907, loss = 0.30208751\n",
      "Iteration 908, loss = 0.30194252\n",
      "Iteration 909, loss = 0.30179941\n",
      "Iteration 910, loss = 0.30167649\n",
      "Iteration 911, loss = 0.30154512\n",
      "Iteration 912, loss = 0.30144668\n",
      "Iteration 913, loss = 0.30133637\n",
      "Iteration 914, loss = 0.30120798\n",
      "Iteration 915, loss = 0.30108792\n",
      "Iteration 916, loss = 0.30100645\n",
      "Iteration 917, loss = 0.30086913\n",
      "Iteration 918, loss = 0.30083351\n",
      "Iteration 919, loss = 0.30064029\n",
      "Iteration 920, loss = 0.30063711\n",
      "Iteration 921, loss = 0.30037843\n",
      "Iteration 922, loss = 0.30029200\n",
      "Iteration 923, loss = 0.30017875\n",
      "Iteration 924, loss = 0.30009583\n",
      "Iteration 925, loss = 0.29991428\n",
      "Iteration 926, loss = 0.29979384\n",
      "Iteration 927, loss = 0.29967542\n",
      "Iteration 928, loss = 0.29956847\n",
      "Iteration 929, loss = 0.29947581\n",
      "Iteration 930, loss = 0.29933559\n",
      "Iteration 931, loss = 0.29926918\n",
      "Iteration 932, loss = 0.29909220\n",
      "Iteration 933, loss = 0.29899771\n",
      "Iteration 934, loss = 0.29888316\n",
      "Iteration 935, loss = 0.29876757\n",
      "Iteration 936, loss = 0.29861627\n",
      "Iteration 937, loss = 0.29855864\n",
      "Iteration 938, loss = 0.29846386\n",
      "Iteration 939, loss = 0.29831482\n",
      "Iteration 940, loss = 0.29815873\n",
      "Iteration 941, loss = 0.29805114\n",
      "Iteration 942, loss = 0.29793600\n",
      "Iteration 943, loss = 0.29780321\n",
      "Iteration 944, loss = 0.29771379\n",
      "Iteration 945, loss = 0.29756020\n",
      "Iteration 946, loss = 0.29748179\n",
      "Iteration 947, loss = 0.29738232\n",
      "Iteration 948, loss = 0.29721900\n",
      "Iteration 949, loss = 0.29710925\n",
      "Iteration 950, loss = 0.29703665\n",
      "Iteration 951, loss = 0.29694239\n",
      "Iteration 952, loss = 0.29675419\n",
      "Iteration 953, loss = 0.29661249\n",
      "Iteration 954, loss = 0.29659671\n",
      "Iteration 955, loss = 0.29645811\n",
      "Iteration 956, loss = 0.29630294\n",
      "Iteration 957, loss = 0.29616350\n",
      "Iteration 958, loss = 0.29610784\n",
      "Iteration 959, loss = 0.29601972\n",
      "Iteration 960, loss = 0.29580325\n",
      "Iteration 961, loss = 0.29569631\n",
      "Iteration 962, loss = 0.29555335\n",
      "Iteration 963, loss = 0.29549092\n",
      "Iteration 964, loss = 0.29534427\n",
      "Iteration 965, loss = 0.29523464\n",
      "Iteration 966, loss = 0.29509911\n",
      "Iteration 967, loss = 0.29500838\n",
      "Iteration 968, loss = 0.29489654\n",
      "Iteration 969, loss = 0.29477597\n",
      "Iteration 970, loss = 0.29462012\n",
      "Iteration 971, loss = 0.29458451\n",
      "Iteration 972, loss = 0.29438145\n",
      "Iteration 973, loss = 0.29426077\n",
      "Iteration 974, loss = 0.29415110\n",
      "Iteration 975, loss = 0.29404432\n",
      "Iteration 976, loss = 0.29390568\n",
      "Iteration 977, loss = 0.29378691\n",
      "Iteration 978, loss = 0.29373214\n",
      "Iteration 979, loss = 0.29356012\n",
      "Iteration 980, loss = 0.29346848\n",
      "Iteration 981, loss = 0.29332092\n",
      "Iteration 982, loss = 0.29321673\n",
      "Iteration 983, loss = 0.29318800\n",
      "Iteration 984, loss = 0.29307276\n",
      "Iteration 985, loss = 0.29288163\n",
      "Iteration 986, loss = 0.29273627\n",
      "Iteration 987, loss = 0.29265715\n",
      "Iteration 988, loss = 0.29250253\n",
      "Iteration 989, loss = 0.29241608\n",
      "Iteration 990, loss = 0.29227469\n",
      "Iteration 991, loss = 0.29214645\n",
      "Iteration 992, loss = 0.29206675\n",
      "Iteration 993, loss = 0.29197251\n",
      "Iteration 994, loss = 0.29185437\n",
      "Iteration 995, loss = 0.29169891\n",
      "Iteration 996, loss = 0.29157924\n",
      "Iteration 997, loss = 0.29149723\n",
      "Iteration 998, loss = 0.29134871\n",
      "Iteration 999, loss = 0.29128617\n",
      "Iteration 1000, loss = 0.29113249\n",
      "Iteration 1001, loss = 0.29097550\n",
      "Iteration 1002, loss = 0.29085624\n",
      "Iteration 1003, loss = 0.29081068\n",
      "Iteration 1004, loss = 0.29074894\n",
      "Iteration 1005, loss = 0.29051974\n",
      "Iteration 1006, loss = 0.29041151\n",
      "Iteration 1007, loss = 0.29027763\n",
      "Iteration 1008, loss = 0.29016615\n",
      "Iteration 1009, loss = 0.29005817\n",
      "Iteration 1010, loss = 0.28998461\n",
      "Iteration 1011, loss = 0.28981656\n",
      "Iteration 1012, loss = 0.28973047\n",
      "Iteration 1013, loss = 0.28963209\n",
      "Iteration 1014, loss = 0.28947324\n",
      "Iteration 1015, loss = 0.28935619\n",
      "Iteration 1016, loss = 0.28929734\n",
      "Iteration 1017, loss = 0.28921789\n",
      "Iteration 1018, loss = 0.28900546\n",
      "Iteration 1019, loss = 0.28891788\n",
      "Iteration 1020, loss = 0.28877833\n",
      "Iteration 1021, loss = 0.28873164\n",
      "Iteration 1022, loss = 0.28857795\n",
      "Iteration 1023, loss = 0.28844438\n",
      "Iteration 1024, loss = 0.28834564\n",
      "Iteration 1025, loss = 0.28816939\n",
      "Iteration 1026, loss = 0.28806722\n",
      "Iteration 1027, loss = 0.28794344\n",
      "Iteration 1028, loss = 0.28791529\n",
      "Iteration 1029, loss = 0.28773926\n",
      "Iteration 1030, loss = 0.28759019\n",
      "Iteration 1031, loss = 0.28746112\n",
      "Iteration 1032, loss = 0.28738176\n",
      "Iteration 1033, loss = 0.28723785\n",
      "Iteration 1034, loss = 0.28715430\n",
      "Iteration 1035, loss = 0.28702968\n",
      "Iteration 1036, loss = 0.28689816\n",
      "Iteration 1037, loss = 0.28682603\n",
      "Iteration 1038, loss = 0.28666278\n",
      "Iteration 1039, loss = 0.28656522\n",
      "Iteration 1040, loss = 0.28644143\n",
      "Iteration 1041, loss = 0.28632028\n",
      "Iteration 1042, loss = 0.28618002\n",
      "Iteration 1043, loss = 0.28610133\n",
      "Iteration 1044, loss = 0.28596029\n",
      "Iteration 1045, loss = 0.28581678\n",
      "Iteration 1046, loss = 0.28571772\n",
      "Iteration 1047, loss = 0.28564802\n",
      "Iteration 1048, loss = 0.28555069\n",
      "Iteration 1049, loss = 0.28534654\n",
      "Iteration 1050, loss = 0.28522204\n",
      "Iteration 1051, loss = 0.28512815\n",
      "Iteration 1052, loss = 0.28521523\n",
      "Iteration 1053, loss = 0.28489926\n",
      "Iteration 1054, loss = 0.28474659\n",
      "Iteration 1055, loss = 0.28473512\n",
      "Iteration 1056, loss = 0.28452108\n",
      "Iteration 1057, loss = 0.28439661\n",
      "Iteration 1058, loss = 0.28429125\n",
      "Iteration 1059, loss = 0.28419233\n",
      "Iteration 1060, loss = 0.28405498\n",
      "Iteration 1061, loss = 0.28396849\n",
      "Iteration 1062, loss = 0.28385745\n",
      "Iteration 1063, loss = 0.28370445\n",
      "Iteration 1064, loss = 0.28355816\n",
      "Iteration 1065, loss = 0.28345504\n",
      "Iteration 1066, loss = 0.28337286\n",
      "Iteration 1067, loss = 0.28322992\n",
      "Iteration 1068, loss = 0.28310874\n",
      "Iteration 1069, loss = 0.28300611\n",
      "Iteration 1070, loss = 0.28286394\n",
      "Iteration 1071, loss = 0.28277881\n",
      "Iteration 1072, loss = 0.28262437\n",
      "Iteration 1073, loss = 0.28256364\n",
      "Iteration 1074, loss = 0.28244698\n",
      "Iteration 1075, loss = 0.28227726\n",
      "Iteration 1076, loss = 0.28216895\n",
      "Iteration 1077, loss = 0.28205725\n",
      "Iteration 1078, loss = 0.28193274\n",
      "Iteration 1079, loss = 0.28179381\n",
      "Iteration 1080, loss = 0.28166826\n",
      "Iteration 1081, loss = 0.28157221\n",
      "Iteration 1082, loss = 0.28145726\n",
      "Iteration 1083, loss = 0.28128835\n",
      "Iteration 1084, loss = 0.28120341\n",
      "Iteration 1085, loss = 0.28111026\n",
      "Iteration 1086, loss = 0.28094709\n",
      "Iteration 1087, loss = 0.28083506\n",
      "Iteration 1088, loss = 0.28070292\n",
      "Iteration 1089, loss = 0.28060770\n",
      "Iteration 1090, loss = 0.28049977\n",
      "Iteration 1091, loss = 0.28033981\n",
      "Iteration 1092, loss = 0.28032705\n",
      "Iteration 1093, loss = 0.28010538\n",
      "Iteration 1094, loss = 0.27996906\n",
      "Iteration 1095, loss = 0.27988730\n",
      "Iteration 1096, loss = 0.27976911\n",
      "Iteration 1097, loss = 0.27964257\n",
      "Iteration 1098, loss = 0.27952762\n",
      "Iteration 1099, loss = 0.27936193\n",
      "Iteration 1100, loss = 0.27931098\n",
      "Iteration 1101, loss = 0.27912667\n",
      "Iteration 1102, loss = 0.27901273\n",
      "Iteration 1103, loss = 0.27890230\n",
      "Iteration 1104, loss = 0.27877569\n",
      "Iteration 1105, loss = 0.27866789\n",
      "Iteration 1106, loss = 0.27859340\n",
      "Iteration 1107, loss = 0.27840828\n",
      "Iteration 1108, loss = 0.27840171\n",
      "Iteration 1109, loss = 0.27816455\n",
      "Iteration 1110, loss = 0.27808039\n",
      "Iteration 1111, loss = 0.27794055\n",
      "Iteration 1112, loss = 0.27782134\n",
      "Iteration 1113, loss = 0.27772209\n",
      "Iteration 1114, loss = 0.27760099\n",
      "Iteration 1115, loss = 0.27744814\n",
      "Iteration 1116, loss = 0.27734259\n",
      "Iteration 1117, loss = 0.27732677\n",
      "Iteration 1118, loss = 0.27735584\n",
      "Iteration 1119, loss = 0.27694473\n",
      "Iteration 1120, loss = 0.27683268\n",
      "Iteration 1121, loss = 0.27675767\n",
      "Iteration 1122, loss = 0.27665369\n",
      "Iteration 1123, loss = 0.27651452\n",
      "Iteration 1124, loss = 0.27641607\n",
      "Iteration 1125, loss = 0.27619973\n",
      "Iteration 1126, loss = 0.27619073\n",
      "Iteration 1127, loss = 0.27608076\n",
      "Iteration 1128, loss = 0.27585110\n",
      "Iteration 1129, loss = 0.27574883\n",
      "Iteration 1130, loss = 0.27561418\n",
      "Iteration 1131, loss = 0.27549028\n",
      "Iteration 1132, loss = 0.27537032\n",
      "Iteration 1133, loss = 0.27524877\n",
      "Iteration 1134, loss = 0.27511643\n",
      "Iteration 1135, loss = 0.27502302\n",
      "Iteration 1136, loss = 0.27488475\n",
      "Iteration 1137, loss = 0.27476517\n",
      "Iteration 1138, loss = 0.27462664\n",
      "Iteration 1139, loss = 0.27453291\n",
      "Iteration 1140, loss = 0.27440516\n",
      "Iteration 1141, loss = 0.27427603\n",
      "Iteration 1142, loss = 0.27416494\n",
      "Iteration 1143, loss = 0.27403767\n",
      "Iteration 1144, loss = 0.27393696\n",
      "Iteration 1145, loss = 0.27379966\n",
      "Iteration 1146, loss = 0.27366646\n",
      "Iteration 1147, loss = 0.27353863\n",
      "Iteration 1148, loss = 0.27343954\n",
      "Iteration 1149, loss = 0.27338063\n",
      "Iteration 1150, loss = 0.27322792\n",
      "Iteration 1151, loss = 0.27309908\n",
      "Iteration 1152, loss = 0.27304586\n",
      "Iteration 1153, loss = 0.27289493\n",
      "Iteration 1154, loss = 0.27275813\n",
      "Iteration 1155, loss = 0.27261641\n",
      "Iteration 1156, loss = 0.27249641\n",
      "Iteration 1157, loss = 0.27236574\n",
      "Iteration 1158, loss = 0.27225838\n",
      "Iteration 1159, loss = 0.27212329\n",
      "Iteration 1160, loss = 0.27201093\n",
      "Iteration 1161, loss = 0.27193391\n",
      "Iteration 1162, loss = 0.27179440\n",
      "Iteration 1163, loss = 0.27167720\n",
      "Iteration 1164, loss = 0.27149572\n",
      "Iteration 1165, loss = 0.27140747\n",
      "Iteration 1166, loss = 0.27125543\n",
      "Iteration 1167, loss = 0.27113103\n",
      "Iteration 1168, loss = 0.27105563\n",
      "Iteration 1169, loss = 0.27091325\n",
      "Iteration 1170, loss = 0.27078615\n",
      "Iteration 1171, loss = 0.27065912\n",
      "Iteration 1172, loss = 0.27057493\n",
      "Iteration 1173, loss = 0.27044756\n",
      "Iteration 1174, loss = 0.27031181\n",
      "Iteration 1175, loss = 0.27018378\n",
      "Iteration 1176, loss = 0.27005398\n",
      "Iteration 1177, loss = 0.26993375\n",
      "Iteration 1178, loss = 0.26987745\n",
      "Iteration 1179, loss = 0.26972019\n",
      "Iteration 1180, loss = 0.26961661\n",
      "Iteration 1181, loss = 0.26947309\n",
      "Iteration 1182, loss = 0.26935401\n",
      "Iteration 1183, loss = 0.26922869\n",
      "Iteration 1184, loss = 0.26907867\n",
      "Iteration 1185, loss = 0.26894075\n",
      "Iteration 1186, loss = 0.26888290\n",
      "Iteration 1187, loss = 0.26872459\n",
      "Iteration 1188, loss = 0.26859653\n",
      "Iteration 1189, loss = 0.26851010\n",
      "Iteration 1190, loss = 0.26835929\n",
      "Iteration 1191, loss = 0.26823989\n",
      "Iteration 1192, loss = 0.26817811\n",
      "Iteration 1193, loss = 0.26802131\n",
      "Iteration 1194, loss = 0.26793154\n",
      "Iteration 1195, loss = 0.26779654\n",
      "Iteration 1196, loss = 0.26765256\n",
      "Iteration 1197, loss = 0.26751370\n",
      "Iteration 1198, loss = 0.26739787\n",
      "Iteration 1199, loss = 0.26730003\n",
      "Iteration 1200, loss = 0.26715289\n",
      "Iteration 1201, loss = 0.26704638\n",
      "Iteration 1202, loss = 0.26689282\n",
      "Iteration 1203, loss = 0.26679796\n",
      "Iteration 1204, loss = 0.26669353\n",
      "Iteration 1205, loss = 0.26657755\n",
      "Iteration 1206, loss = 0.26662794\n",
      "Iteration 1207, loss = 0.26629670\n",
      "Iteration 1208, loss = 0.26618597\n",
      "Iteration 1209, loss = 0.26611418\n",
      "Iteration 1210, loss = 0.26597869\n",
      "Iteration 1211, loss = 0.26583714\n",
      "Iteration 1212, loss = 0.26576381\n",
      "Iteration 1213, loss = 0.26560157\n",
      "Iteration 1214, loss = 0.26549547\n",
      "Iteration 1215, loss = 0.26534362\n",
      "Iteration 1216, loss = 0.26526158\n",
      "Iteration 1217, loss = 0.26513019\n",
      "Iteration 1218, loss = 0.26504511\n",
      "Iteration 1219, loss = 0.26495666\n",
      "Iteration 1220, loss = 0.26476666\n",
      "Iteration 1221, loss = 0.26465836\n",
      "Iteration 1222, loss = 0.26451353\n",
      "Iteration 1223, loss = 0.26440064\n",
      "Iteration 1224, loss = 0.26432010\n",
      "Iteration 1225, loss = 0.26420113\n",
      "Iteration 1226, loss = 0.26406628\n",
      "Iteration 1227, loss = 0.26396152\n",
      "Iteration 1228, loss = 0.26382687\n",
      "Iteration 1229, loss = 0.26377276\n",
      "Iteration 1230, loss = 0.26361434\n",
      "Iteration 1231, loss = 0.26345153\n",
      "Iteration 1232, loss = 0.26337886\n",
      "Iteration 1233, loss = 0.26325039\n",
      "Iteration 1234, loss = 0.26312968\n",
      "Iteration 1235, loss = 0.26301869\n",
      "Iteration 1236, loss = 0.26289717\n",
      "Iteration 1237, loss = 0.26281239\n",
      "Iteration 1238, loss = 0.26263263\n",
      "Iteration 1239, loss = 0.26261128\n",
      "Iteration 1240, loss = 0.26244999\n",
      "Iteration 1241, loss = 0.26228148\n",
      "Iteration 1242, loss = 0.26218715\n",
      "Iteration 1243, loss = 0.26211012\n",
      "Iteration 1244, loss = 0.26196291\n",
      "Iteration 1245, loss = 0.26189877\n",
      "Iteration 1246, loss = 0.26175294\n",
      "Iteration 1247, loss = 0.26175097\n",
      "Iteration 1248, loss = 0.26147587\n",
      "Iteration 1249, loss = 0.26132978\n",
      "Iteration 1250, loss = 0.26122748\n",
      "Iteration 1251, loss = 0.26110540\n",
      "Iteration 1252, loss = 0.26097416\n",
      "Iteration 1253, loss = 0.26088196\n",
      "Iteration 1254, loss = 0.26075023\n",
      "Iteration 1255, loss = 0.26062970\n",
      "Iteration 1256, loss = 0.26053855\n",
      "Iteration 1257, loss = 0.26040437\n",
      "Iteration 1258, loss = 0.26025168\n",
      "Iteration 1259, loss = 0.26021119\n",
      "Iteration 1260, loss = 0.26006086\n",
      "Iteration 1261, loss = 0.25996708\n",
      "Iteration 1262, loss = 0.25985082\n",
      "Iteration 1263, loss = 0.25969386\n",
      "Iteration 1264, loss = 0.25956659\n",
      "Iteration 1265, loss = 0.25944971\n",
      "Iteration 1266, loss = 0.25933963\n",
      "Iteration 1267, loss = 0.25926529\n",
      "Iteration 1268, loss = 0.25907564\n",
      "Iteration 1269, loss = 0.25897223\n",
      "Iteration 1270, loss = 0.25889293\n",
      "Iteration 1271, loss = 0.25873350\n",
      "Iteration 1272, loss = 0.25863174\n",
      "Iteration 1739, loss = 0.14453024\n",
      "Iteration 1740, loss = 0.14440468\n",
      "Iteration 1741, loss = 0.14424574\n",
      "Iteration 1742, loss = 0.14406942\n",
      "Iteration 1743, loss = 0.14393183\n",
      "Iteration 1744, loss = 0.14381020\n",
      "Iteration 1745, loss = 0.14357920\n",
      "Iteration 1746, loss = 0.14341419\n",
      "Iteration 1747, loss = 0.14323164\n",
      "Iteration 1748, loss = 0.14320442\n",
      "Iteration 1749, loss = 0.14300950\n",
      "Iteration 1750, loss = 0.14292706\n",
      "Iteration 1751, loss = 0.14261742\n",
      "Iteration 1752, loss = 0.14264616\n",
      "Iteration 1753, loss = 0.14241579\n",
      "Iteration 1754, loss = 0.14223659\n",
      "Iteration 1755, loss = 0.14199529\n",
      "Iteration 1756, loss = 0.14197179\n",
      "Iteration 1757, loss = 0.14171280\n",
      "Iteration 1758, loss = 0.14169374\n",
      "Iteration 1759, loss = 0.14141642\n",
      "Iteration 1760, loss = 0.14132406\n",
      "Iteration 1761, loss = 0.14105940\n",
      "Iteration 1762, loss = 0.14099878\n",
      "Iteration 1763, loss = 0.14082014\n",
      "Iteration 1764, loss = 0.14064968\n",
      "Iteration 1765, loss = 0.14055065\n",
      "Iteration 1766, loss = 0.14035474\n",
      "Iteration 1767, loss = 0.14009902\n",
      "Iteration 1768, loss = 0.14002582\n",
      "Iteration 1769, loss = 0.13987728\n",
      "Iteration 1770, loss = 0.13961589\n",
      "Iteration 1771, loss = 0.13948345\n",
      "Iteration 1772, loss = 0.13951860\n",
      "Iteration 1773, loss = 0.13935694\n",
      "Iteration 1774, loss = 0.13909794\n",
      "Iteration 1775, loss = 0.13888629\n",
      "Iteration 1776, loss = 0.13878039\n",
      "Iteration 1777, loss = 0.13866176\n",
      "Iteration 1778, loss = 0.13861624\n",
      "Iteration 1779, loss = 0.13829286\n",
      "Iteration 1780, loss = 0.13814269\n",
      "Iteration 1781, loss = 0.13804075\n",
      "Iteration 1782, loss = 0.13799701\n",
      "Iteration 1783, loss = 0.13770095\n",
      "Iteration 1784, loss = 0.13764225\n",
      "Iteration 1785, loss = 0.13738760\n",
      "Iteration 1786, loss = 0.13726107\n",
      "Iteration 1787, loss = 0.13728623\n",
      "Iteration 1788, loss = 0.13704925\n",
      "Iteration 1789, loss = 0.13680084\n",
      "Iteration 1790, loss = 0.13659021\n",
      "Iteration 1791, loss = 0.13647851\n",
      "Iteration 1792, loss = 0.13629513\n",
      "Iteration 1793, loss = 0.13612649\n",
      "Iteration 1794, loss = 0.13606632\n",
      "Iteration 1795, loss = 0.13585315\n",
      "Iteration 1796, loss = 0.13588529\n",
      "Iteration 1797, loss = 0.13561816\n",
      "Iteration 1798, loss = 0.13553687\n",
      "Iteration 1799, loss = 0.13533306\n",
      "Iteration 1800, loss = 0.13518025\n",
      "Iteration 1801, loss = 0.13490182\n",
      "Iteration 1802, loss = 0.13487208\n",
      "Iteration 1803, loss = 0.13467856\n",
      "Iteration 1804, loss = 0.13446349\n",
      "Iteration 1805, loss = 0.13429138\n",
      "Iteration 1806, loss = 0.13433978\n",
      "Iteration 1807, loss = 0.13400306\n",
      "Iteration 1808, loss = 0.13387047\n",
      "Iteration 1809, loss = 0.13371068\n",
      "Iteration 1810, loss = 0.13366146\n",
      "Iteration 1811, loss = 0.13344429\n",
      "Iteration 1812, loss = 0.13336591\n",
      "Iteration 1813, loss = 0.13309888\n",
      "Iteration 1814, loss = 0.13302528\n",
      "Iteration 1815, loss = 0.13280191\n",
      "Iteration 1816, loss = 0.13278126\n",
      "Iteration 1817, loss = 0.13254971\n",
      "Iteration 1818, loss = 0.13238784\n",
      "Iteration 1819, loss = 0.13240560\n",
      "Iteration 1820, loss = 0.13223114\n",
      "Iteration 1821, loss = 0.13195498\n",
      "Iteration 1822, loss = 0.13182878\n",
      "Iteration 1823, loss = 0.13186519\n",
      "Iteration 1824, loss = 0.13151926\n",
      "Iteration 1825, loss = 0.13144934\n",
      "Iteration 1826, loss = 0.13122088\n",
      "Iteration 1827, loss = 0.13108997\n",
      "Iteration 1828, loss = 0.13096806\n",
      "Iteration 1829, loss = 0.13088276\n",
      "Iteration 1830, loss = 0.13061327\n",
      "Iteration 1831, loss = 0.13047017\n",
      "Iteration 1832, loss = 0.13034303\n",
      "Iteration 1833, loss = 0.13050656\n",
      "Iteration 1834, loss = 0.13010946\n",
      "Iteration 1835, loss = 0.13032236\n",
      "Iteration 1836, loss = 0.12984533\n",
      "Iteration 1837, loss = 0.12964723\n",
      "Iteration 1838, loss = 0.12953943\n",
      "Iteration 1839, loss = 0.12930291\n",
      "Iteration 1840, loss = 0.12921899\n",
      "Iteration 1841, loss = 0.12909267\n",
      "Iteration 1842, loss = 0.12906106\n",
      "Iteration 1843, loss = 0.12877849\n",
      "Iteration 1844, loss = 0.12869100\n",
      "Iteration 1845, loss = 0.12849469\n",
      "Iteration 1846, loss = 0.12824815\n",
      "Iteration 1847, loss = 0.12813614\n",
      "Iteration 1848, loss = 0.12799629\n",
      "Iteration 1849, loss = 0.12790578\n",
      "Iteration 1850, loss = 0.12780862\n",
      "Iteration 1851, loss = 0.12753444\n",
      "Iteration 1852, loss = 0.12739443\n",
      "Iteration 1853, loss = 0.12732569\n",
      "Iteration 1854, loss = 0.12718194\n",
      "Iteration 1855, loss = 0.12720846\n",
      "Iteration 1856, loss = 0.12697087\n",
      "Iteration 1857, loss = 0.12678474\n",
      "Iteration 1858, loss = 0.12655839\n",
      "Iteration 1859, loss = 0.12642864\n",
      "Iteration 1860, loss = 0.12640668\n",
      "Iteration 1861, loss = 0.12623731\n",
      "Iteration 1862, loss = 0.12595642\n",
      "Iteration 1863, loss = 0.12588437\n",
      "Iteration 1864, loss = 0.12587066\n",
      "Iteration 1865, loss = 0.12550465\n",
      "Iteration 1866, loss = 0.12550414\n",
      "Iteration 1867, loss = 0.12536932\n",
      "Iteration 1868, loss = 0.12515721\n",
      "Iteration 1869, loss = 0.12504035\n",
      "Iteration 1870, loss = 0.12499442\n",
      "Iteration 1871, loss = 0.12480759\n",
      "Iteration 1872, loss = 0.12459441\n",
      "Iteration 1873, loss = 0.12433201\n",
      "Iteration 1874, loss = 0.12438256\n",
      "Iteration 1875, loss = 0.12411893\n",
      "Iteration 1876, loss = 0.12393244\n",
      "Iteration 1877, loss = 0.12396983\n",
      "Iteration 1878, loss = 0.12364430\n",
      "Iteration 1879, loss = 0.12401297\n",
      "Iteration 1880, loss = 0.12351691\n",
      "Iteration 1881, loss = 0.12323630\n",
      "Iteration 1882, loss = 0.12311082\n",
      "Iteration 1883, loss = 0.12299428\n",
      "Iteration 1884, loss = 0.12287974\n",
      "Iteration 1885, loss = 0.12285252\n",
      "Iteration 1886, loss = 0.12257676\n",
      "Iteration 1887, loss = 0.12247228\n",
      "Iteration 1888, loss = 0.12231734\n",
      "Iteration 1889, loss = 0.12227557\n",
      "Iteration 1890, loss = 0.12200930\n",
      "Iteration 1891, loss = 0.12195010\n",
      "Iteration 1892, loss = 0.12182467\n",
      "Iteration 1893, loss = 0.12171549\n",
      "Iteration 1894, loss = 0.12154513\n",
      "Iteration 1895, loss = 0.12138578\n",
      "Iteration 1896, loss = 0.12140073\n",
      "Iteration 1897, loss = 0.12112733\n",
      "Iteration 1898, loss = 0.12091588\n",
      "Iteration 1899, loss = 0.12075733\n",
      "Iteration 1900, loss = 0.12067354\n",
      "Iteration 1901, loss = 0.12048143\n",
      "Iteration 1902, loss = 0.12043276\n",
      "Iteration 1903, loss = 0.12023704\n",
      "Iteration 1904, loss = 0.12014210\n",
      "Iteration 1905, loss = 0.12000002\n",
      "Iteration 1906, loss = 0.11991865\n",
      "Iteration 1907, loss = 0.11975733\n",
      "Iteration 1908, loss = 0.11951062\n",
      "Iteration 1909, loss = 0.11945410\n",
      "Iteration 1910, loss = 0.11923703\n",
      "Iteration 1911, loss = 0.11924682\n",
      "Iteration 1912, loss = 0.11902790\n",
      "Iteration 1913, loss = 0.11890426\n",
      "Iteration 1914, loss = 0.11873761\n",
      "Iteration 1915, loss = 0.11856470\n",
      "Iteration 1916, loss = 0.11846510\n",
      "Iteration 1917, loss = 0.11853923\n",
      "Iteration 1918, loss = 0.11816899\n",
      "Iteration 1919, loss = 0.11808119\n",
      "Iteration 1920, loss = 0.11807192\n",
      "Iteration 1921, loss = 0.11785969\n",
      "Iteration 1922, loss = 0.11764372\n",
      "Iteration 1923, loss = 0.11767425\n",
      "Iteration 1924, loss = 0.11746937\n",
      "Iteration 1925, loss = 0.11730848\n",
      "Iteration 1926, loss = 0.11726982\n",
      "Iteration 1927, loss = 0.11700285\n",
      "Iteration 1928, loss = 0.11706164\n",
      "Iteration 1929, loss = 0.11672160\n",
      "Iteration 1930, loss = 0.11668683\n",
      "Iteration 1931, loss = 0.11645181\n",
      "Iteration 1932, loss = 0.11640792\n",
      "Iteration 1933, loss = 0.11619485\n",
      "Iteration 1934, loss = 0.11611999\n",
      "Iteration 1935, loss = 0.11598606\n",
      "Iteration 1936, loss = 0.11586045\n",
      "Iteration 1937, loss = 0.11566415\n",
      "Iteration 1938, loss = 0.11570654\n",
      "Iteration 1939, loss = 0.11544242\n",
      "Iteration 1940, loss = 0.11528394\n",
      "Iteration 1941, loss = 0.11530108\n",
      "Iteration 1942, loss = 0.11508532\n",
      "Iteration 1943, loss = 0.11509730\n",
      "Iteration 1944, loss = 0.11496114\n",
      "Iteration 1945, loss = 0.11503447\n",
      "Iteration 1946, loss = 0.11462026\n",
      "Iteration 1947, loss = 0.11442495\n",
      "Iteration 1948, loss = 0.11436167\n",
      "Iteration 1949, loss = 0.11420113\n",
      "Iteration 1950, loss = 0.11412901\n",
      "Iteration 1951, loss = 0.11388705\n",
      "Iteration 1952, loss = 0.11380786\n",
      "Iteration 1953, loss = 0.11367453\n",
      "Iteration 1954, loss = 0.11366857\n",
      "Iteration 1955, loss = 0.11341052\n",
      "Iteration 1956, loss = 0.11345356\n",
      "Iteration 1957, loss = 0.11324304\n",
      "Iteration 1958, loss = 0.11316484\n",
      "Iteration 1959, loss = 0.11294514\n",
      "Iteration 1960, loss = 0.11287063\n",
      "Iteration 1961, loss = 0.11279563\n",
      "Iteration 1962, loss = 0.11259940\n",
      "Iteration 1963, loss = 0.11252096\n",
      "Iteration 1964, loss = 0.11247509\n",
      "Iteration 1965, loss = 0.11230401\n",
      "Iteration 1966, loss = 0.11206402\n",
      "Iteration 1967, loss = 0.11195322\n",
      "Iteration 1968, loss = 0.11196830\n",
      "Iteration 1969, loss = 0.11173542\n",
      "Iteration 1970, loss = 0.11160748\n",
      "Iteration 1971, loss = 0.11162098\n",
      "Iteration 1972, loss = 0.11136250\n",
      "Iteration 1973, loss = 0.11120385\n",
      "Iteration 1974, loss = 0.11109817\n",
      "Iteration 1975, loss = 0.11098208\n",
      "Iteration 1976, loss = 0.11086445\n",
      "Iteration 1977, loss = 0.11093438\n",
      "Iteration 1978, loss = 0.11070130\n",
      "Iteration 1979, loss = 0.11051375\n",
      "Iteration 1980, loss = 0.11051572\n",
      "Iteration 1981, loss = 0.11030203\n",
      "Iteration 1982, loss = 0.11016656\n",
      "Iteration 1983, loss = 0.11005662\n",
      "Iteration 1984, loss = 0.10989229\n",
      "Iteration 1985, loss = 0.10979536\n",
      "Iteration 1986, loss = 0.10974826\n",
      "Iteration 1987, loss = 0.10965382\n",
      "Iteration 1988, loss = 0.10953013\n",
      "Iteration 1989, loss = 0.10928865\n",
      "Iteration 1990, loss = 0.10918968\n",
      "Iteration 1991, loss = 0.10914260\n",
      "Iteration 1992, loss = 0.10898421\n",
      "Iteration 1993, loss = 0.10894920\n",
      "Iteration 1994, loss = 0.10885056\n",
      "Iteration 1995, loss = 0.10861378\n",
      "Iteration 1996, loss = 0.10856598\n",
      "Iteration 1997, loss = 0.10838687\n",
      "Iteration 1998, loss = 0.10839471\n",
      "Iteration 1999, loss = 0.10822541\n",
      "Iteration 2000, loss = 0.10813756\n",
      "Iteration 2001, loss = 0.10787828\n",
      "Iteration 2002, loss = 0.10810453\n",
      "Iteration 2003, loss = 0.10772492\n",
      "Iteration 2004, loss = 0.10762827\n",
      "Iteration 2005, loss = 0.10755644\n",
      "Iteration 2006, loss = 0.10752070\n",
      "Iteration 2007, loss = 0.10728370\n",
      "Iteration 2008, loss = 0.10726382\n",
      "Iteration 2009, loss = 0.10723598\n",
      "Iteration 2010, loss = 0.10692950\n",
      "Iteration 2011, loss = 0.10696961\n",
      "Iteration 2012, loss = 0.10673678\n",
      "Iteration 2013, loss = 0.10661006\n",
      "Iteration 2014, loss = 0.10651416\n",
      "Iteration 2015, loss = 0.10637949\n",
      "Iteration 2016, loss = 0.10631019\n",
      "Iteration 2017, loss = 0.10631588\n",
      "Iteration 2018, loss = 0.10611179\n",
      "Iteration 2019, loss = 0.10592572\n",
      "Iteration 2020, loss = 0.10580614\n",
      "Iteration 2021, loss = 0.10569155\n",
      "Iteration 2022, loss = 0.10561582\n",
      "Iteration 2023, loss = 0.10555287\n",
      "Iteration 2024, loss = 0.10548783\n",
      "Iteration 2025, loss = 0.10531563\n",
      "Iteration 2026, loss = 0.10519717\n",
      "Iteration 2027, loss = 0.10507901\n",
      "Iteration 2028, loss = 0.10496654\n",
      "Iteration 2029, loss = 0.10489120\n",
      "Iteration 2030, loss = 0.10478568\n",
      "Iteration 2031, loss = 0.10463736\n",
      "Iteration 2032, loss = 0.10456344\n",
      "Iteration 2033, loss = 0.10451894\n",
      "Iteration 2034, loss = 0.10429795\n",
      "Iteration 2035, loss = 0.10423280\n",
      "Iteration 2036, loss = 0.10407994\n",
      "Iteration 2037, loss = 0.10398239\n",
      "Iteration 2038, loss = 0.10388036\n",
      "Iteration 2039, loss = 0.10385793\n",
      "Iteration 2040, loss = 0.10364980\n",
      "Iteration 2041, loss = 0.10364865\n",
      "Iteration 2042, loss = 0.10350467\n",
      "Iteration 2043, loss = 0.10344638\n",
      "Iteration 2044, loss = 0.10333477\n",
      "Iteration 2045, loss = 0.10322055\n",
      "Iteration 2046, loss = 0.10306879\n",
      "Iteration 2047, loss = 0.10302731\n",
      "Iteration 2048, loss = 0.10285388\n",
      "Iteration 2049, loss = 0.10299866\n",
      "Iteration 2050, loss = 0.10276366\n",
      "Iteration 2051, loss = 0.10282318\n",
      "Iteration 2052, loss = 0.10252184\n",
      "Iteration 2053, loss = 0.10240416\n",
      "Iteration 2054, loss = 0.10232711\n",
      "Iteration 2055, loss = 0.10223830\n",
      "Iteration 2056, loss = 0.10214253\n",
      "Iteration 2057, loss = 0.10196188\n",
      "Iteration 2058, loss = 0.10181831\n",
      "Iteration 2059, loss = 0.10174292\n",
      "Iteration 2060, loss = 0.10166652\n",
      "Iteration 2061, loss = 0.10159109\n",
      "Iteration 2062, loss = 0.10141501\n",
      "Iteration 2063, loss = 0.10136423\n",
      "Iteration 2064, loss = 0.10129090\n",
      "Iteration 2065, loss = 0.10112566\n",
      "Iteration 2066, loss = 0.10117281\n",
      "Iteration 2067, loss = 0.10093147\n",
      "Iteration 2068, loss = 0.10092948\n",
      "Iteration 2069, loss = 0.10079664\n",
      "Iteration 2070, loss = 0.10067186\n",
      "Iteration 2071, loss = 0.10060377\n",
      "Iteration 2072, loss = 0.10053611\n",
      "Iteration 2073, loss = 0.10037769\n",
      "Iteration 2074, loss = 0.10034823\n",
      "Iteration 2075, loss = 0.10012565\n",
      "Iteration 2076, loss = 0.10004608\n",
      "Iteration 2077, loss = 0.09996037\n",
      "Iteration 2078, loss = 0.09987857\n",
      "Iteration 2079, loss = 0.09978638\n",
      "Iteration 2080, loss = 0.09979163\n",
      "Iteration 2081, loss = 0.09978414\n",
      "Iteration 2082, loss = 0.09965466\n",
      "Iteration 2083, loss = 0.09953001\n",
      "Iteration 2084, loss = 0.09930150\n",
      "Iteration 2085, loss = 0.09939701\n",
      "Iteration 2086, loss = 0.09908864\n",
      "Iteration 2087, loss = 0.09897698\n",
      "Iteration 2088, loss = 0.09903758\n",
      "Iteration 2089, loss = 0.09879045\n",
      "Iteration 2090, loss = 0.09869952\n",
      "Iteration 2091, loss = 0.09867715\n",
      "Iteration 2092, loss = 0.09849891\n",
      "Iteration 2093, loss = 0.09845907\n",
      "Iteration 2094, loss = 0.09838270\n",
      "Iteration 2095, loss = 0.09827182\n",
      "Iteration 2096, loss = 0.09812947\n",
      "Iteration 2097, loss = 0.09802777\n",
      "Iteration 2098, loss = 0.09813830\n",
      "Iteration 2099, loss = 0.09781845\n",
      "Iteration 2100, loss = 0.09775036\n",
      "Iteration 2101, loss = 0.09787927\n",
      "Iteration 2102, loss = 0.09755104\n",
      "Iteration 2103, loss = 0.09747585\n",
      "Iteration 2104, loss = 0.09753034\n",
      "Iteration 2105, loss = 0.09725902\n",
      "Iteration 2106, loss = 0.09719892\n",
      "Iteration 2107, loss = 0.09723731\n",
      "Iteration 2108, loss = 0.09708637\n",
      "Iteration 2109, loss = 0.09703166\n",
      "Iteration 2110, loss = 0.09705910\n",
      "Iteration 2111, loss = 0.09683394\n",
      "Iteration 2112, loss = 0.09665353\n",
      "Iteration 2113, loss = 0.09678814\n",
      "Iteration 2114, loss = 0.09649945\n",
      "Iteration 2115, loss = 0.09641894\n",
      "Iteration 2116, loss = 0.09631112\n",
      "Iteration 2117, loss = 0.09622974\n",
      "Iteration 2118, loss = 0.09614285\n",
      "Iteration 2119, loss = 0.09614532\n",
      "Iteration 2120, loss = 0.09600883\n",
      "Iteration 2121, loss = 0.09589569\n",
      "Iteration 2122, loss = 0.09589205\n",
      "Iteration 2123, loss = 0.09571423\n",
      "Iteration 2124, loss = 0.09557096\n",
      "Iteration 2125, loss = 0.09556082\n",
      "Iteration 2126, loss = 0.09539843\n",
      "Iteration 2127, loss = 0.09534508\n",
      "Iteration 2128, loss = 0.09526583\n",
      "Iteration 2129, loss = 0.09518339\n",
      "Iteration 2130, loss = 0.09509202\n",
      "Iteration 2131, loss = 0.09503387\n",
      "Iteration 2132, loss = 0.09493468\n",
      "Iteration 2133, loss = 0.09485665\n",
      "Iteration 2134, loss = 0.09483646\n",
      "Iteration 2135, loss = 0.09468961\n",
      "Iteration 2136, loss = 0.09475672\n",
      "Iteration 2137, loss = 0.09461309\n",
      "Iteration 2138, loss = 0.09436135\n",
      "Iteration 2139, loss = 0.09434207\n",
      "Iteration 2140, loss = 0.09437967\n",
      "Iteration 2141, loss = 0.09417276\n",
      "Iteration 2142, loss = 0.09407083\n",
      "Iteration 2143, loss = 0.09398657\n",
      "Iteration 2144, loss = 0.09384297\n",
      "Iteration 2145, loss = 0.09376691\n",
      "Iteration 2146, loss = 0.09379037\n",
      "Iteration 2147, loss = 0.09362195\n",
      "Iteration 2148, loss = 0.09363918\n",
      "Iteration 2149, loss = 0.09348582\n",
      "Iteration 2150, loss = 0.09337321\n",
      "Iteration 2151, loss = 0.09330074\n",
      "Iteration 2152, loss = 0.09321314\n",
      "Iteration 2153, loss = 0.09323119\n",
      "Iteration 2154, loss = 0.09335581\n",
      "Iteration 2155, loss = 0.09303680\n",
      "Iteration 2156, loss = 0.09289155\n",
      "Iteration 2157, loss = 0.09290502\n",
      "Iteration 2158, loss = 0.09288869\n",
      "Iteration 2159, loss = 0.09266247\n",
      "Iteration 2160, loss = 0.09257498\n",
      "Iteration 2161, loss = 0.09254019\n",
      "Iteration 2162, loss = 0.09237419\n",
      "Iteration 2163, loss = 0.09237463\n",
      "Iteration 2164, loss = 0.09229676\n",
      "Iteration 2165, loss = 0.09225256\n",
      "Iteration 2166, loss = 0.09216208\n",
      "Iteration 2167, loss = 0.09212446\n",
      "Iteration 2168, loss = 0.09201280\n",
      "Iteration 2169, loss = 0.09191245\n",
      "Iteration 2170, loss = 0.09182433\n",
      "Iteration 2171, loss = 0.09181142\n",
      "Iteration 2172, loss = 0.09164575\n",
      "Iteration 2173, loss = 0.09166969\n",
      "Iteration 2174, loss = 0.09149565\n",
      "Iteration 2175, loss = 0.09141002\n",
      "Iteration 2176, loss = 0.09134171\n",
      "Iteration 2177, loss = 0.09147470\n",
      "Iteration 2178, loss = 0.09120796\n",
      "Iteration 2179, loss = 0.09111345\n",
      "Iteration 2180, loss = 0.09103965\n",
      "Iteration 2181, loss = 0.09102343\n",
      "Iteration 2182, loss = 0.09088788\n",
      "Iteration 2183, loss = 0.09081903\n",
      "Iteration 2184, loss = 0.09074439\n",
      "Iteration 2185, loss = 0.09065667\n",
      "Iteration 2186, loss = 0.09064386\n",
      "Iteration 2187, loss = 0.09050930\n",
      "Iteration 2188, loss = 0.09044692\n",
      "Iteration 2189, loss = 0.09049623\n",
      "Iteration 2190, loss = 0.09026913\n",
      "Iteration 2191, loss = 0.09023787\n",
      "Iteration 2192, loss = 0.09009657\n",
      "Iteration 2193, loss = 0.09009255\n",
      "Iteration 2194, loss = 0.08999194\n",
      "Iteration 2195, loss = 0.08995443\n",
      "Iteration 2196, loss = 0.08982335\n",
      "Iteration 2197, loss = 0.08984574\n",
      "Iteration 2198, loss = 0.08967224\n",
      "Iteration 2199, loss = 0.08961981\n",
      "Iteration 2200, loss = 0.08950677\n",
      "Iteration 2201, loss = 0.08946576\n",
      "Iteration 2202, loss = 0.08939510\n",
      "Iteration 2203, loss = 0.08931981\n",
      "Iteration 2204, loss = 0.08931571\n",
      "Iteration 2205, loss = 0.08920586\n",
      "Iteration 2206, loss = 0.08928089\n",
      "Iteration 2207, loss = 0.08904150\n",
      "Iteration 2208, loss = 0.08902807\n",
      "Iteration 2209, loss = 0.08887682\n",
      "Iteration 2210, loss = 0.08884135\n",
      "Iteration 2211, loss = 0.08875381\n",
      "Iteration 2212, loss = 0.08868473\n",
      "Iteration 2213, loss = 0.08861677\n",
      "Iteration 2214, loss = 0.08858472\n",
      "Iteration 2215, loss = 0.08846734\n",
      "Iteration 2216, loss = 0.08841770\n",
      "Iteration 2217, loss = 0.08828322\n",
      "Iteration 2218, loss = 0.08829355\n",
      "Iteration 485, loss = 0.31196841\n",
      "Iteration 486, loss = 0.31183635\n",
      "Iteration 487, loss = 0.31171538\n",
      "Iteration 488, loss = 0.31160964\n",
      "Iteration 489, loss = 0.31155768\n",
      "Iteration 490, loss = 0.31140593\n",
      "Iteration 491, loss = 0.31132127\n",
      "Iteration 492, loss = 0.31121367\n",
      "Iteration 493, loss = 0.31111202\n",
      "Iteration 494, loss = 0.31099268\n",
      "Iteration 495, loss = 0.31090384\n",
      "Iteration 496, loss = 0.31081483\n",
      "Iteration 497, loss = 0.31070721\n",
      "Iteration 498, loss = 0.31061241\n",
      "Iteration 499, loss = 0.31050241\n",
      "Iteration 500, loss = 0.31045291\n",
      "Iteration 501, loss = 0.31031709\n",
      "Iteration 502, loss = 0.31021113\n",
      "Iteration 503, loss = 0.31013689\n",
      "Iteration 504, loss = 0.31000986\n",
      "Iteration 505, loss = 0.30991144\n",
      "Iteration 506, loss = 0.30981250\n",
      "Iteration 507, loss = 0.30972565\n",
      "Iteration 508, loss = 0.30961827\n",
      "Iteration 509, loss = 0.30952084\n",
      "Iteration 510, loss = 0.30942738\n",
      "Iteration 511, loss = 0.30932840\n",
      "Iteration 512, loss = 0.30923854\n",
      "Iteration 513, loss = 0.30913381\n",
      "Iteration 514, loss = 0.30903375\n",
      "Iteration 515, loss = 0.30893277\n",
      "Iteration 516, loss = 0.30883843\n",
      "Iteration 517, loss = 0.30875257\n",
      "Iteration 518, loss = 0.30863663\n",
      "Iteration 519, loss = 0.30858629\n",
      "Iteration 520, loss = 0.30848012\n",
      "Iteration 521, loss = 0.30838681\n",
      "Iteration 522, loss = 0.30825370\n",
      "Iteration 523, loss = 0.30816308\n",
      "Iteration 524, loss = 0.30805783\n",
      "Iteration 525, loss = 0.30796452\n",
      "Iteration 526, loss = 0.30787640\n",
      "Iteration 527, loss = 0.30780808\n",
      "Iteration 528, loss = 0.30768796\n",
      "Iteration 529, loss = 0.30759911\n",
      "Iteration 530, loss = 0.30750010\n",
      "Iteration 531, loss = 0.30740445\n",
      "Iteration 532, loss = 0.30732803\n",
      "Iteration 533, loss = 0.30724386\n",
      "Iteration 534, loss = 0.30712637\n",
      "Iteration 535, loss = 0.30702217\n",
      "Iteration 536, loss = 0.30693305\n",
      "Iteration 537, loss = 0.30686062\n",
      "Iteration 538, loss = 0.30681326\n",
      "Iteration 539, loss = 0.30664674\n",
      "Iteration 540, loss = 0.30659941\n",
      "Iteration 541, loss = 0.30646809\n",
      "Iteration 542, loss = 0.30635763\n",
      "Iteration 543, loss = 0.30627168\n",
      "Iteration 544, loss = 0.30618278\n",
      "Iteration 545, loss = 0.30607020\n",
      "Iteration 546, loss = 0.30598761\n",
      "Iteration 547, loss = 0.30590760\n",
      "Iteration 548, loss = 0.30581019\n",
      "Iteration 549, loss = 0.30572057\n",
      "Iteration 550, loss = 0.30566431\n",
      "Iteration 551, loss = 0.30553239\n",
      "Iteration 552, loss = 0.30542686\n",
      "Iteration 553, loss = 0.30533090\n",
      "Iteration 554, loss = 0.30523062\n",
      "Iteration 555, loss = 0.30515342\n",
      "Iteration 556, loss = 0.30506811\n",
      "Iteration 557, loss = 0.30495585\n",
      "Iteration 558, loss = 0.30486424\n",
      "Iteration 559, loss = 0.30476276\n",
      "Iteration 560, loss = 0.30468215\n",
      "Iteration 561, loss = 0.30460433\n",
      "Iteration 562, loss = 0.30450803\n",
      "Iteration 563, loss = 0.30442843\n",
      "Iteration 564, loss = 0.30431602\n",
      "Iteration 565, loss = 0.30420531\n",
      "Iteration 566, loss = 0.30413840\n",
      "Iteration 567, loss = 0.30409476\n",
      "Iteration 568, loss = 0.30394778\n",
      "Iteration 569, loss = 0.30384485\n",
      "Iteration 570, loss = 0.30376182\n",
      "Iteration 571, loss = 0.30366139\n",
      "Iteration 572, loss = 0.30357879\n",
      "Iteration 573, loss = 0.30348878\n",
      "Iteration 574, loss = 0.30341861\n",
      "Iteration 575, loss = 0.30331792\n",
      "Iteration 576, loss = 0.30323347\n",
      "Iteration 577, loss = 0.30316763\n",
      "Iteration 578, loss = 0.30302146\n",
      "Iteration 579, loss = 0.30296055\n",
      "Iteration 580, loss = 0.30285516\n",
      "Iteration 581, loss = 0.30279196\n",
      "Iteration 582, loss = 0.30265115\n",
      "Iteration 583, loss = 0.30258577\n",
      "Iteration 584, loss = 0.30250736\n",
      "Iteration 585, loss = 0.30239055\n",
      "Iteration 586, loss = 0.30230481\n",
      "Iteration 587, loss = 0.30221534\n",
      "Iteration 588, loss = 0.30210706\n",
      "Iteration 589, loss = 0.30204122\n",
      "Iteration 590, loss = 0.30194163\n",
      "Iteration 591, loss = 0.30185001\n",
      "Iteration 592, loss = 0.30178036\n",
      "Iteration 593, loss = 0.30164918\n",
      "Iteration 594, loss = 0.30156871\n",
      "Iteration 595, loss = 0.30146379\n",
      "Iteration 596, loss = 0.30140347\n",
      "Iteration 597, loss = 0.30130735\n",
      "Iteration 598, loss = 0.30122047\n",
      "Iteration 599, loss = 0.30109722\n",
      "Iteration 600, loss = 0.30100529\n",
      "Iteration 601, loss = 0.30092146\n",
      "Iteration 602, loss = 0.30082624\n",
      "Iteration 603, loss = 0.30075720\n",
      "Iteration 604, loss = 0.30064243\n",
      "Iteration 605, loss = 0.30054861\n",
      "Iteration 606, loss = 0.30045825\n",
      "Iteration 607, loss = 0.30036769\n",
      "Iteration 608, loss = 0.30028989\n",
      "Iteration 609, loss = 0.30015448\n",
      "Iteration 610, loss = 0.30009042\n",
      "Iteration 611, loss = 0.29999051\n",
      "Iteration 612, loss = 0.29988499\n",
      "Iteration 613, loss = 0.29979746\n",
      "Iteration 614, loss = 0.29969972\n",
      "Iteration 615, loss = 0.29960046\n",
      "Iteration 616, loss = 0.29950887\n",
      "Iteration 617, loss = 0.29942579\n",
      "Iteration 618, loss = 0.29931312\n",
      "Iteration 619, loss = 0.29922490\n",
      "Iteration 620, loss = 0.29916705\n",
      "Iteration 621, loss = 0.29906313\n",
      "Iteration 622, loss = 0.29895444\n",
      "Iteration 623, loss = 0.29885024\n",
      "Iteration 624, loss = 0.29875650\n",
      "Iteration 625, loss = 0.29867515\n",
      "Iteration 626, loss = 0.29858166\n",
      "Iteration 627, loss = 0.29847056\n",
      "Iteration 628, loss = 0.29839558\n",
      "Iteration 629, loss = 0.29829985\n",
      "Iteration 630, loss = 0.29822002\n",
      "Iteration 631, loss = 0.29812596\n",
      "Iteration 632, loss = 0.29802165\n",
      "Iteration 633, loss = 0.29794734\n",
      "Iteration 634, loss = 0.29783469\n",
      "Iteration 635, loss = 0.29774430\n",
      "Iteration 636, loss = 0.29765923\n",
      "Iteration 637, loss = 0.29756502\n",
      "Iteration 638, loss = 0.29746670\n",
      "Iteration 639, loss = 0.29739217\n",
      "Iteration 640, loss = 0.29732997\n",
      "Iteration 641, loss = 0.29718925\n",
      "Iteration 642, loss = 0.29710274\n",
      "Iteration 643, loss = 0.29699303\n",
      "Iteration 644, loss = 0.29689147\n",
      "Iteration 645, loss = 0.29681869\n",
      "Iteration 646, loss = 0.29670813\n",
      "Iteration 647, loss = 0.29663964\n",
      "Iteration 648, loss = 0.29652700\n",
      "Iteration 649, loss = 0.29644664\n",
      "Iteration 650, loss = 0.29634051\n",
      "Iteration 651, loss = 0.29622900\n",
      "Iteration 652, loss = 0.29616413\n",
      "Iteration 653, loss = 0.29605058\n",
      "Iteration 654, loss = 0.29594533\n",
      "Iteration 655, loss = 0.29585820\n",
      "Iteration 656, loss = 0.29577963\n",
      "Iteration 657, loss = 0.29567242\n",
      "Iteration 658, loss = 0.29557851\n",
      "Iteration 659, loss = 0.29548927\n",
      "Iteration 660, loss = 0.29539708\n",
      "Iteration 661, loss = 0.29528316\n",
      "Iteration 662, loss = 0.29519217\n",
      "Iteration 663, loss = 0.29509537\n",
      "Iteration 664, loss = 0.29499060\n",
      "Iteration 665, loss = 0.29491697\n",
      "Iteration 666, loss = 0.29480396\n",
      "Iteration 667, loss = 0.29471601\n",
      "Iteration 668, loss = 0.29461771\n",
      "Iteration 669, loss = 0.29455088\n",
      "Iteration 670, loss = 0.29441411\n",
      "Iteration 671, loss = 0.29437383\n",
      "Iteration 672, loss = 0.29424608\n",
      "Iteration 673, loss = 0.29415677\n",
      "Iteration 674, loss = 0.29407651\n",
      "Iteration 675, loss = 0.29400214\n",
      "Iteration 676, loss = 0.29387776\n",
      "Iteration 677, loss = 0.29376892\n",
      "Iteration 678, loss = 0.29369315\n",
      "Iteration 679, loss = 0.29364271\n",
      "Iteration 680, loss = 0.29349133\n",
      "Iteration 681, loss = 0.29339011\n",
      "Iteration 682, loss = 0.29330062\n",
      "Iteration 683, loss = 0.29323089\n",
      "Iteration 684, loss = 0.29311043\n",
      "Iteration 685, loss = 0.29300321\n",
      "Iteration 686, loss = 0.29290403\n",
      "Iteration 687, loss = 0.29287565\n",
      "Iteration 688, loss = 0.29281571\n",
      "Iteration 689, loss = 0.29262819\n",
      "Iteration 690, loss = 0.29254185\n",
      "Iteration 691, loss = 0.29246303\n",
      "Iteration 692, loss = 0.29234063\n",
      "Iteration 693, loss = 0.29226968\n",
      "Iteration 694, loss = 0.29215123\n",
      "Iteration 695, loss = 0.29205842\n",
      "Iteration 696, loss = 0.29201850\n",
      "Iteration 697, loss = 0.29187693\n",
      "Iteration 698, loss = 0.29181010\n",
      "Iteration 699, loss = 0.29169095\n",
      "Iteration 700, loss = 0.29158597\n",
      "Iteration 701, loss = 0.29151563\n",
      "Iteration 702, loss = 0.29141625\n",
      "Iteration 703, loss = 0.29132440\n",
      "Iteration 704, loss = 0.29122535\n",
      "Iteration 705, loss = 0.29113996\n",
      "Iteration 706, loss = 0.29109291\n",
      "Iteration 707, loss = 0.29095860\n",
      "Iteration 708, loss = 0.29083720\n",
      "Iteration 709, loss = 0.29083169\n",
      "Iteration 710, loss = 0.29065484\n",
      "Iteration 711, loss = 0.29057488\n",
      "Iteration 712, loss = 0.29052419\n",
      "Iteration 713, loss = 0.29036645\n",
      "Iteration 714, loss = 0.29028650\n",
      "Iteration 715, loss = 0.29018098\n",
      "Iteration 716, loss = 0.29010987\n",
      "Iteration 717, loss = 0.29001482\n",
      "Iteration 718, loss = 0.28991539\n",
      "Iteration 719, loss = 0.28980887\n",
      "Iteration 720, loss = 0.28973501\n",
      "Iteration 721, loss = 0.28962862\n",
      "Iteration 722, loss = 0.28952582\n",
      "Iteration 723, loss = 0.28946815\n",
      "Iteration 724, loss = 0.28935216\n",
      "Iteration 725, loss = 0.28927648\n",
      "Iteration 726, loss = 0.28917361\n",
      "Iteration 727, loss = 0.28910297\n",
      "Iteration 728, loss = 0.28900764\n",
      "Iteration 729, loss = 0.28888852\n",
      "Iteration 730, loss = 0.28880214\n",
      "Iteration 731, loss = 0.28870862\n",
      "Iteration 732, loss = 0.28859993\n",
      "Iteration 733, loss = 0.28851007\n",
      "Iteration 734, loss = 0.28842934\n",
      "Iteration 735, loss = 0.28832536\n",
      "Iteration 736, loss = 0.28823140\n",
      "Iteration 737, loss = 0.28816454\n",
      "Iteration 738, loss = 0.28805966\n",
      "Iteration 739, loss = 0.28795503\n",
      "Iteration 740, loss = 0.28785899\n",
      "Iteration 741, loss = 0.28780478\n",
      "Iteration 742, loss = 0.28769538\n",
      "Iteration 743, loss = 0.28760419\n",
      "Iteration 744, loss = 0.28750902\n",
      "Iteration 745, loss = 0.28745957\n",
      "Iteration 746, loss = 0.28732915\n",
      "Iteration 747, loss = 0.28721619\n",
      "Iteration 748, loss = 0.28712852\n",
      "Iteration 749, loss = 0.28706054\n",
      "Iteration 750, loss = 0.28695644\n",
      "Iteration 751, loss = 0.28686764\n",
      "Iteration 752, loss = 0.28678233\n",
      "Iteration 753, loss = 0.28668371\n",
      "Iteration 754, loss = 0.28661981\n",
      "Iteration 755, loss = 0.28648060\n",
      "Iteration 756, loss = 0.28641301\n",
      "Iteration 757, loss = 0.28631237\n",
      "Iteration 758, loss = 0.28623112\n",
      "Iteration 759, loss = 0.28612553\n",
      "Iteration 760, loss = 0.28604777\n",
      "Iteration 761, loss = 0.28597187\n",
      "Iteration 762, loss = 0.28587787\n",
      "Iteration 763, loss = 0.28578367\n",
      "Iteration 764, loss = 0.28570204\n",
      "Iteration 765, loss = 0.28560850\n",
      "Iteration 766, loss = 0.28549702\n",
      "Iteration 767, loss = 0.28547289\n",
      "Iteration 768, loss = 0.28531707\n",
      "Iteration 769, loss = 0.28521457\n",
      "Iteration 770, loss = 0.28513332\n",
      "Iteration 771, loss = 0.28506281\n",
      "Iteration 772, loss = 0.28496549\n",
      "Iteration 773, loss = 0.28487563\n",
      "Iteration 774, loss = 0.28479209\n",
      "Iteration 775, loss = 0.28469911\n",
      "Iteration 776, loss = 0.28462259\n",
      "Iteration 777, loss = 0.28452664\n",
      "Iteration 778, loss = 0.28449774\n",
      "Iteration 779, loss = 0.28434338\n",
      "Iteration 780, loss = 0.28425946\n",
      "Iteration 781, loss = 0.28416170\n",
      "Iteration 782, loss = 0.28406389\n",
      "Iteration 783, loss = 0.28398659\n",
      "Iteration 784, loss = 0.28388933\n",
      "Iteration 785, loss = 0.28380996\n",
      "Iteration 786, loss = 0.28373364\n",
      "Iteration 787, loss = 0.28364642\n",
      "Iteration 788, loss = 0.28354222\n",
      "Iteration 789, loss = 0.28345979\n",
      "Iteration 790, loss = 0.28335522\n",
      "Iteration 791, loss = 0.28328175\n",
      "Iteration 792, loss = 0.28329534\n",
      "Iteration 793, loss = 0.28309379\n",
      "Iteration 794, loss = 0.28304635\n",
      "Iteration 795, loss = 0.28296076\n",
      "Iteration 796, loss = 0.28283310\n",
      "Iteration 797, loss = 0.28274863\n",
      "Iteration 798, loss = 0.28268615\n",
      "Iteration 799, loss = 0.28256557\n",
      "Iteration 800, loss = 0.28251223\n",
      "Iteration 801, loss = 0.28255074\n",
      "Iteration 802, loss = 0.28232438\n",
      "Iteration 803, loss = 0.28221963\n",
      "Iteration 804, loss = 0.28214052\n",
      "Iteration 805, loss = 0.28203688\n",
      "Iteration 806, loss = 0.28192667\n",
      "Iteration 807, loss = 0.28185164\n",
      "Iteration 808, loss = 0.28179564\n",
      "Iteration 809, loss = 0.28167504\n",
      "Iteration 810, loss = 0.28158259\n",
      "Iteration 811, loss = 0.28150773\n",
      "Iteration 812, loss = 0.28140657\n",
      "Iteration 813, loss = 0.28131310\n",
      "Iteration 814, loss = 0.28123751\n",
      "Iteration 815, loss = 0.28117110\n",
      "Iteration 816, loss = 0.28105309\n",
      "Iteration 817, loss = 0.28095619\n",
      "Iteration 818, loss = 0.28088620\n",
      "Iteration 819, loss = 0.28078875\n",
      "Iteration 820, loss = 0.28071503\n",
      "Iteration 821, loss = 0.28061432\n",
      "Iteration 822, loss = 0.28049546\n",
      "Iteration 823, loss = 0.28041128\n",
      "Iteration 824, loss = 0.28033908\n",
      "Iteration 825, loss = 0.28024523\n",
      "Iteration 826, loss = 0.28014895\n",
      "Iteration 827, loss = 0.28010261\n",
      "Iteration 828, loss = 0.27996032\n",
      "Iteration 829, loss = 0.27989387\n",
      "Iteration 830, loss = 0.27978739\n",
      "Iteration 831, loss = 0.27971581\n",
      "Iteration 832, loss = 0.27962796\n",
      "Iteration 833, loss = 0.27951040\n",
      "Iteration 834, loss = 0.27942779\n",
      "Iteration 835, loss = 0.27933396\n",
      "Iteration 836, loss = 0.27923860\n",
      "Iteration 837, loss = 0.27915221\n",
      "Iteration 838, loss = 0.27907191\n",
      "Iteration 839, loss = 0.27895503\n",
      "Iteration 840, loss = 0.27890703\n",
      "Iteration 841, loss = 0.27878851\n",
      "Iteration 842, loss = 0.27869904\n",
      "Iteration 843, loss = 0.27867840\n",
      "Iteration 844, loss = 0.27852668\n",
      "Iteration 845, loss = 0.27842798\n",
      "Iteration 846, loss = 0.27838317\n",
      "Iteration 847, loss = 0.27824835\n",
      "Iteration 848, loss = 0.27815144\n",
      "Iteration 849, loss = 0.27808696\n",
      "Iteration 850, loss = 0.27798252\n",
      "Iteration 851, loss = 0.27797527\n",
      "Iteration 852, loss = 0.27781088\n",
      "Iteration 853, loss = 0.27769481\n",
      "Iteration 854, loss = 0.27762709\n",
      "Iteration 855, loss = 0.27752079\n",
      "Iteration 856, loss = 0.27741727\n",
      "Iteration 857, loss = 0.27731994\n",
      "Iteration 858, loss = 0.27724419\n",
      "Iteration 859, loss = 0.27714418\n",
      "Iteration 860, loss = 0.27707219\n",
      "Iteration 861, loss = 0.27696032\n",
      "Iteration 862, loss = 0.27687110\n",
      "Iteration 863, loss = 0.27677062\n",
      "Iteration 864, loss = 0.27669551\n",
      "Iteration 865, loss = 0.27660386\n",
      "Iteration 866, loss = 0.27653055\n",
      "Iteration 867, loss = 0.27639944\n",
      "Iteration 868, loss = 0.27631523\n",
      "Iteration 869, loss = 0.27621878\n",
      "Iteration 870, loss = 0.27614105\n",
      "Iteration 871, loss = 0.27604385\n",
      "Iteration 872, loss = 0.27593943\n",
      "Iteration 873, loss = 0.27589303\n",
      "Iteration 874, loss = 0.27576278\n",
      "Iteration 875, loss = 0.27571854\n",
      "Iteration 876, loss = 0.27557494\n",
      "Iteration 877, loss = 0.27548718\n",
      "Iteration 878, loss = 0.27541766\n",
      "Iteration 879, loss = 0.27535265\n",
      "Iteration 880, loss = 0.27522700\n",
      "Iteration 881, loss = 0.27515785\n",
      "Iteration 882, loss = 0.27502647\n",
      "Iteration 883, loss = 0.27497473\n",
      "Iteration 884, loss = 0.27486647\n",
      "Iteration 885, loss = 0.27475978\n",
      "Iteration 886, loss = 0.27470451\n",
      "Iteration 887, loss = 0.27460889\n",
      "Iteration 888, loss = 0.27448420\n",
      "Iteration 889, loss = 0.27441973\n",
      "Iteration 890, loss = 0.27436723\n",
      "Iteration 891, loss = 0.27422953\n",
      "Iteration 892, loss = 0.27411709\n",
      "Iteration 893, loss = 0.27411433\n",
      "Iteration 894, loss = 0.27399101\n",
      "Iteration 895, loss = 0.27385120\n",
      "Iteration 896, loss = 0.27377017\n",
      "Iteration 897, loss = 0.27365485\n",
      "Iteration 898, loss = 0.27359162\n",
      "Iteration 899, loss = 0.27348633\n",
      "Iteration 900, loss = 0.27339819\n",
      "Iteration 901, loss = 0.27329759\n",
      "Iteration 902, loss = 0.27322466\n",
      "Iteration 903, loss = 0.27313304\n",
      "Iteration 904, loss = 0.27303573\n",
      "Iteration 905, loss = 0.27292311\n",
      "Iteration 906, loss = 0.27291621\n",
      "Iteration 907, loss = 0.27275872\n",
      "Iteration 908, loss = 0.27266399\n",
      "Iteration 909, loss = 0.27259515\n",
      "Iteration 910, loss = 0.27247276\n",
      "Iteration 911, loss = 0.27240677\n",
      "Iteration 912, loss = 0.27228697\n",
      "Iteration 913, loss = 0.27221050\n",
      "Iteration 914, loss = 0.27209596\n",
      "Iteration 915, loss = 0.27199951\n",
      "Iteration 916, loss = 0.27191273\n",
      "Iteration 917, loss = 0.27190084\n",
      "Iteration 918, loss = 0.27173808\n",
      "Iteration 919, loss = 0.27165279\n",
      "Iteration 920, loss = 0.27153529\n",
      "Iteration 921, loss = 0.27146145\n",
      "Iteration 922, loss = 0.27136466\n",
      "Iteration 923, loss = 0.27124455\n",
      "Iteration 924, loss = 0.27116004\n",
      "Iteration 925, loss = 0.27110377\n",
      "Iteration 926, loss = 0.27096994\n",
      "Iteration 927, loss = 0.27087407\n",
      "Iteration 928, loss = 0.27077715\n",
      "Iteration 929, loss = 0.27067673\n",
      "Iteration 930, loss = 0.27061558\n",
      "Iteration 931, loss = 0.27046810\n",
      "Iteration 932, loss = 0.27038250\n",
      "Iteration 933, loss = 0.27033445\n",
      "Iteration 934, loss = 0.27019768\n",
      "Iteration 935, loss = 0.27014304\n",
      "Iteration 936, loss = 0.26999632\n",
      "Iteration 937, loss = 0.26991193\n",
      "Iteration 938, loss = 0.26981992\n",
      "Iteration 939, loss = 0.26974725\n",
      "Iteration 940, loss = 0.26964143\n",
      "Iteration 941, loss = 0.26956334\n",
      "Iteration 942, loss = 0.26943150\n",
      "Iteration 943, loss = 0.26932469\n",
      "Iteration 944, loss = 0.26921849\n",
      "Iteration 945, loss = 0.26911130\n",
      "Iteration 946, loss = 0.26900654\n",
      "Iteration 947, loss = 0.26893593\n",
      "Iteration 948, loss = 0.26883316\n",
      "Iteration 949, loss = 0.26873163\n",
      "Iteration 950, loss = 0.26863615\n",
      "Iteration 951, loss = 0.26857377\n",
      "Iteration 952, loss = 0.26841083\n",
      "Iteration 953, loss = 0.26832497\n",
      "Iteration 954, loss = 0.26821376\n",
      "Iteration 955, loss = 0.26815478\n",
      "Iteration 956, loss = 0.26803685\n",
      "Iteration 957, loss = 0.26791067\n",
      "Iteration 958, loss = 0.26784027\n",
      "Iteration 959, loss = 0.26771370\n",
      "Iteration 960, loss = 0.26761753\n",
      "Iteration 961, loss = 0.26751763\n",
      "Iteration 962, loss = 0.26743412\n",
      "Iteration 963, loss = 0.26733370\n",
      "Iteration 964, loss = 0.26721863\n",
      "Iteration 965, loss = 0.26712767\n",
      "Iteration 966, loss = 0.26702881\n",
      "Iteration 967, loss = 0.26693901\n",
      "Iteration 968, loss = 0.26682760\n",
      "Iteration 969, loss = 0.26674258\n",
      "Iteration 970, loss = 0.26662672\n",
      "Iteration 971, loss = 0.26654243\n",
      "Iteration 972, loss = 0.26645108\n",
      "Iteration 973, loss = 0.26635209\n",
      "Iteration 974, loss = 0.26624205\n",
      "Iteration 975, loss = 0.26618318\n",
      "Iteration 976, loss = 0.26604603\n",
      "Iteration 977, loss = 0.26595476\n",
      "Iteration 978, loss = 0.26587622\n",
      "Iteration 979, loss = 0.26574529\n",
      "Iteration 980, loss = 0.26564862\n",
      "Iteration 778, loss = 0.30105644\n",
      "Iteration 779, loss = 0.30094029\n",
      "Iteration 780, loss = 0.30081745\n",
      "Iteration 781, loss = 0.30065058\n",
      "Iteration 782, loss = 0.30056535\n",
      "Iteration 783, loss = 0.30047479\n",
      "Iteration 784, loss = 0.30025489\n",
      "Iteration 785, loss = 0.30014383\n",
      "Iteration 786, loss = 0.30001135\n",
      "Iteration 787, loss = 0.29987139\n",
      "Iteration 788, loss = 0.29972996\n",
      "Iteration 789, loss = 0.29962499\n",
      "Iteration 790, loss = 0.29947406\n",
      "Iteration 791, loss = 0.29940261\n",
      "Iteration 792, loss = 0.29923740\n",
      "Iteration 793, loss = 0.29916107\n",
      "Iteration 794, loss = 0.29890513\n",
      "Iteration 795, loss = 0.29883790\n",
      "Iteration 796, loss = 0.29864811\n",
      "Iteration 797, loss = 0.29850341\n",
      "Iteration 798, loss = 0.29856499\n",
      "Iteration 799, loss = 0.29829617\n",
      "Iteration 800, loss = 0.29812972\n",
      "Iteration 801, loss = 0.29801902\n",
      "Iteration 802, loss = 0.29785292\n",
      "Iteration 803, loss = 0.29775505\n",
      "Iteration 804, loss = 0.29760272\n",
      "Iteration 805, loss = 0.29745589\n",
      "Iteration 806, loss = 0.29735744\n",
      "Iteration 807, loss = 0.29720567\n",
      "Iteration 808, loss = 0.29704547\n",
      "Iteration 809, loss = 0.29696243\n",
      "Iteration 810, loss = 0.29679870\n",
      "Iteration 811, loss = 0.29666598\n",
      "Iteration 812, loss = 0.29654423\n",
      "Iteration 813, loss = 0.29639413\n",
      "Iteration 814, loss = 0.29626122\n",
      "Iteration 815, loss = 0.29613587\n",
      "Iteration 816, loss = 0.29598318\n",
      "Iteration 817, loss = 0.29586327\n",
      "Iteration 818, loss = 0.29571936\n",
      "Iteration 819, loss = 0.29562563\n",
      "Iteration 820, loss = 0.29545387\n",
      "Iteration 821, loss = 0.29532721\n",
      "Iteration 822, loss = 0.29517521\n",
      "Iteration 823, loss = 0.29507863\n",
      "Iteration 824, loss = 0.29496972\n",
      "Iteration 825, loss = 0.29477651\n",
      "Iteration 826, loss = 0.29478438\n",
      "Iteration 827, loss = 0.29453961\n",
      "Iteration 828, loss = 0.29440742\n",
      "Iteration 829, loss = 0.29430520\n",
      "Iteration 830, loss = 0.29414116\n",
      "Iteration 831, loss = 0.29399898\n",
      "Iteration 832, loss = 0.29389010\n",
      "Iteration 833, loss = 0.29373066\n",
      "Iteration 834, loss = 0.29369316\n",
      "Iteration 835, loss = 0.29348905\n",
      "Iteration 836, loss = 0.29335760\n",
      "Iteration 837, loss = 0.29320556\n",
      "Iteration 838, loss = 0.29304860\n",
      "Iteration 839, loss = 0.29293153\n",
      "Iteration 840, loss = 0.29281869\n",
      "Iteration 841, loss = 0.29269801\n",
      "Iteration 842, loss = 0.29251685\n",
      "Iteration 843, loss = 0.29238709\n",
      "Iteration 844, loss = 0.29226338\n",
      "Iteration 845, loss = 0.29213377\n",
      "Iteration 846, loss = 0.29199504\n",
      "Iteration 847, loss = 0.29185552\n",
      "Iteration 848, loss = 0.29172462\n",
      "Iteration 849, loss = 0.29157726\n",
      "Iteration 850, loss = 0.29147486\n",
      "Iteration 851, loss = 0.29133003\n",
      "Iteration 852, loss = 0.29119759\n",
      "Iteration 853, loss = 0.29107302\n",
      "Iteration 854, loss = 0.29091882\n",
      "Iteration 855, loss = 0.29077248\n",
      "Iteration 856, loss = 0.29067453\n",
      "Iteration 857, loss = 0.29050857\n",
      "Iteration 858, loss = 0.29040545\n",
      "Iteration 859, loss = 0.29023949\n",
      "Iteration 860, loss = 0.29009161\n",
      "Iteration 861, loss = 0.29005390\n",
      "Iteration 862, loss = 0.28985504\n",
      "Iteration 863, loss = 0.28978372\n",
      "Iteration 864, loss = 0.28956906\n",
      "Iteration 865, loss = 0.28945440\n",
      "Iteration 866, loss = 0.28931339\n",
      "Iteration 867, loss = 0.28917678\n",
      "Iteration 868, loss = 0.28906099\n",
      "Iteration 869, loss = 0.28898863\n",
      "Iteration 870, loss = 0.28884830\n",
      "Iteration 871, loss = 0.28866288\n",
      "Iteration 872, loss = 0.28848714\n",
      "Iteration 873, loss = 0.28833690\n",
      "Iteration 874, loss = 0.28820534\n",
      "Iteration 875, loss = 0.28808981\n",
      "Iteration 876, loss = 0.28799408\n",
      "Iteration 877, loss = 0.28782284\n",
      "Iteration 878, loss = 0.28764992\n",
      "Iteration 879, loss = 0.28759137\n",
      "Iteration 880, loss = 0.28740351\n",
      "Iteration 881, loss = 0.28724524\n",
      "Iteration 882, loss = 0.28713460\n",
      "Iteration 883, loss = 0.28697741\n",
      "Iteration 884, loss = 0.28694216\n",
      "Iteration 885, loss = 0.28675282\n",
      "Iteration 886, loss = 0.28657978\n",
      "Iteration 887, loss = 0.28646169\n",
      "Iteration 888, loss = 0.28628910\n",
      "Iteration 889, loss = 0.28616731\n",
      "Iteration 890, loss = 0.28601038\n",
      "Iteration 891, loss = 0.28590019\n",
      "Iteration 892, loss = 0.28572163\n",
      "Iteration 893, loss = 0.28562789\n",
      "Iteration 894, loss = 0.28547857\n",
      "Iteration 895, loss = 0.28539739\n",
      "Iteration 896, loss = 0.28517510\n",
      "Iteration 897, loss = 0.28503494\n",
      "Iteration 898, loss = 0.28489922\n",
      "Iteration 899, loss = 0.28479139\n",
      "Iteration 900, loss = 0.28463796\n",
      "Iteration 901, loss = 0.28452160\n",
      "Iteration 902, loss = 0.28435837\n",
      "Iteration 903, loss = 0.28425793\n",
      "Iteration 904, loss = 0.28409112\n",
      "Iteration 905, loss = 0.28394403\n",
      "Iteration 906, loss = 0.28386553\n",
      "Iteration 907, loss = 0.28369923\n",
      "Iteration 908, loss = 0.28352001\n",
      "Iteration 909, loss = 0.28342693\n",
      "Iteration 910, loss = 0.28334258\n",
      "Iteration 911, loss = 0.28312706\n",
      "Iteration 912, loss = 0.28299474\n",
      "Iteration 913, loss = 0.28285683\n",
      "Iteration 914, loss = 0.28273579\n",
      "Iteration 915, loss = 0.28262054\n",
      "Iteration 916, loss = 0.28244260\n",
      "Iteration 917, loss = 0.28236078\n",
      "Iteration 918, loss = 0.28214409\n",
      "Iteration 919, loss = 0.28201104\n",
      "Iteration 920, loss = 0.28191772\n",
      "Iteration 921, loss = 0.28172969\n",
      "Iteration 922, loss = 0.28160982\n",
      "Iteration 923, loss = 0.28148674\n",
      "Iteration 924, loss = 0.28142291\n",
      "Iteration 925, loss = 0.28117912\n",
      "Iteration 926, loss = 0.28107213\n",
      "Iteration 927, loss = 0.28092239\n",
      "Iteration 928, loss = 0.28078864\n",
      "Iteration 929, loss = 0.28065709\n",
      "Iteration 930, loss = 0.28049855\n",
      "Iteration 931, loss = 0.28041480\n",
      "Iteration 932, loss = 0.28024193\n",
      "Iteration 933, loss = 0.28014921\n",
      "Iteration 934, loss = 0.27996523\n",
      "Iteration 935, loss = 0.27980945\n",
      "Iteration 936, loss = 0.27965871\n",
      "Iteration 937, loss = 0.27953546\n",
      "Iteration 938, loss = 0.27937165\n",
      "Iteration 939, loss = 0.27924150\n",
      "Iteration 940, loss = 0.27909819\n",
      "Iteration 941, loss = 0.27895549\n",
      "Iteration 942, loss = 0.27883737\n",
      "Iteration 943, loss = 0.27868250\n",
      "Iteration 944, loss = 0.27854491\n",
      "Iteration 945, loss = 0.27843352\n",
      "Iteration 946, loss = 0.27826465\n",
      "Iteration 947, loss = 0.27816118\n",
      "Iteration 948, loss = 0.27797477\n",
      "Iteration 949, loss = 0.27783025\n",
      "Iteration 950, loss = 0.27770101\n",
      "Iteration 951, loss = 0.27771982\n",
      "Iteration 952, loss = 0.27759210\n",
      "Iteration 953, loss = 0.27731219\n",
      "Iteration 954, loss = 0.27711568\n",
      "Iteration 955, loss = 0.27697380\n",
      "Iteration 956, loss = 0.27685535\n",
      "Iteration 957, loss = 0.27670072\n",
      "Iteration 958, loss = 0.27654823\n",
      "Iteration 959, loss = 0.27646684\n",
      "Iteration 960, loss = 0.27625280\n",
      "Iteration 961, loss = 0.27616112\n",
      "Iteration 962, loss = 0.27604031\n",
      "Iteration 963, loss = 0.27584040\n",
      "Iteration 964, loss = 0.27569440\n",
      "Iteration 965, loss = 0.27553379\n",
      "Iteration 966, loss = 0.27547167\n",
      "Iteration 967, loss = 0.27524151\n",
      "Iteration 968, loss = 0.27511617\n",
      "Iteration 969, loss = 0.27498753\n",
      "Iteration 970, loss = 0.27486443\n",
      "Iteration 971, loss = 0.27468568\n",
      "Iteration 972, loss = 0.27455973\n",
      "Iteration 973, loss = 0.27440465\n",
      "Iteration 974, loss = 0.27428504\n",
      "Iteration 975, loss = 0.27413979\n",
      "Iteration 976, loss = 0.27398289\n",
      "Iteration 977, loss = 0.27385025\n",
      "Iteration 978, loss = 0.27367836\n",
      "Iteration 979, loss = 0.27351587\n",
      "Iteration 980, loss = 0.27342421\n",
      "Iteration 981, loss = 0.27325882\n",
      "Iteration 982, loss = 0.27312769\n",
      "Iteration 983, loss = 0.27295916\n",
      "Iteration 984, loss = 0.27284036\n",
      "Iteration 985, loss = 0.27268103\n",
      "Iteration 986, loss = 0.27252723\n",
      "Iteration 987, loss = 0.27236427\n",
      "Iteration 988, loss = 0.27224071\n",
      "Iteration 989, loss = 0.27207685\n",
      "Iteration 990, loss = 0.27200865\n",
      "Iteration 991, loss = 0.27181811\n",
      "Iteration 992, loss = 0.27170495\n",
      "Iteration 993, loss = 0.27151201\n",
      "Iteration 994, loss = 0.27137523\n",
      "Iteration 995, loss = 0.27124825\n",
      "Iteration 996, loss = 0.27112835\n",
      "Iteration 997, loss = 0.27097433\n",
      "Iteration 998, loss = 0.27079755\n",
      "Iteration 999, loss = 0.27068134\n",
      "Iteration 1000, loss = 0.27053046\n",
      "Iteration 1001, loss = 0.27036632\n",
      "Iteration 1002, loss = 0.27020463\n",
      "Iteration 1003, loss = 0.27010734\n",
      "Iteration 1004, loss = 0.26997138\n",
      "Iteration 1005, loss = 0.26984592\n",
      "Iteration 1006, loss = 0.26963565\n",
      "Iteration 1007, loss = 0.26947971\n",
      "Iteration 1008, loss = 0.26935974\n",
      "Iteration 1009, loss = 0.26923120\n",
      "Iteration 1010, loss = 0.26905951\n",
      "Iteration 1011, loss = 0.26894254\n",
      "Iteration 1012, loss = 0.26883002\n",
      "Iteration 1013, loss = 0.26863087\n",
      "Iteration 1014, loss = 0.26849401\n",
      "Iteration 1015, loss = 0.26838230\n",
      "Iteration 1016, loss = 0.26822398\n",
      "Iteration 1017, loss = 0.26805065\n",
      "Iteration 1018, loss = 0.26789713\n",
      "Iteration 1019, loss = 0.26774088\n",
      "Iteration 1020, loss = 0.26764273\n",
      "Iteration 1021, loss = 0.26743857\n",
      "Iteration 1022, loss = 0.26731689\n",
      "Iteration 1023, loss = 0.26722576\n",
      "Iteration 1024, loss = 0.26705989\n",
      "Iteration 1025, loss = 0.26689783\n",
      "Iteration 1026, loss = 0.26675498\n",
      "Iteration 1027, loss = 0.26661743\n",
      "Iteration 1028, loss = 0.26643936\n",
      "Iteration 1029, loss = 0.26640586\n",
      "Iteration 1030, loss = 0.26623914\n",
      "Iteration 1031, loss = 0.26603194\n",
      "Iteration 1032, loss = 0.26586288\n",
      "Iteration 1033, loss = 0.26576775\n",
      "Iteration 1034, loss = 0.26559970\n",
      "Iteration 1035, loss = 0.26541951\n",
      "Iteration 1036, loss = 0.26534093\n",
      "Iteration 1037, loss = 0.26515766\n",
      "Iteration 1038, loss = 0.26500098\n",
      "Iteration 1039, loss = 0.26485674\n",
      "Iteration 1040, loss = 0.26469753\n",
      "Iteration 1041, loss = 0.26457107\n",
      "Iteration 1042, loss = 0.26449045\n",
      "Iteration 1043, loss = 0.26431307\n",
      "Iteration 1044, loss = 0.26411577\n",
      "Iteration 1045, loss = 0.26396320\n",
      "Iteration 1046, loss = 0.26387656\n",
      "Iteration 1047, loss = 0.26368541\n",
      "Iteration 1048, loss = 0.26356203\n",
      "Iteration 1049, loss = 0.26341901\n",
      "Iteration 1050, loss = 0.26333937\n",
      "Iteration 1051, loss = 0.26310909\n",
      "Iteration 1052, loss = 0.26299091\n",
      "Iteration 1053, loss = 0.26290308\n",
      "Iteration 1054, loss = 0.26272941\n",
      "Iteration 1055, loss = 0.26251882\n",
      "Iteration 1056, loss = 0.26238460\n",
      "Iteration 1057, loss = 0.26227488\n",
      "Iteration 1058, loss = 0.26214582\n",
      "Iteration 1059, loss = 0.26196322\n",
      "Iteration 1060, loss = 0.26180959\n",
      "Iteration 1061, loss = 0.26167994\n",
      "Iteration 1062, loss = 0.26153516\n",
      "Iteration 1063, loss = 0.26141033\n",
      "Iteration 1064, loss = 0.26139258\n",
      "Iteration 1065, loss = 0.26109252\n",
      "Iteration 1066, loss = 0.26097132\n",
      "Iteration 1067, loss = 0.26084524\n",
      "Iteration 1068, loss = 0.26069049\n",
      "Iteration 1069, loss = 0.26052171\n",
      "Iteration 1070, loss = 0.26044282\n",
      "Iteration 1071, loss = 0.26023347\n",
      "Iteration 1072, loss = 0.26010588\n",
      "Iteration 1073, loss = 0.25998237\n",
      "Iteration 1074, loss = 0.25980415\n",
      "Iteration 1075, loss = 0.25969587\n",
      "Iteration 1076, loss = 0.25950463\n",
      "Iteration 1077, loss = 0.25935387\n",
      "Iteration 1078, loss = 0.25920794\n",
      "Iteration 1079, loss = 0.25907942\n",
      "Iteration 1080, loss = 0.25895146\n",
      "Iteration 1081, loss = 0.25879907\n",
      "Iteration 1082, loss = 0.25869815\n",
      "Iteration 1083, loss = 0.25849290\n",
      "Iteration 1084, loss = 0.25836562\n",
      "Iteration 1085, loss = 0.25818910\n",
      "Iteration 1086, loss = 0.25808986\n",
      "Iteration 1087, loss = 0.25796316\n",
      "Iteration 1088, loss = 0.25777294\n",
      "Iteration 1089, loss = 0.25762596\n",
      "Iteration 1090, loss = 0.25747183\n",
      "Iteration 1091, loss = 0.25737263\n",
      "Iteration 1092, loss = 0.25721477\n",
      "Iteration 1093, loss = 0.25708136\n",
      "Iteration 1094, loss = 0.25695995\n",
      "Iteration 1095, loss = 0.25680977\n",
      "Iteration 1096, loss = 0.25661212\n",
      "Iteration 1097, loss = 0.25651270\n",
      "Iteration 1098, loss = 0.25633448\n",
      "Iteration 1099, loss = 0.25622900\n",
      "Iteration 1100, loss = 0.25604579\n",
      "Iteration 1101, loss = 0.25593337\n",
      "Iteration 1102, loss = 0.25577784\n",
      "Iteration 1103, loss = 0.25564839\n",
      "Iteration 1104, loss = 0.25548090\n",
      "Iteration 1105, loss = 0.25536704\n",
      "Iteration 1106, loss = 0.25520371\n",
      "Iteration 1107, loss = 0.25506824\n",
      "Iteration 1108, loss = 0.25490054\n",
      "Iteration 1109, loss = 0.25480499\n",
      "Iteration 1110, loss = 0.25462844\n",
      "Iteration 1111, loss = 0.25453410\n",
      "Iteration 1112, loss = 0.25436857\n",
      "Iteration 1113, loss = 0.25417193\n",
      "Iteration 1114, loss = 0.25408135\n",
      "Iteration 1115, loss = 0.25391269\n",
      "Iteration 1116, loss = 0.25375720\n",
      "Iteration 1117, loss = 0.25360617\n",
      "Iteration 1118, loss = 0.25351354\n",
      "Iteration 1119, loss = 0.25331375\n",
      "Iteration 1120, loss = 0.25318680\n",
      "Iteration 1121, loss = 0.25313953\n",
      "Iteration 1122, loss = 0.25296026\n",
      "Iteration 1123, loss = 0.25278990\n",
      "Iteration 1124, loss = 0.25263943\n",
      "Iteration 1125, loss = 0.25248555\n",
      "Iteration 1126, loss = 0.25234606\n",
      "Iteration 1127, loss = 0.25220802\n",
      "Iteration 1128, loss = 0.25202873\n",
      "Iteration 1129, loss = 0.25192317\n",
      "Iteration 1130, loss = 0.25183335\n",
      "Iteration 1131, loss = 0.25162495\n",
      "Iteration 1132, loss = 0.25146560\n",
      "Iteration 1133, loss = 0.25131974\n",
      "Iteration 1134, loss = 0.25125887\n",
      "Iteration 1135, loss = 0.25106258\n",
      "Iteration 1136, loss = 0.25096111\n",
      "Iteration 1137, loss = 0.25078740\n",
      "Iteration 1138, loss = 0.25064792\n",
      "Iteration 1139, loss = 0.25047373\n",
      "Iteration 1140, loss = 0.25034456\n",
      "Iteration 1141, loss = 0.25023847\n",
      "Iteration 1142, loss = 0.25008587\n",
      "Iteration 1143, loss = 0.24996281\n",
      "Iteration 1144, loss = 0.24980490\n",
      "Iteration 1145, loss = 0.24964050\n",
      "Iteration 1146, loss = 0.24949797\n",
      "Iteration 1147, loss = 0.24936960\n",
      "Iteration 1148, loss = 0.24927524\n",
      "Iteration 1149, loss = 0.24907261\n",
      "Iteration 1150, loss = 0.24898238\n",
      "Iteration 1151, loss = 0.24878193\n",
      "Iteration 1152, loss = 0.24862145\n",
      "Iteration 1153, loss = 0.24859752\n",
      "Iteration 1154, loss = 0.24837667\n",
      "Iteration 1155, loss = 0.24820361\n",
      "Iteration 1156, loss = 0.24812497\n",
      "Iteration 1157, loss = 0.24799237\n",
      "Iteration 1158, loss = 0.24775693\n",
      "Iteration 1159, loss = 0.24763806\n",
      "Iteration 1160, loss = 0.24749275\n",
      "Iteration 1161, loss = 0.24736140\n",
      "Iteration 1162, loss = 0.24723328\n",
      "Iteration 1163, loss = 0.24707889\n",
      "Iteration 1164, loss = 0.24693467\n",
      "Iteration 1165, loss = 0.24681967\n",
      "Iteration 1166, loss = 0.24666678\n",
      "Iteration 1167, loss = 0.24650644\n",
      "Iteration 1168, loss = 0.24634334\n",
      "Iteration 1169, loss = 0.24621643\n",
      "Iteration 1170, loss = 0.24605926\n",
      "Iteration 1171, loss = 0.24591074\n",
      "Iteration 1172, loss = 0.24578769\n",
      "Iteration 1173, loss = 0.24561605\n",
      "Iteration 1174, loss = 0.24550394\n",
      "Iteration 1175, loss = 0.24533769\n",
      "Iteration 1176, loss = 0.24519905\n",
      "Iteration 1177, loss = 0.24515984\n",
      "Iteration 1178, loss = 0.24496209\n",
      "Iteration 1179, loss = 0.24477164\n",
      "Iteration 1180, loss = 0.24471230\n",
      "Iteration 1181, loss = 0.24449717\n",
      "Iteration 1182, loss = 0.24439021\n",
      "Iteration 1183, loss = 0.24422837\n",
      "Iteration 1184, loss = 0.24406594\n",
      "Iteration 1185, loss = 0.24393825\n",
      "Iteration 1186, loss = 0.24378989\n",
      "Iteration 1187, loss = 0.24361466\n",
      "Iteration 1188, loss = 0.24347303\n",
      "Iteration 1189, loss = 0.24333355\n",
      "Iteration 1190, loss = 0.24327965\n",
      "Iteration 1191, loss = 0.24310638\n",
      "Iteration 1192, loss = 0.24298940\n",
      "Iteration 1193, loss = 0.24281914\n",
      "Iteration 1194, loss = 0.24262436\n",
      "Iteration 1195, loss = 0.24251936\n",
      "Iteration 1196, loss = 0.24234679\n",
      "Iteration 1197, loss = 0.24219849\n",
      "Iteration 1198, loss = 0.24205725\n",
      "Iteration 1199, loss = 0.24191174\n",
      "Iteration 1200, loss = 0.24176892\n",
      "Iteration 1201, loss = 0.24162201\n",
      "Iteration 1202, loss = 0.24151690\n",
      "Iteration 1203, loss = 0.24136844\n",
      "Iteration 1204, loss = 0.24125208\n",
      "Iteration 1205, loss = 0.24118910\n",
      "Iteration 1206, loss = 0.24099556\n",
      "Iteration 1207, loss = 0.24078614\n",
      "Iteration 1208, loss = 0.24062442\n",
      "Iteration 1209, loss = 0.24051828\n",
      "Iteration 1210, loss = 0.24038681\n",
      "Iteration 1211, loss = 0.24019044\n",
      "Iteration 1212, loss = 0.24009073\n",
      "Iteration 1213, loss = 0.23995270\n",
      "Iteration 1214, loss = 0.23977901\n",
      "Iteration 1215, loss = 0.23965904\n",
      "Iteration 1216, loss = 0.23949597\n",
      "Iteration 1217, loss = 0.23933124\n",
      "Iteration 1218, loss = 0.23922876\n",
      "Iteration 1219, loss = 0.23909018\n",
      "Iteration 1220, loss = 0.23891457\n",
      "Iteration 1221, loss = 0.23883937\n",
      "Iteration 1222, loss = 0.23867464\n",
      "Iteration 1223, loss = 0.23852125\n",
      "Iteration 1224, loss = 0.23835185\n",
      "Iteration 1225, loss = 0.23818222\n",
      "Iteration 1226, loss = 0.23806427\n",
      "Iteration 1227, loss = 0.23792659\n",
      "Iteration 1228, loss = 0.23778993\n",
      "Iteration 1229, loss = 0.23765934\n",
      "Iteration 1230, loss = 0.23751250\n",
      "Iteration 1231, loss = 0.23743856\n",
      "Iteration 1232, loss = 0.23723661\n",
      "Iteration 1233, loss = 0.23717614\n",
      "Iteration 1234, loss = 0.23699719\n",
      "Iteration 1235, loss = 0.23683605\n",
      "Iteration 1236, loss = 0.23670796\n",
      "Iteration 1237, loss = 0.23653121\n",
      "Iteration 1238, loss = 0.23642913\n",
      "Iteration 1239, loss = 0.23628191\n",
      "Iteration 1240, loss = 0.23613681\n",
      "Iteration 1241, loss = 0.23596623\n",
      "Iteration 1242, loss = 0.23582811\n",
      "Iteration 1243, loss = 0.23569585\n",
      "Iteration 1244, loss = 0.23557833\n",
      "Iteration 1245, loss = 0.23549890\n",
      "Iteration 1246, loss = 0.23526677\n",
      "Iteration 1247, loss = 0.23511481\n",
      "Iteration 1248, loss = 0.23511896\n",
      "Iteration 1249, loss = 0.23487898\n",
      "Iteration 1250, loss = 0.23481914\n",
      "Iteration 1251, loss = 0.23459218\n",
      "Iteration 1252, loss = 0.23445228\n",
      "Iteration 1253, loss = 0.23433252\n",
      "Iteration 1254, loss = 0.23428344\n",
      "Iteration 1255, loss = 0.23403041\n",
      "Iteration 1256, loss = 0.23392929\n",
      "Iteration 1257, loss = 0.23375297\n",
      "Iteration 1258, loss = 0.23360451\n",
      "Iteration 1259, loss = 0.23350063\n",
      "Iteration 1260, loss = 0.23344338\n",
      "Iteration 1261, loss = 0.23319470\n",
      "Iteration 1262, loss = 0.23306038\n",
      "Iteration 1263, loss = 0.23301761\n",
      "Iteration 1264, loss = 0.23278998\n",
      "Iteration 1314, loss = 0.24271516\n",
      "Iteration 1315, loss = 0.24261983\n",
      "Iteration 1316, loss = 0.24246568\n",
      "Iteration 1317, loss = 0.24241258\n",
      "Iteration 1318, loss = 0.24226396\n",
      "Iteration 1319, loss = 0.24219883\n",
      "Iteration 1320, loss = 0.24204798\n",
      "Iteration 1321, loss = 0.24192853\n",
      "Iteration 1322, loss = 0.24193270\n",
      "Iteration 1323, loss = 0.24171444\n",
      "Iteration 1324, loss = 0.24155961\n",
      "Iteration 1325, loss = 0.24149640\n",
      "Iteration 1326, loss = 0.24135327\n",
      "Iteration 1327, loss = 0.24133196\n",
      "Iteration 1328, loss = 0.24112175\n",
      "Iteration 1329, loss = 0.24102362\n",
      "Iteration 1330, loss = 0.24092195\n",
      "Iteration 1331, loss = 0.24085485\n",
      "Iteration 1332, loss = 0.24071124\n",
      "Iteration 1333, loss = 0.24057133\n",
      "Iteration 1334, loss = 0.24044571\n",
      "Iteration 1335, loss = 0.24035821\n",
      "Iteration 1336, loss = 0.24026398\n",
      "Iteration 1337, loss = 0.24011125\n",
      "Iteration 1338, loss = 0.23999173\n",
      "Iteration 1339, loss = 0.23989288\n",
      "Iteration 1340, loss = 0.23980485\n",
      "Iteration 1341, loss = 0.23969966\n",
      "Iteration 1342, loss = 0.23954199\n",
      "Iteration 1343, loss = 0.23943339\n",
      "Iteration 1344, loss = 0.23932045\n",
      "Iteration 1345, loss = 0.23920875\n",
      "Iteration 1346, loss = 0.23910632\n",
      "Iteration 1347, loss = 0.23902880\n",
      "Iteration 1348, loss = 0.23889806\n",
      "Iteration 1349, loss = 0.23879989\n",
      "Iteration 1350, loss = 0.23867301\n",
      "Iteration 1351, loss = 0.23855474\n",
      "Iteration 1352, loss = 0.23844762\n",
      "Iteration 1353, loss = 0.23833625\n",
      "Iteration 1354, loss = 0.23825424\n",
      "Iteration 1355, loss = 0.23811168\n",
      "Iteration 1356, loss = 0.23798132\n",
      "Iteration 1357, loss = 0.23791125\n",
      "Iteration 1358, loss = 0.23783090\n",
      "Iteration 1359, loss = 0.23766479\n",
      "Iteration 1360, loss = 0.23753772\n",
      "Iteration 1361, loss = 0.23747466\n",
      "Iteration 1362, loss = 0.23736329\n",
      "Iteration 1363, loss = 0.23726666\n",
      "Iteration 1364, loss = 0.23719400\n",
      "Iteration 1365, loss = 0.23698736\n",
      "Iteration 1366, loss = 0.23693700\n",
      "Iteration 1367, loss = 0.23685624\n",
      "Iteration 1368, loss = 0.23670090\n",
      "Iteration 1369, loss = 0.23654614\n",
      "Iteration 1370, loss = 0.23643476\n",
      "Iteration 1371, loss = 0.23633545\n",
      "Iteration 1372, loss = 0.23622840\n",
      "Iteration 1373, loss = 0.23611696\n",
      "Iteration 1374, loss = 0.23599757\n",
      "Iteration 1375, loss = 0.23590188\n",
      "Iteration 1376, loss = 0.23575826\n",
      "Iteration 1377, loss = 0.23566474\n",
      "Iteration 1378, loss = 0.23557430\n",
      "Iteration 1379, loss = 0.23546505\n",
      "Iteration 1380, loss = 0.23535895\n",
      "Iteration 1381, loss = 0.23522219\n",
      "Iteration 1382, loss = 0.23513751\n",
      "Iteration 1383, loss = 0.23499495\n",
      "Iteration 1384, loss = 0.23489149\n",
      "Iteration 1385, loss = 0.23485167\n",
      "Iteration 1386, loss = 0.23466704\n",
      "Iteration 1387, loss = 0.23456869\n",
      "Iteration 1388, loss = 0.23444114\n",
      "Iteration 1389, loss = 0.23433798\n",
      "Iteration 1390, loss = 0.23422656\n",
      "Iteration 1391, loss = 0.23411911\n",
      "Iteration 1392, loss = 0.23405715\n",
      "Iteration 1393, loss = 0.23387443\n",
      "Iteration 1394, loss = 0.23378402\n",
      "Iteration 1395, loss = 0.23375590\n",
      "Iteration 1396, loss = 0.23362320\n",
      "Iteration 1397, loss = 0.23356339\n",
      "Iteration 1398, loss = 0.23337943\n",
      "Iteration 1399, loss = 0.23325415\n",
      "Iteration 1400, loss = 0.23315761\n",
      "Iteration 1401, loss = 0.23305896\n",
      "Iteration 1402, loss = 0.23293616\n",
      "Iteration 1403, loss = 0.23283585\n",
      "Iteration 1404, loss = 0.23269587\n",
      "Iteration 1405, loss = 0.23258519\n",
      "Iteration 1406, loss = 0.23248320\n",
      "Iteration 1407, loss = 0.23241919\n",
      "Iteration 1408, loss = 0.23226088\n",
      "Iteration 1409, loss = 0.23221285\n",
      "Iteration 1410, loss = 0.23203637\n",
      "Iteration 1411, loss = 0.23191114\n",
      "Iteration 1412, loss = 0.23179767\n",
      "Iteration 1413, loss = 0.23169695\n",
      "Iteration 1414, loss = 0.23159016\n",
      "Iteration 1415, loss = 0.23157614\n",
      "Iteration 1416, loss = 0.23136891\n",
      "Iteration 1417, loss = 0.23129427\n",
      "Iteration 1418, loss = 0.23121846\n",
      "Iteration 1419, loss = 0.23108680\n",
      "Iteration 1420, loss = 0.23096354\n",
      "Iteration 1421, loss = 0.23079665\n",
      "Iteration 1422, loss = 0.23071655\n",
      "Iteration 1423, loss = 0.23065466\n",
      "Iteration 1424, loss = 0.23055070\n",
      "Iteration 1425, loss = 0.23038460\n",
      "Iteration 1426, loss = 0.23027856\n",
      "Iteration 1427, loss = 0.23017597\n",
      "Iteration 1428, loss = 0.23007123\n",
      "Iteration 1429, loss = 0.22997097\n",
      "Iteration 1430, loss = 0.22981892\n",
      "Iteration 1431, loss = 0.22970987\n",
      "Iteration 1432, loss = 0.22964975\n",
      "Iteration 1433, loss = 0.22951568\n",
      "Iteration 1434, loss = 0.22938273\n",
      "Iteration 1435, loss = 0.22929337\n",
      "Iteration 1436, loss = 0.22918234\n",
      "Iteration 1437, loss = 0.22903517\n",
      "Iteration 1438, loss = 0.22894592\n",
      "Iteration 1439, loss = 0.22880851\n",
      "Iteration 1440, loss = 0.22877718\n",
      "Iteration 1441, loss = 0.22862449\n",
      "Iteration 1442, loss = 0.22851974\n",
      "Iteration 1443, loss = 0.22852045\n",
      "Iteration 1444, loss = 0.22830627\n",
      "Iteration 1445, loss = 0.22816350\n",
      "Iteration 1446, loss = 0.22806498\n",
      "Iteration 1447, loss = 0.22794091\n",
      "Iteration 1448, loss = 0.22783385\n",
      "Iteration 1449, loss = 0.22772522\n",
      "Iteration 1450, loss = 0.22761923\n",
      "Iteration 1451, loss = 0.22755063\n",
      "Iteration 1452, loss = 0.22744484\n",
      "Iteration 1453, loss = 0.22731443\n",
      "Iteration 1454, loss = 0.22719713\n",
      "Iteration 1455, loss = 0.22705769\n",
      "Iteration 1456, loss = 0.22694794\n",
      "Iteration 1457, loss = 0.22686022\n",
      "Iteration 1458, loss = 0.22671236\n",
      "Iteration 1459, loss = 0.22660163\n",
      "Iteration 1460, loss = 0.22647993\n",
      "Iteration 1461, loss = 0.22639858\n",
      "Iteration 1462, loss = 0.22629862\n",
      "Iteration 1463, loss = 0.22618690\n",
      "Iteration 1464, loss = 0.22614358\n",
      "Iteration 1465, loss = 0.22594847\n",
      "Iteration 1466, loss = 0.22586869\n",
      "Iteration 1467, loss = 0.22575186\n",
      "Iteration 1468, loss = 0.22564988\n",
      "Iteration 1469, loss = 0.22558566\n",
      "Iteration 1470, loss = 0.22538285\n",
      "Iteration 1471, loss = 0.22528607\n",
      "Iteration 1472, loss = 0.22520327\n",
      "Iteration 1473, loss = 0.22512823\n",
      "Iteration 1474, loss = 0.22498315\n",
      "Iteration 1475, loss = 0.22479238\n",
      "Iteration 1476, loss = 0.22475793\n",
      "Iteration 1477, loss = 0.22463922\n",
      "Iteration 1478, loss = 0.22445708\n",
      "Iteration 1479, loss = 0.22435175\n",
      "Iteration 1480, loss = 0.22430725\n",
      "Iteration 1481, loss = 0.22416901\n",
      "Iteration 1482, loss = 0.22403684\n",
      "Iteration 1483, loss = 0.22390275\n",
      "Iteration 1484, loss = 0.22378234\n",
      "Iteration 1485, loss = 0.22372744\n",
      "Iteration 1486, loss = 0.22356434\n",
      "Iteration 1487, loss = 0.22344484\n",
      "Iteration 1488, loss = 0.22337023\n",
      "Iteration 1489, loss = 0.22326772\n",
      "Iteration 1490, loss = 0.22312389\n",
      "Iteration 1491, loss = 0.22299465\n",
      "Iteration 1492, loss = 0.22297097\n",
      "Iteration 1493, loss = 0.22280161\n",
      "Iteration 1494, loss = 0.22266960\n",
      "Iteration 1495, loss = 0.22262970\n",
      "Iteration 1496, loss = 0.22244305\n",
      "Iteration 1497, loss = 0.22235559\n",
      "Iteration 1498, loss = 0.22222477\n",
      "Iteration 1499, loss = 0.22212279\n",
      "Iteration 1500, loss = 0.22198301\n",
      "Iteration 1501, loss = 0.22193443\n",
      "Iteration 1502, loss = 0.22177298\n",
      "Iteration 1503, loss = 0.22172917\n",
      "Iteration 1504, loss = 0.22165359\n",
      "Iteration 1505, loss = 0.22144387\n",
      "Iteration 1506, loss = 0.22131146\n",
      "Iteration 1507, loss = 0.22127406\n",
      "Iteration 1508, loss = 0.22111224\n",
      "Iteration 1509, loss = 0.22111016\n",
      "Iteration 1510, loss = 0.22088505\n",
      "Iteration 1511, loss = 0.22077126\n",
      "Iteration 1512, loss = 0.22066425\n",
      "Iteration 1513, loss = 0.22060184\n",
      "Iteration 1514, loss = 0.22044498\n",
      "Iteration 1515, loss = 0.22030489\n",
      "Iteration 1516, loss = 0.22026147\n",
      "Iteration 1517, loss = 0.22017453\n",
      "Iteration 1518, loss = 0.22004900\n",
      "Iteration 1519, loss = 0.22009744\n",
      "Iteration 1520, loss = 0.21978559\n",
      "Iteration 1521, loss = 0.21968148\n",
      "Iteration 1522, loss = 0.21954259\n",
      "Iteration 1523, loss = 0.21939071\n",
      "Iteration 1524, loss = 0.21932971\n",
      "Iteration 1525, loss = 0.21915489\n",
      "Iteration 1526, loss = 0.21911659\n",
      "Iteration 1527, loss = 0.21896909\n",
      "Iteration 1528, loss = 0.21885940\n",
      "Iteration 1529, loss = 0.21877952\n",
      "Iteration 1530, loss = 0.21862173\n",
      "Iteration 1531, loss = 0.21857608\n",
      "Iteration 1532, loss = 0.21842223\n",
      "Iteration 1533, loss = 0.21841937\n",
      "Iteration 1534, loss = 0.21815794\n",
      "Iteration 1535, loss = 0.21805614\n",
      "Iteration 1536, loss = 0.21795902\n",
      "Iteration 1537, loss = 0.21785387\n",
      "Iteration 1538, loss = 0.21775853\n",
      "Iteration 1539, loss = 0.21759807\n",
      "Iteration 1540, loss = 0.21749454\n",
      "Iteration 1541, loss = 0.21741140\n",
      "Iteration 1542, loss = 0.21728012\n",
      "Iteration 1543, loss = 0.21718109\n",
      "Iteration 1544, loss = 0.21704592\n",
      "Iteration 1545, loss = 0.21693602\n",
      "Iteration 1546, loss = 0.21685101\n",
      "Iteration 1547, loss = 0.21680395\n",
      "Iteration 1548, loss = 0.21662029\n",
      "Iteration 1549, loss = 0.21649967\n",
      "Iteration 1550, loss = 0.21638158\n",
      "Iteration 1551, loss = 0.21627416\n",
      "Iteration 1552, loss = 0.21617461\n",
      "Iteration 1553, loss = 0.21611981\n",
      "Iteration 1554, loss = 0.21600536\n",
      "Iteration 1555, loss = 0.21583028\n",
      "Iteration 1556, loss = 0.21574081\n",
      "Iteration 1557, loss = 0.21561622\n",
      "Iteration 1558, loss = 0.21547439\n",
      "Iteration 1559, loss = 0.21544887\n",
      "Iteration 1560, loss = 0.21538422\n",
      "Iteration 1561, loss = 0.21518650\n",
      "Iteration 1562, loss = 0.21504808\n",
      "Iteration 1563, loss = 0.21492267\n",
      "Iteration 1564, loss = 0.21482315\n",
      "Iteration 1565, loss = 0.21472728\n",
      "Iteration 1566, loss = 0.21461714\n",
      "Iteration 1567, loss = 0.21453641\n",
      "Iteration 1568, loss = 0.21437130\n",
      "Iteration 1569, loss = 0.21432848\n",
      "Iteration 1570, loss = 0.21414294\n",
      "Iteration 1571, loss = 0.21417156\n",
      "Iteration 1572, loss = 0.21392962\n",
      "Iteration 1573, loss = 0.21380488\n",
      "Iteration 1574, loss = 0.21377667\n",
      "Iteration 1575, loss = 0.21363809\n",
      "Iteration 1576, loss = 0.21350714\n",
      "Iteration 1577, loss = 0.21352742\n",
      "Iteration 1578, loss = 0.21332062\n",
      "Iteration 1579, loss = 0.21316687\n",
      "Iteration 1580, loss = 0.21314069\n",
      "Iteration 1581, loss = 0.21296083\n",
      "Iteration 1582, loss = 0.21287048\n",
      "Iteration 1583, loss = 0.21271001\n",
      "Iteration 1584, loss = 0.21259764\n",
      "Iteration 1585, loss = 0.21263707\n",
      "Iteration 1586, loss = 0.21238647\n",
      "Iteration 1587, loss = 0.21230997\n",
      "Iteration 1588, loss = 0.21220427\n",
      "Iteration 1589, loss = 0.21202990\n",
      "Iteration 1590, loss = 0.21196197\n",
      "Iteration 1591, loss = 0.21197576\n",
      "Iteration 1592, loss = 0.21169973\n",
      "Iteration 1593, loss = 0.21161961\n",
      "Iteration 1594, loss = 0.21152804\n",
      "Iteration 1595, loss = 0.21148744\n",
      "Iteration 1596, loss = 0.21130818\n",
      "Iteration 1597, loss = 0.21116370\n",
      "Iteration 1598, loss = 0.21103634\n",
      "Iteration 1599, loss = 0.21094032\n",
      "Iteration 1600, loss = 0.21086685\n",
      "Iteration 1601, loss = 0.21074084\n",
      "Iteration 1602, loss = 0.21059430\n",
      "Iteration 1603, loss = 0.21051788\n",
      "Iteration 1604, loss = 0.21041631\n",
      "Iteration 1605, loss = 0.21029070\n",
      "Iteration 1606, loss = 0.21019083\n",
      "Iteration 1607, loss = 0.21005478\n",
      "Iteration 1608, loss = 0.20995919\n",
      "Iteration 1609, loss = 0.20983557\n",
      "Iteration 1610, loss = 0.20975193\n",
      "Iteration 1611, loss = 0.20960388\n",
      "Iteration 1612, loss = 0.20954310\n",
      "Iteration 1613, loss = 0.20937605\n",
      "Iteration 1614, loss = 0.20928900\n",
      "Iteration 1615, loss = 0.20917451\n",
      "Iteration 1616, loss = 0.20907984\n",
      "Iteration 1617, loss = 0.20895211\n",
      "Iteration 1618, loss = 0.20893628\n",
      "Iteration 1619, loss = 0.20871872\n",
      "Iteration 1620, loss = 0.20864742\n",
      "Iteration 1621, loss = 0.20852668\n",
      "Iteration 1622, loss = 0.20841130\n",
      "Iteration 1623, loss = 0.20831699\n",
      "Iteration 1624, loss = 0.20819278\n",
      "Iteration 1625, loss = 0.20807707\n",
      "Iteration 1626, loss = 0.20797037\n",
      "Iteration 1627, loss = 0.20784741\n",
      "Iteration 1628, loss = 0.20774725\n",
      "Iteration 1629, loss = 0.20760345\n",
      "Iteration 1630, loss = 0.20752993\n",
      "Iteration 1631, loss = 0.20739367\n",
      "Iteration 1632, loss = 0.20726320\n",
      "Iteration 1633, loss = 0.20719990\n",
      "Iteration 1634, loss = 0.20710020\n",
      "Iteration 1635, loss = 0.20699607\n",
      "Iteration 1636, loss = 0.20691797\n",
      "Iteration 1637, loss = 0.20677822\n",
      "Iteration 1638, loss = 0.20665142\n",
      "Iteration 1639, loss = 0.20652776\n",
      "Iteration 1640, loss = 0.20642403\n",
      "Iteration 1641, loss = 0.20643665\n",
      "Iteration 1642, loss = 0.20636671\n",
      "Iteration 1643, loss = 0.20608578\n",
      "Iteration 1644, loss = 0.20597291\n",
      "Iteration 1645, loss = 0.20589649\n",
      "Iteration 1646, loss = 0.20570942\n",
      "Iteration 1647, loss = 0.20565207\n",
      "Iteration 1648, loss = 0.20554191\n",
      "Iteration 1649, loss = 0.20544553\n",
      "Iteration 1650, loss = 0.20531925\n",
      "Iteration 1651, loss = 0.20518984\n",
      "Iteration 1652, loss = 0.20512935\n",
      "Iteration 1653, loss = 0.20495415\n",
      "Iteration 1654, loss = 0.20487648\n",
      "Iteration 1655, loss = 0.20476976\n",
      "Iteration 1656, loss = 0.20466824\n",
      "Iteration 1657, loss = 0.20464057\n",
      "Iteration 1658, loss = 0.20447755\n",
      "Iteration 1659, loss = 0.20431640\n",
      "Iteration 1660, loss = 0.20422962\n",
      "Iteration 1661, loss = 0.20412844\n",
      "Iteration 1662, loss = 0.20400284\n",
      "Iteration 1663, loss = 0.20384481\n",
      "Iteration 1664, loss = 0.20378594\n",
      "Iteration 1665, loss = 0.20367281\n",
      "Iteration 1666, loss = 0.20363157\n",
      "Iteration 1667, loss = 0.20350610\n",
      "Iteration 1668, loss = 0.20330985\n",
      "Iteration 1669, loss = 0.20325960\n",
      "Iteration 1670, loss = 0.20310268\n",
      "Iteration 1671, loss = 0.20299481\n",
      "Iteration 1672, loss = 0.20300317\n",
      "Iteration 1673, loss = 0.20275035\n",
      "Iteration 1674, loss = 0.20265628\n",
      "Iteration 1675, loss = 0.20253447\n",
      "Iteration 1676, loss = 0.20241017\n",
      "Iteration 1677, loss = 0.20232306\n",
      "Iteration 1678, loss = 0.20228292\n",
      "Iteration 1679, loss = 0.20210387\n",
      "Iteration 1680, loss = 0.20200710\n",
      "Iteration 1681, loss = 0.20197238\n",
      "Iteration 1682, loss = 0.20174243\n",
      "Iteration 1683, loss = 0.20167260\n",
      "Iteration 1684, loss = 0.20155623\n",
      "Iteration 1685, loss = 0.20144083\n",
      "Iteration 1686, loss = 0.20130814\n",
      "Iteration 1687, loss = 0.20135840\n",
      "Iteration 1688, loss = 0.20125987\n",
      "Iteration 1689, loss = 0.20101712\n",
      "Iteration 1690, loss = 0.20089486\n",
      "Iteration 1691, loss = 0.20078144\n",
      "Iteration 1692, loss = 0.20065965\n",
      "Iteration 1693, loss = 0.20058708\n",
      "Iteration 1694, loss = 0.20042353\n",
      "Iteration 1695, loss = 0.20032723\n",
      "Iteration 1696, loss = 0.20020886\n",
      "Iteration 1697, loss = 0.20007279\n",
      "Iteration 1698, loss = 0.20006025\n",
      "Iteration 1699, loss = 0.19992576\n",
      "Iteration 1700, loss = 0.19983621\n",
      "Iteration 1701, loss = 0.19977470\n",
      "Iteration 1702, loss = 0.19953736\n",
      "Iteration 1703, loss = 0.19943978\n",
      "Iteration 1704, loss = 0.19928802\n",
      "Iteration 1705, loss = 0.19916809\n",
      "Iteration 1706, loss = 0.19930352\n",
      "Iteration 1707, loss = 0.19902123\n",
      "Iteration 1708, loss = 0.19889473\n",
      "Iteration 1709, loss = 0.19877032\n",
      "Iteration 1710, loss = 0.19867783\n",
      "Iteration 1711, loss = 0.19859358\n",
      "Iteration 1712, loss = 0.19846554\n",
      "Iteration 1713, loss = 0.19833581\n",
      "Iteration 1714, loss = 0.19823991\n",
      "Iteration 1715, loss = 0.19811246\n",
      "Iteration 1716, loss = 0.19802002\n",
      "Iteration 1717, loss = 0.19787722\n",
      "Iteration 1718, loss = 0.19777947\n",
      "Iteration 1719, loss = 0.19771922\n",
      "Iteration 1720, loss = 0.19756033\n",
      "Iteration 1721, loss = 0.19751286\n",
      "Iteration 1722, loss = 0.19734095\n",
      "Iteration 1723, loss = 0.19732102\n",
      "Iteration 1724, loss = 0.19708398\n",
      "Iteration 1725, loss = 0.19698379\n",
      "Iteration 1726, loss = 0.19703616\n",
      "Iteration 1727, loss = 0.19677020\n",
      "Iteration 1728, loss = 0.19673255\n",
      "Iteration 1729, loss = 0.19661575\n",
      "Iteration 1730, loss = 0.19645183\n",
      "Iteration 1731, loss = 0.19644186\n",
      "Iteration 1732, loss = 0.19634006\n",
      "Iteration 1733, loss = 0.19620467\n",
      "Iteration 1734, loss = 0.19606040\n",
      "Iteration 1735, loss = 0.19595047\n",
      "Iteration 1736, loss = 0.19581487\n",
      "Iteration 1737, loss = 0.19567338\n",
      "Iteration 1738, loss = 0.19562347\n",
      "Iteration 1739, loss = 0.19549959\n",
      "Iteration 1740, loss = 0.19536711\n",
      "Iteration 1741, loss = 0.19529840\n",
      "Iteration 1742, loss = 0.19513039\n",
      "Iteration 1743, loss = 0.19516471\n",
      "Iteration 1744, loss = 0.19495588\n",
      "Iteration 1745, loss = 0.19487842\n",
      "Iteration 1746, loss = 0.19472811\n",
      "Iteration 1747, loss = 0.19459300\n",
      "Iteration 1748, loss = 0.19451939\n",
      "Iteration 1749, loss = 0.19441688\n",
      "Iteration 1750, loss = 0.19431282\n",
      "Iteration 1751, loss = 0.19417983\n",
      "Iteration 1752, loss = 0.19407872\n",
      "Iteration 1753, loss = 0.19409351\n",
      "Iteration 1754, loss = 0.19395959\n",
      "Iteration 1755, loss = 0.19385300\n",
      "Iteration 1756, loss = 0.19361083\n",
      "Iteration 1757, loss = 0.19359387\n",
      "Iteration 1758, loss = 0.19340523\n",
      "Iteration 1759, loss = 0.19327652\n",
      "Iteration 1760, loss = 0.19315913\n",
      "Iteration 1761, loss = 0.19311052\n",
      "Iteration 1762, loss = 0.19294566\n",
      "Iteration 1763, loss = 0.19286399\n",
      "Iteration 1764, loss = 0.19272728\n",
      "Iteration 1765, loss = 0.19260360\n",
      "Iteration 1766, loss = 0.19252028\n",
      "Iteration 1767, loss = 0.19242633\n",
      "Iteration 1768, loss = 0.19235642\n",
      "Iteration 1769, loss = 0.19216703\n",
      "Iteration 1770, loss = 0.19204489\n",
      "Iteration 1771, loss = 0.19200093\n",
      "Iteration 1772, loss = 0.19181037\n",
      "Iteration 1773, loss = 0.19174452\n",
      "Iteration 1774, loss = 0.19164509\n",
      "Iteration 1775, loss = 0.19159082\n",
      "Iteration 1776, loss = 0.19137152\n",
      "Iteration 1777, loss = 0.19135202\n",
      "Iteration 1778, loss = 0.19115353\n",
      "Iteration 1779, loss = 0.19112348\n",
      "Iteration 1780, loss = 0.19097339\n",
      "Iteration 1781, loss = 0.19082554\n",
      "Iteration 1782, loss = 0.19071069\n",
      "Iteration 1783, loss = 0.19058847\n",
      "Iteration 1784, loss = 0.19059048\n",
      "Iteration 1785, loss = 0.19036508\n",
      "Iteration 1786, loss = 0.19028998\n",
      "Iteration 1787, loss = 0.19015206\n",
      "Iteration 1788, loss = 0.19007132\n",
      "Iteration 1789, loss = 0.18992739\n",
      "Iteration 1790, loss = 0.18982137\n",
      "Iteration 1791, loss = 0.18975570\n",
      "Iteration 1792, loss = 0.18961449\n",
      "Iteration 1793, loss = 0.18947228\n",
      "Iteration 981, loss = 0.26559351\n",
      "Iteration 982, loss = 0.26545638\n",
      "Iteration 983, loss = 0.26535378\n",
      "Iteration 984, loss = 0.26529678\n",
      "Iteration 985, loss = 0.26516121\n",
      "Iteration 986, loss = 0.26507034\n",
      "Iteration 987, loss = 0.26501216\n",
      "Iteration 988, loss = 0.26487519\n",
      "Iteration 989, loss = 0.26476935\n",
      "Iteration 990, loss = 0.26470369\n",
      "Iteration 991, loss = 0.26457509\n",
      "Iteration 992, loss = 0.26449081\n",
      "Iteration 993, loss = 0.26441051\n",
      "Iteration 994, loss = 0.26429747\n",
      "Iteration 995, loss = 0.26417835\n",
      "Iteration 996, loss = 0.26408503\n",
      "Iteration 997, loss = 0.26405766\n",
      "Iteration 998, loss = 0.26389072\n",
      "Iteration 999, loss = 0.26381497\n",
      "Iteration 1000, loss = 0.26369020\n",
      "Iteration 1001, loss = 0.26357365\n",
      "Iteration 1002, loss = 0.26349466\n",
      "Iteration 1003, loss = 0.26340221\n",
      "Iteration 1004, loss = 0.26329613\n",
      "Iteration 1005, loss = 0.26321534\n",
      "Iteration 1006, loss = 0.26310275\n",
      "Iteration 1007, loss = 0.26302296\n",
      "Iteration 1008, loss = 0.26289993\n",
      "Iteration 1009, loss = 0.26285794\n",
      "Iteration 1010, loss = 0.26271637\n",
      "Iteration 1011, loss = 0.26261217\n",
      "Iteration 1012, loss = 0.26253437\n",
      "Iteration 1013, loss = 0.26242871\n",
      "Iteration 1014, loss = 0.26232033\n",
      "Iteration 1015, loss = 0.26223753\n",
      "Iteration 1016, loss = 0.26216082\n",
      "Iteration 1017, loss = 0.26205905\n",
      "Iteration 1018, loss = 0.26195817\n",
      "Iteration 1019, loss = 0.26186158\n",
      "Iteration 1020, loss = 0.26171503\n",
      "Iteration 1021, loss = 0.26166616\n",
      "Iteration 1022, loss = 0.26152304\n",
      "Iteration 1023, loss = 0.26144524\n",
      "Iteration 1024, loss = 0.26135297\n",
      "Iteration 1025, loss = 0.26126288\n",
      "Iteration 1026, loss = 0.26117318\n",
      "Iteration 1027, loss = 0.26104003\n",
      "Iteration 1028, loss = 0.26096084\n",
      "Iteration 1029, loss = 0.26087750\n",
      "Iteration 1030, loss = 0.26073769\n",
      "Iteration 1031, loss = 0.26064264\n",
      "Iteration 1032, loss = 0.26061155\n",
      "Iteration 1033, loss = 0.26044648\n",
      "Iteration 1034, loss = 0.26040589\n",
      "Iteration 1035, loss = 0.26024191\n",
      "Iteration 1036, loss = 0.26015365\n",
      "Iteration 1037, loss = 0.26005027\n",
      "Iteration 1038, loss = 0.25997025\n",
      "Iteration 1039, loss = 0.25985972\n",
      "Iteration 1040, loss = 0.25980292\n",
      "Iteration 1041, loss = 0.25964655\n",
      "Iteration 1042, loss = 0.25954540\n",
      "Iteration 1043, loss = 0.25945402\n",
      "Iteration 1044, loss = 0.25938372\n",
      "Iteration 1045, loss = 0.25924013\n",
      "Iteration 1046, loss = 0.25921237\n",
      "Iteration 1047, loss = 0.25906322\n",
      "Iteration 1048, loss = 0.25894609\n",
      "Iteration 1049, loss = 0.25889452\n",
      "Iteration 1050, loss = 0.25874706\n",
      "Iteration 1051, loss = 0.25865653\n",
      "Iteration 1052, loss = 0.25857912\n",
      "Iteration 1053, loss = 0.25845081\n",
      "Iteration 1054, loss = 0.25837006\n",
      "Iteration 1055, loss = 0.25824635\n",
      "Iteration 1056, loss = 0.25817976\n",
      "Iteration 1057, loss = 0.25804993\n",
      "Iteration 1058, loss = 0.25796170\n",
      "Iteration 1059, loss = 0.25786340\n",
      "Iteration 1060, loss = 0.25775738\n",
      "Iteration 1061, loss = 0.25766247\n",
      "Iteration 1062, loss = 0.25757180\n",
      "Iteration 1063, loss = 0.25755411\n",
      "Iteration 1064, loss = 0.25745016\n",
      "Iteration 1065, loss = 0.25725953\n",
      "Iteration 1066, loss = 0.25717018\n",
      "Iteration 1067, loss = 0.25707879\n",
      "Iteration 1068, loss = 0.25698512\n",
      "Iteration 1069, loss = 0.25688696\n",
      "Iteration 1070, loss = 0.25678529\n",
      "Iteration 1071, loss = 0.25671157\n",
      "Iteration 1072, loss = 0.25660903\n",
      "Iteration 1073, loss = 0.25649742\n",
      "Iteration 1074, loss = 0.25639353\n",
      "Iteration 1075, loss = 0.25631411\n",
      "Iteration 1076, loss = 0.25627051\n",
      "Iteration 1077, loss = 0.25614518\n",
      "Iteration 1078, loss = 0.25598770\n",
      "Iteration 1079, loss = 0.25593172\n",
      "Iteration 1080, loss = 0.25580785\n",
      "Iteration 1081, loss = 0.25573076\n",
      "Iteration 1082, loss = 0.25561530\n",
      "Iteration 1083, loss = 0.25554541\n",
      "Iteration 1084, loss = 0.25545245\n",
      "Iteration 1085, loss = 0.25534354\n",
      "Iteration 1086, loss = 0.25523654\n",
      "Iteration 1087, loss = 0.25512360\n",
      "Iteration 1088, loss = 0.25505027\n",
      "Iteration 1089, loss = 0.25493991\n",
      "Iteration 1090, loss = 0.25486272\n",
      "Iteration 1091, loss = 0.25476186\n",
      "Iteration 1092, loss = 0.25465618\n",
      "Iteration 1093, loss = 0.25454702\n",
      "Iteration 1094, loss = 0.25448133\n",
      "Iteration 1095, loss = 0.25435754\n",
      "Iteration 1096, loss = 0.25426840\n",
      "Iteration 1097, loss = 0.25417613\n",
      "Iteration 1098, loss = 0.25408805\n",
      "Iteration 1099, loss = 0.25399708\n",
      "Iteration 1100, loss = 0.25386883\n",
      "Iteration 1101, loss = 0.25376947\n",
      "Iteration 1102, loss = 0.25368261\n",
      "Iteration 1103, loss = 0.25357747\n",
      "Iteration 1104, loss = 0.25352609\n",
      "Iteration 1105, loss = 0.25339878\n",
      "Iteration 1106, loss = 0.25328317\n",
      "Iteration 1107, loss = 0.25320568\n",
      "Iteration 1108, loss = 0.25313823\n",
      "Iteration 1109, loss = 0.25300869\n",
      "Iteration 1110, loss = 0.25291114\n",
      "Iteration 1111, loss = 0.25280249\n",
      "Iteration 1112, loss = 0.25271356\n",
      "Iteration 1113, loss = 0.25265700\n",
      "Iteration 1114, loss = 0.25254097\n",
      "Iteration 1115, loss = 0.25246479\n",
      "Iteration 1116, loss = 0.25233183\n",
      "Iteration 1117, loss = 0.25229362\n",
      "Iteration 1118, loss = 0.25212788\n",
      "Iteration 1119, loss = 0.25202993\n",
      "Iteration 1120, loss = 0.25198601\n",
      "Iteration 1121, loss = 0.25189215\n",
      "Iteration 1122, loss = 0.25178649\n",
      "Iteration 1123, loss = 0.25165523\n",
      "Iteration 1124, loss = 0.25157060\n",
      "Iteration 1125, loss = 0.25145125\n",
      "Iteration 1126, loss = 0.25138160\n",
      "Iteration 1127, loss = 0.25128058\n",
      "Iteration 1128, loss = 0.25120428\n",
      "Iteration 1129, loss = 0.25110579\n",
      "Iteration 1130, loss = 0.25099799\n",
      "Iteration 1131, loss = 0.25088407\n",
      "Iteration 1132, loss = 0.25080430\n",
      "Iteration 1133, loss = 0.25071051\n",
      "Iteration 1134, loss = 0.25063632\n",
      "Iteration 1135, loss = 0.25050915\n",
      "Iteration 1136, loss = 0.25044988\n",
      "Iteration 1137, loss = 0.25032199\n",
      "Iteration 1138, loss = 0.25022317\n",
      "Iteration 1139, loss = 0.25015206\n",
      "Iteration 1140, loss = 0.25003706\n",
      "Iteration 1141, loss = 0.24992046\n",
      "Iteration 1142, loss = 0.24984508\n",
      "Iteration 1143, loss = 0.24974210\n",
      "Iteration 1144, loss = 0.24964818\n",
      "Iteration 1145, loss = 0.24954509\n",
      "Iteration 1146, loss = 0.24950072\n",
      "Iteration 1147, loss = 0.24934447\n",
      "Iteration 1148, loss = 0.24927057\n",
      "Iteration 1149, loss = 0.24919705\n",
      "Iteration 1150, loss = 0.24906045\n",
      "Iteration 1151, loss = 0.24895345\n",
      "Iteration 1152, loss = 0.24889651\n",
      "Iteration 1153, loss = 0.24877354\n",
      "Iteration 1154, loss = 0.24868846\n",
      "Iteration 1155, loss = 0.24859811\n",
      "Iteration 1156, loss = 0.24852261\n",
      "Iteration 1157, loss = 0.24841433\n",
      "Iteration 1158, loss = 0.24836774\n",
      "Iteration 1159, loss = 0.24820108\n",
      "Iteration 1160, loss = 0.24813757\n",
      "Iteration 1161, loss = 0.24802639\n",
      "Iteration 1162, loss = 0.24797560\n",
      "Iteration 1163, loss = 0.24779473\n",
      "Iteration 1164, loss = 0.24782641\n",
      "Iteration 1165, loss = 0.24769783\n",
      "Iteration 1166, loss = 0.24753490\n",
      "Iteration 1167, loss = 0.24744969\n",
      "Iteration 1168, loss = 0.24732237\n",
      "Iteration 1169, loss = 0.24725231\n",
      "Iteration 1170, loss = 0.24716348\n",
      "Iteration 1171, loss = 0.24707053\n",
      "Iteration 1172, loss = 0.24696266\n",
      "Iteration 1173, loss = 0.24686025\n",
      "Iteration 1174, loss = 0.24680910\n",
      "Iteration 1175, loss = 0.24668276\n",
      "Iteration 1176, loss = 0.24661088\n",
      "Iteration 1177, loss = 0.24647594\n",
      "Iteration 1178, loss = 0.24639504\n",
      "Iteration 1179, loss = 0.24629282\n",
      "Iteration 1180, loss = 0.24619164\n",
      "Iteration 1181, loss = 0.24612489\n",
      "Iteration 1182, loss = 0.24604838\n",
      "Iteration 1183, loss = 0.24589556\n",
      "Iteration 1184, loss = 0.24582681\n",
      "Iteration 1185, loss = 0.24576603\n",
      "Iteration 1186, loss = 0.24564790\n",
      "Iteration 1187, loss = 0.24553792\n",
      "Iteration 1188, loss = 0.24544332\n",
      "Iteration 1189, loss = 0.24533325\n",
      "Iteration 1190, loss = 0.24527820\n",
      "Iteration 1191, loss = 0.24513838\n",
      "Iteration 1192, loss = 0.24505084\n",
      "Iteration 1193, loss = 0.24499346\n",
      "Iteration 1194, loss = 0.24486674\n",
      "Iteration 1195, loss = 0.24476509\n",
      "Iteration 1196, loss = 0.24470414\n",
      "Iteration 1197, loss = 0.24459634\n",
      "Iteration 1198, loss = 0.24450426\n",
      "Iteration 1199, loss = 0.24441693\n",
      "Iteration 1200, loss = 0.24430170\n",
      "Iteration 1201, loss = 0.24421191\n",
      "Iteration 1202, loss = 0.24409236\n",
      "Iteration 1203, loss = 0.24400293\n",
      "Iteration 1204, loss = 0.24391166\n",
      "Iteration 1205, loss = 0.24379238\n",
      "Iteration 1206, loss = 0.24371862\n",
      "Iteration 1207, loss = 0.24361797\n",
      "Iteration 1208, loss = 0.24351789\n",
      "Iteration 1209, loss = 0.24344576\n",
      "Iteration 1210, loss = 0.24332498\n",
      "Iteration 1211, loss = 0.24330659\n",
      "Iteration 1212, loss = 0.24317290\n",
      "Iteration 1213, loss = 0.24305252\n",
      "Iteration 1214, loss = 0.24297181\n",
      "Iteration 1215, loss = 0.24286133\n",
      "Iteration 1216, loss = 0.24274142\n",
      "Iteration 1217, loss = 0.24266208\n",
      "Iteration 1218, loss = 0.24254918\n",
      "Iteration 1219, loss = 0.24247617\n",
      "Iteration 1220, loss = 0.24233975\n",
      "Iteration 1221, loss = 0.24227741\n",
      "Iteration 1222, loss = 0.24216525\n",
      "Iteration 1223, loss = 0.24206651\n",
      "Iteration 1224, loss = 0.24197302\n",
      "Iteration 1225, loss = 0.24185355\n",
      "Iteration 1226, loss = 0.24178883\n",
      "Iteration 1227, loss = 0.24169296\n",
      "Iteration 1228, loss = 0.24158561\n",
      "Iteration 1229, loss = 0.24149346\n",
      "Iteration 1230, loss = 0.24139166\n",
      "Iteration 1231, loss = 0.24128873\n",
      "Iteration 1232, loss = 0.24118353\n",
      "Iteration 1233, loss = 0.24109944\n",
      "Iteration 1234, loss = 0.24098593\n",
      "Iteration 1235, loss = 0.24090316\n",
      "Iteration 1236, loss = 0.24080908\n",
      "Iteration 1237, loss = 0.24071570\n",
      "Iteration 1238, loss = 0.24062573\n",
      "Iteration 1239, loss = 0.24051742\n",
      "Iteration 1240, loss = 0.24043257\n",
      "Iteration 1241, loss = 0.24036225\n",
      "Iteration 1242, loss = 0.24025663\n",
      "Iteration 1243, loss = 0.24014388\n",
      "Iteration 1244, loss = 0.24006605\n",
      "Iteration 1245, loss = 0.23997771\n",
      "Iteration 1246, loss = 0.23992806\n",
      "Iteration 1247, loss = 0.23976856\n",
      "Iteration 1248, loss = 0.23968600\n",
      "Iteration 1249, loss = 0.23962819\n",
      "Iteration 1250, loss = 0.23954316\n",
      "Iteration 1251, loss = 0.23943055\n",
      "Iteration 1252, loss = 0.23933421\n",
      "Iteration 1253, loss = 0.23922923\n",
      "Iteration 1254, loss = 0.23918801\n",
      "Iteration 1255, loss = 0.23905552\n",
      "Iteration 1256, loss = 0.23898414\n",
      "Iteration 1257, loss = 0.23888653\n",
      "Iteration 1258, loss = 0.23877536\n",
      "Iteration 1259, loss = 0.23867174\n",
      "Iteration 1260, loss = 0.23857217\n",
      "Iteration 1261, loss = 0.23849257\n",
      "Iteration 1262, loss = 0.23843603\n",
      "Iteration 1263, loss = 0.23835484\n",
      "Iteration 1264, loss = 0.23823207\n",
      "Iteration 1265, loss = 0.23812409\n",
      "Iteration 1266, loss = 0.23803436\n",
      "Iteration 1267, loss = 0.23793012\n",
      "Iteration 1268, loss = 0.23797563\n",
      "Iteration 1269, loss = 0.23776004\n",
      "Iteration 1270, loss = 0.23768496\n",
      "Iteration 1271, loss = 0.23756689\n",
      "Iteration 1272, loss = 0.23748090\n",
      "Iteration 1273, loss = 0.23740830\n",
      "Iteration 1274, loss = 0.23730404\n",
      "Iteration 1275, loss = 0.23724374\n",
      "Iteration 1276, loss = 0.23712146\n",
      "Iteration 1277, loss = 0.23706375\n",
      "Iteration 1278, loss = 0.23693984\n",
      "Iteration 1279, loss = 0.23682735\n",
      "Iteration 1280, loss = 0.23673786\n",
      "Iteration 1281, loss = 0.23664501\n",
      "Iteration 1282, loss = 0.23656271\n",
      "Iteration 1283, loss = 0.23649199\n",
      "Iteration 1284, loss = 0.23643174\n",
      "Iteration 1285, loss = 0.23631891\n",
      "Iteration 1286, loss = 0.23624297\n",
      "Iteration 1287, loss = 0.23613570\n",
      "Iteration 1288, loss = 0.23601041\n",
      "Iteration 1289, loss = 0.23593659\n",
      "Iteration 1290, loss = 0.23582628\n",
      "Iteration 1291, loss = 0.23578746\n",
      "Iteration 1292, loss = 0.23563897\n",
      "Iteration 1293, loss = 0.23559130\n",
      "Iteration 1294, loss = 0.23550326\n",
      "Iteration 1295, loss = 0.23536939\n",
      "Iteration 1296, loss = 0.23531226\n",
      "Iteration 1297, loss = 0.23518875\n",
      "Iteration 1298, loss = 0.23509716\n",
      "Iteration 1299, loss = 0.23502540\n",
      "Iteration 1300, loss = 0.23495776\n",
      "Iteration 1301, loss = 0.23487347\n",
      "Iteration 1302, loss = 0.23474800\n",
      "Iteration 1303, loss = 0.23463277\n",
      "Iteration 1304, loss = 0.23456820\n",
      "Iteration 1305, loss = 0.23446917\n",
      "Iteration 1306, loss = 0.23437443\n",
      "Iteration 1307, loss = 0.23427416\n",
      "Iteration 1308, loss = 0.23417379\n",
      "Iteration 1309, loss = 0.23410089\n",
      "Iteration 1310, loss = 0.23400166\n",
      "Iteration 1311, loss = 0.23389469\n",
      "Iteration 1312, loss = 0.23382300\n",
      "Iteration 1313, loss = 0.23373106\n",
      "Iteration 1314, loss = 0.23367107\n",
      "Iteration 1315, loss = 0.23360582\n",
      "Iteration 1316, loss = 0.23344205\n",
      "Iteration 1317, loss = 0.23337222\n",
      "Iteration 1318, loss = 0.23326671\n",
      "Iteration 1319, loss = 0.23318004\n",
      "Iteration 1320, loss = 0.23306936\n",
      "Iteration 1321, loss = 0.23300950\n",
      "Iteration 1322, loss = 0.23294877\n",
      "Iteration 1323, loss = 0.23282224\n",
      "Iteration 1324, loss = 0.23280682\n",
      "Iteration 1325, loss = 0.23265873\n",
      "Iteration 1326, loss = 0.23254116\n",
      "Iteration 1327, loss = 0.23247399\n",
      "Iteration 1328, loss = 0.23237067\n",
      "Iteration 1329, loss = 0.23228484\n",
      "Iteration 1330, loss = 0.23219084\n",
      "Iteration 1331, loss = 0.23217586\n",
      "Iteration 1332, loss = 0.23206633\n",
      "Iteration 1333, loss = 0.23191177\n",
      "Iteration 1334, loss = 0.23185163\n",
      "Iteration 1335, loss = 0.23171906\n",
      "Iteration 1336, loss = 0.23162308\n",
      "Iteration 1337, loss = 0.23157005\n",
      "Iteration 1338, loss = 0.23149119\n",
      "Iteration 1339, loss = 0.23134317\n",
      "Iteration 1340, loss = 0.23129240\n",
      "Iteration 1341, loss = 0.23119707\n",
      "Iteration 1342, loss = 0.23113995\n",
      "Iteration 1343, loss = 0.23098153\n",
      "Iteration 1344, loss = 0.23091118\n",
      "Iteration 1345, loss = 0.23082820\n",
      "Iteration 1346, loss = 0.23077440\n",
      "Iteration 1347, loss = 0.23060336\n",
      "Iteration 1348, loss = 0.23052827\n",
      "Iteration 1349, loss = 0.23048420\n",
      "Iteration 1350, loss = 0.23034673\n",
      "Iteration 1351, loss = 0.23025843\n",
      "Iteration 1352, loss = 0.23017874\n",
      "Iteration 1353, loss = 0.23009017\n",
      "Iteration 1354, loss = 0.22999416\n",
      "Iteration 1355, loss = 0.22988097\n",
      "Iteration 1356, loss = 0.22980143\n",
      "Iteration 1357, loss = 0.22974636\n",
      "Iteration 1358, loss = 0.22964000\n",
      "Iteration 1359, loss = 0.22953603\n",
      "Iteration 1360, loss = 0.22944282\n",
      "Iteration 1361, loss = 0.22933286\n",
      "Iteration 1362, loss = 0.22926000\n",
      "Iteration 1363, loss = 0.22916580\n",
      "Iteration 1364, loss = 0.22907196\n",
      "Iteration 1365, loss = 0.22896514\n",
      "Iteration 1366, loss = 0.22887137\n",
      "Iteration 1367, loss = 0.22880642\n",
      "Iteration 1368, loss = 0.22870603\n",
      "Iteration 1369, loss = 0.22863096\n",
      "Iteration 1370, loss = 0.22855221\n",
      "Iteration 1371, loss = 0.22846288\n",
      "Iteration 1372, loss = 0.22833510\n",
      "Iteration 1373, loss = 0.22827498\n",
      "Iteration 1374, loss = 0.22815439\n",
      "Iteration 1375, loss = 0.22806411\n",
      "Iteration 1376, loss = 0.22798849\n",
      "Iteration 1377, loss = 0.22790976\n",
      "Iteration 1378, loss = 0.22778043\n",
      "Iteration 1379, loss = 0.22771790\n",
      "Iteration 1380, loss = 0.22762084\n",
      "Iteration 1381, loss = 0.22757298\n",
      "Iteration 1382, loss = 0.22745684\n",
      "Iteration 1383, loss = 0.22738128\n",
      "Iteration 1384, loss = 0.22728199\n",
      "Iteration 1385, loss = 0.22716025\n",
      "Iteration 1386, loss = 0.22706158\n",
      "Iteration 1387, loss = 0.22701639\n",
      "Iteration 1388, loss = 0.22691042\n",
      "Iteration 1389, loss = 0.22680245\n",
      "Iteration 1390, loss = 0.22674163\n",
      "Iteration 1391, loss = 0.22661719\n",
      "Iteration 1392, loss = 0.22654602\n",
      "Iteration 1393, loss = 0.22642723\n",
      "Iteration 1394, loss = 0.22642379\n",
      "Iteration 1395, loss = 0.22630356\n",
      "Iteration 1396, loss = 0.22615868\n",
      "Iteration 1397, loss = 0.22608132\n",
      "Iteration 1398, loss = 0.22603317\n",
      "Iteration 1399, loss = 0.22588478\n",
      "Iteration 1400, loss = 0.22583998\n",
      "Iteration 1401, loss = 0.22569263\n",
      "Iteration 1402, loss = 0.22560151\n",
      "Iteration 1403, loss = 0.22554562\n",
      "Iteration 1404, loss = 0.22539180\n",
      "Iteration 1405, loss = 0.22541792\n",
      "Iteration 1406, loss = 0.22522420\n",
      "Iteration 1407, loss = 0.22515131\n",
      "Iteration 1408, loss = 0.22502263\n",
      "Iteration 1409, loss = 0.22496172\n",
      "Iteration 1410, loss = 0.22485376\n",
      "Iteration 1411, loss = 0.22474325\n",
      "Iteration 1412, loss = 0.22468518\n",
      "Iteration 1413, loss = 0.22457166\n",
      "Iteration 1414, loss = 0.22450183\n",
      "Iteration 1415, loss = 0.22443547\n",
      "Iteration 1416, loss = 0.22437731\n",
      "Iteration 1417, loss = 0.22423825\n",
      "Iteration 1418, loss = 0.22410043\n",
      "Iteration 1419, loss = 0.22402375\n",
      "Iteration 1420, loss = 0.22389813\n",
      "Iteration 1421, loss = 0.22382315\n",
      "Iteration 1422, loss = 0.22373576\n",
      "Iteration 1423, loss = 0.22363892\n",
      "Iteration 1424, loss = 0.22353874\n",
      "Iteration 1425, loss = 0.22344772\n",
      "Iteration 1426, loss = 0.22336976\n",
      "Iteration 1427, loss = 0.22324476\n",
      "Iteration 1428, loss = 0.22314119\n",
      "Iteration 1429, loss = 0.22309038\n",
      "Iteration 1430, loss = 0.22295748\n",
      "Iteration 1431, loss = 0.22288545\n",
      "Iteration 1432, loss = 0.22279427\n",
      "Iteration 1433, loss = 0.22272266\n",
      "Iteration 1434, loss = 0.22260737\n",
      "Iteration 1435, loss = 0.22249679\n",
      "Iteration 1436, loss = 0.22239555\n",
      "Iteration 1437, loss = 0.22233318\n",
      "Iteration 1438, loss = 0.22220801\n",
      "Iteration 1439, loss = 0.22212826\n",
      "Iteration 1440, loss = 0.22199548\n",
      "Iteration 1441, loss = 0.22189680\n",
      "Iteration 1442, loss = 0.22181157\n",
      "Iteration 1443, loss = 0.22172491\n",
      "Iteration 1444, loss = 0.22161531\n",
      "Iteration 1445, loss = 0.22155522\n",
      "Iteration 1446, loss = 0.22142276\n",
      "Iteration 1447, loss = 0.22135689\n",
      "Iteration 1448, loss = 0.22124302\n",
      "Iteration 1449, loss = 0.22114597\n",
      "Iteration 1450, loss = 0.22108389\n",
      "Iteration 1451, loss = 0.22097536\n",
      "Iteration 1452, loss = 0.22085197\n",
      "Iteration 1453, loss = 0.22078546\n",
      "Iteration 1454, loss = 0.22068082\n",
      "Iteration 1455, loss = 0.22059915\n",
      "Iteration 1456, loss = 0.22059343\n",
      "Iteration 1457, loss = 0.22038602\n",
      "Iteration 1458, loss = 0.22031304\n",
      "Iteration 1459, loss = 0.22020265\n",
      "Iteration 1460, loss = 0.22011679\n",
      "Iteration 1461, loss = 0.22012246\n",
      "Iteration 2598, loss = 0.12813770\n",
      "Iteration 2599, loss = 0.12818047\n",
      "Iteration 2600, loss = 0.12799457\n",
      "Iteration 2601, loss = 0.12797977\n",
      "Iteration 2602, loss = 0.12791161\n",
      "Iteration 2603, loss = 0.12791213\n",
      "Iteration 2604, loss = 0.12768930\n",
      "Iteration 2605, loss = 0.12767824\n",
      "Iteration 2606, loss = 0.12751021\n",
      "Iteration 2607, loss = 0.12748258\n",
      "Iteration 2608, loss = 0.12745946\n",
      "Iteration 2609, loss = 0.12733432\n",
      "Iteration 2610, loss = 0.12720551\n",
      "Iteration 2611, loss = 0.12721141\n",
      "Iteration 2612, loss = 0.12703602\n",
      "Iteration 2613, loss = 0.12700375\n",
      "Iteration 2614, loss = 0.12693495\n",
      "Iteration 2615, loss = 0.12678873\n",
      "Iteration 2616, loss = 0.12675554\n",
      "Iteration 2617, loss = 0.12673062\n",
      "Iteration 2618, loss = 0.12665620\n",
      "Iteration 2619, loss = 0.12649237\n",
      "Iteration 2620, loss = 0.12656882\n",
      "Iteration 2621, loss = 0.12635911\n",
      "Iteration 2622, loss = 0.12628858\n",
      "Iteration 2623, loss = 0.12620063\n",
      "Iteration 2624, loss = 0.12612116\n",
      "Iteration 2625, loss = 0.12606989\n",
      "Iteration 2626, loss = 0.12598524\n",
      "Iteration 2627, loss = 0.12583215\n",
      "Iteration 2628, loss = 0.12574498\n",
      "Iteration 2629, loss = 0.12574857\n",
      "Iteration 2630, loss = 0.12564233\n",
      "Iteration 2631, loss = 0.12552165\n",
      "Iteration 2632, loss = 0.12547206\n",
      "Iteration 2633, loss = 0.12539051\n",
      "Iteration 2634, loss = 0.12533903\n",
      "Iteration 2635, loss = 0.12520504\n",
      "Iteration 2636, loss = 0.12512755\n",
      "Iteration 2637, loss = 0.12507751\n",
      "Iteration 2638, loss = 0.12511185\n",
      "Iteration 2639, loss = 0.12490256\n",
      "Iteration 2640, loss = 0.12486329\n",
      "Iteration 2641, loss = 0.12474101\n",
      "Iteration 2642, loss = 0.12470535\n",
      "Iteration 2643, loss = 0.12459196\n",
      "Iteration 2644, loss = 0.12456214\n",
      "Iteration 2645, loss = 0.12447947\n",
      "Iteration 2646, loss = 0.12440835\n",
      "Iteration 2647, loss = 0.12436917\n",
      "Iteration 2648, loss = 0.12419484\n",
      "Iteration 2649, loss = 0.12414492\n",
      "Iteration 2650, loss = 0.12414189\n",
      "Iteration 2651, loss = 0.12400319\n",
      "Iteration 2652, loss = 0.12393087\n",
      "Iteration 2653, loss = 0.12385542\n",
      "Iteration 2654, loss = 0.12375398\n",
      "Iteration 2655, loss = 0.12366753\n",
      "Iteration 2656, loss = 0.12367606\n",
      "Iteration 2657, loss = 0.12349469\n",
      "Iteration 2658, loss = 0.12341230\n",
      "Iteration 2659, loss = 0.12333411\n",
      "Iteration 2660, loss = 0.12334452\n",
      "Iteration 2661, loss = 0.12324237\n",
      "Iteration 2662, loss = 0.12317785\n",
      "Iteration 2663, loss = 0.12308805\n",
      "Iteration 2664, loss = 0.12295496\n",
      "Iteration 2665, loss = 0.12290570\n",
      "Iteration 2666, loss = 0.12287090\n",
      "Iteration 2667, loss = 0.12283363\n",
      "Iteration 2668, loss = 0.12268009\n",
      "Iteration 2669, loss = 0.12256482\n",
      "Iteration 2670, loss = 0.12251553\n",
      "Iteration 2671, loss = 0.12244528\n",
      "Iteration 2672, loss = 0.12241183\n",
      "Iteration 2673, loss = 0.12227901\n",
      "Iteration 2674, loss = 0.12227609\n",
      "Iteration 2675, loss = 0.12223927\n",
      "Iteration 2676, loss = 0.12205018\n",
      "Iteration 2677, loss = 0.12196073\n",
      "Iteration 2678, loss = 0.12194065\n",
      "Iteration 2679, loss = 0.12182235\n",
      "Iteration 2680, loss = 0.12179888\n",
      "Iteration 2681, loss = 0.12165650\n",
      "Iteration 2682, loss = 0.12169222\n",
      "Iteration 2683, loss = 0.12152740\n",
      "Iteration 2684, loss = 0.12143019\n",
      "Iteration 2685, loss = 0.12142590\n",
      "Iteration 2686, loss = 0.12131035\n",
      "Iteration 2687, loss = 0.12133236\n",
      "Iteration 2688, loss = 0.12120983\n",
      "Iteration 2689, loss = 0.12114112\n",
      "Iteration 2690, loss = 0.12104341\n",
      "Iteration 2691, loss = 0.12102220\n",
      "Iteration 2692, loss = 0.12084050\n",
      "Iteration 2693, loss = 0.12081050\n",
      "Iteration 2694, loss = 0.12069686\n",
      "Iteration 2695, loss = 0.12065039\n",
      "Iteration 2696, loss = 0.12055052\n",
      "Iteration 2697, loss = 0.12052208\n",
      "Iteration 2698, loss = 0.12047477\n",
      "Iteration 2699, loss = 0.12033594\n",
      "Iteration 2700, loss = 0.12024928\n",
      "Iteration 2701, loss = 0.12016409\n",
      "Iteration 2702, loss = 0.12014153\n",
      "Iteration 2703, loss = 0.12007455\n",
      "Iteration 2704, loss = 0.12001458\n",
      "Iteration 2705, loss = 0.11990226\n",
      "Iteration 2706, loss = 0.11980697\n",
      "Iteration 2707, loss = 0.11972412\n",
      "Iteration 2708, loss = 0.11973497\n",
      "Iteration 2709, loss = 0.11959407\n",
      "Iteration 2710, loss = 0.11963678\n",
      "Iteration 2711, loss = 0.11939413\n",
      "Iteration 2712, loss = 0.11941478\n",
      "Iteration 2713, loss = 0.11941069\n",
      "Iteration 2714, loss = 0.11922551\n",
      "Iteration 2715, loss = 0.11916403\n",
      "Iteration 2716, loss = 0.11906560\n",
      "Iteration 2717, loss = 0.11919261\n",
      "Iteration 2718, loss = 0.11900398\n",
      "Iteration 2719, loss = 0.11893526\n",
      "Iteration 2720, loss = 0.11889154\n",
      "Iteration 2721, loss = 0.11867597\n",
      "Iteration 2722, loss = 0.11873246\n",
      "Iteration 2723, loss = 0.11859171\n",
      "Iteration 2724, loss = 0.11850112\n",
      "Iteration 2725, loss = 0.11840564\n",
      "Iteration 2726, loss = 0.11841365\n",
      "Iteration 2727, loss = 0.11824239\n",
      "Iteration 2728, loss = 0.11827052\n",
      "Iteration 2729, loss = 0.11813101\n",
      "Iteration 2730, loss = 0.11801799\n",
      "Iteration 2731, loss = 0.11795794\n",
      "Iteration 2732, loss = 0.11788842\n",
      "Iteration 2733, loss = 0.11776546\n",
      "Iteration 2734, loss = 0.11772323\n",
      "Iteration 2735, loss = 0.11766389\n",
      "Iteration 2736, loss = 0.11758239\n",
      "Iteration 2737, loss = 0.11752816\n",
      "Iteration 2738, loss = 0.11740582\n",
      "Iteration 2739, loss = 0.11734070\n",
      "Iteration 2740, loss = 0.11738423\n",
      "Iteration 2741, loss = 0.11718927\n",
      "Iteration 2742, loss = 0.11710057\n",
      "Iteration 2743, loss = 0.11709151\n",
      "Iteration 2744, loss = 0.11701283\n",
      "Iteration 2745, loss = 0.11700514\n",
      "Iteration 2746, loss = 0.11686641\n",
      "Iteration 2747, loss = 0.11675972\n",
      "Iteration 2748, loss = 0.11677308\n",
      "Iteration 2749, loss = 0.11669267\n",
      "Iteration 2750, loss = 0.11670549\n",
      "Iteration 2751, loss = 0.11655197\n",
      "Iteration 2752, loss = 0.11640956\n",
      "Iteration 2753, loss = 0.11642061\n",
      "Iteration 2754, loss = 0.11623425\n",
      "Iteration 2755, loss = 0.11622614\n",
      "Iteration 2756, loss = 0.11610317\n",
      "Iteration 2757, loss = 0.11605430\n",
      "Iteration 2758, loss = 0.11596264\n",
      "Iteration 2759, loss = 0.11586766\n",
      "Iteration 2760, loss = 0.11579268\n",
      "Iteration 2761, loss = 0.11573372\n",
      "Iteration 2762, loss = 0.11568081\n",
      "Iteration 2763, loss = 0.11559172\n",
      "Iteration 2764, loss = 0.11554915\n",
      "Iteration 2765, loss = 0.11545409\n",
      "Iteration 2766, loss = 0.11539853\n",
      "Iteration 2767, loss = 0.11530085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75350947\n",
      "Iteration 2, loss = 0.75158366\n",
      "Iteration 3, loss = 0.74861454\n",
      "Iteration 4, loss = 0.74509227\n",
      "Iteration 5, loss = 0.74119842\n",
      "Iteration 6, loss = 0.73670018\n",
      "Iteration 7, loss = 0.73236769\n",
      "Iteration 8, loss = 0.72794837\n",
      "Iteration 9, loss = 0.72348634\n",
      "Iteration 10, loss = 0.71917923\n",
      "Iteration 11, loss = 0.71464482\n",
      "Iteration 12, loss = 0.71031837\n",
      "Iteration 13, loss = 0.70610381\n",
      "Iteration 14, loss = 0.70185067\n",
      "Iteration 15, loss = 0.69783805\n",
      "Iteration 16, loss = 0.69365295\n",
      "Iteration 17, loss = 0.68961000\n",
      "Iteration 18, loss = 0.68571135\n",
      "Iteration 19, loss = 0.68161710\n",
      "Iteration 20, loss = 0.67771487\n",
      "Iteration 21, loss = 0.67379927\n",
      "Iteration 22, loss = 0.66990981\n",
      "Iteration 23, loss = 0.66613735\n",
      "Iteration 24, loss = 0.66225337\n",
      "Iteration 25, loss = 0.65840490\n",
      "Iteration 26, loss = 0.65455998\n",
      "Iteration 27, loss = 0.65072230\n",
      "Iteration 28, loss = 0.64691479\n",
      "Iteration 29, loss = 0.64316692\n",
      "Iteration 30, loss = 0.63928490\n",
      "Iteration 31, loss = 0.63555833\n",
      "Iteration 32, loss = 0.63165660\n",
      "Iteration 33, loss = 0.62796920\n",
      "Iteration 34, loss = 0.62414308\n",
      "Iteration 35, loss = 0.62035325\n",
      "Iteration 36, loss = 0.61662036\n",
      "Iteration 37, loss = 0.61281010\n",
      "Iteration 38, loss = 0.60910826\n",
      "Iteration 39, loss = 0.60528213\n",
      "Iteration 40, loss = 0.60153010\n",
      "Iteration 41, loss = 0.59780614\n",
      "Iteration 42, loss = 0.59407143\n",
      "Iteration 43, loss = 0.59039526\n",
      "Iteration 44, loss = 0.58670999\n",
      "Iteration 45, loss = 0.58298054\n",
      "Iteration 46, loss = 0.57941144\n",
      "Iteration 47, loss = 0.57576281\n",
      "Iteration 48, loss = 0.57224440\n",
      "Iteration 49, loss = 0.56857103\n",
      "Iteration 50, loss = 0.56510029\n",
      "Iteration 51, loss = 0.56168648\n",
      "Iteration 52, loss = 0.55819789\n",
      "Iteration 53, loss = 0.55467561\n",
      "Iteration 54, loss = 0.55137418\n",
      "Iteration 55, loss = 0.54796142\n",
      "Iteration 56, loss = 0.54468433\n",
      "Iteration 57, loss = 0.54135571\n",
      "Iteration 58, loss = 0.53807928\n",
      "Iteration 59, loss = 0.53497026\n",
      "Iteration 60, loss = 0.53182064\n",
      "Iteration 61, loss = 0.52871624\n",
      "Iteration 62, loss = 0.52572427\n",
      "Iteration 63, loss = 0.52264440\n",
      "Iteration 64, loss = 0.51982985\n",
      "Iteration 65, loss = 0.51686695\n",
      "Iteration 66, loss = 0.51400227\n",
      "Iteration 67, loss = 0.51126931\n",
      "Iteration 68, loss = 0.50849300\n",
      "Iteration 69, loss = 0.50574389\n",
      "Iteration 70, loss = 0.50318624\n",
      "Iteration 71, loss = 0.50060252\n",
      "Iteration 72, loss = 0.49795014\n",
      "Iteration 73, loss = 0.49542913\n",
      "Iteration 74, loss = 0.49304978\n",
      "Iteration 75, loss = 0.49066318\n",
      "Iteration 76, loss = 0.48826741\n",
      "Iteration 77, loss = 0.48594177\n",
      "Iteration 78, loss = 0.48373532\n",
      "Iteration 79, loss = 0.48156191\n",
      "Iteration 80, loss = 0.47936592\n",
      "Iteration 81, loss = 0.47730531\n",
      "Iteration 82, loss = 0.47528035\n",
      "Iteration 83, loss = 0.47317253\n",
      "Iteration 84, loss = 0.47116004\n",
      "Iteration 85, loss = 0.46930184\n",
      "Iteration 86, loss = 0.46738353\n",
      "Iteration 87, loss = 0.46552656\n",
      "Iteration 88, loss = 0.46374126\n",
      "Iteration 89, loss = 0.46203328\n",
      "Iteration 90, loss = 0.46029155\n",
      "Iteration 91, loss = 0.45857711\n",
      "Iteration 92, loss = 0.45697720\n",
      "Iteration 93, loss = 0.45537375\n",
      "Iteration 94, loss = 0.45390349\n",
      "Iteration 95, loss = 0.45227060\n",
      "Iteration 96, loss = 0.45076555\n",
      "Iteration 97, loss = 0.44934278\n",
      "Iteration 98, loss = 0.44795838\n",
      "Iteration 99, loss = 0.44648143\n",
      "Iteration 100, loss = 0.44516746\n",
      "Iteration 101, loss = 0.44391759\n",
      "Iteration 102, loss = 0.44261491\n",
      "Iteration 103, loss = 0.44134153\n",
      "Iteration 104, loss = 0.44010256\n",
      "Iteration 105, loss = 0.43883231\n",
      "Iteration 106, loss = 0.43773628\n",
      "Iteration 107, loss = 0.43659370\n",
      "Iteration 108, loss = 0.43545148\n",
      "Iteration 109, loss = 0.43436792\n",
      "Iteration 110, loss = 0.43327718\n",
      "Iteration 111, loss = 0.43222208\n",
      "Iteration 112, loss = 0.43125159\n",
      "Iteration 113, loss = 0.43018305\n",
      "Iteration 114, loss = 0.42916831\n",
      "Iteration 115, loss = 0.42824046\n",
      "Iteration 116, loss = 0.42723887\n",
      "Iteration 117, loss = 0.42634719\n",
      "Iteration 118, loss = 0.42544738\n",
      "Iteration 119, loss = 0.42456065\n",
      "Iteration 120, loss = 0.42366140\n",
      "Iteration 121, loss = 0.42280704\n",
      "Iteration 122, loss = 0.42192651\n",
      "Iteration 123, loss = 0.42113136\n",
      "Iteration 124, loss = 0.42032319\n",
      "Iteration 125, loss = 0.41952482\n",
      "Iteration 126, loss = 0.41872378\n",
      "Iteration 127, loss = 0.41798163\n",
      "Iteration 128, loss = 0.41723353\n",
      "Iteration 129, loss = 0.41641420\n",
      "Iteration 130, loss = 0.41573808\n",
      "Iteration 131, loss = 0.41501784\n",
      "Iteration 132, loss = 0.41427990\n",
      "Iteration 133, loss = 0.41360356\n",
      "Iteration 134, loss = 0.41294889\n",
      "Iteration 135, loss = 0.41226814\n",
      "Iteration 136, loss = 0.41160270\n",
      "Iteration 137, loss = 0.41091699\n",
      "Iteration 138, loss = 0.41027397\n",
      "Iteration 139, loss = 0.40966442\n",
      "Iteration 140, loss = 0.40898468\n",
      "Iteration 141, loss = 0.40839998\n",
      "Iteration 142, loss = 0.40780071\n",
      "Iteration 143, loss = 0.40719629\n",
      "Iteration 144, loss = 0.40661369\n",
      "Iteration 145, loss = 0.40604362\n",
      "Iteration 146, loss = 0.40544350\n",
      "Iteration 147, loss = 0.40486335\n",
      "Iteration 148, loss = 0.40431912\n",
      "Iteration 149, loss = 0.40379604\n",
      "Iteration 150, loss = 0.40322989\n",
      "Iteration 151, loss = 0.40272989\n",
      "Iteration 152, loss = 0.40215251\n",
      "Iteration 153, loss = 0.40161630\n",
      "Iteration 154, loss = 0.40109079\n",
      "Iteration 155, loss = 0.40058723\n",
      "Iteration 156, loss = 0.40005738\n",
      "Iteration 157, loss = 0.39956626\n",
      "Iteration 158, loss = 0.39904470\n",
      "Iteration 159, loss = 0.39853449\n",
      "Iteration 160, loss = 0.39803073\n",
      "Iteration 161, loss = 0.39755778\n",
      "Iteration 162, loss = 0.39706413\n",
      "Iteration 163, loss = 0.39662042\n",
      "Iteration 164, loss = 0.39615793\n",
      "Iteration 165, loss = 0.39568964\n",
      "Iteration 166, loss = 0.39517060\n",
      "Iteration 167, loss = 0.39472563\n",
      "Iteration 168, loss = 0.39425065\n",
      "Iteration 169, loss = 0.39383388\n",
      "Iteration 170, loss = 0.39337705\n",
      "Iteration 171, loss = 0.39294908\n",
      "Iteration 172, loss = 0.39248813\n",
      "Iteration 173, loss = 0.39203560\n",
      "Iteration 174, loss = 0.39166436\n",
      "Iteration 175, loss = 0.39122092\n",
      "Iteration 176, loss = 0.39080757\n",
      "Iteration 177, loss = 0.39038564\n",
      "Iteration 178, loss = 0.38995539\n",
      "Iteration 179, loss = 0.38952682\n",
      "Iteration 180, loss = 0.38913854\n",
      "Iteration 181, loss = 0.38878590\n",
      "Iteration 182, loss = 0.38835574\n",
      "Iteration 183, loss = 0.38795352\n",
      "Iteration 184, loss = 0.38758605\n",
      "Iteration 185, loss = 0.38719381\n",
      "Iteration 186, loss = 0.38680823\n",
      "Iteration 187, loss = 0.38641224\n",
      "Iteration 188, loss = 0.38605270\n",
      "Iteration 189, loss = 0.38571557\n",
      "Iteration 190, loss = 0.38537173\n",
      "Iteration 191, loss = 0.38496464\n",
      "Iteration 192, loss = 0.38460284\n",
      "Iteration 193, loss = 0.38423984\n",
      "Iteration 194, loss = 0.38387600\n",
      "Iteration 195, loss = 0.38355093\n",
      "Iteration 196, loss = 0.38320910\n",
      "Iteration 197, loss = 0.38289760\n",
      "Iteration 198, loss = 0.38254241\n",
      "Iteration 199, loss = 0.38216981\n",
      "Iteration 200, loss = 0.38187336\n",
      "Iteration 201, loss = 0.38153732\n",
      "Iteration 202, loss = 0.38120568\n",
      "Iteration 203, loss = 0.38088763\n",
      "Iteration 204, loss = 0.38055813\n",
      "Iteration 205, loss = 0.38029731\n",
      "Iteration 206, loss = 0.37993174\n",
      "Iteration 207, loss = 0.37965242\n",
      "Iteration 208, loss = 0.37944114\n",
      "Iteration 209, loss = 0.37903493\n",
      "Iteration 210, loss = 0.37873487\n",
      "Iteration 211, loss = 0.37843686\n",
      "Iteration 212, loss = 0.37814596\n",
      "Iteration 213, loss = 0.37783517\n",
      "Iteration 214, loss = 0.37753422\n",
      "Iteration 215, loss = 0.37726474\n",
      "Iteration 216, loss = 0.37698241\n",
      "Iteration 217, loss = 0.37667497\n",
      "Iteration 218, loss = 0.37638018\n",
      "Iteration 219, loss = 0.37612391\n",
      "Iteration 220, loss = 0.37583562\n",
      "Iteration 221, loss = 0.37559260\n",
      "Iteration 222, loss = 0.37526794\n",
      "Iteration 223, loss = 0.37500812\n",
      "Iteration 224, loss = 0.37473391\n",
      "Iteration 225, loss = 0.37447358\n",
      "Iteration 226, loss = 0.37417186\n",
      "Iteration 227, loss = 0.37391863\n",
      "Iteration 228, loss = 0.37364407\n",
      "Iteration 229, loss = 0.37337352\n",
      "Iteration 230, loss = 0.37309212\n",
      "Iteration 231, loss = 0.37283418\n",
      "Iteration 232, loss = 0.37259386\n",
      "Iteration 233, loss = 0.37231589\n",
      "Iteration 234, loss = 0.37205016\n",
      "Iteration 235, loss = 0.37182650\n",
      "Iteration 236, loss = 0.37157389\n",
      "Iteration 237, loss = 0.37130333\n",
      "Iteration 238, loss = 0.37105066\n",
      "Iteration 239, loss = 0.37080395\n",
      "Iteration 240, loss = 0.37056283\n",
      "Iteration 241, loss = 0.37031525\n",
      "Iteration 242, loss = 0.37006031\n",
      "Iteration 243, loss = 0.36983728\n",
      "Iteration 244, loss = 0.36957925\n",
      "Iteration 245, loss = 0.36932946\n",
      "Iteration 246, loss = 0.36908154\n",
      "Iteration 247, loss = 0.36884853\n",
      "Iteration 248, loss = 0.36862425\n",
      "Iteration 249, loss = 0.36837476\n",
      "Iteration 250, loss = 0.36818146\n",
      "Iteration 251, loss = 0.36791235\n",
      "Iteration 252, loss = 0.36768283\n",
      "Iteration 253, loss = 0.36747640\n",
      "Iteration 254, loss = 0.36723678\n",
      "Iteration 255, loss = 0.36700736\n",
      "Iteration 256, loss = 0.36677205\n",
      "Iteration 257, loss = 0.36656169\n",
      "Iteration 258, loss = 0.36634923\n",
      "Iteration 259, loss = 0.36612146\n",
      "Iteration 260, loss = 0.36593954\n",
      "Iteration 261, loss = 0.36569931\n",
      "Iteration 262, loss = 0.36550748\n",
      "Iteration 263, loss = 0.36526786\n",
      "Iteration 264, loss = 0.36504614\n",
      "Iteration 265, loss = 0.36482651\n",
      "Iteration 266, loss = 0.36463841\n",
      "Iteration 267, loss = 0.36442627\n",
      "Iteration 268, loss = 0.36422632\n",
      "Iteration 269, loss = 0.36400336\n",
      "Iteration 270, loss = 0.36381499\n",
      "Iteration 271, loss = 0.36361109\n",
      "Iteration 272, loss = 0.36341351\n",
      "Iteration 273, loss = 0.36321622\n",
      "Iteration 274, loss = 0.36301289\n",
      "Iteration 275, loss = 0.36282155\n",
      "Iteration 276, loss = 0.36263200\n",
      "Iteration 277, loss = 0.36241270\n",
      "Iteration 278, loss = 0.36221357\n",
      "Iteration 279, loss = 0.36204405\n",
      "Iteration 280, loss = 0.36181910\n",
      "Iteration 281, loss = 0.36168525\n",
      "Iteration 282, loss = 0.36145292\n",
      "Iteration 283, loss = 0.36126166\n",
      "Iteration 284, loss = 0.36106650\n",
      "Iteration 285, loss = 0.36087600\n",
      "Iteration 286, loss = 0.36068004\n",
      "Iteration 287, loss = 0.36050278\n",
      "Iteration 288, loss = 0.36034604\n",
      "Iteration 289, loss = 0.36011329\n",
      "Iteration 290, loss = 0.35993904\n",
      "Iteration 291, loss = 0.35975424\n",
      "Iteration 292, loss = 0.35957604\n",
      "Iteration 293, loss = 0.35936291\n",
      "Iteration 294, loss = 0.35917939\n",
      "Iteration 295, loss = 0.35899883\n",
      "Iteration 296, loss = 0.35881068\n",
      "Iteration 297, loss = 0.35863756\n",
      "Iteration 298, loss = 0.35845179\n",
      "Iteration 299, loss = 0.35827320\n",
      "Iteration 300, loss = 0.35807441\n",
      "Iteration 301, loss = 0.35792611\n",
      "Iteration 302, loss = 0.35771640\n",
      "Iteration 303, loss = 0.35757517\n",
      "Iteration 304, loss = 0.35736449\n",
      "Iteration 305, loss = 0.35716059\n",
      "Iteration 306, loss = 0.35700275\n",
      "Iteration 307, loss = 0.35681211\n",
      "Iteration 308, loss = 0.35663811\n",
      "Iteration 309, loss = 0.35646027\n",
      "Iteration 310, loss = 0.35630069\n",
      "Iteration 311, loss = 0.35610286\n",
      "Iteration 312, loss = 0.35593586\n",
      "Iteration 313, loss = 0.35577322\n",
      "Iteration 314, loss = 0.35559800\n",
      "Iteration 315, loss = 0.35542020\n",
      "Iteration 316, loss = 0.35527414\n",
      "Iteration 317, loss = 0.35509562\n",
      "Iteration 318, loss = 0.35491738\n",
      "Iteration 319, loss = 0.35473718\n",
      "Iteration 320, loss = 0.35458156\n",
      "Iteration 321, loss = 0.35441127\n",
      "Iteration 144, loss = 0.38005711\n",
      "Iteration 145, loss = 0.37955505\n",
      "Iteration 146, loss = 0.37904406\n",
      "Iteration 147, loss = 0.37857541\n",
      "Iteration 148, loss = 0.37810773\n",
      "Iteration 149, loss = 0.37761700\n",
      "Iteration 150, loss = 0.37715362\n",
      "Iteration 151, loss = 0.37675241\n",
      "Iteration 152, loss = 0.37629291\n",
      "Iteration 153, loss = 0.37584110\n",
      "Iteration 154, loss = 0.37540955\n",
      "Iteration 155, loss = 0.37496949\n",
      "Iteration 156, loss = 0.37460473\n",
      "Iteration 157, loss = 0.37413373\n",
      "Iteration 158, loss = 0.37374023\n",
      "Iteration 159, loss = 0.37336184\n",
      "Iteration 160, loss = 0.37292348\n",
      "Iteration 161, loss = 0.37255818\n",
      "Iteration 162, loss = 0.37216430\n",
      "Iteration 163, loss = 0.37179274\n",
      "Iteration 164, loss = 0.37142596\n",
      "Iteration 165, loss = 0.37103909\n",
      "Iteration 166, loss = 0.37066996\n",
      "Iteration 167, loss = 0.37030069\n",
      "Iteration 168, loss = 0.36996614\n",
      "Iteration 169, loss = 0.36960792\n",
      "Iteration 170, loss = 0.36927696\n",
      "Iteration 171, loss = 0.36893712\n",
      "Iteration 172, loss = 0.36856254\n",
      "Iteration 173, loss = 0.36823366\n",
      "Iteration 174, loss = 0.36789249\n",
      "Iteration 175, loss = 0.36756977\n",
      "Iteration 176, loss = 0.36725142\n",
      "Iteration 177, loss = 0.36693245\n",
      "Iteration 178, loss = 0.36659806\n",
      "Iteration 179, loss = 0.36628127\n",
      "Iteration 180, loss = 0.36597460\n",
      "Iteration 181, loss = 0.36566382\n",
      "Iteration 182, loss = 0.36537672\n",
      "Iteration 183, loss = 0.36506215\n",
      "Iteration 184, loss = 0.36477020\n",
      "Iteration 185, loss = 0.36445878\n",
      "Iteration 186, loss = 0.36416264\n",
      "Iteration 187, loss = 0.36388032\n",
      "Iteration 188, loss = 0.36361197\n",
      "Iteration 189, loss = 0.36333598\n",
      "Iteration 190, loss = 0.36303365\n",
      "Iteration 191, loss = 0.36274849\n",
      "Iteration 192, loss = 0.36246817\n",
      "Iteration 193, loss = 0.36219558\n",
      "Iteration 194, loss = 0.36194100\n",
      "Iteration 195, loss = 0.36165346\n",
      "Iteration 196, loss = 0.36140430\n",
      "Iteration 197, loss = 0.36114071\n",
      "Iteration 198, loss = 0.36087006\n",
      "Iteration 199, loss = 0.36061749\n",
      "Iteration 200, loss = 0.36035998\n",
      "Iteration 201, loss = 0.36013933\n",
      "Iteration 202, loss = 0.35986213\n",
      "Iteration 203, loss = 0.35959855\n",
      "Iteration 204, loss = 0.35931730\n",
      "Iteration 205, loss = 0.35909916\n",
      "Iteration 206, loss = 0.35887590\n",
      "Iteration 207, loss = 0.35858854\n",
      "Iteration 208, loss = 0.35835487\n",
      "Iteration 209, loss = 0.35811194\n",
      "Iteration 210, loss = 0.35788197\n",
      "Iteration 211, loss = 0.35764789\n",
      "Iteration 212, loss = 0.35739301\n",
      "Iteration 213, loss = 0.35718418\n",
      "Iteration 214, loss = 0.35696798\n",
      "Iteration 215, loss = 0.35670970\n",
      "Iteration 216, loss = 0.35646913\n",
      "Iteration 217, loss = 0.35625867\n",
      "Iteration 218, loss = 0.35603484\n",
      "Iteration 219, loss = 0.35580775\n",
      "Iteration 220, loss = 0.35558596\n",
      "Iteration 221, loss = 0.35535296\n",
      "Iteration 222, loss = 0.35515398\n",
      "Iteration 223, loss = 0.35492497\n",
      "Iteration 224, loss = 0.35469325\n",
      "Iteration 225, loss = 0.35449825\n",
      "Iteration 226, loss = 0.35428126\n",
      "Iteration 227, loss = 0.35410364\n",
      "Iteration 228, loss = 0.35387405\n",
      "Iteration 229, loss = 0.35364105\n",
      "Iteration 230, loss = 0.35342944\n",
      "Iteration 231, loss = 0.35322885\n",
      "Iteration 232, loss = 0.35302598\n",
      "Iteration 233, loss = 0.35281300\n",
      "Iteration 234, loss = 0.35261257\n",
      "Iteration 235, loss = 0.35241039\n",
      "Iteration 236, loss = 0.35221858\n",
      "Iteration 237, loss = 0.35202703\n",
      "Iteration 238, loss = 0.35179627\n",
      "Iteration 239, loss = 0.35160557\n",
      "Iteration 240, loss = 0.35140051\n",
      "Iteration 241, loss = 0.35123094\n",
      "Iteration 242, loss = 0.35103543\n",
      "Iteration 243, loss = 0.35084620\n",
      "Iteration 244, loss = 0.35063826\n",
      "Iteration 245, loss = 0.35043318\n",
      "Iteration 246, loss = 0.35027586\n",
      "Iteration 247, loss = 0.35011199\n",
      "Iteration 248, loss = 0.34986969\n",
      "Iteration 249, loss = 0.34969543\n",
      "Iteration 250, loss = 0.34950932\n",
      "Iteration 251, loss = 0.34932554\n",
      "Iteration 252, loss = 0.34913687\n",
      "Iteration 253, loss = 0.34896024\n",
      "Iteration 254, loss = 0.34879871\n",
      "Iteration 255, loss = 0.34860352\n",
      "Iteration 256, loss = 0.34839352\n",
      "Iteration 257, loss = 0.34824199\n",
      "Iteration 258, loss = 0.34805301\n",
      "Iteration 259, loss = 0.34788058\n",
      "Iteration 260, loss = 0.34769088\n",
      "Iteration 261, loss = 0.34752898\n",
      "Iteration 262, loss = 0.34733364\n",
      "Iteration 263, loss = 0.34718067\n",
      "Iteration 264, loss = 0.34699665\n",
      "Iteration 265, loss = 0.34681650\n",
      "Iteration 266, loss = 0.34664904\n",
      "Iteration 267, loss = 0.34647333\n",
      "Iteration 268, loss = 0.34629390\n",
      "Iteration 269, loss = 0.34612896\n",
      "Iteration 270, loss = 0.34597598\n",
      "Iteration 271, loss = 0.34579534\n",
      "Iteration 272, loss = 0.34563213\n",
      "Iteration 273, loss = 0.34547764\n",
      "Iteration 274, loss = 0.34530279\n",
      "Iteration 275, loss = 0.34515502\n",
      "Iteration 276, loss = 0.34495806\n",
      "Iteration 277, loss = 0.34479932\n",
      "Iteration 278, loss = 0.34466343\n",
      "Iteration 279, loss = 0.34452768\n",
      "Iteration 280, loss = 0.34431535\n",
      "Iteration 281, loss = 0.34415002\n",
      "Iteration 282, loss = 0.34398117\n",
      "Iteration 283, loss = 0.34382152\n",
      "Iteration 284, loss = 0.34366898\n",
      "Iteration 285, loss = 0.34351645\n",
      "Iteration 286, loss = 0.34336434\n",
      "Iteration 287, loss = 0.34319317\n",
      "Iteration 288, loss = 0.34307828\n",
      "Iteration 289, loss = 0.34289350\n",
      "Iteration 290, loss = 0.34273167\n",
      "Iteration 291, loss = 0.34257337\n",
      "Iteration 292, loss = 0.34242749\n",
      "Iteration 293, loss = 0.34228043\n",
      "Iteration 294, loss = 0.34212636\n",
      "Iteration 295, loss = 0.34197843\n",
      "Iteration 296, loss = 0.34181546\n",
      "Iteration 297, loss = 0.34168306\n",
      "Iteration 298, loss = 0.34152347\n",
      "Iteration 299, loss = 0.34137634\n",
      "Iteration 300, loss = 0.34123439\n",
      "Iteration 301, loss = 0.34107022\n",
      "Iteration 302, loss = 0.34092834\n",
      "Iteration 303, loss = 0.34078325\n",
      "Iteration 304, loss = 0.34063565\n",
      "Iteration 305, loss = 0.34049149\n",
      "Iteration 306, loss = 0.34036403\n",
      "Iteration 307, loss = 0.34020385\n",
      "Iteration 308, loss = 0.34006461\n",
      "Iteration 309, loss = 0.33990510\n",
      "Iteration 310, loss = 0.33977483\n",
      "Iteration 311, loss = 0.33962725\n",
      "Iteration 312, loss = 0.33950233\n",
      "Iteration 313, loss = 0.33936734\n",
      "Iteration 314, loss = 0.33922371\n",
      "Iteration 315, loss = 0.33907069\n",
      "Iteration 316, loss = 0.33893777\n",
      "Iteration 317, loss = 0.33881340\n",
      "Iteration 318, loss = 0.33866024\n",
      "Iteration 319, loss = 0.33852789\n",
      "Iteration 320, loss = 0.33839591\n",
      "Iteration 321, loss = 0.33825568\n",
      "Iteration 322, loss = 0.33811963\n",
      "Iteration 323, loss = 0.33799381\n",
      "Iteration 324, loss = 0.33784861\n",
      "Iteration 325, loss = 0.33773339\n",
      "Iteration 326, loss = 0.33759662\n",
      "Iteration 327, loss = 0.33747166\n",
      "Iteration 328, loss = 0.33733414\n",
      "Iteration 329, loss = 0.33718637\n",
      "Iteration 330, loss = 0.33706567\n",
      "Iteration 331, loss = 0.33692120\n",
      "Iteration 332, loss = 0.33681090\n",
      "Iteration 333, loss = 0.33672002\n",
      "Iteration 334, loss = 0.33655321\n",
      "Iteration 335, loss = 0.33642061\n",
      "Iteration 336, loss = 0.33629589\n",
      "Iteration 337, loss = 0.33616690\n",
      "Iteration 338, loss = 0.33602072\n",
      "Iteration 339, loss = 0.33589592\n",
      "Iteration 340, loss = 0.33576811\n",
      "Iteration 341, loss = 0.33564719\n",
      "Iteration 342, loss = 0.33551896\n",
      "Iteration 343, loss = 0.33539116\n",
      "Iteration 344, loss = 0.33526311\n",
      "Iteration 345, loss = 0.33514710\n",
      "Iteration 346, loss = 0.33501761\n",
      "Iteration 347, loss = 0.33489510\n",
      "Iteration 348, loss = 0.33476846\n",
      "Iteration 349, loss = 0.33464615\n",
      "Iteration 350, loss = 0.33453324\n",
      "Iteration 351, loss = 0.33440322\n",
      "Iteration 352, loss = 0.33427670\n",
      "Iteration 353, loss = 0.33415659\n",
      "Iteration 354, loss = 0.33402790\n",
      "Iteration 355, loss = 0.33391305\n",
      "Iteration 356, loss = 0.33379187\n",
      "Iteration 357, loss = 0.33365816\n",
      "Iteration 358, loss = 0.33355714\n",
      "Iteration 359, loss = 0.33344025\n",
      "Iteration 360, loss = 0.33331078\n",
      "Iteration 361, loss = 0.33318774\n",
      "Iteration 362, loss = 0.33306771\n",
      "Iteration 363, loss = 0.33296002\n",
      "Iteration 364, loss = 0.33283107\n",
      "Iteration 365, loss = 0.33272098\n",
      "Iteration 366, loss = 0.33258965\n",
      "Iteration 367, loss = 0.33247013\n",
      "Iteration 368, loss = 0.33235600\n",
      "Iteration 369, loss = 0.33224447\n",
      "Iteration 370, loss = 0.33214673\n",
      "Iteration 371, loss = 0.33201505\n",
      "Iteration 372, loss = 0.33191036\n",
      "Iteration 373, loss = 0.33179312\n",
      "Iteration 374, loss = 0.33166892\n",
      "Iteration 375, loss = 0.33156149\n",
      "Iteration 376, loss = 0.33144606\n",
      "Iteration 377, loss = 0.33132829\n",
      "Iteration 378, loss = 0.33121172\n",
      "Iteration 379, loss = 0.33111292\n",
      "Iteration 380, loss = 0.33098600\n",
      "Iteration 381, loss = 0.33087219\n",
      "Iteration 382, loss = 0.33077406\n",
      "Iteration 383, loss = 0.33066514\n",
      "Iteration 384, loss = 0.33055708\n",
      "Iteration 385, loss = 0.33043310\n",
      "Iteration 386, loss = 0.33033174\n",
      "Iteration 387, loss = 0.33020960\n",
      "Iteration 388, loss = 0.33011758\n",
      "Iteration 389, loss = 0.33000106\n",
      "Iteration 390, loss = 0.32988344\n",
      "Iteration 391, loss = 0.32977154\n",
      "Iteration 392, loss = 0.32966714\n",
      "Iteration 393, loss = 0.32955198\n",
      "Iteration 394, loss = 0.32945448\n",
      "Iteration 395, loss = 0.32933771\n",
      "Iteration 396, loss = 0.32922380\n",
      "Iteration 397, loss = 0.32911425\n",
      "Iteration 398, loss = 0.32901714\n",
      "Iteration 399, loss = 0.32891050\n",
      "Iteration 400, loss = 0.32879719\n",
      "Iteration 401, loss = 0.32871926\n",
      "Iteration 402, loss = 0.32857349\n",
      "Iteration 403, loss = 0.32847379\n",
      "Iteration 404, loss = 0.32836970\n",
      "Iteration 405, loss = 0.32826500\n",
      "Iteration 406, loss = 0.32814682\n",
      "Iteration 407, loss = 0.32803972\n",
      "Iteration 408, loss = 0.32794590\n",
      "Iteration 409, loss = 0.32786103\n",
      "Iteration 410, loss = 0.32772580\n",
      "Iteration 411, loss = 0.32762300\n",
      "Iteration 412, loss = 0.32752246\n",
      "Iteration 413, loss = 0.32741522\n",
      "Iteration 414, loss = 0.32732608\n",
      "Iteration 415, loss = 0.32720766\n",
      "Iteration 416, loss = 0.32710335\n",
      "Iteration 417, loss = 0.32699325\n",
      "Iteration 418, loss = 0.32688420\n",
      "Iteration 419, loss = 0.32678640\n",
      "Iteration 420, loss = 0.32667693\n",
      "Iteration 421, loss = 0.32660181\n",
      "Iteration 422, loss = 0.32651283\n",
      "Iteration 423, loss = 0.32638193\n",
      "Iteration 424, loss = 0.32628397\n",
      "Iteration 425, loss = 0.32618596\n",
      "Iteration 426, loss = 0.32608478\n",
      "Iteration 427, loss = 0.32597192\n",
      "Iteration 428, loss = 0.32589577\n",
      "Iteration 429, loss = 0.32578412\n",
      "Iteration 430, loss = 0.32566888\n",
      "Iteration 431, loss = 0.32558309\n",
      "Iteration 432, loss = 0.32548190\n",
      "Iteration 433, loss = 0.32537921\n",
      "Iteration 434, loss = 0.32528338\n",
      "Iteration 435, loss = 0.32518751\n",
      "Iteration 436, loss = 0.32509988\n",
      "Iteration 437, loss = 0.32498100\n",
      "Iteration 438, loss = 0.32488258\n",
      "Iteration 439, loss = 0.32478014\n",
      "Iteration 440, loss = 0.32468866\n",
      "Iteration 441, loss = 0.32458874\n",
      "Iteration 442, loss = 0.32449254\n",
      "Iteration 443, loss = 0.32439480\n",
      "Iteration 444, loss = 0.32430172\n",
      "Iteration 445, loss = 0.32419772\n",
      "Iteration 446, loss = 0.32410324\n",
      "Iteration 447, loss = 0.32402156\n",
      "Iteration 448, loss = 0.32391051\n",
      "Iteration 449, loss = 0.32381305\n",
      "Iteration 450, loss = 0.32370800\n",
      "Iteration 451, loss = 0.32362530\n",
      "Iteration 452, loss = 0.32351445\n",
      "Iteration 453, loss = 0.32342359\n",
      "Iteration 454, loss = 0.32332836\n",
      "Iteration 455, loss = 0.32326725\n",
      "Iteration 456, loss = 0.32314190\n",
      "Iteration 457, loss = 0.32303923\n",
      "Iteration 458, loss = 0.32294698\n",
      "Iteration 459, loss = 0.32284104\n",
      "Iteration 460, loss = 0.32276589\n",
      "Iteration 461, loss = 0.32266006\n",
      "Iteration 462, loss = 0.32255680\n",
      "Iteration 463, loss = 0.32246826\n",
      "Iteration 464, loss = 0.32236762\n",
      "Iteration 465, loss = 0.32227251\n",
      "Iteration 466, loss = 0.32218601\n",
      "Iteration 467, loss = 0.32209121\n",
      "Iteration 468, loss = 0.32198705\n",
      "Iteration 469, loss = 0.32191226\n",
      "Iteration 470, loss = 0.32179965\n",
      "Iteration 471, loss = 0.32169605\n",
      "Iteration 472, loss = 0.32163510\n",
      "Iteration 473, loss = 0.32150619\n",
      "Iteration 474, loss = 0.32142469\n",
      "Iteration 475, loss = 0.32132979\n",
      "Iteration 476, loss = 0.32122143\n",
      "Iteration 477, loss = 0.32112758\n",
      "Iteration 478, loss = 0.32104074\n",
      "Iteration 479, loss = 0.32094113\n",
      "Iteration 480, loss = 0.32085014\n",
      "Iteration 481, loss = 0.32074881\n",
      "Iteration 482, loss = 0.32067630\n",
      "Iteration 483, loss = 0.32056797\n",
      "Iteration 484, loss = 0.32047333\n",
      "Iteration 485, loss = 0.32038777\n",
      "Iteration 486, loss = 0.32030900\n",
      "Iteration 487, loss = 0.32019595\n",
      "Iteration 488, loss = 0.32009071\n",
      "Iteration 489, loss = 0.32001085\n",
      "Iteration 490, loss = 0.31991697\n",
      "Iteration 491, loss = 0.31981458\n",
      "Iteration 492, loss = 0.31973107\n",
      "Iteration 493, loss = 0.31962817\n",
      "Iteration 494, loss = 0.31956824\n",
      "Iteration 495, loss = 0.31945853\n",
      "Iteration 496, loss = 0.31935190\n",
      "Iteration 497, loss = 0.31926427\n",
      "Iteration 498, loss = 0.31917600\n",
      "Iteration 499, loss = 0.31913146\n",
      "Iteration 500, loss = 0.31899523\n",
      "Iteration 501, loss = 0.31889775\n",
      "Iteration 502, loss = 0.31879777\n",
      "Iteration 503, loss = 0.31869687\n",
      "Iteration 504, loss = 0.31861870\n",
      "Iteration 505, loss = 0.31853502\n",
      "Iteration 506, loss = 0.31843243\n",
      "Iteration 507, loss = 0.31835442\n",
      "Iteration 508, loss = 0.31824007\n",
      "Iteration 509, loss = 0.31814112\n",
      "Iteration 510, loss = 0.31805707\n",
      "Iteration 511, loss = 0.31797034\n",
      "Iteration 512, loss = 0.31787149\n",
      "Iteration 513, loss = 0.31778643\n",
      "Iteration 514, loss = 0.31768013\n",
      "Iteration 515, loss = 0.31760194\n",
      "Iteration 516, loss = 0.31750305\n",
      "Iteration 517, loss = 0.31741172\n",
      "Iteration 518, loss = 0.31732208\n",
      "Iteration 519, loss = 0.31722953\n",
      "Iteration 520, loss = 0.31714168\n",
      "Iteration 521, loss = 0.31703972\n",
      "Iteration 522, loss = 0.31696363\n",
      "Iteration 523, loss = 0.31687348\n",
      "Iteration 524, loss = 0.31676808\n",
      "Iteration 525, loss = 0.31667383\n",
      "Iteration 526, loss = 0.31657815\n",
      "Iteration 527, loss = 0.31651211\n",
      "Iteration 528, loss = 0.31640266\n",
      "Iteration 529, loss = 0.31630684\n",
      "Iteration 530, loss = 0.31621457\n",
      "Iteration 531, loss = 0.31612629\n",
      "Iteration 532, loss = 0.31603151\n",
      "Iteration 533, loss = 0.31593987\n",
      "Iteration 534, loss = 0.31585972\n",
      "Iteration 535, loss = 0.31575909\n",
      "Iteration 536, loss = 0.31567385\n",
      "Iteration 537, loss = 0.31556855\n",
      "Iteration 538, loss = 0.31550141\n",
      "Iteration 539, loss = 0.31539768\n",
      "Iteration 540, loss = 0.31530457\n",
      "Iteration 541, loss = 0.31520340\n",
      "Iteration 542, loss = 0.31510146\n",
      "Iteration 543, loss = 0.31503077\n",
      "Iteration 544, loss = 0.31493031\n",
      "Iteration 545, loss = 0.31484108\n",
      "Iteration 546, loss = 0.31475887\n",
      "Iteration 547, loss = 0.31465883\n",
      "Iteration 548, loss = 0.31455955\n",
      "Iteration 549, loss = 0.31448259\n",
      "Iteration 550, loss = 0.31438301\n",
      "Iteration 551, loss = 0.31429113\n",
      "Iteration 552, loss = 0.31420599\n",
      "Iteration 553, loss = 0.31410558\n",
      "Iteration 554, loss = 0.31402262\n",
      "Iteration 555, loss = 0.31396314\n",
      "Iteration 556, loss = 0.31384694\n",
      "Iteration 557, loss = 0.31376278\n",
      "Iteration 558, loss = 0.31366853\n",
      "Iteration 559, loss = 0.31357554\n",
      "Iteration 560, loss = 0.31347945\n",
      "Iteration 561, loss = 0.31339025\n",
      "Iteration 562, loss = 0.31331408\n",
      "Iteration 563, loss = 0.31322118\n",
      "Iteration 564, loss = 0.31312127\n",
      "Iteration 565, loss = 0.31303711\n",
      "Iteration 566, loss = 0.31294550\n",
      "Iteration 567, loss = 0.31288278\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.84270584\n",
      "Iteration 2, loss = 0.83755798\n",
      "Iteration 3, loss = 0.82975217\n",
      "Iteration 4, loss = 0.82057741\n",
      "Iteration 5, loss = 0.81077245\n",
      "Iteration 6, loss = 0.80104614\n",
      "Iteration 7, loss = 0.79101263\n",
      "Iteration 8, loss = 0.78159498\n",
      "Iteration 9, loss = 0.77274882\n",
      "Iteration 10, loss = 0.76454783\n",
      "Iteration 11, loss = 0.75688793\n",
      "Iteration 12, loss = 0.74980370\n",
      "Iteration 13, loss = 0.74331776\n",
      "Iteration 14, loss = 0.73715940\n",
      "Iteration 15, loss = 0.73134969\n",
      "Iteration 16, loss = 0.72585868\n",
      "Iteration 17, loss = 0.72082357\n",
      "Iteration 18, loss = 0.71579544\n",
      "Iteration 19, loss = 0.71112121\n",
      "Iteration 20, loss = 0.70673441\n",
      "Iteration 21, loss = 0.70215659\n",
      "Iteration 22, loss = 0.69808823\n",
      "Iteration 23, loss = 0.69394766\n",
      "Iteration 24, loss = 0.68991848\n",
      "Iteration 25, loss = 0.68602005\n",
      "Iteration 26, loss = 0.68210308\n",
      "Iteration 27, loss = 0.67829097\n",
      "Iteration 28, loss = 0.67443652\n",
      "Iteration 29, loss = 0.67063816\n",
      "Iteration 30, loss = 0.66701564\n",
      "Iteration 31, loss = 0.66340324\n",
      "Iteration 32, loss = 0.65953395\n",
      "Iteration 33, loss = 0.65582743\n",
      "Iteration 34, loss = 0.65203185\n",
      "Iteration 35, loss = 0.64846940\n",
      "Iteration 36, loss = 0.64472495\n",
      "Iteration 37, loss = 0.64116234\n",
      "Iteration 38, loss = 0.63740633\n",
      "Iteration 39, loss = 0.63373552\n",
      "Iteration 40, loss = 0.63012407\n",
      "Iteration 41, loss = 0.62643965\n",
      "Iteration 42, loss = 0.62277090\n",
      "Iteration 43, loss = 0.61914349\n",
      "Iteration 44, loss = 0.61551935\n",
      "Iteration 45, loss = 0.61187992\n",
      "Iteration 46, loss = 0.60812205\n",
      "Iteration 47, loss = 0.60454687\n",
      "Iteration 48, loss = 0.60092170\n",
      "Iteration 49, loss = 0.59719445\n",
      "Iteration 50, loss = 0.59361052\n",
      "Iteration 51, loss = 0.59004321\n",
      "Iteration 52, loss = 0.58639871\n",
      "Iteration 53, loss = 0.58285373\n",
      "Iteration 54, loss = 0.57929129\n",
      "Iteration 55, loss = 0.57568955\n",
      "Iteration 56, loss = 0.57227823\n",
      "Iteration 57, loss = 0.56877273\n",
      "Iteration 58, loss = 0.56532876\n",
      "Iteration 59, loss = 0.56189948\n",
      "Iteration 60, loss = 0.55849644\n",
      "Iteration 61, loss = 0.55514925\n",
      "Iteration 62, loss = 0.55190991\n",
      "Iteration 63, loss = 0.54852243\n",
      "Iteration 64, loss = 0.54527460\n",
      "Iteration 65, loss = 0.54208324\n",
      "Iteration 66, loss = 0.53898597\n",
      "Iteration 67, loss = 0.53580787\n",
      "Iteration 68, loss = 0.53262555\n",
      "Iteration 69, loss = 0.52968530\n",
      "Iteration 70, loss = 0.52670189\n",
      "Iteration 71, loss = 0.52378489\n",
      "Iteration 72, loss = 0.52085877Iteration 1273, loss = 0.25858757\n",
      "Iteration 1274, loss = 0.25847735\n",
      "Iteration 1275, loss = 0.25828389\n",
      "Iteration 1276, loss = 0.25814086\n",
      "Iteration 1277, loss = 0.25804474\n",
      "Iteration 1278, loss = 0.25795553\n",
      "Iteration 1279, loss = 0.25780071\n",
      "Iteration 1280, loss = 0.25767884\n",
      "Iteration 1281, loss = 0.25756298\n",
      "Iteration 1282, loss = 0.25748096\n",
      "Iteration 1283, loss = 0.25734474\n",
      "Iteration 1284, loss = 0.25723087\n",
      "Iteration 1285, loss = 0.25711817\n",
      "Iteration 1286, loss = 0.25711571\n",
      "Iteration 1287, loss = 0.25690582\n",
      "Iteration 1288, loss = 0.25677755\n",
      "Iteration 1289, loss = 0.25663445\n",
      "Iteration 1290, loss = 0.25654480\n",
      "Iteration 1291, loss = 0.25640465\n",
      "Iteration 1292, loss = 0.25628466\n",
      "Iteration 1293, loss = 0.25615481\n",
      "Iteration 1294, loss = 0.25603036\n",
      "Iteration 1295, loss = 0.25593697\n",
      "Iteration 1296, loss = 0.25581586\n",
      "Iteration 1297, loss = 0.25569375\n",
      "Iteration 1298, loss = 0.25557167\n",
      "Iteration 1299, loss = 0.25547345\n",
      "Iteration 1300, loss = 0.25533672\n",
      "Iteration 1301, loss = 0.25524707\n",
      "Iteration 1302, loss = 0.25514194\n",
      "Iteration 1303, loss = 0.25501787\n",
      "Iteration 1304, loss = 0.25490274\n",
      "Iteration 1305, loss = 0.25477686\n",
      "Iteration 1306, loss = 0.25467275\n",
      "Iteration 1307, loss = 0.25456500\n",
      "Iteration 1308, loss = 0.25444840\n",
      "Iteration 1309, loss = 0.25434666\n",
      "Iteration 1310, loss = 0.25426065\n",
      "Iteration 1311, loss = 0.25415864\n",
      "Iteration 1312, loss = 0.25395265\n",
      "Iteration 1313, loss = 0.25384112\n",
      "Iteration 1314, loss = 0.25371787\n",
      "Iteration 1315, loss = 0.25359841\n",
      "Iteration 1316, loss = 0.25349201\n",
      "Iteration 1317, loss = 0.25343276\n",
      "Iteration 1318, loss = 0.25324682\n",
      "Iteration 1319, loss = 0.25315117\n",
      "Iteration 1320, loss = 0.25302917\n",
      "Iteration 1321, loss = 0.25296307\n",
      "Iteration 1322, loss = 0.25279420\n",
      "Iteration 1323, loss = 0.25269226\n",
      "Iteration 1324, loss = 0.25254764\n",
      "Iteration 1325, loss = 0.25243242\n",
      "Iteration 1326, loss = 0.25237786\n",
      "Iteration 1327, loss = 0.25222160\n",
      "Iteration 1328, loss = 0.25210771\n",
      "Iteration 1329, loss = 0.25196620\n",
      "Iteration 1330, loss = 0.25186234\n",
      "Iteration 1331, loss = 0.25174890\n",
      "Iteration 1332, loss = 0.25180344\n",
      "Iteration 1333, loss = 0.25152191\n",
      "Iteration 1334, loss = 0.25142389\n",
      "Iteration 1335, loss = 0.25128878\n",
      "Iteration 1336, loss = 0.25115898\n",
      "Iteration 1337, loss = 0.25107315\n",
      "Iteration 1338, loss = 0.25095470\n",
      "Iteration 1339, loss = 0.25081965\n",
      "Iteration 1340, loss = 0.25070849\n",
      "Iteration 1341, loss = 0.25060796\n",
      "Iteration 1342, loss = 0.25046511\n",
      "Iteration 1343, loss = 0.25042449\n",
      "Iteration 1344, loss = 0.25026752\n",
      "Iteration 1345, loss = 0.25012569\n",
      "Iteration 1346, loss = 0.25002599\n",
      "Iteration 1347, loss = 0.24990141\n",
      "Iteration 1348, loss = 0.24981268\n",
      "Iteration 1349, loss = 0.24969123\n",
      "Iteration 1350, loss = 0.24956847\n",
      "Iteration 1351, loss = 0.24943849\n",
      "Iteration 1352, loss = 0.24931509\n",
      "Iteration 1353, loss = 0.24920097\n",
      "Iteration 1354, loss = 0.24914720\n",
      "Iteration 1355, loss = 0.24897424\n",
      "Iteration 1356, loss = 0.24886004\n",
      "Iteration 1357, loss = 0.24874695\n",
      "Iteration 1358, loss = 0.24864236\n",
      "Iteration 1359, loss = 0.24850917\n",
      "Iteration 1360, loss = 0.24843011\n",
      "Iteration 1361, loss = 0.24835940\n",
      "Iteration 1362, loss = 0.24815244\n",
      "Iteration 1363, loss = 0.24807552\n",
      "Iteration 1364, loss = 0.24798100\n",
      "Iteration 1365, loss = 0.24781195\n",
      "Iteration 1366, loss = 0.24775615\n",
      "Iteration 1367, loss = 0.24772900\n",
      "Iteration 1368, loss = 0.24756887\n",
      "Iteration 1369, loss = 0.24736988\n",
      "Iteration 1370, loss = 0.24721576\n",
      "Iteration 1371, loss = 0.24715755\n",
      "Iteration 1372, loss = 0.24698848\n",
      "Iteration 1373, loss = 0.24691687\n",
      "Iteration 1374, loss = 0.24677717\n",
      "Iteration 1375, loss = 0.24662390\n",
      "Iteration 1376, loss = 0.24659310\n",
      "Iteration 1377, loss = 0.24644083\n",
      "Iteration 1378, loss = 0.24631352\n",
      "Iteration 1379, loss = 0.24621857\n",
      "Iteration 1380, loss = 0.24608060\n",
      "Iteration 1381, loss = 0.24594656\n",
      "Iteration 1382, loss = 0.24581507\n",
      "Iteration 1383, loss = 0.24568624\n",
      "Iteration 1384, loss = 0.24556103\n",
      "Iteration 1385, loss = 0.24546159\n",
      "Iteration 1386, loss = 0.24534064\n",
      "Iteration 1387, loss = 0.24518743\n",
      "Iteration 1388, loss = 0.24511218\n",
      "Iteration 1389, loss = 0.24499479\n",
      "Iteration 1390, loss = 0.24488792\n",
      "Iteration 1391, loss = 0.24480481\n",
      "Iteration 1392, loss = 0.24470355\n",
      "Iteration 1393, loss = 0.24453912\n",
      "Iteration 1394, loss = 0.24445231\n",
      "Iteration 1395, loss = 0.24427828\n",
      "Iteration 1396, loss = 0.24416565\n",
      "Iteration 1397, loss = 0.24403281\n",
      "Iteration 1398, loss = 0.24392388\n",
      "Iteration 1399, loss = 0.24382380\n",
      "Iteration 1400, loss = 0.24370141\n",
      "Iteration 1401, loss = 0.24358714\n",
      "Iteration 1402, loss = 0.24347274\n",
      "Iteration 1403, loss = 0.24336925\n",
      "Iteration 1404, loss = 0.24325909\n",
      "Iteration 1405, loss = 0.24316976\n",
      "Iteration 1406, loss = 0.24307126\n",
      "Iteration 1407, loss = 0.24290120\n",
      "Iteration 1408, loss = 0.24283264\n",
      "Iteration 1409, loss = 0.24273015\n",
      "Iteration 1410, loss = 0.24260017\n",
      "Iteration 1411, loss = 0.24241774\n",
      "Iteration 1412, loss = 0.24234147\n",
      "Iteration 1413, loss = 0.24220281\n",
      "Iteration 1414, loss = 0.24206830\n",
      "Iteration 1415, loss = 0.24193226\n",
      "Iteration 1416, loss = 0.24181843\n",
      "Iteration 1417, loss = 0.24173026\n",
      "Iteration 1418, loss = 0.24161104\n",
      "Iteration 1419, loss = 0.24146613\n",
      "Iteration 1420, loss = 0.24140758\n",
      "Iteration 1421, loss = 0.24124204\n",
      "Iteration 1422, loss = 0.24113461\n",
      "Iteration 1423, loss = 0.24102976\n",
      "Iteration 1424, loss = 0.24101079\n",
      "Iteration 1425, loss = 0.24078621\n",
      "Iteration 1426, loss = 0.24068953\n",
      "Iteration 1427, loss = 0.24060181\n",
      "Iteration 1428, loss = 0.24043764\n",
      "Iteration 1429, loss = 0.24034173\n",
      "Iteration 1430, loss = 0.24019968\n",
      "Iteration 1431, loss = 0.24015951\n",
      "Iteration 1432, loss = 0.23999604\n",
      "Iteration 1433, loss = 0.23996579\n",
      "Iteration 1434, loss = 0.23977529\n",
      "Iteration 1435, loss = 0.23974751\n",
      "Iteration 1436, loss = 0.23955942\n",
      "Iteration 1437, loss = 0.23940009\n",
      "Iteration 1438, loss = 0.23934121\n",
      "Iteration 1439, loss = 0.23928030\n",
      "Iteration 1440, loss = 0.23907912\n",
      "Iteration 1441, loss = 0.23893700\n",
      "Iteration 1442, loss = 0.23888916\n",
      "Iteration 1443, loss = 0.23872524\n",
      "Iteration 1444, loss = 0.23859549\n",
      "Iteration 1445, loss = 0.23863634\n",
      "Iteration 1446, loss = 0.23836861\n",
      "Iteration 1447, loss = 0.23827872\n",
      "Iteration 1448, loss = 0.23812761\n",
      "Iteration 1449, loss = 0.23804787\n",
      "Iteration 1450, loss = 0.23792920\n",
      "Iteration 1451, loss = 0.23782576\n",
      "Iteration 1452, loss = 0.23770432\n",
      "Iteration 1453, loss = 0.23766592\n",
      "Iteration 1454, loss = 0.23746524\n",
      "Iteration 1455, loss = 0.23738004\n",
      "Iteration 1456, loss = 0.23725127\n",
      "Iteration 1457, loss = 0.23709031\n",
      "Iteration 1458, loss = 0.23705549\n",
      "Iteration 1459, loss = 0.23689494\n",
      "Iteration 1460, loss = 0.23676896\n",
      "Iteration 1461, loss = 0.23663864\n",
      "Iteration 1462, loss = 0.23650784\n",
      "Iteration 1463, loss = 0.23645138\n",
      "Iteration 1464, loss = 0.23629260\n",
      "Iteration 1465, loss = 0.23627477\n",
      "Iteration 1466, loss = 0.23604336\n",
      "Iteration 1467, loss = 0.23597705\n",
      "Iteration 1468, loss = 0.23583856\n",
      "Iteration 1469, loss = 0.23570223\n",
      "Iteration 1470, loss = 0.23568070\n",
      "Iteration 1471, loss = 0.23548498\n",
      "Iteration 1472, loss = 0.23541603\n",
      "Iteration 1473, loss = 0.23535014\n",
      "Iteration 1474, loss = 0.23514077\n",
      "Iteration 1475, loss = 0.23505932\n",
      "Iteration 1476, loss = 0.23491184\n",
      "Iteration 1477, loss = 0.23482264\n",
      "Iteration 1478, loss = 0.23471304\n",
      "Iteration 1479, loss = 0.23462071\n",
      "Iteration 1480, loss = 0.23446371\n",
      "Iteration 1481, loss = 0.23438155\n",
      "Iteration 1482, loss = 0.23423835\n",
      "Iteration 1483, loss = 0.23412542\n",
      "Iteration 1484, loss = 0.23401999\n",
      "Iteration 1485, loss = 0.23386418\n",
      "Iteration 1486, loss = 0.23384176\n",
      "Iteration 1487, loss = 0.23363732\n",
      "Iteration 1488, loss = 0.23355580\n",
      "Iteration 1489, loss = 0.23352283\n",
      "Iteration 1490, loss = 0.23337216\n",
      "Iteration 1491, loss = 0.23322821\n",
      "Iteration 1492, loss = 0.23307600\n",
      "Iteration 1493, loss = 0.23301774\n",
      "Iteration 1494, loss = 0.23286938\n",
      "Iteration 1495, loss = 0.23275478\n",
      "Iteration 1496, loss = 0.23274773\n",
      "Iteration 1497, loss = 0.23255143\n",
      "Iteration 1498, loss = 0.23240082\n",
      "Iteration 1499, loss = 0.23241630\n",
      "Iteration 1500, loss = 0.23212970\n",
      "Iteration 1501, loss = 0.23203224\n",
      "Iteration 1502, loss = 0.23190601\n",
      "Iteration 1503, loss = 0.23176809\n",
      "Iteration 1504, loss = 0.23164420\n",
      "Iteration 1505, loss = 0.23154084\n",
      "Iteration 1506, loss = 0.23149959\n",
      "Iteration 1507, loss = 0.23131148\n",
      "Iteration 1508, loss = 0.23121522\n",
      "Iteration 1509, loss = 0.23106539\n",
      "Iteration 1510, loss = 0.23106298\n",
      "Iteration 1511, loss = 0.23094530\n",
      "Iteration 1512, loss = 0.23081556\n",
      "Iteration 1513, loss = 0.23065567\n",
      "Iteration 1514, loss = 0.23052195\n",
      "Iteration 1515, loss = 0.23039305\n",
      "Iteration 1516, loss = 0.23033133\n",
      "Iteration 1517, loss = 0.23051416\n",
      "Iteration 1518, loss = 0.23015935\n",
      "Iteration 1519, loss = 0.22994439\n",
      "Iteration 1520, loss = 0.22990140\n",
      "Iteration 1521, loss = 0.22972865\n",
      "Iteration 1522, loss = 0.22958983\n",
      "Iteration 1523, loss = 0.22947545\n",
      "Iteration 1524, loss = 0.22934739\n",
      "Iteration 1525, loss = 0.22926217\n",
      "Iteration 1526, loss = 0.22910749\n",
      "Iteration 1527, loss = 0.22900444\n",
      "Iteration 1528, loss = 0.22886815\n",
      "Iteration 1529, loss = 0.22879641\n",
      "Iteration 1530, loss = 0.22873616\n",
      "Iteration 1531, loss = 0.22854873\n",
      "Iteration 1532, loss = 0.22843532\n",
      "Iteration 1533, loss = 0.22833468\n",
      "Iteration 1534, loss = 0.22816658\n",
      "Iteration 1535, loss = 0.22812557\n",
      "Iteration 1536, loss = 0.22811282\n",
      "Iteration 1537, loss = 0.22793369\n",
      "Iteration 1538, loss = 0.22773540\n",
      "Iteration 1539, loss = 0.22761533\n",
      "Iteration 1540, loss = 0.22754992\n",
      "Iteration 1541, loss = 0.22747973\n",
      "Iteration 1542, loss = 0.22729949\n",
      "Iteration 1543, loss = 0.22721696\n",
      "Iteration 1544, loss = 0.22708769\n",
      "Iteration 1545, loss = 0.22700871\n",
      "Iteration 1546, loss = 0.22682766\n",
      "Iteration 1547, loss = 0.22680129\n",
      "Iteration 1548, loss = 0.22655702\n",
      "Iteration 1549, loss = 0.22645538\n",
      "Iteration 1550, loss = 0.22639955\n",
      "Iteration 1551, loss = 0.22621692\n",
      "Iteration 1552, loss = 0.22616998\n",
      "Iteration 1553, loss = 0.22605983\n",
      "Iteration 1554, loss = 0.22602324\n",
      "Iteration 1555, loss = 0.22581278\n",
      "Iteration 1556, loss = 0.22573194\n",
      "Iteration 1557, loss = 0.22560926\n",
      "Iteration 1558, loss = 0.22555458\n",
      "Iteration 1559, loss = 0.22532382\n",
      "Iteration 1560, loss = 0.22522580\n",
      "Iteration 1561, loss = 0.22507943\n",
      "Iteration 1562, loss = 0.22500829\n",
      "Iteration 1563, loss = 0.22490565\n",
      "Iteration 1564, loss = 0.22475101\n",
      "Iteration 1565, loss = 0.22463760\n",
      "Iteration 1566, loss = 0.22451761\n",
      "Iteration 1567, loss = 0.22444123\n",
      "Iteration 1568, loss = 0.22430603\n",
      "Iteration 1569, loss = 0.22422198\n",
      "Iteration 1570, loss = 0.22409132\n",
      "Iteration 1571, loss = 0.22398456\n",
      "Iteration 1572, loss = 0.22383690\n",
      "Iteration 1573, loss = 0.22379956\n",
      "Iteration 1574, loss = 0.22368102\n",
      "Iteration 1575, loss = 0.22355512\n",
      "Iteration 1576, loss = 0.22343870\n",
      "Iteration 1577, loss = 0.22330138\n",
      "Iteration 1578, loss = 0.22318206\n",
      "Iteration 1579, loss = 0.22307520\n",
      "Iteration 1580, loss = 0.22298243\n",
      "Iteration 1581, loss = 0.22283515\n",
      "Iteration 1582, loss = 0.22270873\n",
      "Iteration 1583, loss = 0.22263861\n",
      "Iteration 1584, loss = 0.22250578\n",
      "Iteration 1585, loss = 0.22239389\n",
      "Iteration 1586, loss = 0.22235072\n",
      "Iteration 1587, loss = 0.22220411\n",
      "Iteration 1588, loss = 0.22210369\n",
      "Iteration 1589, loss = 0.22200789\n",
      "Iteration 1590, loss = 0.22189578\n",
      "Iteration 1591, loss = 0.22172638\n",
      "Iteration 1592, loss = 0.22165377\n",
      "Iteration 1593, loss = 0.22148658\n",
      "Iteration 1594, loss = 0.22136992\n",
      "Iteration 1595, loss = 0.22126703\n",
      "Iteration 1596, loss = 0.22117802\n",
      "Iteration 1597, loss = 0.22108364\n",
      "Iteration 1598, loss = 0.22098760\n",
      "Iteration 1599, loss = 0.22078720\n",
      "Iteration 1600, loss = 0.22069646\n",
      "Iteration 1601, loss = 0.22057564\n",
      "Iteration 1602, loss = 0.22042919\n",
      "Iteration 1603, loss = 0.22040484\n",
      "Iteration 1604, loss = 0.22025867\n",
      "Iteration 1605, loss = 0.22008907\n",
      "Iteration 1606, loss = 0.21999741\n",
      "Iteration 1607, loss = 0.21998260\n",
      "Iteration 1608, loss = 0.21978547\n",
      "Iteration 1609, loss = 0.21973087\n",
      "Iteration 1610, loss = 0.21956327\n",
      "Iteration 1611, loss = 0.21945126\n",
      "Iteration 1612, loss = 0.21935591\n",
      "Iteration 1613, loss = 0.21922158\n",
      "Iteration 1614, loss = 0.21919516\n",
      "Iteration 1615, loss = 0.21902505\n",
      "Iteration 1616, loss = 0.21887522\n",
      "Iteration 1617, loss = 0.21879286\n",
      "Iteration 1618, loss = 0.21864398\n",
      "Iteration 1619, loss = 0.21864574\n",
      "Iteration 1620, loss = 0.21846916\n",
      "Iteration 1621, loss = 0.21834549\n",
      "Iteration 1622, loss = 0.21824526\n",
      "Iteration 1623, loss = 0.21812819\n",
      "Iteration 1624, loss = 0.21807730\n",
      "Iteration 1625, loss = 0.21789856\n",
      "Iteration 1626, loss = 0.21780441\n",
      "Iteration 1627, loss = 0.21772595\n",
      "Iteration 1628, loss = 0.21753485\n",
      "Iteration 1629, loss = 0.21746266\n",
      "Iteration 1630, loss = 0.21735733\n",
      "Iteration 1631, loss = 0.21725717\n",
      "Iteration 1632, loss = 0.21710968\n",
      "Iteration 1633, loss = 0.21699760\n",
      "Iteration 1634, loss = 0.21686967\n",
      "Iteration 1635, loss = 0.21682796\n",
      "Iteration 1636, loss = 0.21668362\n",
      "Iteration 1637, loss = 0.21657009\n",
      "Iteration 1638, loss = 0.21643611\n",
      "Iteration 1639, loss = 0.21635015\n",
      "Iteration 1640, loss = 0.21621262\n",
      "Iteration 1641, loss = 0.21629326\n",
      "Iteration 1642, loss = 0.21597673\n",
      "Iteration 1643, loss = 0.21597327\n",
      "Iteration 1644, loss = 0.21585388\n",
      "Iteration 1645, loss = 0.21569233\n",
      "Iteration 1646, loss = 0.21557880\n",
      "Iteration 1647, loss = 0.21541689\n",
      "Iteration 1648, loss = 0.21533809\n",
      "Iteration 1649, loss = 0.21525846\n",
      "Iteration 1650, loss = 0.21514117\n",
      "Iteration 1651, loss = 0.21501557\n",
      "Iteration 1652, loss = 0.21489977\n",
      "Iteration 1653, loss = 0.21479031\n",
      "Iteration 1654, loss = 0.21467692\n",
      "Iteration 1655, loss = 0.21456802\n",
      "Iteration 1656, loss = 0.21450602\n",
      "Iteration 1657, loss = 0.21438546\n",
      "Iteration 1658, loss = 0.21422452\n",
      "Iteration 1659, loss = 0.21408934\n",
      "Iteration 1660, loss = 0.21398083\n",
      "Iteration 1661, loss = 0.21392816\n",
      "Iteration 1662, loss = 0.21382936\n",
      "Iteration 1663, loss = 0.21369929\n",
      "Iteration 1664, loss = 0.21355975\n",
      "Iteration 1665, loss = 0.21348991\n",
      "Iteration 1666, loss = 0.21332844\n",
      "Iteration 1667, loss = 0.21325067\n",
      "Iteration 1668, loss = 0.21314015\n",
      "Iteration 1669, loss = 0.21300645\n",
      "Iteration 1670, loss = 0.21291990\n",
      "Iteration 1671, loss = 0.21286448\n",
      "Iteration 1672, loss = 0.21268326\n",
      "Iteration 1673, loss = 0.21253603\n",
      "Iteration 1674, loss = 0.21242393\n",
      "Iteration 1675, loss = 0.21241927\n",
      "Iteration 1676, loss = 0.21230663\n",
      "Iteration 1677, loss = 0.21211846\n",
      "Iteration 1678, loss = 0.21208679\n",
      "Iteration 1679, loss = 0.21194044\n",
      "Iteration 1680, loss = 0.21178522\n",
      "Iteration 1681, loss = 0.21175875\n",
      "Iteration 1682, loss = 0.21159115\n",
      "Iteration 1683, loss = 0.21148681\n",
      "Iteration 1684, loss = 0.21135892\n",
      "Iteration 1685, loss = 0.21128149\n",
      "Iteration 1686, loss = 0.21114744\n",
      "Iteration 1687, loss = 0.21104916\n",
      "Iteration 1688, loss = 0.21093609\n",
      "Iteration 1689, loss = 0.21083909\n",
      "Iteration 1690, loss = 0.21067292\n",
      "Iteration 1691, loss = 0.21055736\n",
      "Iteration 1692, loss = 0.21049023\n",
      "Iteration 1693, loss = 0.21042087\n",
      "Iteration 1694, loss = 0.21026647\n",
      "Iteration 1695, loss = 0.21015236\n",
      "Iteration 1696, loss = 0.21014876\n",
      "Iteration 1697, loss = 0.20996618\n",
      "Iteration 1698, loss = 0.20983917\n",
      "Iteration 1699, loss = 0.20973073\n",
      "Iteration 1700, loss = 0.20965315\n",
      "Iteration 1701, loss = 0.20960410\n",
      "Iteration 1702, loss = 0.20941430\n",
      "Iteration 1703, loss = 0.20935049\n",
      "Iteration 1704, loss = 0.20921710\n",
      "Iteration 1705, loss = 0.20907194\n",
      "Iteration 1706, loss = 0.20898152\n",
      "Iteration 1707, loss = 0.20883029\n",
      "Iteration 1708, loss = 0.20882384\n",
      "Iteration 1709, loss = 0.20864422\n",
      "Iteration 1710, loss = 0.20858899\n",
      "Iteration 1711, loss = 0.20843869\n",
      "Iteration 1712, loss = 0.20832764\n",
      "Iteration 1713, loss = 0.20823288\n",
      "Iteration 1714, loss = 0.20825760\n",
      "Iteration 1715, loss = 0.20799329\n",
      "Iteration 1716, loss = 0.20788683\n",
      "Iteration 1717, loss = 0.20781834\n",
      "Iteration 1718, loss = 0.20769175\n",
      "Iteration 1719, loss = 0.20759336\n",
      "Iteration 1720, loss = 0.20747141\n",
      "Iteration 1721, loss = 0.20731201\n",
      "Iteration 1722, loss = 0.20722859\n",
      "Iteration 1723, loss = 0.20711631\n",
      "Iteration 1724, loss = 0.20702744\n",
      "Iteration 1725, loss = 0.20700088\n",
      "Iteration 1726, loss = 0.20683660\n",
      "Iteration 1727, loss = 0.20669502\n",
      "Iteration 1728, loss = 0.20659400\n",
      "Iteration 1729, loss = 0.20648905\n",
      "Iteration 1730, loss = 0.20642646\n",
      "Iteration 1731, loss = 0.20635340\n",
      "Iteration 1732, loss = 0.20628995\n",
      "Iteration 1733, loss = 0.20604475\n",
      "Iteration 1734, loss = 0.20598617\n",
      "Iteration 1735, loss = 0.20585612\n",
      "Iteration 1736, loss = 0.20579246\n",
      "Iteration 1737, loss = 0.20563738\n",
      "Iteration 1738, loss = 0.20558033\n",
      "Iteration 1739, loss = 0.20543294\n",
      "Iteration 1740, loss = 0.20535431\n",
      "Iteration 1741, loss = 0.20517222\n",
      "Iteration 1742, loss = 0.20508617\n",
      "Iteration 1743, loss = 0.20495540\n",
      "Iteration 1744, loss = 0.20485665\n",
      "Iteration 1745, loss = 0.20480430\n",
      "Iteration 1746, loss = 0.20469496\n",
      "Iteration 1747, loss = 0.20452224\n",
      "Iteration 1748, loss = 0.20441636\n",
      "Iteration 1749, loss = 0.20437578\n",
      "Iteration 1750, loss = 0.20427124\n",
      "Iteration 1751, loss = 0.20414886\n",
      "Iteration 1752, loss = 0.20410160\n",
      "Iteration 253, loss = 0.41210561\n",
      "Iteration 254, loss = 0.41185843\n",
      "Iteration 255, loss = 0.41163330\n",
      "Iteration 256, loss = 0.41134438\n",
      "Iteration 257, loss = 0.41111835\n",
      "Iteration 258, loss = 0.41087269\n",
      "Iteration 259, loss = 0.41068373\n",
      "Iteration 260, loss = 0.41038067\n",
      "Iteration 261, loss = 0.41011178\n",
      "Iteration 262, loss = 0.40986245\n",
      "Iteration 263, loss = 0.40961272\n",
      "Iteration 264, loss = 0.40937072\n",
      "Iteration 265, loss = 0.40914280\n",
      "Iteration 266, loss = 0.40887673\n",
      "Iteration 267, loss = 0.40862664\n",
      "Iteration 268, loss = 0.40842371\n",
      "Iteration 269, loss = 0.40817559\n",
      "Iteration 270, loss = 0.40791919\n",
      "Iteration 271, loss = 0.40769783\n",
      "Iteration 272, loss = 0.40745986\n",
      "Iteration 273, loss = 0.40723072\n",
      "Iteration 274, loss = 0.40698462\n",
      "Iteration 275, loss = 0.40677063\n",
      "Iteration 276, loss = 0.40653552\n",
      "Iteration 277, loss = 0.40631100\n",
      "Iteration 278, loss = 0.40607042\n",
      "Iteration 279, loss = 0.40586259\n",
      "Iteration 280, loss = 0.40564513\n",
      "Iteration 281, loss = 0.40541645\n",
      "Iteration 282, loss = 0.40519229\n",
      "Iteration 283, loss = 0.40495399\n",
      "Iteration 284, loss = 0.40472793\n",
      "Iteration 285, loss = 0.40453309\n",
      "Iteration 286, loss = 0.40428539\n",
      "Iteration 287, loss = 0.40411644\n",
      "Iteration 288, loss = 0.40387028\n",
      "Iteration 289, loss = 0.40365538\n",
      "Iteration 290, loss = 0.40345005\n",
      "Iteration 291, loss = 0.40321349\n",
      "Iteration 292, loss = 0.40300787\n",
      "Iteration 293, loss = 0.40285208\n",
      "Iteration 294, loss = 0.40255096\n",
      "Iteration 295, loss = 0.40235830\n",
      "Iteration 296, loss = 0.40217544\n",
      "Iteration 297, loss = 0.40194365\n",
      "Iteration 298, loss = 0.40172214\n",
      "Iteration 299, loss = 0.40154615\n",
      "Iteration 300, loss = 0.40134346\n",
      "Iteration 301, loss = 0.40112446\n",
      "Iteration 302, loss = 0.40088783\n",
      "Iteration 303, loss = 0.40068456\n",
      "Iteration 304, loss = 0.40051892\n",
      "Iteration 305, loss = 0.40034288\n",
      "Iteration 306, loss = 0.40012706\n",
      "Iteration 307, loss = 0.39988986\n",
      "Iteration 308, loss = 0.39972602\n",
      "Iteration 309, loss = 0.39947791\n",
      "Iteration 310, loss = 0.39930723\n",
      "Iteration 311, loss = 0.39908608\n",
      "Iteration 312, loss = 0.39887418\n",
      "Iteration 313, loss = 0.39869118\n",
      "Iteration 314, loss = 0.39847761\n",
      "Iteration 315, loss = 0.39829511\n",
      "Iteration 316, loss = 0.39808963\n",
      "Iteration 317, loss = 0.39790741\n",
      "Iteration 318, loss = 0.39770079\n",
      "Iteration 319, loss = 0.39751072\n",
      "Iteration 320, loss = 0.39729785\n",
      "Iteration 321, loss = 0.39713343\n",
      "Iteration 322, loss = 0.39693393\n",
      "Iteration 323, loss = 0.39674255\n",
      "Iteration 324, loss = 0.39655100\n",
      "Iteration 325, loss = 0.39637456\n",
      "Iteration 326, loss = 0.39618563\n",
      "Iteration 327, loss = 0.39597372\n",
      "Iteration 328, loss = 0.39581383\n",
      "Iteration 329, loss = 0.39562894\n",
      "Iteration 330, loss = 0.39542026\n",
      "Iteration 331, loss = 0.39529604\n",
      "Iteration 332, loss = 0.39506092\n",
      "Iteration 333, loss = 0.39486194\n",
      "Iteration 334, loss = 0.39466252\n",
      "Iteration 335, loss = 0.39452361\n",
      "Iteration 336, loss = 0.39435025\n",
      "Iteration 337, loss = 0.39411813\n",
      "Iteration 338, loss = 0.39393565\n",
      "Iteration 339, loss = 0.39376172\n",
      "Iteration 340, loss = 0.39363833\n",
      "Iteration 341, loss = 0.39339189\n",
      "Iteration 342, loss = 0.39320012\n",
      "Iteration 343, loss = 0.39304635\n",
      "Iteration 344, loss = 0.39285673\n",
      "Iteration 345, loss = 0.39268139\n",
      "Iteration 346, loss = 0.39250632\n",
      "Iteration 347, loss = 0.39231001\n",
      "Iteration 348, loss = 0.39216073\n",
      "Iteration 349, loss = 0.39196134\n",
      "Iteration 350, loss = 0.39175201\n",
      "Iteration 351, loss = 0.39159169\n",
      "Iteration 352, loss = 0.39144087\n",
      "Iteration 353, loss = 0.39126801\n",
      "Iteration 354, loss = 0.39107138\n",
      "Iteration 355, loss = 0.39092852\n",
      "Iteration 356, loss = 0.39073705\n",
      "Iteration 357, loss = 0.39053341\n",
      "Iteration 358, loss = 0.39041215\n",
      "Iteration 359, loss = 0.39019507\n",
      "Iteration 360, loss = 0.39004843\n",
      "Iteration 361, loss = 0.38988679\n",
      "Iteration 362, loss = 0.38967389\n",
      "Iteration 363, loss = 0.38953178\n",
      "Iteration 364, loss = 0.38932667\n",
      "Iteration 365, loss = 0.38915573\n",
      "Iteration 366, loss = 0.38898841\n",
      "Iteration 367, loss = 0.38880717\n",
      "Iteration 368, loss = 0.38866270\n",
      "Iteration 369, loss = 0.38846195\n",
      "Iteration 370, loss = 0.38832139\n",
      "Iteration 371, loss = 0.38813318\n",
      "Iteration 372, loss = 0.38794747\n",
      "Iteration 373, loss = 0.38779488\n",
      "Iteration 374, loss = 0.38761149\n",
      "Iteration 375, loss = 0.38745895\n",
      "Iteration 376, loss = 0.38727722\n",
      "Iteration 377, loss = 0.38711505\n",
      "Iteration 378, loss = 0.38696188\n",
      "Iteration 379, loss = 0.38679329\n",
      "Iteration 380, loss = 0.38662446\n",
      "Iteration 381, loss = 0.38645437\n",
      "Iteration 382, loss = 0.38627959\n",
      "Iteration 383, loss = 0.38610904\n",
      "Iteration 384, loss = 0.38598474\n",
      "Iteration 385, loss = 0.38580660\n",
      "Iteration 386, loss = 0.38561339\n",
      "Iteration 387, loss = 0.38545855\n",
      "Iteration 388, loss = 0.38531483\n",
      "Iteration 389, loss = 0.38514473\n",
      "Iteration 390, loss = 0.38496774\n",
      "Iteration 391, loss = 0.38481898\n",
      "Iteration 392, loss = 0.38469012\n",
      "Iteration 393, loss = 0.38452335\n",
      "Iteration 394, loss = 0.38431239\n",
      "Iteration 395, loss = 0.38416567\n",
      "Iteration 396, loss = 0.38399052\n",
      "Iteration 397, loss = 0.38383996\n",
      "Iteration 398, loss = 0.38367922\n",
      "Iteration 399, loss = 0.38353019\n",
      "Iteration 400, loss = 0.38338239\n",
      "Iteration 401, loss = 0.38322221\n",
      "Iteration 402, loss = 0.38304357\n",
      "Iteration 403, loss = 0.38290389\n",
      "Iteration 404, loss = 0.38276650\n",
      "Iteration 405, loss = 0.38257313\n",
      "Iteration 406, loss = 0.38243279\n",
      "Iteration 407, loss = 0.38224531\n",
      "Iteration 408, loss = 0.38209237\n",
      "Iteration 409, loss = 0.38195721\n",
      "Iteration 410, loss = 0.38176997\n",
      "Iteration 411, loss = 0.38172612\n",
      "Iteration 412, loss = 0.38146081\n",
      "Iteration 413, loss = 0.38133205\n",
      "Iteration 414, loss = 0.38120296\n",
      "Iteration 415, loss = 0.38101098\n",
      "Iteration 416, loss = 0.38083729\n",
      "Iteration 417, loss = 0.38069551\n",
      "Iteration 418, loss = 0.38053352\n",
      "Iteration 419, loss = 0.38039775\n",
      "Iteration 420, loss = 0.38023573\n",
      "Iteration 421, loss = 0.38014494\n",
      "Iteration 422, loss = 0.37992510\n",
      "Iteration 423, loss = 0.37978110\n",
      "Iteration 424, loss = 0.37961726\n",
      "Iteration 425, loss = 0.37948367\n",
      "Iteration 426, loss = 0.37931859\n",
      "Iteration 427, loss = 0.37918478\n",
      "Iteration 428, loss = 0.37901032\n",
      "Iteration 429, loss = 0.37889205\n",
      "Iteration 430, loss = 0.37875160\n",
      "Iteration 431, loss = 0.37859394\n",
      "Iteration 432, loss = 0.37841750\n",
      "Iteration 433, loss = 0.37825925\n",
      "Iteration 434, loss = 0.37811614\n",
      "Iteration 435, loss = 0.37797129\n",
      "Iteration 436, loss = 0.37781453\n",
      "Iteration 437, loss = 0.37766304\n",
      "Iteration 438, loss = 0.37755409\n",
      "Iteration 439, loss = 0.37738847\n",
      "Iteration 440, loss = 0.37722480\n",
      "Iteration 441, loss = 0.37709837\n",
      "Iteration 442, loss = 0.37692246\n",
      "Iteration 443, loss = 0.37678737\n",
      "Iteration 444, loss = 0.37664922\n",
      "Iteration 445, loss = 0.37648950\n",
      "Iteration 446, loss = 0.37635031\n",
      "Iteration 447, loss = 0.37620026\n",
      "Iteration 448, loss = 0.37606470\n",
      "Iteration 449, loss = 0.37592683\n",
      "Iteration 450, loss = 0.37575200\n",
      "Iteration 451, loss = 0.37563784\n",
      "Iteration 452, loss = 0.37546910\n",
      "Iteration 453, loss = 0.37530975\n",
      "Iteration 454, loss = 0.37518676\n",
      "Iteration 455, loss = 0.37503066\n",
      "Iteration 456, loss = 0.37488323\n",
      "Iteration 457, loss = 0.37478726\n",
      "Iteration 458, loss = 0.37459525\n",
      "Iteration 459, loss = 0.37445518\n",
      "Iteration 460, loss = 0.37433733\n",
      "Iteration 461, loss = 0.37418568\n",
      "Iteration 462, loss = 0.37402400\n",
      "Iteration 463, loss = 0.37391281\n",
      "Iteration 464, loss = 0.37378538\n",
      "Iteration 465, loss = 0.37360177\n",
      "Iteration 466, loss = 0.37344859\n",
      "Iteration 467, loss = 0.37330617\n",
      "Iteration 468, loss = 0.37319930\n",
      "Iteration 469, loss = 0.37304911\n",
      "Iteration 470, loss = 0.37286515\n",
      "Iteration 471, loss = 0.37273171\n",
      "Iteration 472, loss = 0.37258998\n",
      "Iteration 473, loss = 0.37245246\n",
      "Iteration 474, loss = 0.37232063\n",
      "Iteration 475, loss = 0.37217295\n",
      "Iteration 476, loss = 0.37204297\n",
      "Iteration 477, loss = 0.37188662\n",
      "Iteration 478, loss = 0.37175532\n",
      "Iteration 479, loss = 0.37162960\n",
      "Iteration 480, loss = 0.37147006\n",
      "Iteration 481, loss = 0.37133737\n",
      "Iteration 482, loss = 0.37117216\n",
      "Iteration 483, loss = 0.37102643\n",
      "Iteration 484, loss = 0.37088698\n",
      "Iteration 485, loss = 0.37073395\n",
      "Iteration 486, loss = 0.37058777\n",
      "Iteration 487, loss = 0.37050577\n",
      "Iteration 488, loss = 0.37032339\n",
      "Iteration 489, loss = 0.37019224\n",
      "Iteration 490, loss = 0.37002440\n",
      "Iteration 491, loss = 0.36986568\n",
      "Iteration 492, loss = 0.36974836\n",
      "Iteration 493, loss = 0.36958651\n",
      "Iteration 494, loss = 0.36945997\n",
      "Iteration 495, loss = 0.36930711\n",
      "Iteration 496, loss = 0.36917558\n",
      "Iteration 497, loss = 0.36903709\n",
      "Iteration 498, loss = 0.36891691\n",
      "Iteration 499, loss = 0.36873951\n",
      "Iteration 500, loss = 0.36861033\n",
      "Iteration 501, loss = 0.36846491\n",
      "Iteration 502, loss = 0.36832949\n",
      "Iteration 503, loss = 0.36818440\n",
      "Iteration 504, loss = 0.36804773\n",
      "Iteration 505, loss = 0.36791519\n",
      "Iteration 506, loss = 0.36775229\n",
      "Iteration 507, loss = 0.36761194\n",
      "Iteration 508, loss = 0.36750753\n",
      "Iteration 509, loss = 0.36734947\n",
      "Iteration 510, loss = 0.36718900\n",
      "Iteration 511, loss = 0.36710386\n",
      "Iteration 512, loss = 0.36692743\n",
      "Iteration 513, loss = 0.36677271\n",
      "Iteration 514, loss = 0.36663579\n",
      "Iteration 515, loss = 0.36650902\n",
      "Iteration 516, loss = 0.36635060\n",
      "Iteration 517, loss = 0.36622821\n",
      "Iteration 518, loss = 0.36607725\n",
      "Iteration 519, loss = 0.36597194\n",
      "Iteration 520, loss = 0.36580970\n",
      "Iteration 521, loss = 0.36575137\n",
      "Iteration 522, loss = 0.36554067\n",
      "Iteration 523, loss = 0.36540700\n",
      "Iteration 524, loss = 0.36530051\n",
      "Iteration 525, loss = 0.36512792\n",
      "Iteration 526, loss = 0.36498786\n",
      "Iteration 527, loss = 0.36492507\n",
      "Iteration 528, loss = 0.36470245\n",
      "Iteration 529, loss = 0.36460157\n",
      "Iteration 530, loss = 0.36443366\n",
      "Iteration 531, loss = 0.36429443\n",
      "Iteration 532, loss = 0.36415389\n",
      "Iteration 533, loss = 0.36402480\n",
      "Iteration 534, loss = 0.36389257\n",
      "Iteration 535, loss = 0.36375364\n",
      "Iteration 536, loss = 0.36362511\n",
      "Iteration 537, loss = 0.36348509\n",
      "Iteration 538, loss = 0.36335273\n",
      "Iteration 539, loss = 0.36323327\n",
      "Iteration 540, loss = 0.36308523\n",
      "Iteration 541, loss = 0.36298080\n",
      "Iteration 542, loss = 0.36280036\n",
      "Iteration 543, loss = 0.36269881\n",
      "Iteration 544, loss = 0.36257500\n",
      "Iteration 545, loss = 0.36240612\n",
      "Iteration 546, loss = 0.36226903\n",
      "Iteration 547, loss = 0.36214220\n",
      "Iteration 548, loss = 0.36200157\n",
      "Iteration 549, loss = 0.36187931\n",
      "Iteration 550, loss = 0.36175523\n",
      "Iteration 551, loss = 0.36159481\n",
      "Iteration 552, loss = 0.36146508\n",
      "Iteration 553, loss = 0.36132427\n",
      "Iteration 554, loss = 0.36119293\n",
      "Iteration 555, loss = 0.36107063\n",
      "Iteration 556, loss = 0.36092671\n",
      "Iteration 557, loss = 0.36081120\n",
      "Iteration 558, loss = 0.36067693\n",
      "Iteration 559, loss = 0.36053665\n",
      "Iteration 560, loss = 0.36040530\n",
      "Iteration 561, loss = 0.36029452\n",
      "Iteration 562, loss = 0.36012886\n",
      "Iteration 563, loss = 0.36001582\n",
      "Iteration 564, loss = 0.35987063\n",
      "Iteration 565, loss = 0.35973749\n",
      "Iteration 566, loss = 0.35961894\n",
      "Iteration 567, loss = 0.35947637\n",
      "Iteration 568, loss = 0.35941127\n",
      "Iteration 569, loss = 0.35920412\n",
      "Iteration 570, loss = 0.35908652\n",
      "Iteration 571, loss = 0.35894702\n",
      "Iteration 572, loss = 0.35881819\n",
      "Iteration 573, loss = 0.35873409\n",
      "Iteration 574, loss = 0.35856549\n",
      "Iteration 575, loss = 0.35841736\n",
      "Iteration 576, loss = 0.35827797\n",
      "Iteration 577, loss = 0.35815516\n",
      "Iteration 578, loss = 0.35804665\n",
      "Iteration 579, loss = 0.35791650\n",
      "Iteration 580, loss = 0.35777104\n",
      "Iteration 581, loss = 0.35764962\n",
      "Iteration 582, loss = 0.35749144\n",
      "Iteration 583, loss = 0.35745016\n",
      "Iteration 584, loss = 0.35722662\n",
      "Iteration 585, loss = 0.35709021\n",
      "Iteration 586, loss = 0.35699002\n",
      "Iteration 587, loss = 0.35684177\n",
      "Iteration 588, loss = 0.35671013\n",
      "Iteration 589, loss = 0.35659977\n",
      "Iteration 590, loss = 0.35650120\n",
      "Iteration 591, loss = 0.35632003\n",
      "Iteration 592, loss = 0.35620253\n",
      "Iteration 593, loss = 0.35604776\n",
      "Iteration 594, loss = 0.35593436\n",
      "Iteration 595, loss = 0.35582665\n",
      "Iteration 596, loss = 0.35567563\n",
      "Iteration 597, loss = 0.35558129\n",
      "Iteration 598, loss = 0.35541840\n",
      "Iteration 599, loss = 0.35528676\n",
      "Iteration 600, loss = 0.35517801\n",
      "Iteration 601, loss = 0.35504094\n",
      "Iteration 602, loss = 0.35493898\n",
      "Iteration 603, loss = 0.35476870\n",
      "Iteration 604, loss = 0.35462837\n",
      "Iteration 605, loss = 0.35468877\n",
      "Iteration 606, loss = 0.35437719\n",
      "Iteration 607, loss = 0.35428442\n",
      "Iteration 608, loss = 0.35412408\n",
      "Iteration 609, loss = 0.35401856\n",
      "Iteration 610, loss = 0.35387330\n",
      "Iteration 611, loss = 0.35376628\n",
      "Iteration 612, loss = 0.35360747\n",
      "Iteration 613, loss = 0.35348969\n",
      "Iteration 614, loss = 0.35336192\n",
      "Iteration 615, loss = 0.35322998\n",
      "Iteration 616, loss = 0.35311535\n",
      "Iteration 617, loss = 0.35297450\n",
      "Iteration 618, loss = 0.35283779\n",
      "Iteration 619, loss = 0.35270629\n",
      "Iteration 620, loss = 0.35259817\n",
      "Iteration 621, loss = 0.35246948\n",
      "Iteration 622, loss = 0.35232075\n",
      "Iteration 623, loss = 0.35219590\n",
      "Iteration 624, loss = 0.35209491\n",
      "Iteration 625, loss = 0.35195841\n",
      "Iteration 626, loss = 0.35182832\n",
      "Iteration 627, loss = 0.35167859\n",
      "Iteration 628, loss = 0.35164114\n",
      "Iteration 629, loss = 0.35145197\n",
      "Iteration 630, loss = 0.35131775\n",
      "Iteration 631, loss = 0.35120395\n",
      "Iteration 632, loss = 0.35103245\n",
      "Iteration 633, loss = 0.35090731\n",
      "Iteration 634, loss = 0.35079278\n",
      "Iteration 635, loss = 0.35064986\n",
      "Iteration 636, loss = 0.35051354\n",
      "Iteration 637, loss = 0.35038835\n",
      "Iteration 638, loss = 0.35025543\n",
      "Iteration 639, loss = 0.35015132\n",
      "Iteration 640, loss = 0.35000776\n",
      "Iteration 641, loss = 0.34988125\n",
      "Iteration 642, loss = 0.34973076\n",
      "Iteration 643, loss = 0.34964208\n",
      "Iteration 644, loss = 0.34946839\n",
      "Iteration 645, loss = 0.34938720\n",
      "Iteration 646, loss = 0.34922351\n",
      "Iteration 647, loss = 0.34909637\n",
      "Iteration 648, loss = 0.34901553\n",
      "Iteration 649, loss = 0.34881968\n",
      "Iteration 650, loss = 0.34870495\n",
      "Iteration 651, loss = 0.34855564\n",
      "Iteration 652, loss = 0.34844523\n",
      "Iteration 653, loss = 0.34831207\n",
      "Iteration 654, loss = 0.34818856\n",
      "Iteration 655, loss = 0.34805544\n",
      "Iteration 656, loss = 0.34791202\n",
      "Iteration 657, loss = 0.34780567\n",
      "Iteration 658, loss = 0.34765241\n",
      "Iteration 659, loss = 0.34751397\n",
      "Iteration 660, loss = 0.34741954\n",
      "Iteration 661, loss = 0.34732685\n",
      "Iteration 662, loss = 0.34716404\n",
      "Iteration 663, loss = 0.34700075\n",
      "Iteration 664, loss = 0.34689134\n",
      "Iteration 665, loss = 0.34676017\n",
      "Iteration 666, loss = 0.34664173\n",
      "Iteration 667, loss = 0.34649061\n",
      "Iteration 668, loss = 0.34639882\n",
      "Iteration 669, loss = 0.34621963\n",
      "Iteration 670, loss = 0.34612809\n",
      "Iteration 671, loss = 0.34597605\n",
      "Iteration 672, loss = 0.34583376\n",
      "Iteration 673, loss = 0.34570708\n",
      "Iteration 674, loss = 0.34560997\n",
      "Iteration 675, loss = 0.34547600\n",
      "Iteration 676, loss = 0.34534906\n",
      "Iteration 677, loss = 0.34525988\n",
      "Iteration 678, loss = 0.34505614\n",
      "Iteration 679, loss = 0.34494647\n",
      "Iteration 680, loss = 0.34480648\n",
      "Iteration 681, loss = 0.34466287\n",
      "Iteration 682, loss = 0.34453070\n",
      "Iteration 683, loss = 0.34442029\n",
      "Iteration 684, loss = 0.34428011\n",
      "Iteration 685, loss = 0.34419649\n",
      "Iteration 686, loss = 0.34402133\n",
      "Iteration 687, loss = 0.34391855\n",
      "Iteration 688, loss = 0.34376075\n",
      "Iteration 689, loss = 0.34364024\n",
      "Iteration 690, loss = 0.34350839\n",
      "Iteration 691, loss = 0.34342580\n",
      "Iteration 692, loss = 0.34325282\n",
      "Iteration 693, loss = 0.34315723\n",
      "Iteration 694, loss = 0.34298079\n",
      "Iteration 695, loss = 0.34286553\n",
      "Iteration 696, loss = 0.34279606\n",
      "Iteration 697, loss = 0.34263384\n",
      "Iteration 698, loss = 0.34249352\n",
      "Iteration 699, loss = 0.34232453\n",
      "Iteration 700, loss = 0.34224661\n",
      "Iteration 701, loss = 0.34206687\n",
      "Iteration 702, loss = 0.34193249\n",
      "Iteration 703, loss = 0.34180670\n",
      "Iteration 704, loss = 0.34166124\n",
      "Iteration 705, loss = 0.34156606\n",
      "Iteration 706, loss = 0.34142973\n",
      "Iteration 707, loss = 0.34129073\n",
      "Iteration 708, loss = 0.34117606\n",
      "Iteration 709, loss = 0.34102511\n",
      "Iteration 710, loss = 0.34092039\n",
      "Iteration 711, loss = 0.34079036\n",
      "Iteration 712, loss = 0.34065064\n",
      "Iteration 713, loss = 0.34052217\n",
      "Iteration 714, loss = 0.34037760\n",
      "Iteration 715, loss = 0.34025078\n",
      "Iteration 716, loss = 0.34013880\n",
      "Iteration 717, loss = 0.33998418\n",
      "Iteration 718, loss = 0.33988882\n",
      "Iteration 719, loss = 0.33977132\n",
      "Iteration 720, loss = 0.33969128\n",
      "Iteration 721, loss = 0.33951221\n",
      "Iteration 722, loss = 0.33936482\n",
      "Iteration 723, loss = 0.33922529\n",
      "Iteration 724, loss = 0.33910344\n",
      "Iteration 725, loss = 0.33895811\n",
      "Iteration 726, loss = 0.33883772\n",
      "Iteration 727, loss = 0.33871377\n",
      "Iteration 728, loss = 0.33859661\n",
      "Iteration 729, loss = 0.33849205\n",
      "Iteration 730, loss = 0.33834014\n",
      "Iteration 731, loss = 0.33821199\n",
      "Iteration 732, loss = 0.33805852\n",
      "Iteration 733, loss = 0.33793344\n",
      "Iteration 734, loss = 0.33779903\n",
      "Iteration 735, loss = 0.33767069\n",
      "Iteration 736, loss = 0.33755147\n",
      "Iteration 737, loss = 0.33741681\n",
      "Iteration 738, loss = 0.33729146\n",
      "Iteration 739, loss = 0.33719005\n",
      "Iteration 740, loss = 0.33702706\n",
      "Iteration 741, loss = 0.33691075\n",
      "Iteration 742, loss = 0.33676126\n",
      "Iteration 743, loss = 0.33664283\n",
      "Iteration 744, loss = 0.33650247\n",
      "Iteration 745, loss = 0.33637710\n",
      "Iteration 746, loss = 0.33627160\n",
      "Iteration 747, loss = 0.33611989\n",
      "Iteration 748, loss = 0.33601860\n",
      "Iteration 2219, loss = 0.08824222\n",
      "Iteration 2220, loss = 0.08815852\n",
      "Iteration 2221, loss = 0.08800046\n",
      "Iteration 2222, loss = 0.08795115\n",
      "Iteration 2223, loss = 0.08787562\n",
      "Iteration 2224, loss = 0.08785302\n",
      "Iteration 2225, loss = 0.08776717\n",
      "Iteration 2226, loss = 0.08772699\n",
      "Iteration 2227, loss = 0.08781880\n",
      "Iteration 2228, loss = 0.08764756\n",
      "Iteration 2229, loss = 0.08746681\n",
      "Iteration 2230, loss = 0.08748014\n",
      "Iteration 2231, loss = 0.08742099\n",
      "Iteration 2232, loss = 0.08730326\n",
      "Iteration 2233, loss = 0.08721548\n",
      "Iteration 2234, loss = 0.08716778\n",
      "Iteration 2235, loss = 0.08708974\n",
      "Iteration 2236, loss = 0.08700266\n",
      "Iteration 2237, loss = 0.08704217\n",
      "Iteration 2238, loss = 0.08695557\n",
      "Iteration 2239, loss = 0.08689376\n",
      "Iteration 2240, loss = 0.08672516\n",
      "Iteration 2241, loss = 0.08669151\n",
      "Iteration 2242, loss = 0.08666082\n",
      "Iteration 2243, loss = 0.08654751\n",
      "Iteration 2244, loss = 0.08649641\n",
      "Iteration 2245, loss = 0.08647829\n",
      "Iteration 2246, loss = 0.08645548\n",
      "Iteration 2247, loss = 0.08625310\n",
      "Iteration 2248, loss = 0.08625225\n",
      "Iteration 2249, loss = 0.08612859\n",
      "Iteration 2250, loss = 0.08606949\n",
      "Iteration 2251, loss = 0.08605373\n",
      "Iteration 2252, loss = 0.08597286\n",
      "Iteration 2253, loss = 0.08590539\n",
      "Iteration 2254, loss = 0.08585259\n",
      "Iteration 2255, loss = 0.08586486\n",
      "Iteration 2256, loss = 0.08575536\n",
      "Iteration 2257, loss = 0.08562221\n",
      "Iteration 2258, loss = 0.08567298\n",
      "Iteration 2259, loss = 0.08563914\n",
      "Iteration 2260, loss = 0.08565267\n",
      "Iteration 2261, loss = 0.08543418\n",
      "Iteration 2262, loss = 0.08539611\n",
      "Iteration 2263, loss = 0.08525809\n",
      "Iteration 2264, loss = 0.08518946\n",
      "Iteration 2265, loss = 0.08519466\n",
      "Iteration 2266, loss = 0.08511021\n",
      "Iteration 2267, loss = 0.08504069\n",
      "Iteration 2268, loss = 0.08505510\n",
      "Iteration 2269, loss = 0.08491184\n",
      "Iteration 2270, loss = 0.08486538\n",
      "Iteration 2271, loss = 0.08485647\n",
      "Iteration 2272, loss = 0.08470837\n",
      "Iteration 2273, loss = 0.08460730\n",
      "Iteration 2274, loss = 0.08466891\n",
      "Iteration 2275, loss = 0.08457360\n",
      "Iteration 2276, loss = 0.08445855\n",
      "Iteration 2277, loss = 0.08442476\n",
      "Iteration 2278, loss = 0.08438587\n",
      "Iteration 2279, loss = 0.08426907\n",
      "Iteration 2280, loss = 0.08435984\n",
      "Iteration 2281, loss = 0.08424218\n",
      "Iteration 2282, loss = 0.08418751\n",
      "Iteration 2283, loss = 0.08403714\n",
      "Iteration 2284, loss = 0.08399392\n",
      "Iteration 2285, loss = 0.08403861\n",
      "Iteration 2286, loss = 0.08385942\n",
      "Iteration 2287, loss = 0.08387978\n",
      "Iteration 2288, loss = 0.08374441\n",
      "Iteration 2289, loss = 0.08375362\n",
      "Iteration 2290, loss = 0.08369854\n",
      "Iteration 2291, loss = 0.08357926\n",
      "Iteration 2292, loss = 0.08354265\n",
      "Iteration 2293, loss = 0.08348038\n",
      "Iteration 2294, loss = 0.08354661\n",
      "Iteration 2295, loss = 0.08333798\n",
      "Iteration 2296, loss = 0.08339080\n",
      "Iteration 2297, loss = 0.08322871\n",
      "Iteration 2298, loss = 0.08317567\n",
      "Iteration 2299, loss = 0.08310526\n",
      "Iteration 2300, loss = 0.08317311\n",
      "Iteration 2301, loss = 0.08308229\n",
      "Iteration 2302, loss = 0.08297521\n",
      "Iteration 2303, loss = 0.08287928\n",
      "Iteration 2304, loss = 0.08304083\n",
      "Iteration 2305, loss = 0.08282516\n",
      "Iteration 2306, loss = 0.08276712\n",
      "Iteration 2307, loss = 0.08274322\n",
      "Iteration 2308, loss = 0.08261865\n",
      "Iteration 2309, loss = 0.08252010\n",
      "Iteration 2310, loss = 0.08251790\n",
      "Iteration 2311, loss = 0.08244748\n",
      "Iteration 2312, loss = 0.08247614\n",
      "Iteration 2313, loss = 0.08246576\n",
      "Iteration 2314, loss = 0.08239311\n",
      "Iteration 2315, loss = 0.08222161\n",
      "Iteration 2316, loss = 0.08215908\n",
      "Iteration 2317, loss = 0.08221886\n",
      "Iteration 2318, loss = 0.08206318\n",
      "Iteration 2319, loss = 0.08199774\n",
      "Iteration 2320, loss = 0.08201728\n",
      "Iteration 2321, loss = 0.08195370\n",
      "Iteration 2322, loss = 0.08184951\n",
      "Iteration 2323, loss = 0.08184738\n",
      "Iteration 2324, loss = 0.08172733\n",
      "Iteration 2325, loss = 0.08184503\n",
      "Iteration 2326, loss = 0.08165533\n",
      "Iteration 2327, loss = 0.08168711\n",
      "Iteration 2328, loss = 0.08153786\n",
      "Iteration 2329, loss = 0.08166018\n",
      "Iteration 2330, loss = 0.08143568\n",
      "Iteration 2331, loss = 0.08143439\n",
      "Iteration 2332, loss = 0.08145192\n",
      "Iteration 2333, loss = 0.08135956\n",
      "Iteration 2334, loss = 0.08137651\n",
      "Iteration 2335, loss = 0.08115918\n",
      "Iteration 2336, loss = 0.08115295\n",
      "Iteration 2337, loss = 0.08114823\n",
      "Iteration 2338, loss = 0.08107653\n",
      "Iteration 2339, loss = 0.08104928\n",
      "Iteration 2340, loss = 0.08092609\n",
      "Iteration 2341, loss = 0.08092114\n",
      "Iteration 2342, loss = 0.08085150\n",
      "Iteration 2343, loss = 0.08074031\n",
      "Iteration 2344, loss = 0.08070203\n",
      "Iteration 2345, loss = 0.08069322\n",
      "Iteration 2346, loss = 0.08061028\n",
      "Iteration 2347, loss = 0.08055455\n",
      "Iteration 2348, loss = 0.08056524\n",
      "Iteration 2349, loss = 0.08052414\n",
      "Iteration 2350, loss = 0.08048684\n",
      "Iteration 2351, loss = 0.08037827\n",
      "Iteration 2352, loss = 0.08033070\n",
      "Iteration 2353, loss = 0.08025402\n",
      "Iteration 2354, loss = 0.08018576\n",
      "Iteration 2355, loss = 0.08014843\n",
      "Iteration 2356, loss = 0.08009885\n",
      "Iteration 2357, loss = 0.08010493\n",
      "Iteration 2358, loss = 0.07996498\n",
      "Iteration 2359, loss = 0.07992309\n",
      "Iteration 2360, loss = 0.07986069\n",
      "Iteration 2361, loss = 0.07981917\n",
      "Iteration 2362, loss = 0.07983890\n",
      "Iteration 2363, loss = 0.07972823\n",
      "Iteration 2364, loss = 0.07969343\n",
      "Iteration 2365, loss = 0.07981110\n",
      "Iteration 2366, loss = 0.07957617\n",
      "Iteration 2367, loss = 0.07958451\n",
      "Iteration 2368, loss = 0.07955797\n",
      "Iteration 2369, loss = 0.07947728\n",
      "Iteration 2370, loss = 0.07937026\n",
      "Iteration 2371, loss = 0.07938990\n",
      "Iteration 2372, loss = 0.07930263\n",
      "Iteration 2373, loss = 0.07921323\n",
      "Iteration 2374, loss = 0.07916334\n",
      "Iteration 2375, loss = 0.07915442\n",
      "Iteration 2376, loss = 0.07917055\n",
      "Iteration 2377, loss = 0.07912400\n",
      "Iteration 2378, loss = 0.07904135\n",
      "Iteration 2379, loss = 0.07893789\n",
      "Iteration 2380, loss = 0.07892871\n",
      "Iteration 2381, loss = 0.07885082\n",
      "Iteration 2382, loss = 0.07887135\n",
      "Iteration 2383, loss = 0.07878478\n",
      "Iteration 2384, loss = 0.07871735\n",
      "Iteration 2385, loss = 0.07863874\n",
      "Iteration 2386, loss = 0.07858652\n",
      "Iteration 2387, loss = 0.07859767\n",
      "Iteration 2388, loss = 0.07860918\n",
      "Iteration 2389, loss = 0.07844482\n",
      "Iteration 2390, loss = 0.07842044\n",
      "Iteration 2391, loss = 0.07836219\n",
      "Iteration 2392, loss = 0.07839340\n",
      "Iteration 2393, loss = 0.07831473\n",
      "Iteration 2394, loss = 0.07825145\n",
      "Iteration 2395, loss = 0.07823029\n",
      "Iteration 2396, loss = 0.07828488\n",
      "Iteration 2397, loss = 0.07808547\n",
      "Iteration 2398, loss = 0.07806005\n",
      "Iteration 2399, loss = 0.07800164\n",
      "Iteration 2400, loss = 0.07796279\n",
      "Iteration 2401, loss = 0.07795399\n",
      "Iteration 2402, loss = 0.07785831\n",
      "Iteration 2403, loss = 0.07786034\n",
      "Iteration 2404, loss = 0.07774478\n",
      "Iteration 2405, loss = 0.07776211\n",
      "Iteration 2406, loss = 0.07787512\n",
      "Iteration 2407, loss = 0.07765609\n",
      "Iteration 2408, loss = 0.07758636\n",
      "Iteration 2409, loss = 0.07761351\n",
      "Iteration 2410, loss = 0.07753371\n",
      "Iteration 2411, loss = 0.07745561\n",
      "Iteration 2412, loss = 0.07742000\n",
      "Iteration 2413, loss = 0.07739100\n",
      "Iteration 2414, loss = 0.07732370\n",
      "Iteration 2415, loss = 0.07733099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79166063\n",
      "Iteration 2, loss = 0.78843540\n",
      "Iteration 3, loss = 0.78337510\n",
      "Iteration 4, loss = 0.77714546\n",
      "Iteration 5, loss = 0.77061988\n",
      "Iteration 6, loss = 0.76356184\n",
      "Iteration 7, loss = 0.75614663\n",
      "Iteration 8, loss = 0.74874618\n",
      "Iteration 9, loss = 0.74142831\n",
      "Iteration 10, loss = 0.73437308\n",
      "Iteration 11, loss = 0.72703266\n",
      "Iteration 12, loss = 0.72009997\n",
      "Iteration 13, loss = 0.71346682\n",
      "Iteration 14, loss = 0.70711236\n",
      "Iteration 15, loss = 0.70081664\n",
      "Iteration 16, loss = 0.69507518\n",
      "Iteration 17, loss = 0.68904850\n",
      "Iteration 18, loss = 0.68341955\n",
      "Iteration 19, loss = 0.67798548\n",
      "Iteration 20, loss = 0.67259654\n",
      "Iteration 21, loss = 0.66727856\n",
      "Iteration 22, loss = 0.66223906\n",
      "Iteration 23, loss = 0.65745450\n",
      "Iteration 24, loss = 0.65253052\n",
      "Iteration 25, loss = 0.64804948\n",
      "Iteration 26, loss = 0.64315158\n",
      "Iteration 27, loss = 0.63885176\n",
      "Iteration 28, loss = 0.63422982\n",
      "Iteration 29, loss = 0.62995247\n",
      "Iteration 30, loss = 0.62557299\n",
      "Iteration 31, loss = 0.62140022\n",
      "Iteration 32, loss = 0.61723586\n",
      "Iteration 33, loss = 0.61313210\n",
      "Iteration 34, loss = 0.60914527\n",
      "Iteration 35, loss = 0.60498946\n",
      "Iteration 36, loss = 0.60107755\n",
      "Iteration 37, loss = 0.59729287\n",
      "Iteration 38, loss = 0.59340651\n",
      "Iteration 39, loss = 0.58949789\n",
      "Iteration 40, loss = 0.58592599\n",
      "Iteration 41, loss = 0.58224359\n",
      "Iteration 42, loss = 0.57861080\n",
      "Iteration 43, loss = 0.57518321\n",
      "Iteration 44, loss = 0.57161285\n",
      "Iteration 45, loss = 0.56816196\n",
      "Iteration 46, loss = 0.56474519\n",
      "Iteration 47, loss = 0.56140542\n",
      "Iteration 48, loss = 0.55802709\n",
      "Iteration 49, loss = 0.55482055\n",
      "Iteration 50, loss = 0.55168030\n",
      "Iteration 51, loss = 0.54852000\n",
      "Iteration 52, loss = 0.54534669\n",
      "Iteration 53, loss = 0.54234157\n",
      "Iteration 54, loss = 0.53934474\n",
      "Iteration 55, loss = 0.53634667\n",
      "Iteration 56, loss = 0.53335788\n",
      "Iteration 57, loss = 0.53051377\n",
      "Iteration 58, loss = 0.52767015\n",
      "Iteration 59, loss = 0.52475540\n",
      "Iteration 60, loss = 0.52211312\n",
      "Iteration 61, loss = 0.51931340\n",
      "Iteration 62, loss = 0.51674687\n",
      "Iteration 63, loss = 0.51400246\n",
      "Iteration 64, loss = 0.51144032\n",
      "Iteration 65, loss = 0.50900861\n",
      "Iteration 66, loss = 0.50646473\n",
      "Iteration 67, loss = 0.50404815\n",
      "Iteration 68, loss = 0.50166880\n",
      "Iteration 69, loss = 0.49928579\n",
      "Iteration 70, loss = 0.49698377\n",
      "Iteration 71, loss = 0.49470886\n",
      "Iteration 72, loss = 0.49255807\n",
      "Iteration 73, loss = 0.49031878\n",
      "Iteration 74, loss = 0.48824523\n",
      "Iteration 75, loss = 0.48612108\n",
      "Iteration 76, loss = 0.48410726\n",
      "Iteration 77, loss = 0.48203400\n",
      "Iteration 78, loss = 0.48010623\n",
      "Iteration 79, loss = 0.47819480\n",
      "Iteration 80, loss = 0.47631315\n",
      "Iteration 81, loss = 0.47436526\n",
      "Iteration 82, loss = 0.47262401\n",
      "Iteration 83, loss = 0.47078828\n",
      "Iteration 84, loss = 0.46905824\n",
      "Iteration 85, loss = 0.46733128\n",
      "Iteration 86, loss = 0.46564618\n",
      "Iteration 87, loss = 0.46403592\n",
      "Iteration 88, loss = 0.46239812\n",
      "Iteration 89, loss = 0.46080284\n",
      "Iteration 90, loss = 0.45923929\n",
      "Iteration 91, loss = 0.45771889\n",
      "Iteration 92, loss = 0.45628180\n",
      "Iteration 93, loss = 0.45482869\n",
      "Iteration 94, loss = 0.45334869\n",
      "Iteration 95, loss = 0.45195191\n",
      "Iteration 96, loss = 0.45056244\n",
      "Iteration 97, loss = 0.44930001\n",
      "Iteration 98, loss = 0.44792875\n",
      "Iteration 99, loss = 0.44664646\n",
      "Iteration 100, loss = 0.44534180\n",
      "Iteration 101, loss = 0.44413187\n",
      "Iteration 102, loss = 0.44287887\n",
      "Iteration 103, loss = 0.44178126\n",
      "Iteration 104, loss = 0.44054336\n",
      "Iteration 105, loss = 0.43935295\n",
      "Iteration 106, loss = 0.43817932\n",
      "Iteration 107, loss = 0.43711489\n",
      "Iteration 108, loss = 0.43605895\n",
      "Iteration 109, loss = 0.43492392\n",
      "Iteration 110, loss = 0.43399087\n",
      "Iteration 111, loss = 0.43288477\n",
      "Iteration 112, loss = 0.43185060\n",
      "Iteration 113, loss = 0.43086638\n",
      "Iteration 114, loss = 0.42989908\n",
      "Iteration 115, loss = 0.42895421\n",
      "Iteration 116, loss = 0.42799441\n",
      "Iteration 117, loss = 0.42701934\n",
      "Iteration 118, loss = 0.42614531\n",
      "Iteration 119, loss = 0.42525437\n",
      "Iteration 120, loss = 0.42433725\n",
      "Iteration 121, loss = 0.42346916\n",
      "Iteration 122, loss = 0.42262569\n",
      "Iteration 123, loss = 0.42187507\n",
      "Iteration 124, loss = 0.42096210\n",
      "Iteration 125, loss = 0.42016648\n",
      "Iteration 126, loss = 0.41932972\n",
      "Iteration 127, loss = 0.41858834\n",
      "Iteration 128, loss = 0.41780543\n",
      "Iteration 129, loss = 0.41703386\n",
      "Iteration 130, loss = 0.41633804\n",
      "Iteration 131, loss = 0.41562014\n",
      "Iteration 132, loss = 0.41482850\n",
      "Iteration 133, loss = 0.41415424\n",
      "Iteration 134, loss = 0.41341882\n",
      "Iteration 135, loss = 0.41273672\n",
      "Iteration 136, loss = 0.41205096\n",
      "Iteration 137, loss = 0.41137113\n",
      "Iteration 138, loss = 0.41066906\n",
      "Iteration 139, loss = 0.41006850\n",
      "Iteration 140, loss = 0.40938326\n",
      "Iteration 141, loss = 0.40878264\n",
      "Iteration 142, loss = 0.40812075\n",
      "Iteration 143, loss = 0.40753415\n",
      "Iteration 144, loss = 0.40687352\n",
      "Iteration 145, loss = 0.40627658\n",
      "Iteration 146, loss = 0.40564159\n",
      "Iteration 147, loss = 0.40507076\n",
      "Iteration 148, loss = 0.40448619\n",
      "Iteration 149, loss = 0.40389432\n",
      "Iteration 150, loss = 0.40332484\n",
      "Iteration 151, loss = 0.40269927\n",
      "Iteration 152, loss = 0.40215457\n",
      "Iteration 153, loss = 0.40154976\n",
      "Iteration 154, loss = 0.40101980\n",
      "Iteration 155, loss = 0.40047551\n",
      "Iteration 156, loss = 0.39992396\n",
      "Iteration 157, loss = 0.39943725\n",
      "Iteration 158, loss = 0.39884550\n",
      "Iteration 159, loss = 0.39834330\n",
      "Iteration 160, loss = 0.39779948\n",
      "Iteration 161, loss = 0.39725979\n",
      "Iteration 162, loss = 0.39675183\n",
      "Iteration 163, loss = 0.39627394\n",
      "Iteration 164, loss = 0.39573173\n",
      "Iteration 165, loss = 0.39527723\n",
      "Iteration 166, loss = 0.39472546\n",
      "Iteration 167, loss = 0.39425430\n",
      "Iteration 168, loss = 0.39378655\n",
      "Iteration 169, loss = 0.39327629\n",
      "Iteration 170, loss = 0.39280558\n",
      "Iteration 171, loss = 0.39235660\n",
      "Iteration 172, loss = 0.39187063\n",
      "Iteration 173, loss = 0.39144039\n",
      "Iteration 174, loss = 0.39095471\n",
      "Iteration 175, loss = 0.39054193\n",
      "Iteration 176, loss = 0.39003484\n",
      "Iteration 177, loss = 0.38957690\n",
      "Iteration 178, loss = 0.38913032\n",
      "Iteration 179, loss = 0.38868932\n",
      "Iteration 180, loss = 0.38824134\n",
      "Iteration 181, loss = 0.38780270\n",
      "Iteration 182, loss = 0.38737284\n",
      "Iteration 183, loss = 0.38697579\n",
      "Iteration 184, loss = 0.38651443\n",
      "Iteration 185, loss = 0.38607544\n",
      "Iteration 186, loss = 0.38567167\n",
      "Iteration 187, loss = 0.38527316\n",
      "Iteration 188, loss = 0.38487868\n",
      "Iteration 189, loss = 0.38444322\n",
      "Iteration 190, loss = 0.38403904\n",
      "Iteration 191, loss = 0.38364459\n",
      "Iteration 192, loss = 0.38322611\n",
      "Iteration 193, loss = 0.38281693\n",
      "Iteration 194, loss = 0.38239159\n",
      "Iteration 195, loss = 0.38203261\n",
      "Iteration 196, loss = 0.38161817\n",
      "Iteration 197, loss = 0.38126665\n",
      "Iteration 198, loss = 0.38088836\n",
      "Iteration 199, loss = 0.38043585\n",
      "Iteration 200, loss = 0.38011559\n",
      "Iteration 201, loss = 0.37969534\n",
      "Iteration 202, loss = 0.37932245\n",
      "Iteration 203, loss = 0.37894480\n",
      "Iteration 204, loss = 0.37857254\n",
      "Iteration 205, loss = 0.37820096\n",
      "Iteration 206, loss = 0.37781206\n",
      "Iteration 207, loss = 0.37747917\n",
      "Iteration 208, loss = 0.37717800\n",
      "Iteration 209, loss = 0.37678707\n",
      "Iteration 210, loss = 0.37641240\n",
      "Iteration 211, loss = 0.37606237\n",
      "Iteration 212, loss = 0.37569245\n",
      "Iteration 213, loss = 0.37531052\n",
      "Iteration 214, loss = 0.37498407\n",
      "Iteration 215, loss = 0.37465426\n",
      "Iteration 216, loss = 0.37429417\n",
      "Iteration 217, loss = 0.37395562\n",
      "Iteration 218, loss = 0.37362231\n",
      "Iteration 219, loss = 0.37329085\n",
      "Iteration 220, loss = 0.37296962\n",
      "Iteration 221, loss = 0.37262919\n",
      "Iteration 222, loss = 0.37229569\n",
      "Iteration 223, loss = 0.37198421\n",
      "Iteration 224, loss = 0.37164618\n",
      "Iteration 225, loss = 0.37132804\n",
      "Iteration 226, loss = 0.37102933\n",
      "Iteration 227, loss = 0.37069626\n",
      "Iteration 228, loss = 0.37037678\n",
      "Iteration 229, loss = 0.37006074\n",
      "Iteration 230, loss = 0.36973278\n",
      "Iteration 231, loss = 0.36944762\n",
      "Iteration 232, loss = 0.36913271\n",
      "Iteration 233, loss = 0.36888441\n",
      "Iteration 234, loss = 0.36858595\n",
      "Iteration 235, loss = 0.36821068\n",
      "Iteration 236, loss = 0.36791318\n",
      "Iteration 237, loss = 0.36763540\n",
      "Iteration 238, loss = 0.36732791\n",
      "Iteration 239, loss = 0.36701742\n",
      "Iteration 240, loss = 0.36671764\n",
      "Iteration 241, loss = 0.36643307\n",
      "Iteration 242, loss = 0.36613536\n",
      "Iteration 243, loss = 0.36585184\n",
      "Iteration 244, loss = 0.36555548\n",
      "Iteration 245, loss = 0.36529097\n",
      "Iteration 246, loss = 0.36501018\n",
      "Iteration 247, loss = 0.36469217\n",
      "Iteration 248, loss = 0.36455630\n",
      "Iteration 249, loss = 0.36415353\n",
      "Iteration 250, loss = 0.36387036\n",
      "Iteration 251, loss = 0.36356519\n",
      "Iteration 252, loss = 0.36331018\n",
      "Iteration 253, loss = 0.36305164\n",
      "Iteration 254, loss = 0.36276038\n",
      "Iteration 255, loss = 0.36253049\n",
      "Iteration 256, loss = 0.36225436\n",
      "Iteration 257, loss = 0.36195518\n",
      "Iteration 258, loss = 0.36176842\n",
      "Iteration 259, loss = 0.36146240\n",
      "Iteration 260, loss = 0.36118435\n",
      "Iteration 261, loss = 0.36096315\n",
      "Iteration 262, loss = 0.36068919\n",
      "Iteration 263, loss = 0.36041766\n",
      "Iteration 264, loss = 0.36018374\n",
      "Iteration 265, loss = 0.35990963\n",
      "Iteration 266, loss = 0.35967762\n",
      "Iteration 267, loss = 0.35942945\n",
      "Iteration 268, loss = 0.35918892\n",
      "Iteration 269, loss = 0.35893689\n",
      "Iteration 270, loss = 0.35866909\n",
      "Iteration 271, loss = 0.35845517\n",
      "Iteration 272, loss = 0.35819403\n",
      "Iteration 273, loss = 0.35803528\n",
      "Iteration 274, loss = 0.35776636\n",
      "Iteration 275, loss = 0.35749248\n",
      "Iteration 276, loss = 0.35727560\n",
      "Iteration 277, loss = 0.35702869\n",
      "Iteration 278, loss = 0.35676972\n",
      "Iteration 279, loss = 0.35658012\n",
      "Iteration 280, loss = 0.35631601\n",
      "Iteration 281, loss = 0.35610455\n",
      "Iteration 282, loss = 0.35587033\n",
      "Iteration 283, loss = 0.35566552\n",
      "Iteration 284, loss = 0.35543633\n",
      "Iteration 285, loss = 0.35521361\n",
      "Iteration 286, loss = 0.35501740\n",
      "Iteration 287, loss = 0.35476574\n",
      "Iteration 288, loss = 0.35452733\n",
      "Iteration 289, loss = 0.35430589\n",
      "Iteration 290, loss = 0.35410439\n",
      "Iteration 291, loss = 0.35388352\n",
      "Iteration 292, loss = 0.35366783\n",
      "Iteration 293, loss = 0.35343055\n",
      "Iteration 1462, loss = 0.21997484\n",
      "Iteration 1463, loss = 0.21991350\n",
      "Iteration 1464, loss = 0.21976645\n",
      "Iteration 1465, loss = 0.21967368\n",
      "Iteration 1466, loss = 0.21955007\n",
      "Iteration 1467, loss = 0.21947057\n",
      "Iteration 1468, loss = 0.21936422\n",
      "Iteration 1469, loss = 0.21926788\n",
      "Iteration 1470, loss = 0.21921656\n",
      "Iteration 1471, loss = 0.21911301\n",
      "Iteration 1472, loss = 0.21904418\n",
      "Iteration 1473, loss = 0.21889618\n",
      "Iteration 1474, loss = 0.21881409\n",
      "Iteration 1475, loss = 0.21873937\n",
      "Iteration 1476, loss = 0.21864210\n",
      "Iteration 1477, loss = 0.21853342\n",
      "Iteration 1478, loss = 0.21842615\n",
      "Iteration 1479, loss = 0.21836738\n",
      "Iteration 1480, loss = 0.21831497\n",
      "Iteration 1481, loss = 0.21816153\n",
      "Iteration 1482, loss = 0.21808281\n",
      "Iteration 1483, loss = 0.21799391\n",
      "Iteration 1484, loss = 0.21789531\n",
      "Iteration 1485, loss = 0.21781794\n",
      "Iteration 1486, loss = 0.21771582\n",
      "Iteration 1487, loss = 0.21761242\n",
      "Iteration 1488, loss = 0.21751306\n",
      "Iteration 1489, loss = 0.21743568\n",
      "Iteration 1490, loss = 0.21733350\n",
      "Iteration 1491, loss = 0.21721683\n",
      "Iteration 1492, loss = 0.21720407\n",
      "Iteration 1493, loss = 0.21708205\n",
      "Iteration 1494, loss = 0.21695390\n",
      "Iteration 1495, loss = 0.21686577\n",
      "Iteration 1496, loss = 0.21679000\n",
      "Iteration 1497, loss = 0.21669661\n",
      "Iteration 1498, loss = 0.21660198\n",
      "Iteration 1499, loss = 0.21648580\n",
      "Iteration 1500, loss = 0.21642759\n",
      "Iteration 1501, loss = 0.21631202\n",
      "Iteration 1502, loss = 0.21622522\n",
      "Iteration 1503, loss = 0.21611451\n",
      "Iteration 1504, loss = 0.21606075\n",
      "Iteration 1505, loss = 0.21596044\n",
      "Iteration 1506, loss = 0.21586573\n",
      "Iteration 1507, loss = 0.21586591\n",
      "Iteration 1508, loss = 0.21569967\n",
      "Iteration 1509, loss = 0.21555519\n",
      "Iteration 1510, loss = 0.21547709\n",
      "Iteration 1511, loss = 0.21540364\n",
      "Iteration 1512, loss = 0.21531496\n",
      "Iteration 1513, loss = 0.21521253\n",
      "Iteration 1514, loss = 0.21515513\n",
      "Iteration 1515, loss = 0.21506165\n",
      "Iteration 1516, loss = 0.21493539\n",
      "Iteration 1517, loss = 0.21482680\n",
      "Iteration 1518, loss = 0.21473659\n",
      "Iteration 1519, loss = 0.21464602\n",
      "Iteration 1520, loss = 0.21458244\n",
      "Iteration 1521, loss = 0.21448347\n",
      "Iteration 1522, loss = 0.21437364\n",
      "Iteration 1523, loss = 0.21428013\n",
      "Iteration 1524, loss = 0.21418683\n",
      "Iteration 1525, loss = 0.21408250\n",
      "Iteration 1526, loss = 0.21400811\n",
      "Iteration 1527, loss = 0.21391697\n",
      "Iteration 1528, loss = 0.21382281\n",
      "Iteration 1529, loss = 0.21374202\n",
      "Iteration 1530, loss = 0.21362237\n",
      "Iteration 1531, loss = 0.21354031\n",
      "Iteration 1532, loss = 0.21346216\n",
      "Iteration 1533, loss = 0.21335838\n",
      "Iteration 1534, loss = 0.21327638\n",
      "Iteration 1535, loss = 0.21317800\n",
      "Iteration 1536, loss = 0.21310103\n",
      "Iteration 1537, loss = 0.21302770\n",
      "Iteration 1538, loss = 0.21290581\n",
      "Iteration 1539, loss = 0.21284140\n",
      "Iteration 1540, loss = 0.21276360\n",
      "Iteration 1541, loss = 0.21270213\n",
      "Iteration 1542, loss = 0.21254376\n",
      "Iteration 1543, loss = 0.21245524\n",
      "Iteration 1544, loss = 0.21236683\n",
      "Iteration 1545, loss = 0.21227475\n",
      "Iteration 1546, loss = 0.21224043\n",
      "Iteration 1547, loss = 0.21211220\n",
      "Iteration 1548, loss = 0.21200973\n",
      "Iteration 1549, loss = 0.21193427\n",
      "Iteration 1550, loss = 0.21187331\n",
      "Iteration 1551, loss = 0.21170444\n",
      "Iteration 1552, loss = 0.21163456\n",
      "Iteration 1553, loss = 0.21155930\n",
      "Iteration 1554, loss = 0.21145819\n",
      "Iteration 1555, loss = 0.21136643\n",
      "Iteration 1556, loss = 0.21125562\n",
      "Iteration 1557, loss = 0.21119239\n",
      "Iteration 1558, loss = 0.21108603\n",
      "Iteration 1559, loss = 0.21099022\n",
      "Iteration 1560, loss = 0.21094924\n",
      "Iteration 1561, loss = 0.21081109\n",
      "Iteration 1562, loss = 0.21072841\n",
      "Iteration 1563, loss = 0.21062188\n",
      "Iteration 1564, loss = 0.21067694\n",
      "Iteration 1565, loss = 0.21049993\n",
      "Iteration 1566, loss = 0.21036685\n",
      "Iteration 1567, loss = 0.21030334\n",
      "Iteration 1568, loss = 0.21020589\n",
      "Iteration 1569, loss = 0.21013093\n",
      "Iteration 1570, loss = 0.21003149\n",
      "Iteration 1571, loss = 0.20989650\n",
      "Iteration 1572, loss = 0.20981564\n",
      "Iteration 1573, loss = 0.20975912\n",
      "Iteration 1574, loss = 0.20966758\n",
      "Iteration 1575, loss = 0.20967753\n",
      "Iteration 1576, loss = 0.20947741\n",
      "Iteration 1577, loss = 0.20938844\n",
      "Iteration 1578, loss = 0.20931048\n",
      "Iteration 1579, loss = 0.20921067\n",
      "Iteration 1580, loss = 0.20913697\n",
      "Iteration 1581, loss = 0.20902193\n",
      "Iteration 1582, loss = 0.20892741\n",
      "Iteration 1583, loss = 0.20886879\n",
      "Iteration 1584, loss = 0.20877016\n",
      "Iteration 1585, loss = 0.20871675\n",
      "Iteration 1586, loss = 0.20858086\n",
      "Iteration 1587, loss = 0.20852257\n",
      "Iteration 1588, loss = 0.20841200\n",
      "Iteration 1589, loss = 0.20832330\n",
      "Iteration 1590, loss = 0.20826357\n",
      "Iteration 1591, loss = 0.20814912\n",
      "Iteration 1592, loss = 0.20805790\n",
      "Iteration 1593, loss = 0.20795862\n",
      "Iteration 1594, loss = 0.20788173\n",
      "Iteration 1595, loss = 0.20776780\n",
      "Iteration 1596, loss = 0.20773102\n",
      "Iteration 1597, loss = 0.20764764\n",
      "Iteration 1598, loss = 0.20754747\n",
      "Iteration 1599, loss = 0.20743480\n",
      "Iteration 1600, loss = 0.20736361\n",
      "Iteration 1601, loss = 0.20726548\n",
      "Iteration 1602, loss = 0.20718761\n",
      "Iteration 1603, loss = 0.20708543\n",
      "Iteration 1604, loss = 0.20697816\n",
      "Iteration 1605, loss = 0.20690553\n",
      "Iteration 1606, loss = 0.20682932\n",
      "Iteration 1607, loss = 0.20674670\n",
      "Iteration 1608, loss = 0.20668976\n",
      "Iteration 1609, loss = 0.20656342\n",
      "Iteration 1610, loss = 0.20651207\n",
      "Iteration 1611, loss = 0.20637301\n",
      "Iteration 1612, loss = 0.20630896\n",
      "Iteration 1613, loss = 0.20620691\n",
      "Iteration 1614, loss = 0.20616166\n",
      "Iteration 1615, loss = 0.20608780\n",
      "Iteration 1616, loss = 0.20598436\n",
      "Iteration 1617, loss = 0.20589402\n",
      "Iteration 1618, loss = 0.20581475\n",
      "Iteration 1619, loss = 0.20571126\n",
      "Iteration 1620, loss = 0.20562582\n",
      "Iteration 1621, loss = 0.20555191\n",
      "Iteration 1622, loss = 0.20541905\n",
      "Iteration 1623, loss = 0.20537018\n",
      "Iteration 1624, loss = 0.20523538\n",
      "Iteration 1625, loss = 0.20518501\n",
      "Iteration 1626, loss = 0.20506531\n",
      "Iteration 1627, loss = 0.20499120\n",
      "Iteration 1628, loss = 0.20492352\n",
      "Iteration 1629, loss = 0.20486635\n",
      "Iteration 1630, loss = 0.20476216\n",
      "Iteration 1631, loss = 0.20464465\n",
      "Iteration 1632, loss = 0.20460872\n",
      "Iteration 1633, loss = 0.20446469\n",
      "Iteration 1634, loss = 0.20443002\n",
      "Iteration 1635, loss = 0.20431811\n",
      "Iteration 1636, loss = 0.20425722\n",
      "Iteration 1637, loss = 0.20411687\n",
      "Iteration 1638, loss = 0.20405872\n",
      "Iteration 1639, loss = 0.20397042\n",
      "Iteration 1640, loss = 0.20385749\n",
      "Iteration 1641, loss = 0.20378424\n",
      "Iteration 1642, loss = 0.20374169\n",
      "Iteration 1643, loss = 0.20360920\n",
      "Iteration 1644, loss = 0.20353218\n",
      "Iteration 1645, loss = 0.20347035\n",
      "Iteration 1646, loss = 0.20334333\n",
      "Iteration 1647, loss = 0.20325609\n",
      "Iteration 1648, loss = 0.20317283\n",
      "Iteration 1649, loss = 0.20307819\n",
      "Iteration 1650, loss = 0.20301325\n",
      "Iteration 1651, loss = 0.20289845\n",
      "Iteration 1652, loss = 0.20293316\n",
      "Iteration 1653, loss = 0.20272653\n",
      "Iteration 1654, loss = 0.20264768\n",
      "Iteration 1655, loss = 0.20256937\n",
      "Iteration 1656, loss = 0.20246762\n",
      "Iteration 1657, loss = 0.20237562\n",
      "Iteration 1658, loss = 0.20229784\n",
      "Iteration 1659, loss = 0.20224719\n",
      "Iteration 1660, loss = 0.20210256\n",
      "Iteration 1661, loss = 0.20202736\n",
      "Iteration 1662, loss = 0.20194235\n",
      "Iteration 1663, loss = 0.20187603\n",
      "Iteration 1664, loss = 0.20174938\n",
      "Iteration 1665, loss = 0.20170489\n",
      "Iteration 1666, loss = 0.20158957\n",
      "Iteration 1667, loss = 0.20150343\n",
      "Iteration 1668, loss = 0.20146058\n",
      "Iteration 1669, loss = 0.20134880\n",
      "Iteration 1670, loss = 0.20124467\n",
      "Iteration 1671, loss = 0.20118541\n",
      "Iteration 1672, loss = 0.20109641\n",
      "Iteration 1673, loss = 0.20094943\n",
      "Iteration 1674, loss = 0.20085651\n",
      "Iteration 1675, loss = 0.20078849\n",
      "Iteration 1676, loss = 0.20072630\n",
      "Iteration 1677, loss = 0.20060578\n",
      "Iteration 1678, loss = 0.20051834\n",
      "Iteration 1679, loss = 0.20043712\n",
      "Iteration 1680, loss = 0.20034933\n",
      "Iteration 1681, loss = 0.20025096\n",
      "Iteration 1682, loss = 0.20019045\n",
      "Iteration 1683, loss = 0.20011102\n",
      "Iteration 1684, loss = 0.19997839\n",
      "Iteration 1685, loss = 0.19990119\n",
      "Iteration 1686, loss = 0.19982573\n",
      "Iteration 1687, loss = 0.19972288\n",
      "Iteration 1688, loss = 0.19965274\n",
      "Iteration 1689, loss = 0.19955420\n",
      "Iteration 1690, loss = 0.19944843\n",
      "Iteration 1691, loss = 0.19935342\n",
      "Iteration 1692, loss = 0.19934523\n",
      "Iteration 1693, loss = 0.19919403\n",
      "Iteration 1694, loss = 0.19913963\n",
      "Iteration 1695, loss = 0.19902417\n",
      "Iteration 1696, loss = 0.19892964\n",
      "Iteration 1697, loss = 0.19888886\n",
      "Iteration 1698, loss = 0.19877828\n",
      "Iteration 1699, loss = 0.19867087\n",
      "Iteration 1700, loss = 0.19859086\n",
      "Iteration 1701, loss = 0.19853050\n",
      "Iteration 1702, loss = 0.19841757\n",
      "Iteration 1703, loss = 0.19830645\n",
      "Iteration 1704, loss = 0.19828944\n",
      "Iteration 1705, loss = 0.19812917\n",
      "Iteration 1706, loss = 0.19813114\n",
      "Iteration 1707, loss = 0.19799562\n",
      "Iteration 1708, loss = 0.19788873\n",
      "Iteration 1709, loss = 0.19777312\n",
      "Iteration 1710, loss = 0.19774882\n",
      "Iteration 1711, loss = 0.19761566\n",
      "Iteration 1712, loss = 0.19762798\n",
      "Iteration 1713, loss = 0.19743113\n",
      "Iteration 1714, loss = 0.19738247\n",
      "Iteration 1715, loss = 0.19724491\n",
      "Iteration 1716, loss = 0.19717136\n",
      "Iteration 1717, loss = 0.19710383\n",
      "Iteration 1718, loss = 0.19700444\n",
      "Iteration 1719, loss = 0.19694347\n",
      "Iteration 1720, loss = 0.19683490\n",
      "Iteration 1721, loss = 0.19686699\n",
      "Iteration 1722, loss = 0.19664155\n",
      "Iteration 1723, loss = 0.19657216\n",
      "Iteration 1724, loss = 0.19646420\n",
      "Iteration 1725, loss = 0.19637786\n",
      "Iteration 1726, loss = 0.19630833\n",
      "Iteration 1727, loss = 0.19621174\n",
      "Iteration 1728, loss = 0.19612248\n",
      "Iteration 1729, loss = 0.19601190\n",
      "Iteration 1730, loss = 0.19597636\n",
      "Iteration 1731, loss = 0.19586761\n",
      "Iteration 1732, loss = 0.19576643\n",
      "Iteration 1733, loss = 0.19571404\n",
      "Iteration 1734, loss = 0.19564535\n",
      "Iteration 1735, loss = 0.19549900\n",
      "Iteration 1736, loss = 0.19547274\n",
      "Iteration 1737, loss = 0.19532870\n",
      "Iteration 1738, loss = 0.19523134\n",
      "Iteration 1739, loss = 0.19516554\n",
      "Iteration 1740, loss = 0.19506955\n",
      "Iteration 1741, loss = 0.19498122\n",
      "Iteration 1742, loss = 0.19486524\n",
      "Iteration 1743, loss = 0.19481059\n",
      "Iteration 1744, loss = 0.19468521\n",
      "Iteration 1745, loss = 0.19460601\n",
      "Iteration 1746, loss = 0.19456337\n",
      "Iteration 1747, loss = 0.19442341\n",
      "Iteration 1748, loss = 0.19432267\n",
      "Iteration 1749, loss = 0.19424533\n",
      "Iteration 1750, loss = 0.19418657\n",
      "Iteration 1751, loss = 0.19411299\n",
      "Iteration 1752, loss = 0.19398380\n",
      "Iteration 1753, loss = 0.19397192\n",
      "Iteration 1754, loss = 0.19381027\n",
      "Iteration 1755, loss = 0.19373152\n",
      "Iteration 1756, loss = 0.19365404\n",
      "Iteration 1757, loss = 0.19354022\n",
      "Iteration 1758, loss = 0.19347416\n",
      "Iteration 1759, loss = 0.19335341\n",
      "Iteration 1760, loss = 0.19324401\n",
      "Iteration 1761, loss = 0.19320456\n",
      "Iteration 1762, loss = 0.19313564\n",
      "Iteration 1763, loss = 0.19302238\n",
      "Iteration 1764, loss = 0.19289972\n",
      "Iteration 1765, loss = 0.19280743\n",
      "Iteration 1766, loss = 0.19277591\n",
      "Iteration 1767, loss = 0.19267965\n",
      "Iteration 1768, loss = 0.19254160\n",
      "Iteration 1769, loss = 0.19250748\n",
      "Iteration 1770, loss = 0.19239893\n",
      "Iteration 1771, loss = 0.19227546\n",
      "Iteration 1772, loss = 0.19219306\n",
      "Iteration 1773, loss = 0.19215080\n",
      "Iteration 1774, loss = 0.19202311\n",
      "Iteration 1775, loss = 0.19193949\n",
      "Iteration 1776, loss = 0.19189161\n",
      "Iteration 1777, loss = 0.19175607\n",
      "Iteration 1778, loss = 0.19164785\n",
      "Iteration 1779, loss = 0.19158080\n",
      "Iteration 1780, loss = 0.19150041\n",
      "Iteration 1781, loss = 0.19150096\n",
      "Iteration 1782, loss = 0.19131441\n",
      "Iteration 1783, loss = 0.19129770\n",
      "Iteration 1784, loss = 0.19112453\n",
      "Iteration 1785, loss = 0.19104683\n",
      "Iteration 1786, loss = 0.19097097\n",
      "Iteration 1787, loss = 0.19086504\n",
      "Iteration 1788, loss = 0.19075732\n",
      "Iteration 1789, loss = 0.19068877\n",
      "Iteration 1790, loss = 0.19059697\n",
      "Iteration 1791, loss = 0.19048514\n",
      "Iteration 1792, loss = 0.19040167\n",
      "Iteration 1793, loss = 0.19034615\n",
      "Iteration 1794, loss = 0.19026316\n",
      "Iteration 1795, loss = 0.19012615\n",
      "Iteration 1796, loss = 0.19008670\n",
      "Iteration 1797, loss = 0.19001972\n",
      "Iteration 1798, loss = 0.18991263\n",
      "Iteration 1799, loss = 0.18976220\n",
      "Iteration 1800, loss = 0.18970102\n",
      "Iteration 1801, loss = 0.18960611\n",
      "Iteration 1802, loss = 0.18954265\n",
      "Iteration 1803, loss = 0.18945941\n",
      "Iteration 1804, loss = 0.18935254\n",
      "Iteration 1805, loss = 0.18925687\n",
      "Iteration 1806, loss = 0.18925125\n",
      "Iteration 1807, loss = 0.18906296\n",
      "Iteration 1808, loss = 0.18899714\n",
      "Iteration 1809, loss = 0.18893529\n",
      "Iteration 1810, loss = 0.18883347\n",
      "Iteration 1811, loss = 0.18875006\n",
      "Iteration 1812, loss = 0.18862089\n",
      "Iteration 1813, loss = 0.18857259\n",
      "Iteration 1814, loss = 0.18849108\n",
      "Iteration 1815, loss = 0.18838574\n",
      "Iteration 1816, loss = 0.18835401\n",
      "Iteration 1817, loss = 0.18820513\n",
      "Iteration 1818, loss = 0.18810830\n",
      "Iteration 1819, loss = 0.18804378\n",
      "Iteration 1820, loss = 0.18801838\n",
      "Iteration 1821, loss = 0.18783629\n",
      "Iteration 1822, loss = 0.18775505\n",
      "Iteration 1823, loss = 0.18770027\n",
      "Iteration 1824, loss = 0.18759606\n",
      "Iteration 1825, loss = 0.18748353\n",
      "Iteration 1826, loss = 0.18745222\n",
      "Iteration 1827, loss = 0.18738756\n",
      "Iteration 1828, loss = 0.18728178\n",
      "Iteration 1829, loss = 0.18714776\n",
      "Iteration 1830, loss = 0.18710032\n",
      "Iteration 1831, loss = 0.18697330\n",
      "Iteration 1832, loss = 0.18693298\n",
      "Iteration 1833, loss = 0.18678555\n",
      "Iteration 1834, loss = 0.18673576\n",
      "Iteration 1835, loss = 0.18664301\n",
      "Iteration 1836, loss = 0.18657532\n",
      "Iteration 1837, loss = 0.18644176\n",
      "Iteration 1838, loss = 0.18634546\n",
      "Iteration 1839, loss = 0.18629161\n",
      "Iteration 1840, loss = 0.18618224\n",
      "Iteration 1841, loss = 0.18615948\n",
      "Iteration 1842, loss = 0.18605431\n",
      "Iteration 1843, loss = 0.18592391\n",
      "Iteration 1844, loss = 0.18581757\n",
      "Iteration 1845, loss = 0.18579725\n",
      "Iteration 1846, loss = 0.18573182\n",
      "Iteration 1847, loss = 0.18563589\n",
      "Iteration 1848, loss = 0.18548545\n",
      "Iteration 1849, loss = 0.18545468\n",
      "Iteration 1850, loss = 0.18532796\n",
      "Iteration 1851, loss = 0.18522129\n",
      "Iteration 1852, loss = 0.18514498\n",
      "Iteration 1853, loss = 0.18512203\n",
      "Iteration 1854, loss = 0.18496412\n",
      "Iteration 1855, loss = 0.18496534\n",
      "Iteration 1856, loss = 0.18478791\n",
      "Iteration 1857, loss = 0.18471794\n",
      "Iteration 1858, loss = 0.18465760\n",
      "Iteration 1859, loss = 0.18454845\n",
      "Iteration 1860, loss = 0.18447151\n",
      "Iteration 1861, loss = 0.18440001\n",
      "Iteration 1862, loss = 0.18427629\n",
      "Iteration 1863, loss = 0.18424139\n",
      "Iteration 1864, loss = 0.18418983\n",
      "Iteration 1865, loss = 0.18400425\n",
      "Iteration 1866, loss = 0.18393286\n",
      "Iteration 1867, loss = 0.18391597\n",
      "Iteration 1868, loss = 0.18376244\n",
      "Iteration 1869, loss = 0.18367304\n",
      "Iteration 1870, loss = 0.18357839\n",
      "Iteration 1871, loss = 0.18350662\n",
      "Iteration 1872, loss = 0.18341439\n",
      "Iteration 1873, loss = 0.18331303\n",
      "Iteration 1874, loss = 0.18324926\n",
      "Iteration 1875, loss = 0.18317171\n",
      "Iteration 1876, loss = 0.18304746\n",
      "Iteration 1877, loss = 0.18296474\n",
      "Iteration 1878, loss = 0.18288748\n",
      "Iteration 1879, loss = 0.18278849\n",
      "Iteration 1880, loss = 0.18271976\n",
      "Iteration 1881, loss = 0.18264353\n",
      "Iteration 1882, loss = 0.18256240\n",
      "Iteration 1883, loss = 0.18246118\n",
      "Iteration 1884, loss = 0.18236399\n",
      "Iteration 1885, loss = 0.18230024\n",
      "Iteration 1886, loss = 0.18220092\n",
      "Iteration 1887, loss = 0.18210461\n",
      "Iteration 1888, loss = 0.18200012\n",
      "Iteration 1889, loss = 0.18192995\n",
      "Iteration 1890, loss = 0.18184859\n",
      "Iteration 1891, loss = 0.18174696\n",
      "Iteration 1892, loss = 0.18178314\n",
      "Iteration 1893, loss = 0.18167487\n",
      "Iteration 1894, loss = 0.18150379\n",
      "Iteration 1895, loss = 0.18140181\n",
      "Iteration 1896, loss = 0.18133427\n",
      "Iteration 1897, loss = 0.18123882\n",
      "Iteration 1898, loss = 0.18114859\n",
      "Iteration 1899, loss = 0.18106299\n",
      "Iteration 1900, loss = 0.18095775\n",
      "Iteration 1901, loss = 0.18087233\n",
      "Iteration 1902, loss = 0.18080596\n",
      "Iteration 1903, loss = 0.18074299\n",
      "Iteration 1904, loss = 0.18061393\n",
      "Iteration 1905, loss = 0.18055756\n",
      "Iteration 1906, loss = 0.18044372\n",
      "Iteration 1907, loss = 0.18038999\n",
      "Iteration 1908, loss = 0.18027281\n",
      "Iteration 1909, loss = 0.18016803\n",
      "Iteration 1910, loss = 0.18009086\n",
      "Iteration 1911, loss = 0.18002720\n",
      "Iteration 1912, loss = 0.17990663\n",
      "Iteration 1913, loss = 0.17981435\n",
      "Iteration 1914, loss = 0.17975410\n",
      "Iteration 1915, loss = 0.17966038\n",
      "Iteration 1916, loss = 0.17953702\n",
      "Iteration 1917, loss = 0.17946720\n",
      "Iteration 1918, loss = 0.17937689\n",
      "Iteration 1919, loss = 0.17930028\n",
      "Iteration 1920, loss = 0.17924842\n",
      "Iteration 1921, loss = 0.17912369\n",
      "Iteration 1922, loss = 0.17904782\n",
      "Iteration 1923, loss = 0.17895208\n",
      "Iteration 1924, loss = 0.17893287\n",
      "Iteration 1925, loss = 0.17878152\n",
      "Iteration 1926, loss = 0.17872455\n",
      "Iteration 1927, loss = 0.17858923\n",
      "Iteration 1928, loss = 0.17854022\n",
      "Iteration 1929, loss = 0.17839388\n",
      "Iteration 1930, loss = 0.17839081\n",
      "Iteration 1931, loss = 0.17822898\n",
      "Iteration 1932, loss = 0.17817182\n",
      "Iteration 1933, loss = 0.17811061\n",
      "Iteration 1934, loss = 0.17795788\n",
      "Iteration 1935, loss = 0.17793241\n",
      "Iteration 1936, loss = 0.17779880\n",
      "Iteration 1937, loss = 0.17778418\n",
      "Iteration 1938, loss = 0.17763411\n",
      "Iteration 1939, loss = 0.17755819\n",
      "Iteration 1940, loss = 0.17746765\n",
      "Iteration 1941, loss = 0.17737446\n",
      "Iteration 1265, loss = 0.23265014\n",
      "Iteration 1266, loss = 0.23249841\n",
      "Iteration 1267, loss = 0.23236071\n",
      "Iteration 1268, loss = 0.23222324\n",
      "Iteration 1269, loss = 0.23209757\n",
      "Iteration 1270, loss = 0.23209173\n",
      "Iteration 1271, loss = 0.23179324\n",
      "Iteration 1272, loss = 0.23173069\n",
      "Iteration 1273, loss = 0.23154524\n",
      "Iteration 1274, loss = 0.23139328\n",
      "Iteration 1275, loss = 0.23129847\n",
      "Iteration 1276, loss = 0.23108212\n",
      "Iteration 1277, loss = 0.23102475\n",
      "Iteration 1278, loss = 0.23087795\n",
      "Iteration 1279, loss = 0.23068052\n",
      "Iteration 1280, loss = 0.23054301\n",
      "Iteration 1281, loss = 0.23038773\n",
      "Iteration 1282, loss = 0.23029409\n",
      "Iteration 1283, loss = 0.23010540\n",
      "Iteration 1284, loss = 0.23004003\n",
      "Iteration 1285, loss = 0.22984125\n",
      "Iteration 1286, loss = 0.22972990\n",
      "Iteration 1287, loss = 0.22961030\n",
      "Iteration 1288, loss = 0.22941366\n",
      "Iteration 1289, loss = 0.22932522\n",
      "Iteration 1290, loss = 0.22918546\n",
      "Iteration 1291, loss = 0.22902449\n",
      "Iteration 1292, loss = 0.22884880\n",
      "Iteration 1293, loss = 0.22876187\n",
      "Iteration 1294, loss = 0.22858820\n",
      "Iteration 1295, loss = 0.22849027\n",
      "Iteration 1296, loss = 0.22828064\n",
      "Iteration 1297, loss = 0.22817400\n",
      "Iteration 1298, loss = 0.22803918\n",
      "Iteration 1299, loss = 0.22790854\n",
      "Iteration 1300, loss = 0.22774608\n",
      "Iteration 1301, loss = 0.22758971\n",
      "Iteration 1302, loss = 0.22749407\n",
      "Iteration 1303, loss = 0.22733724\n",
      "Iteration 1304, loss = 0.22720856\n",
      "Iteration 1305, loss = 0.22703156\n",
      "Iteration 1306, loss = 0.22693733\n",
      "Iteration 1307, loss = 0.22678763\n",
      "Iteration 1308, loss = 0.22663032\n",
      "Iteration 1309, loss = 0.22650278\n",
      "Iteration 1310, loss = 0.22632859\n",
      "Iteration 1311, loss = 0.22624109\n",
      "Iteration 1312, loss = 0.22610016\n",
      "Iteration 1313, loss = 0.22597983\n",
      "Iteration 1314, loss = 0.22585378\n",
      "Iteration 1315, loss = 0.22568042\n",
      "Iteration 1316, loss = 0.22552923\n",
      "Iteration 1317, loss = 0.22539429\n",
      "Iteration 1318, loss = 0.22539990\n",
      "Iteration 1319, loss = 0.22516190\n",
      "Iteration 1320, loss = 0.22502819\n",
      "Iteration 1321, loss = 0.22483324\n",
      "Iteration 1322, loss = 0.22473565\n",
      "Iteration 1323, loss = 0.22458499\n",
      "Iteration 1324, loss = 0.22448355\n",
      "Iteration 1325, loss = 0.22431006\n",
      "Iteration 1326, loss = 0.22416484\n",
      "Iteration 1327, loss = 0.22402004\n",
      "Iteration 1328, loss = 0.22389645\n",
      "Iteration 1329, loss = 0.22373182\n",
      "Iteration 1330, loss = 0.22360170\n",
      "Iteration 1331, loss = 0.22349239\n",
      "Iteration 1332, loss = 0.22335042\n",
      "Iteration 1333, loss = 0.22327511\n",
      "Iteration 1334, loss = 0.22316388\n",
      "Iteration 1335, loss = 0.22295158\n",
      "Iteration 1336, loss = 0.22289580\n",
      "Iteration 1337, loss = 0.22266365\n",
      "Iteration 1338, loss = 0.22251916\n",
      "Iteration 1339, loss = 0.22240217\n",
      "Iteration 1340, loss = 0.22224771\n",
      "Iteration 1341, loss = 0.22220016\n",
      "Iteration 1342, loss = 0.22204880\n",
      "Iteration 1343, loss = 0.22188322\n",
      "Iteration 1344, loss = 0.22168760\n",
      "Iteration 1345, loss = 0.22163704\n",
      "Iteration 1346, loss = 0.22147205\n",
      "Iteration 1347, loss = 0.22130425\n",
      "Iteration 1348, loss = 0.22118426\n",
      "Iteration 1349, loss = 0.22105232\n",
      "Iteration 1350, loss = 0.22092377\n",
      "Iteration 1351, loss = 0.22075182\n",
      "Iteration 1352, loss = 0.22065046\n",
      "Iteration 1353, loss = 0.22049511\n",
      "Iteration 1354, loss = 0.22037363\n",
      "Iteration 1355, loss = 0.22022596\n",
      "Iteration 1356, loss = 0.22014482\n",
      "Iteration 1357, loss = 0.22002085\n",
      "Iteration 1358, loss = 0.21983649\n",
      "Iteration 1359, loss = 0.21968631\n",
      "Iteration 1360, loss = 0.21963688\n",
      "Iteration 1361, loss = 0.21939053\n",
      "Iteration 1362, loss = 0.21929144\n",
      "Iteration 1363, loss = 0.21915553\n",
      "Iteration 1364, loss = 0.21899870\n",
      "Iteration 1365, loss = 0.21882472\n",
      "Iteration 1366, loss = 0.21873957\n",
      "Iteration 1367, loss = 0.21858508\n",
      "Iteration 1368, loss = 0.21856182\n",
      "Iteration 1369, loss = 0.21832581\n",
      "Iteration 1370, loss = 0.21820019\n",
      "Iteration 1371, loss = 0.21807150\n",
      "Iteration 1372, loss = 0.21788974\n",
      "Iteration 1373, loss = 0.21786710\n",
      "Iteration 1374, loss = 0.21760156\n",
      "Iteration 1375, loss = 0.21748542\n",
      "Iteration 1376, loss = 0.21734990\n",
      "Iteration 1377, loss = 0.21720254\n",
      "Iteration 1378, loss = 0.21711616\n",
      "Iteration 1379, loss = 0.21702949\n",
      "Iteration 1380, loss = 0.21682069\n",
      "Iteration 1381, loss = 0.21668271\n",
      "Iteration 1382, loss = 0.21652365\n",
      "Iteration 1383, loss = 0.21637520\n",
      "Iteration 1384, loss = 0.21623485\n",
      "Iteration 1385, loss = 0.21612883\n",
      "Iteration 1386, loss = 0.21606398\n",
      "Iteration 1387, loss = 0.21592779\n",
      "Iteration 1388, loss = 0.21569072\n",
      "Iteration 1389, loss = 0.21555744\n",
      "Iteration 1390, loss = 0.21545042\n",
      "Iteration 1391, loss = 0.21549662\n",
      "Iteration 1392, loss = 0.21519705\n",
      "Iteration 1393, loss = 0.21502975\n",
      "Iteration 1394, loss = 0.21500618\n",
      "Iteration 1395, loss = 0.21479554\n",
      "Iteration 1396, loss = 0.21460807\n",
      "Iteration 1397, loss = 0.21449640\n",
      "Iteration 1398, loss = 0.21437967\n",
      "Iteration 1399, loss = 0.21425005\n",
      "Iteration 1400, loss = 0.21407046\n",
      "Iteration 1401, loss = 0.21392163\n",
      "Iteration 1402, loss = 0.21387805\n",
      "Iteration 1403, loss = 0.21371101\n",
      "Iteration 1404, loss = 0.21352146\n",
      "Iteration 1405, loss = 0.21347265\n",
      "Iteration 1406, loss = 0.21323237\n",
      "Iteration 1407, loss = 0.21310285\n",
      "Iteration 1408, loss = 0.21297561\n",
      "Iteration 1409, loss = 0.21285422\n",
      "Iteration 1410, loss = 0.21268424\n",
      "Iteration 1411, loss = 0.21260463\n",
      "Iteration 1412, loss = 0.21245810\n",
      "Iteration 1413, loss = 0.21235830\n",
      "Iteration 1414, loss = 0.21222097\n",
      "Iteration 1415, loss = 0.21204361\n",
      "Iteration 1416, loss = 0.21186374\n",
      "Iteration 1417, loss = 0.21183209\n",
      "Iteration 1418, loss = 0.21168206\n",
      "Iteration 1419, loss = 0.21159875\n",
      "Iteration 1420, loss = 0.21136168\n",
      "Iteration 1421, loss = 0.21127595\n",
      "Iteration 1422, loss = 0.21107190\n",
      "Iteration 1423, loss = 0.21104126\n",
      "Iteration 1424, loss = 0.21081238\n",
      "Iteration 1425, loss = 0.21071007\n",
      "Iteration 1426, loss = 0.21074025\n",
      "Iteration 1427, loss = 0.21041643\n",
      "Iteration 1428, loss = 0.21027723\n",
      "Iteration 1429, loss = 0.21017931\n",
      "Iteration 1430, loss = 0.21004898\n",
      "Iteration 1431, loss = 0.20988432\n",
      "Iteration 1432, loss = 0.20975690\n",
      "Iteration 1433, loss = 0.20967508\n",
      "Iteration 1434, loss = 0.20944549\n",
      "Iteration 1435, loss = 0.20938222\n",
      "Iteration 1436, loss = 0.20918566\n",
      "Iteration 1437, loss = 0.20911239\n",
      "Iteration 1438, loss = 0.20894165\n",
      "Iteration 1439, loss = 0.20881174\n",
      "Iteration 1440, loss = 0.20872619\n",
      "Iteration 1441, loss = 0.20851575\n",
      "Iteration 1442, loss = 0.20841067\n",
      "Iteration 1443, loss = 0.20825011\n",
      "Iteration 1444, loss = 0.20812810\n",
      "Iteration 1445, loss = 0.20801386\n",
      "Iteration 1446, loss = 0.20790528\n",
      "Iteration 1447, loss = 0.20776887\n",
      "Iteration 1448, loss = 0.20757420\n",
      "Iteration 1449, loss = 0.20749455\n",
      "Iteration 1450, loss = 0.20731448\n",
      "Iteration 1451, loss = 0.20718526\n",
      "Iteration 1452, loss = 0.20703343\n",
      "Iteration 1453, loss = 0.20692036\n",
      "Iteration 1454, loss = 0.20680163\n",
      "Iteration 1455, loss = 0.20667723\n",
      "Iteration 1456, loss = 0.20648905\n",
      "Iteration 1457, loss = 0.20655970\n",
      "Iteration 1458, loss = 0.20629165\n",
      "Iteration 1459, loss = 0.20615962\n",
      "Iteration 1460, loss = 0.20604864\n",
      "Iteration 1461, loss = 0.20588965\n",
      "Iteration 1462, loss = 0.20576232\n",
      "Iteration 1463, loss = 0.20562993\n",
      "Iteration 1464, loss = 0.20550812\n",
      "Iteration 1465, loss = 0.20532239\n",
      "Iteration 1466, loss = 0.20532520\n",
      "Iteration 1467, loss = 0.20510231\n",
      "Iteration 1468, loss = 0.20493769\n",
      "Iteration 1469, loss = 0.20481397\n",
      "Iteration 1470, loss = 0.20467236\n",
      "Iteration 1471, loss = 0.20453354\n",
      "Iteration 1472, loss = 0.20441398\n",
      "Iteration 1473, loss = 0.20427733\n",
      "Iteration 1474, loss = 0.20411072\n",
      "Iteration 1475, loss = 0.20400560\n",
      "Iteration 1476, loss = 0.20390032\n",
      "Iteration 1477, loss = 0.20375612\n",
      "Iteration 1478, loss = 0.20361159\n",
      "Iteration 1479, loss = 0.20351980\n",
      "Iteration 1480, loss = 0.20338037\n",
      "Iteration 1481, loss = 0.20323000\n",
      "Iteration 1482, loss = 0.20308605\n",
      "Iteration 1483, loss = 0.20295775\n",
      "Iteration 1484, loss = 0.20287685\n",
      "Iteration 1485, loss = 0.20281536\n",
      "Iteration 1486, loss = 0.20270774\n",
      "Iteration 1487, loss = 0.20242910\n",
      "Iteration 1488, loss = 0.20230317\n",
      "Iteration 1489, loss = 0.20220500\n",
      "Iteration 1490, loss = 0.20209507\n",
      "Iteration 1491, loss = 0.20193894\n",
      "Iteration 1492, loss = 0.20177802\n",
      "Iteration 1493, loss = 0.20170474\n",
      "Iteration 1494, loss = 0.20155593\n",
      "Iteration 1495, loss = 0.20137008\n",
      "Iteration 1496, loss = 0.20126474\n",
      "Iteration 1497, loss = 0.20111586\n",
      "Iteration 1498, loss = 0.20102195\n",
      "Iteration 1499, loss = 0.20095724\n",
      "Iteration 1500, loss = 0.20079721\n",
      "Iteration 1501, loss = 0.20062537\n",
      "Iteration 1502, loss = 0.20049723\n",
      "Iteration 1503, loss = 0.20035558\n",
      "Iteration 1504, loss = 0.20024499\n",
      "Iteration 1505, loss = 0.20008522\n",
      "Iteration 1506, loss = 0.20002016\n",
      "Iteration 1507, loss = 0.19986422\n",
      "Iteration 1508, loss = 0.19970267\n",
      "Iteration 1509, loss = 0.19960495\n",
      "Iteration 1510, loss = 0.19957658\n",
      "Iteration 1511, loss = 0.19934484\n",
      "Iteration 1512, loss = 0.19923403\n",
      "Iteration 1513, loss = 0.19912377\n",
      "Iteration 1514, loss = 0.19896500\n",
      "Iteration 1515, loss = 0.19882708\n",
      "Iteration 1516, loss = 0.19867073\n",
      "Iteration 1517, loss = 0.19855444\n",
      "Iteration 1518, loss = 0.19850495\n",
      "Iteration 1519, loss = 0.19834652\n",
      "Iteration 1520, loss = 0.19816959\n",
      "Iteration 1521, loss = 0.19801499\n",
      "Iteration 1522, loss = 0.19787796\n",
      "Iteration 1523, loss = 0.19778063\n",
      "Iteration 1524, loss = 0.19761916\n",
      "Iteration 1525, loss = 0.19749749\n",
      "Iteration 1526, loss = 0.19736161\n",
      "Iteration 1527, loss = 0.19733219\n",
      "Iteration 1528, loss = 0.19709713\n",
      "Iteration 1529, loss = 0.19696254\n",
      "Iteration 1530, loss = 0.19686920\n",
      "Iteration 1531, loss = 0.19680035\n",
      "Iteration 1532, loss = 0.19658782\n",
      "Iteration 1533, loss = 0.19648144\n",
      "Iteration 1534, loss = 0.19637097\n",
      "Iteration 1535, loss = 0.19620396\n",
      "Iteration 1536, loss = 0.19617102\n",
      "Iteration 1537, loss = 0.19592738\n",
      "Iteration 1538, loss = 0.19582094\n",
      "Iteration 1539, loss = 0.19586700\n",
      "Iteration 1540, loss = 0.19558713\n",
      "Iteration 1541, loss = 0.19538308\n",
      "Iteration 1542, loss = 0.19528486\n",
      "Iteration 1543, loss = 0.19517179\n",
      "Iteration 1544, loss = 0.19505558\n",
      "Iteration 1545, loss = 0.19488482\n",
      "Iteration 1546, loss = 0.19484756\n",
      "Iteration 1547, loss = 0.19466236\n",
      "Iteration 1548, loss = 0.19452585\n",
      "Iteration 1549, loss = 0.19437925\n",
      "Iteration 1550, loss = 0.19425518\n",
      "Iteration 1551, loss = 0.19420194\n",
      "Iteration 1552, loss = 0.19399362\n",
      "Iteration 1553, loss = 0.19384378\n",
      "Iteration 1554, loss = 0.19373942\n",
      "Iteration 1555, loss = 0.19363774\n",
      "Iteration 1556, loss = 0.19348787\n",
      "Iteration 1557, loss = 0.19335839\n",
      "Iteration 1558, loss = 0.19321488\n",
      "Iteration 1559, loss = 0.19309986\n",
      "Iteration 1560, loss = 0.19299900\n",
      "Iteration 1561, loss = 0.19285134\n",
      "Iteration 1562, loss = 0.19273566\n",
      "Iteration 1563, loss = 0.19255783\n",
      "Iteration 1564, loss = 0.19245260\n",
      "Iteration 1565, loss = 0.19235049\n",
      "Iteration 1566, loss = 0.19224289\n",
      "Iteration 1567, loss = 0.19205249\n",
      "Iteration 1568, loss = 0.19195459\n",
      "Iteration 1569, loss = 0.19186776\n",
      "Iteration 1570, loss = 0.19167941\n",
      "Iteration 1571, loss = 0.19154676\n",
      "Iteration 1572, loss = 0.19153129\n",
      "Iteration 1573, loss = 0.19126396\n",
      "Iteration 1574, loss = 0.19118068\n",
      "Iteration 1575, loss = 0.19105868\n",
      "Iteration 1576, loss = 0.19089863\n",
      "Iteration 1577, loss = 0.19091581\n",
      "Iteration 1578, loss = 0.19068689\n",
      "Iteration 1579, loss = 0.19061651\n",
      "Iteration 1580, loss = 0.19039231\n",
      "Iteration 1581, loss = 0.19027249\n",
      "Iteration 1582, loss = 0.19019593\n",
      "Iteration 1583, loss = 0.18997879\n",
      "Iteration 1584, loss = 0.18991218\n",
      "Iteration 1585, loss = 0.18977078\n",
      "Iteration 1586, loss = 0.18958877\n",
      "Iteration 1587, loss = 0.18947115\n",
      "Iteration 1588, loss = 0.18939333\n",
      "Iteration 1589, loss = 0.18923102\n",
      "Iteration 1590, loss = 0.18907573\n",
      "Iteration 1591, loss = 0.18903889\n",
      "Iteration 1592, loss = 0.18883997\n",
      "Iteration 1593, loss = 0.18867737\n",
      "Iteration 1594, loss = 0.18855946\n",
      "Iteration 1595, loss = 0.18844993\n",
      "Iteration 1596, loss = 0.18835612\n",
      "Iteration 1597, loss = 0.18830877\n",
      "Iteration 1598, loss = 0.18806063\n",
      "Iteration 1599, loss = 0.18797852\n",
      "Iteration 1600, loss = 0.18784743\n",
      "Iteration 1601, loss = 0.18768537\n",
      "Iteration 1602, loss = 0.18755582\n",
      "Iteration 1603, loss = 0.18745289\n",
      "Iteration 1604, loss = 0.18733906\n",
      "Iteration 1605, loss = 0.18737453\n",
      "Iteration 1606, loss = 0.18706542\n",
      "Iteration 1607, loss = 0.18717322\n",
      "Iteration 1608, loss = 0.18687656\n",
      "Iteration 1609, loss = 0.18680492\n",
      "Iteration 1610, loss = 0.18658039\n",
      "Iteration 1611, loss = 0.18641479\n",
      "Iteration 1612, loss = 0.18627933\n",
      "Iteration 1613, loss = 0.18615195\n",
      "Iteration 1614, loss = 0.18604568\n",
      "Iteration 1615, loss = 0.18588155\n",
      "Iteration 1616, loss = 0.18576997\n",
      "Iteration 1617, loss = 0.18570411\n",
      "Iteration 1618, loss = 0.18554272\n",
      "Iteration 1619, loss = 0.18539505\n",
      "Iteration 1620, loss = 0.18527844\n",
      "Iteration 1621, loss = 0.18516976\n",
      "Iteration 1622, loss = 0.18511089\n",
      "Iteration 1623, loss = 0.18490119\n",
      "Iteration 1624, loss = 0.18479482\n",
      "Iteration 1625, loss = 0.18472389\n",
      "Iteration 1626, loss = 0.18450623\n",
      "Iteration 1627, loss = 0.18444798\n",
      "Iteration 1628, loss = 0.18431763\n",
      "Iteration 1629, loss = 0.18413277\n",
      "Iteration 1630, loss = 0.18403935\n",
      "Iteration 1631, loss = 0.18390424\n",
      "Iteration 1632, loss = 0.18377901\n",
      "Iteration 1633, loss = 0.18362784\n",
      "Iteration 1634, loss = 0.18353472\n",
      "Iteration 1635, loss = 0.18341719\n",
      "Iteration 1636, loss = 0.18324089\n",
      "Iteration 1637, loss = 0.18313908\n",
      "Iteration 1638, loss = 0.18299161\n",
      "Iteration 1639, loss = 0.18288338\n",
      "Iteration 1640, loss = 0.18275924\n",
      "Iteration 1641, loss = 0.18266578\n",
      "Iteration 1642, loss = 0.18253262\n",
      "Iteration 1643, loss = 0.18240074\n",
      "Iteration 1644, loss = 0.18224060\n",
      "Iteration 1645, loss = 0.18217823\n",
      "Iteration 1646, loss = 0.18209106\n",
      "Iteration 1647, loss = 0.18190558\n",
      "Iteration 1648, loss = 0.18172463\n",
      "Iteration 1649, loss = 0.18165276\n",
      "Iteration 1650, loss = 0.18151210\n",
      "Iteration 1651, loss = 0.18138742\n",
      "Iteration 1652, loss = 0.18126035\n",
      "Iteration 1653, loss = 0.18109573\n",
      "Iteration 1654, loss = 0.18102448\n",
      "Iteration 1655, loss = 0.18089591\n",
      "Iteration 1656, loss = 0.18080250\n",
      "Iteration 1657, loss = 0.18067343\n",
      "Iteration 1658, loss = 0.18061089\n",
      "Iteration 1659, loss = 0.18038590\n",
      "Iteration 1660, loss = 0.18028534\n",
      "Iteration 1661, loss = 0.18028240\n",
      "Iteration 1662, loss = 0.18008104\n",
      "Iteration 1663, loss = 0.17990223\n",
      "Iteration 1664, loss = 0.17977048\n",
      "Iteration 1665, loss = 0.17966897\n",
      "Iteration 1666, loss = 0.17952712\n",
      "Iteration 1667, loss = 0.17943897\n",
      "Iteration 1668, loss = 0.17926309\n",
      "Iteration 1669, loss = 0.17910963\n",
      "Iteration 1670, loss = 0.17902370\n",
      "Iteration 1671, loss = 0.17894392\n",
      "Iteration 1672, loss = 0.17876277\n",
      "Iteration 1673, loss = 0.17862677\n",
      "Iteration 1674, loss = 0.17857040\n",
      "Iteration 1675, loss = 0.17847863\n",
      "Iteration 1676, loss = 0.17828463\n",
      "Iteration 1677, loss = 0.17810126\n",
      "Iteration 1678, loss = 0.17801138\n",
      "Iteration 1679, loss = 0.17798368\n",
      "Iteration 1680, loss = 0.17775339\n",
      "Iteration 1681, loss = 0.17763943\n",
      "Iteration 1682, loss = 0.17748442\n",
      "Iteration 1683, loss = 0.17733903\n",
      "Iteration 1684, loss = 0.17720875\n",
      "Iteration 1685, loss = 0.17707302\n",
      "Iteration 1686, loss = 0.17703948\n",
      "Iteration 1687, loss = 0.17687684\n",
      "Iteration 1688, loss = 0.17672506\n",
      "Iteration 1689, loss = 0.17658405\n",
      "Iteration 1690, loss = 0.17648941\n",
      "Iteration 1691, loss = 0.17631383\n",
      "Iteration 1692, loss = 0.17620374\n",
      "Iteration 1693, loss = 0.17606014\n",
      "Iteration 1694, loss = 0.17594044\n",
      "Iteration 1695, loss = 0.17580709\n",
      "Iteration 1696, loss = 0.17569463\n",
      "Iteration 1697, loss = 0.17554172\n",
      "Iteration 1698, loss = 0.17545939\n",
      "Iteration 1699, loss = 0.17532045\n",
      "Iteration 1700, loss = 0.17517146\n",
      "Iteration 1701, loss = 0.17503377\n",
      "Iteration 1702, loss = 0.17491214\n",
      "Iteration 1703, loss = 0.17485326\n",
      "Iteration 1704, loss = 0.17465812\n",
      "Iteration 1705, loss = 0.17457734\n",
      "Iteration 1706, loss = 0.17440003\n",
      "Iteration 1707, loss = 0.17430198\n",
      "Iteration 1708, loss = 0.17418181\n",
      "Iteration 1709, loss = 0.17409342\n",
      "Iteration 1710, loss = 0.17400492\n",
      "Iteration 1711, loss = 0.17391303\n",
      "Iteration 1712, loss = 0.17369562\n",
      "Iteration 1713, loss = 0.17356909\n",
      "Iteration 1714, loss = 0.17350542\n",
      "Iteration 1715, loss = 0.17342073\n",
      "Iteration 1716, loss = 0.17320137\n",
      "Iteration 1717, loss = 0.17313420\n",
      "Iteration 1718, loss = 0.17293925\n",
      "Iteration 1719, loss = 0.17279835\n",
      "Iteration 1720, loss = 0.17278587\n",
      "Iteration 1721, loss = 0.17258094\n",
      "Iteration 1722, loss = 0.17245613\n",
      "Iteration 1723, loss = 0.17246685\n",
      "Iteration 1724, loss = 0.17223581\n",
      "Iteration 1725, loss = 0.17207327\n",
      "Iteration 1726, loss = 0.17193186\n",
      "Iteration 1727, loss = 0.17181211\n",
      "Iteration 1728, loss = 0.17173207\n",
      "Iteration 1729, loss = 0.17161373\n",
      "Iteration 1730, loss = 0.17146568\n",
      "Iteration 1731, loss = 0.17138583\n",
      "Iteration 1732, loss = 0.17118735\n",
      "Iteration 1733, loss = 0.17107750\n",
      "Iteration 1734, loss = 0.17094762\n",
      "Iteration 1735, loss = 0.17083053\n",
      "Iteration 1736, loss = 0.17067817\n",
      "Iteration 1737, loss = 0.17060214\n",
      "Iteration 1738, loss = 0.17046766\n",
      "Iteration 1739, loss = 0.17032858\n",
      "Iteration 1740, loss = 0.17021404\n",
      "Iteration 1741, loss = 0.17011336\n",
      "Iteration 1742, loss = 0.16999021\n",
      "Iteration 1743, loss = 0.16987536\n",
      "Iteration 1744, loss = 0.16977859\n",
      "Iteration 1794, loss = 0.18939873\n",
      "Iteration 1795, loss = 0.18939286\n",
      "Iteration 1796, loss = 0.18926944\n",
      "Iteration 1797, loss = 0.18908209\n",
      "Iteration 1798, loss = 0.18898108\n",
      "Iteration 1799, loss = 0.18884661\n",
      "Iteration 1800, loss = 0.18874957\n",
      "Iteration 1801, loss = 0.18863063\n",
      "Iteration 1802, loss = 0.18849637\n",
      "Iteration 1803, loss = 0.18837665\n",
      "Iteration 1804, loss = 0.18838796\n",
      "Iteration 1805, loss = 0.18816518\n",
      "Iteration 1806, loss = 0.18803087\n",
      "Iteration 1807, loss = 0.18792839\n",
      "Iteration 1808, loss = 0.18786860\n",
      "Iteration 1809, loss = 0.18783650\n",
      "Iteration 1810, loss = 0.18765767\n",
      "Iteration 1811, loss = 0.18753687\n",
      "Iteration 1812, loss = 0.18740851\n",
      "Iteration 1813, loss = 0.18727935\n",
      "Iteration 1814, loss = 0.18722190\n",
      "Iteration 1815, loss = 0.18703361\n",
      "Iteration 1816, loss = 0.18694709\n",
      "Iteration 1817, loss = 0.18690283\n",
      "Iteration 1818, loss = 0.18674256\n",
      "Iteration 1819, loss = 0.18661542\n",
      "Iteration 1820, loss = 0.18652642\n",
      "Iteration 1821, loss = 0.18637285\n",
      "Iteration 1822, loss = 0.18631253\n",
      "Iteration 1823, loss = 0.18617490\n",
      "Iteration 1824, loss = 0.18610782\n",
      "Iteration 1825, loss = 0.18598088\n",
      "Iteration 1826, loss = 0.18598302\n",
      "Iteration 1827, loss = 0.18576462\n",
      "Iteration 1828, loss = 0.18561711\n",
      "Iteration 1829, loss = 0.18547488\n",
      "Iteration 1830, loss = 0.18550853\n",
      "Iteration 1831, loss = 0.18526374\n",
      "Iteration 1832, loss = 0.18521683\n",
      "Iteration 1833, loss = 0.18506893\n",
      "Iteration 1834, loss = 0.18490162\n",
      "Iteration 1835, loss = 0.18481886\n",
      "Iteration 1836, loss = 0.18469023\n",
      "Iteration 1837, loss = 0.18457291\n",
      "Iteration 1838, loss = 0.18447991\n",
      "Iteration 1839, loss = 0.18440350\n",
      "Iteration 1840, loss = 0.18429922\n",
      "Iteration 1841, loss = 0.18416484\n",
      "Iteration 1842, loss = 0.18405307\n",
      "Iteration 1843, loss = 0.18396367\n",
      "Iteration 1844, loss = 0.18383323\n",
      "Iteration 1845, loss = 0.18378584\n",
      "Iteration 1846, loss = 0.18369611\n",
      "Iteration 1847, loss = 0.18353207\n",
      "Iteration 1848, loss = 0.18337488\n",
      "Iteration 1849, loss = 0.18327465\n",
      "Iteration 1850, loss = 0.18328903\n",
      "Iteration 1851, loss = 0.18304521\n",
      "Iteration 1852, loss = 0.18296912\n",
      "Iteration 1853, loss = 0.18284591\n",
      "Iteration 1854, loss = 0.18276316\n",
      "Iteration 1855, loss = 0.18262164\n",
      "Iteration 1856, loss = 0.18252905\n",
      "Iteration 1857, loss = 0.18236987\n",
      "Iteration 1858, loss = 0.18223935\n",
      "Iteration 1859, loss = 0.18213474\n",
      "Iteration 1860, loss = 0.18209376\n",
      "Iteration 1861, loss = 0.18191634\n",
      "Iteration 1862, loss = 0.18181934\n",
      "Iteration 1863, loss = 0.18174301\n",
      "Iteration 1864, loss = 0.18164544\n",
      "Iteration 1865, loss = 0.18155272\n",
      "Iteration 1866, loss = 0.18141753\n",
      "Iteration 1867, loss = 0.18128869\n",
      "Iteration 1868, loss = 0.18129948\n",
      "Iteration 1869, loss = 0.18106494\n",
      "Iteration 1870, loss = 0.18105848\n",
      "Iteration 1871, loss = 0.18084055\n",
      "Iteration 1872, loss = 0.18077221\n",
      "Iteration 1873, loss = 0.18075343\n",
      "Iteration 1874, loss = 0.18054520\n",
      "Iteration 1875, loss = 0.18047359\n",
      "Iteration 1876, loss = 0.18030699\n",
      "Iteration 1877, loss = 0.18020472\n",
      "Iteration 1878, loss = 0.18005675\n",
      "Iteration 1879, loss = 0.17997618\n",
      "Iteration 1880, loss = 0.17985086\n",
      "Iteration 1881, loss = 0.17980499\n",
      "Iteration 1882, loss = 0.17964107\n",
      "Iteration 1883, loss = 0.17961934\n",
      "Iteration 1884, loss = 0.17943114\n",
      "Iteration 1885, loss = 0.17946437\n",
      "Iteration 1886, loss = 0.17920961\n",
      "Iteration 1887, loss = 0.17910117\n",
      "Iteration 1888, loss = 0.17897901\n",
      "Iteration 1889, loss = 0.17887176\n",
      "Iteration 1890, loss = 0.17882844\n",
      "Iteration 1891, loss = 0.17864851\n",
      "Iteration 1892, loss = 0.17853108\n",
      "Iteration 1893, loss = 0.17844095\n",
      "Iteration 1894, loss = 0.17832278\n",
      "Iteration 1895, loss = 0.17822052\n",
      "Iteration 1896, loss = 0.17809503\n",
      "Iteration 1897, loss = 0.17800790\n",
      "Iteration 1898, loss = 0.17794641\n",
      "Iteration 1899, loss = 0.17776205\n",
      "Iteration 1900, loss = 0.17763858\n",
      "Iteration 1901, loss = 0.17752302\n",
      "Iteration 1902, loss = 0.17742797\n",
      "Iteration 1903, loss = 0.17730704\n",
      "Iteration 1904, loss = 0.17719599\n",
      "Iteration 1905, loss = 0.17708993\n",
      "Iteration 1906, loss = 0.17697497\n",
      "Iteration 1907, loss = 0.17694998\n",
      "Iteration 1908, loss = 0.17675883\n",
      "Iteration 1909, loss = 0.17665448\n",
      "Iteration 1910, loss = 0.17658887\n",
      "Iteration 1911, loss = 0.17643466\n",
      "Iteration 1912, loss = 0.17633490\n",
      "Iteration 1913, loss = 0.17617097\n",
      "Iteration 1914, loss = 0.17612704\n",
      "Iteration 1915, loss = 0.17599269\n",
      "Iteration 1916, loss = 0.17593018\n",
      "Iteration 1917, loss = 0.17575178\n",
      "Iteration 1918, loss = 0.17574359\n",
      "Iteration 1919, loss = 0.17567800\n",
      "Iteration 1920, loss = 0.17542342\n",
      "Iteration 1921, loss = 0.17534123\n",
      "Iteration 1922, loss = 0.17521617\n",
      "Iteration 1923, loss = 0.17518923\n",
      "Iteration 1924, loss = 0.17504439\n",
      "Iteration 1925, loss = 0.17488525\n",
      "Iteration 1926, loss = 0.17476255\n",
      "Iteration 1927, loss = 0.17466098\n",
      "Iteration 1928, loss = 0.17460933\n",
      "Iteration 1929, loss = 0.17446268\n",
      "Iteration 1930, loss = 0.17440968\n",
      "Iteration 1931, loss = 0.17446513\n",
      "Iteration 1932, loss = 0.17422770\n",
      "Iteration 1933, loss = 0.17408234\n",
      "Iteration 1934, loss = 0.17390747\n",
      "Iteration 1935, loss = 0.17380575\n",
      "Iteration 1936, loss = 0.17374432\n",
      "Iteration 1937, loss = 0.17357534\n",
      "Iteration 1938, loss = 0.17354018\n",
      "Iteration 1939, loss = 0.17336601\n",
      "Iteration 1940, loss = 0.17323902\n",
      "Iteration 1941, loss = 0.17310731\n",
      "Iteration 1942, loss = 0.17301175\n",
      "Iteration 1943, loss = 0.17290000\n",
      "Iteration 1944, loss = 0.17294918\n",
      "Iteration 1945, loss = 0.17270394\n",
      "Iteration 1946, loss = 0.17259948\n",
      "Iteration 1947, loss = 0.17246362\n",
      "Iteration 1948, loss = 0.17235198\n",
      "Iteration 1949, loss = 0.17227587\n",
      "Iteration 1950, loss = 0.17213296\n",
      "Iteration 1951, loss = 0.17204548\n",
      "Iteration 1952, loss = 0.17191342\n",
      "Iteration 1953, loss = 0.17182551\n",
      "Iteration 1954, loss = 0.17171164\n",
      "Iteration 1955, loss = 0.17163397\n",
      "Iteration 1956, loss = 0.17149580\n",
      "Iteration 1957, loss = 0.17140982\n",
      "Iteration 1958, loss = 0.17122792\n",
      "Iteration 1959, loss = 0.17113182\n",
      "Iteration 1960, loss = 0.17108357\n",
      "Iteration 1961, loss = 0.17095608\n",
      "Iteration 1962, loss = 0.17085392\n",
      "Iteration 1963, loss = 0.17071639\n",
      "Iteration 1964, loss = 0.17067575\n",
      "Iteration 1965, loss = 0.17049222\n",
      "Iteration 1966, loss = 0.17041756\n",
      "Iteration 1967, loss = 0.17027405\n",
      "Iteration 1968, loss = 0.17018855\n",
      "Iteration 1969, loss = 0.17010126\n",
      "Iteration 1970, loss = 0.16998614\n",
      "Iteration 1971, loss = 0.16990464\n",
      "Iteration 1972, loss = 0.16985873\n",
      "Iteration 1973, loss = 0.16967281\n",
      "Iteration 1974, loss = 0.16963562\n",
      "Iteration 1975, loss = 0.16945739\n",
      "Iteration 1976, loss = 0.16934960\n",
      "Iteration 1977, loss = 0.16924671\n",
      "Iteration 1978, loss = 0.16913675\n",
      "Iteration 1979, loss = 0.16901630\n",
      "Iteration 1980, loss = 0.16901500\n",
      "Iteration 1981, loss = 0.16880062\n",
      "Iteration 1982, loss = 0.16869770\n",
      "Iteration 1983, loss = 0.16856879\n",
      "Iteration 1984, loss = 0.16855715\n",
      "Iteration 1985, loss = 0.16842147\n",
      "Iteration 1986, loss = 0.16825977\n",
      "Iteration 1987, loss = 0.16813886\n",
      "Iteration 1988, loss = 0.16804476\n",
      "Iteration 1989, loss = 0.16797159\n",
      "Iteration 1990, loss = 0.16784947\n",
      "Iteration 1991, loss = 0.16779426\n",
      "Iteration 1992, loss = 0.16760238\n",
      "Iteration 1993, loss = 0.16750055\n",
      "Iteration 1994, loss = 0.16739639\n",
      "Iteration 1995, loss = 0.16729094\n",
      "Iteration 1996, loss = 0.16720372\n",
      "Iteration 1997, loss = 0.16710812\n",
      "Iteration 1998, loss = 0.16697130\n",
      "Iteration 1999, loss = 0.16686942\n",
      "Iteration 2000, loss = 0.16685658\n",
      "Iteration 2001, loss = 0.16664309\n",
      "Iteration 2002, loss = 0.16676135\n",
      "Iteration 2003, loss = 0.16647553\n",
      "Iteration 2004, loss = 0.16635005\n",
      "Iteration 2005, loss = 0.16625251\n",
      "Iteration 2006, loss = 0.16616876\n",
      "Iteration 2007, loss = 0.16600819\n",
      "Iteration 2008, loss = 0.16602579\n",
      "Iteration 2009, loss = 0.16588229\n",
      "Iteration 2010, loss = 0.16582977\n",
      "Iteration 2011, loss = 0.16562274\n",
      "Iteration 2012, loss = 0.16554126\n",
      "Iteration 2013, loss = 0.16541771\n",
      "Iteration 2014, loss = 0.16537352\n",
      "Iteration 2015, loss = 0.16520126\n",
      "Iteration 2016, loss = 0.16512328\n",
      "Iteration 2017, loss = 0.16500728\n",
      "Iteration 2018, loss = 0.16497568\n",
      "Iteration 2019, loss = 0.16478076\n",
      "Iteration 2020, loss = 0.16469525\n",
      "Iteration 2021, loss = 0.16461441\n",
      "Iteration 2022, loss = 0.16453168\n",
      "Iteration 2023, loss = 0.16437852\n",
      "Iteration 2024, loss = 0.16431140\n",
      "Iteration 2025, loss = 0.16420410\n",
      "Iteration 2026, loss = 0.16409967\n",
      "Iteration 2027, loss = 0.16397088\n",
      "Iteration 2028, loss = 0.16390677\n",
      "Iteration 2029, loss = 0.16377962\n",
      "Iteration 2030, loss = 0.16373714\n",
      "Iteration 2031, loss = 0.16368168\n",
      "Iteration 2032, loss = 0.16346349\n",
      "Iteration 2033, loss = 0.16336399\n",
      "Iteration 2034, loss = 0.16332339\n",
      "Iteration 2035, loss = 0.16316075\n",
      "Iteration 2036, loss = 0.16307723\n",
      "Iteration 2037, loss = 0.16297371\n",
      "Iteration 2038, loss = 0.16293222\n",
      "Iteration 2039, loss = 0.16275338\n",
      "Iteration 2040, loss = 0.16266745\n",
      "Iteration 2041, loss = 0.16258643\n",
      "Iteration 2042, loss = 0.16248391\n",
      "Iteration 2043, loss = 0.16240599\n",
      "Iteration 2044, loss = 0.16230364\n",
      "Iteration 2045, loss = 0.16223985\n",
      "Iteration 2046, loss = 0.16215711\n",
      "Iteration 2047, loss = 0.16193690\n",
      "Iteration 2048, loss = 0.16188442\n",
      "Iteration 2049, loss = 0.16172966\n",
      "Iteration 2050, loss = 0.16168001\n",
      "Iteration 2051, loss = 0.16160098\n",
      "Iteration 2052, loss = 0.16153093\n",
      "Iteration 2053, loss = 0.16134728\n",
      "Iteration 2054, loss = 0.16127903\n",
      "Iteration 2055, loss = 0.16116672\n",
      "Iteration 2056, loss = 0.16103489\n",
      "Iteration 2057, loss = 0.16099983\n",
      "Iteration 2058, loss = 0.16084319\n",
      "Iteration 2059, loss = 0.16078013\n",
      "Iteration 2060, loss = 0.16064249\n",
      "Iteration 2061, loss = 0.16050194\n",
      "Iteration 2062, loss = 0.16045842\n",
      "Iteration 2063, loss = 0.16033864\n",
      "Iteration 2064, loss = 0.16024135\n",
      "Iteration 2065, loss = 0.16015463\n",
      "Iteration 2066, loss = 0.16006926\n",
      "Iteration 2067, loss = 0.15994341\n",
      "Iteration 2068, loss = 0.15985242\n",
      "Iteration 2069, loss = 0.15974851\n",
      "Iteration 2070, loss = 0.15960857\n",
      "Iteration 2071, loss = 0.15954747\n",
      "Iteration 2072, loss = 0.15941530\n",
      "Iteration 2073, loss = 0.15934982\n",
      "Iteration 2074, loss = 0.15925003\n",
      "Iteration 2075, loss = 0.15910050\n",
      "Iteration 2076, loss = 0.15903251\n",
      "Iteration 2077, loss = 0.15894655\n",
      "Iteration 2078, loss = 0.15881201\n",
      "Iteration 2079, loss = 0.15872336\n",
      "Iteration 2080, loss = 0.15859008\n",
      "Iteration 2081, loss = 0.15858563\n",
      "Iteration 2082, loss = 0.15841542\n",
      "Iteration 2083, loss = 0.15829136\n",
      "Iteration 2084, loss = 0.15823512\n",
      "Iteration 2085, loss = 0.15811568\n",
      "Iteration 2086, loss = 0.15801845\n",
      "Iteration 2087, loss = 0.15796022\n",
      "Iteration 2088, loss = 0.15778243\n",
      "Iteration 2089, loss = 0.15773651\n",
      "Iteration 2090, loss = 0.15759186\n",
      "Iteration 2091, loss = 0.15753432\n",
      "Iteration 2092, loss = 0.15740152\n",
      "Iteration 2093, loss = 0.15727711\n",
      "Iteration 2094, loss = 0.15717258\n",
      "Iteration 2095, loss = 0.15715119\n",
      "Iteration 2096, loss = 0.15708828\n",
      "Iteration 2097, loss = 0.15690452\n",
      "Iteration 2098, loss = 0.15676032\n",
      "Iteration 2099, loss = 0.15669104\n",
      "Iteration 2100, loss = 0.15660095\n",
      "Iteration 2101, loss = 0.15651074\n",
      "Iteration 2102, loss = 0.15639792\n",
      "Iteration 2103, loss = 0.15630551\n",
      "Iteration 2104, loss = 0.15620371\n",
      "Iteration 2105, loss = 0.15621186\n",
      "Iteration 2106, loss = 0.15608425\n",
      "Iteration 2107, loss = 0.15591724\n",
      "Iteration 2108, loss = 0.15623561\n",
      "Iteration 2109, loss = 0.15570513\n",
      "Iteration 2110, loss = 0.15560292\n",
      "Iteration 2111, loss = 0.15552504\n",
      "Iteration 2112, loss = 0.15541453\n",
      "Iteration 2113, loss = 0.15530494\n",
      "Iteration 2114, loss = 0.15519178\n",
      "Iteration 2115, loss = 0.15508330\n",
      "Iteration 2116, loss = 0.15499355\n",
      "Iteration 2117, loss = 0.15493211\n",
      "Iteration 2118, loss = 0.15479910\n",
      "Iteration 2119, loss = 0.15470948\n",
      "Iteration 2120, loss = 0.15459661\n",
      "Iteration 2121, loss = 0.15451593\n",
      "Iteration 2122, loss = 0.15438615\n",
      "Iteration 2123, loss = 0.15429614\n",
      "Iteration 2124, loss = 0.15419119\n",
      "Iteration 2125, loss = 0.15416126\n",
      "Iteration 2126, loss = 0.15399049\n",
      "Iteration 2127, loss = 0.15387977\n",
      "Iteration 2128, loss = 0.15392087\n",
      "Iteration 2129, loss = 0.15376063\n",
      "Iteration 2130, loss = 0.15372042\n",
      "Iteration 2131, loss = 0.15349154\n",
      "Iteration 2132, loss = 0.15343833\n",
      "Iteration 2133, loss = 0.15328619\n",
      "Iteration 2134, loss = 0.15319831\n",
      "Iteration 2135, loss = 0.15306559\n",
      "Iteration 2136, loss = 0.15302695\n",
      "Iteration 2137, loss = 0.15293943\n",
      "Iteration 2138, loss = 0.15281670\n",
      "Iteration 2139, loss = 0.15272013\n",
      "Iteration 2140, loss = 0.15256928\n",
      "Iteration 2141, loss = 0.15249403\n",
      "Iteration 2142, loss = 0.15238389\n",
      "Iteration 2143, loss = 0.15226051\n",
      "Iteration 2144, loss = 0.15224487\n",
      "Iteration 2145, loss = 0.15215058\n",
      "Iteration 2146, loss = 0.15195349\n",
      "Iteration 2147, loss = 0.15186778\n",
      "Iteration 2148, loss = 0.15178217\n",
      "Iteration 2149, loss = 0.15171653\n",
      "Iteration 2150, loss = 0.15161929\n",
      "Iteration 2151, loss = 0.15147362\n",
      "Iteration 2152, loss = 0.15139014\n",
      "Iteration 2153, loss = 0.15134344\n",
      "Iteration 2154, loss = 0.15124047\n",
      "Iteration 2155, loss = 0.15105739\n",
      "Iteration 2156, loss = 0.15095143\n",
      "Iteration 2157, loss = 0.15099264\n",
      "Iteration 2158, loss = 0.15095583\n",
      "Iteration 2159, loss = 0.15069662\n",
      "Iteration 2160, loss = 0.15063573\n",
      "Iteration 2161, loss = 0.15052476\n",
      "Iteration 2162, loss = 0.15043870\n",
      "Iteration 2163, loss = 0.15029626\n",
      "Iteration 2164, loss = 0.15020707\n",
      "Iteration 2165, loss = 0.15013357\n",
      "Iteration 2166, loss = 0.14999913\n",
      "Iteration 2167, loss = 0.14989075\n",
      "Iteration 2168, loss = 0.14978774\n",
      "Iteration 2169, loss = 0.14971637\n",
      "Iteration 2170, loss = 0.14957436\n",
      "Iteration 2171, loss = 0.14949035\n",
      "Iteration 2172, loss = 0.14938373\n",
      "Iteration 2173, loss = 0.14928651\n",
      "Iteration 2174, loss = 0.14919019\n",
      "Iteration 2175, loss = 0.14917832\n",
      "Iteration 2176, loss = 0.14910190\n",
      "Iteration 2177, loss = 0.14894094\n",
      "Iteration 2178, loss = 0.14881287\n",
      "Iteration 2179, loss = 0.14880181\n",
      "Iteration 2180, loss = 0.14856239\n",
      "Iteration 2181, loss = 0.14851746\n",
      "Iteration 2182, loss = 0.14844425\n",
      "Iteration 2183, loss = 0.14828512\n",
      "Iteration 2184, loss = 0.14822649\n",
      "Iteration 2185, loss = 0.14812663\n",
      "Iteration 2186, loss = 0.14800508\n",
      "Iteration 2187, loss = 0.14791019\n",
      "Iteration 2188, loss = 0.14780660\n",
      "Iteration 2189, loss = 0.14778774\n",
      "Iteration 2190, loss = 0.14764500\n",
      "Iteration 2191, loss = 0.14753387\n",
      "Iteration 2192, loss = 0.14741240\n",
      "Iteration 2193, loss = 0.14733935\n",
      "Iteration 2194, loss = 0.14723468\n",
      "Iteration 2195, loss = 0.14717075\n",
      "Iteration 2196, loss = 0.14711105\n",
      "Iteration 2197, loss = 0.14700617\n",
      "Iteration 2198, loss = 0.14684736\n",
      "Iteration 2199, loss = 0.14676752\n",
      "Iteration 2200, loss = 0.14670381\n",
      "Iteration 2201, loss = 0.14657882\n",
      "Iteration 2202, loss = 0.14652500\n",
      "Iteration 2203, loss = 0.14641454\n",
      "Iteration 2204, loss = 0.14627515\n",
      "Iteration 2205, loss = 0.14621331\n",
      "Iteration 2206, loss = 0.14610978\n",
      "Iteration 2207, loss = 0.14602275\n",
      "Iteration 2208, loss = 0.14589515\n",
      "Iteration 2209, loss = 0.14581481\n",
      "Iteration 2210, loss = 0.14567834\n",
      "Iteration 2211, loss = 0.14559804\n",
      "Iteration 2212, loss = 0.14562957\n",
      "Iteration 2213, loss = 0.14541591\n",
      "Iteration 2214, loss = 0.14539937\n",
      "Iteration 2215, loss = 0.14531289\n",
      "Iteration 2216, loss = 0.14514025\n",
      "Iteration 2217, loss = 0.14508055\n",
      "Iteration 2218, loss = 0.14494879\n",
      "Iteration 2219, loss = 0.14488322\n",
      "Iteration 2220, loss = 0.14477333\n",
      "Iteration 2221, loss = 0.14465023\n",
      "Iteration 2222, loss = 0.14458340\n",
      "Iteration 2223, loss = 0.14446232\n",
      "Iteration 2224, loss = 0.14435651\n",
      "Iteration 2225, loss = 0.14426214\n",
      "Iteration 2226, loss = 0.14417419\n",
      "Iteration 2227, loss = 0.14405608\n",
      "Iteration 2228, loss = 0.14398023\n",
      "Iteration 2229, loss = 0.14391737\n",
      "Iteration 2230, loss = 0.14388343\n",
      "Iteration 2231, loss = 0.14389021\n",
      "Iteration 2232, loss = 0.14359591\n",
      "Iteration 2233, loss = 0.14354375\n",
      "Iteration 2234, loss = 0.14336728\n",
      "Iteration 2235, loss = 0.14333167\n",
      "Iteration 2236, loss = 0.14320860\n",
      "Iteration 2237, loss = 0.14309319\n",
      "Iteration 2238, loss = 0.14305976\n",
      "Iteration 2239, loss = 0.14296932\n",
      "Iteration 2240, loss = 0.14289469\n",
      "Iteration 2241, loss = 0.14270697\n",
      "Iteration 2242, loss = 0.14262419\n",
      "Iteration 2243, loss = 0.14251607\n",
      "Iteration 2244, loss = 0.14246778\n",
      "Iteration 2245, loss = 0.14238778\n",
      "Iteration 2246, loss = 0.14248159\n",
      "Iteration 2247, loss = 0.14212329\n",
      "Iteration 2248, loss = 0.14213366\n",
      "Iteration 2249, loss = 0.14204358\n",
      "Iteration 2250, loss = 0.14187214\n",
      "Iteration 2251, loss = 0.14178517\n",
      "Iteration 2252, loss = 0.14179773\n",
      "Iteration 2253, loss = 0.14162100\n",
      "Iteration 2254, loss = 0.14153001\n",
      "Iteration 2255, loss = 0.14141406\n",
      "Iteration 2256, loss = 0.14131561\n",
      "Iteration 2257, loss = 0.14120317\n",
      "Iteration 2258, loss = 0.14110093\n",
      "Iteration 2259, loss = 0.14101939\n",
      "Iteration 2260, loss = 0.14099540\n",
      "Iteration 2261, loss = 0.14093392\n",
      "Iteration 2262, loss = 0.14072696\n",
      "Iteration 2263, loss = 0.14064622\n",
      "Iteration 2264, loss = 0.14054074\n",
      "Iteration 2265, loss = 0.14050005\n",
      "Iteration 2266, loss = 0.14037127\n",
      "Iteration 2267, loss = 0.14030556\n",
      "Iteration 2268, loss = 0.14018776\n",
      "Iteration 2269, loss = 0.14010257\n",
      "Iteration 2270, loss = 0.14005632\n",
      "Iteration 2271, loss = 0.13991795\n",
      "Iteration 2272, loss = 0.13995451\n",
      "Iteration 2273, loss = 0.13975477\n",
      "Iteration 1942, loss = 0.17733365\n",
      "Iteration 1943, loss = 0.17731920\n",
      "Iteration 1944, loss = 0.17711943\n",
      "Iteration 1945, loss = 0.17699712\n",
      "Iteration 1946, loss = 0.17692990\n",
      "Iteration 1947, loss = 0.17686672\n",
      "Iteration 1948, loss = 0.17674409\n",
      "Iteration 1949, loss = 0.17664981\n",
      "Iteration 1950, loss = 0.17659900\n",
      "Iteration 1951, loss = 0.17656698\n",
      "Iteration 1952, loss = 0.17641545\n",
      "Iteration 1953, loss = 0.17633159\n",
      "Iteration 1954, loss = 0.17626396\n",
      "Iteration 1955, loss = 0.17622560\n",
      "Iteration 1956, loss = 0.17607724\n",
      "Iteration 1957, loss = 0.17604031\n",
      "Iteration 1958, loss = 0.17586342\n",
      "Iteration 1959, loss = 0.17589377\n",
      "Iteration 1960, loss = 0.17572062\n",
      "Iteration 1961, loss = 0.17564554\n",
      "Iteration 1962, loss = 0.17555941\n",
      "Iteration 1963, loss = 0.17551934\n",
      "Iteration 1964, loss = 0.17539209\n",
      "Iteration 1965, loss = 0.17527869\n",
      "Iteration 1966, loss = 0.17520569\n",
      "Iteration 1967, loss = 0.17513371\n",
      "Iteration 1968, loss = 0.17501751\n",
      "Iteration 1969, loss = 0.17495571\n",
      "Iteration 1970, loss = 0.17483987\n",
      "Iteration 1971, loss = 0.17491209\n",
      "Iteration 1972, loss = 0.17466415\n",
      "Iteration 1973, loss = 0.17466871\n",
      "Iteration 1974, loss = 0.17457985\n",
      "Iteration 1975, loss = 0.17446470\n",
      "Iteration 1976, loss = 0.17432466\n",
      "Iteration 1977, loss = 0.17426196\n",
      "Iteration 1978, loss = 0.17420204\n",
      "Iteration 1979, loss = 0.17414484\n",
      "Iteration 1980, loss = 0.17406288\n",
      "Iteration 1981, loss = 0.17389705\n",
      "Iteration 1982, loss = 0.17386073\n",
      "Iteration 1983, loss = 0.17384463\n",
      "Iteration 1984, loss = 0.17365035\n",
      "Iteration 1985, loss = 0.17357334\n",
      "Iteration 1986, loss = 0.17347431\n",
      "Iteration 1987, loss = 0.17351469\n",
      "Iteration 1988, loss = 0.17333000\n",
      "Iteration 1989, loss = 0.17321908\n",
      "Iteration 1990, loss = 0.17317903\n",
      "Iteration 1991, loss = 0.17307012\n",
      "Iteration 1992, loss = 0.17304842\n",
      "Iteration 1993, loss = 0.17291350\n",
      "Iteration 1994, loss = 0.17279947\n",
      "Iteration 1995, loss = 0.17277148\n",
      "Iteration 1996, loss = 0.17274950\n",
      "Iteration 1997, loss = 0.17259131\n",
      "Iteration 1998, loss = 0.17249066\n",
      "Iteration 1999, loss = 0.17239372\n",
      "Iteration 2000, loss = 0.17230643\n",
      "Iteration 2001, loss = 0.17222966\n",
      "Iteration 2002, loss = 0.17213602\n",
      "Iteration 2003, loss = 0.17219782\n",
      "Iteration 2004, loss = 0.17196037\n",
      "Iteration 2005, loss = 0.17191917\n",
      "Iteration 2006, loss = 0.17185638\n",
      "Iteration 2007, loss = 0.17171830\n",
      "Iteration 2008, loss = 0.17170870\n",
      "Iteration 2009, loss = 0.17156557\n",
      "Iteration 2010, loss = 0.17161649\n",
      "Iteration 2011, loss = 0.17137739\n",
      "Iteration 2012, loss = 0.17137972\n",
      "Iteration 2013, loss = 0.17130574\n",
      "Iteration 2014, loss = 0.17118969\n",
      "Iteration 2015, loss = 0.17112840\n",
      "Iteration 2016, loss = 0.17107827\n",
      "Iteration 2017, loss = 0.17090788\n",
      "Iteration 2018, loss = 0.17088147\n",
      "Iteration 2019, loss = 0.17071443\n",
      "Iteration 2020, loss = 0.17069727\n",
      "Iteration 2021, loss = 0.17057591\n",
      "Iteration 2022, loss = 0.17047445\n",
      "Iteration 2023, loss = 0.17041460\n",
      "Iteration 2024, loss = 0.17030772\n",
      "Iteration 2025, loss = 0.17023307\n",
      "Iteration 2026, loss = 0.17015211\n",
      "Iteration 2027, loss = 0.17007857\n",
      "Iteration 2028, loss = 0.17006253\n",
      "Iteration 2029, loss = 0.16990560\n",
      "Iteration 2030, loss = 0.16990221\n",
      "Iteration 2031, loss = 0.16993789\n",
      "Iteration 2032, loss = 0.16966024\n",
      "Iteration 2033, loss = 0.16961943\n",
      "Iteration 2034, loss = 0.16949583\n",
      "Iteration 2035, loss = 0.16947304\n",
      "Iteration 2036, loss = 0.16936009\n",
      "Iteration 2037, loss = 0.16932329\n",
      "Iteration 2038, loss = 0.16920151\n",
      "Iteration 2039, loss = 0.16912247\n",
      "Iteration 2040, loss = 0.16903215\n",
      "Iteration 2041, loss = 0.16908690\n",
      "Iteration 2042, loss = 0.16887290\n",
      "Iteration 2043, loss = 0.16880226\n",
      "Iteration 2044, loss = 0.16874804\n",
      "Iteration 2045, loss = 0.16861065\n",
      "Iteration 2046, loss = 0.16854775\n",
      "Iteration 2047, loss = 0.16844855\n",
      "Iteration 2048, loss = 0.16836001\n",
      "Iteration 2049, loss = 0.16827998\n",
      "Iteration 2050, loss = 0.16826424\n",
      "Iteration 2051, loss = 0.16813628\n",
      "Iteration 2052, loss = 0.16806536\n",
      "Iteration 2053, loss = 0.16800804\n",
      "Iteration 2054, loss = 0.16789044\n",
      "Iteration 2055, loss = 0.16782339\n",
      "Iteration 2056, loss = 0.16773125\n",
      "Iteration 2057, loss = 0.16765770\n",
      "Iteration 2058, loss = 0.16767343\n",
      "Iteration 2059, loss = 0.16752044\n",
      "Iteration 2060, loss = 0.16742113\n",
      "Iteration 2061, loss = 0.16732918\n",
      "Iteration 2062, loss = 0.16733034\n",
      "Iteration 2063, loss = 0.16716877\n",
      "Iteration 2064, loss = 0.16712781\n",
      "Iteration 2065, loss = 0.16736392\n",
      "Iteration 2066, loss = 0.16694477\n",
      "Iteration 2067, loss = 0.16684938\n",
      "Iteration 2068, loss = 0.16683905\n",
      "Iteration 2069, loss = 0.16673295\n",
      "Iteration 2070, loss = 0.16665624\n",
      "Iteration 2071, loss = 0.16659124\n",
      "Iteration 2072, loss = 0.16652579\n",
      "Iteration 2073, loss = 0.16642825\n",
      "Iteration 2074, loss = 0.16642579\n",
      "Iteration 2075, loss = 0.16627551\n",
      "Iteration 2076, loss = 0.16619562\n",
      "Iteration 2077, loss = 0.16614123\n",
      "Iteration 2078, loss = 0.16598385\n",
      "Iteration 2079, loss = 0.16594529\n",
      "Iteration 2080, loss = 0.16585667\n",
      "Iteration 2081, loss = 0.16580734\n",
      "Iteration 2082, loss = 0.16573807\n",
      "Iteration 2083, loss = 0.16567524\n",
      "Iteration 2084, loss = 0.16553603\n",
      "Iteration 2085, loss = 0.16546263\n",
      "Iteration 2086, loss = 0.16539339\n",
      "Iteration 2087, loss = 0.16530462\n",
      "Iteration 2088, loss = 0.16523454\n",
      "Iteration 2089, loss = 0.16520911\n",
      "Iteration 2090, loss = 0.16507208\n",
      "Iteration 2091, loss = 0.16500271\n",
      "Iteration 2092, loss = 0.16493072\n",
      "Iteration 2093, loss = 0.16484039\n",
      "Iteration 2094, loss = 0.16480239\n",
      "Iteration 2095, loss = 0.16470540\n",
      "Iteration 2096, loss = 0.16463791\n",
      "Iteration 2097, loss = 0.16456716\n",
      "Iteration 2098, loss = 0.16442809\n",
      "Iteration 2099, loss = 0.16433469\n",
      "Iteration 2100, loss = 0.16441474\n",
      "Iteration 2101, loss = 0.16422033\n",
      "Iteration 2102, loss = 0.16411677\n",
      "Iteration 2103, loss = 0.16416896\n",
      "Iteration 2104, loss = 0.16402294\n",
      "Iteration 2105, loss = 0.16387132\n",
      "Iteration 2106, loss = 0.16380878\n",
      "Iteration 2107, loss = 0.16374105\n",
      "Iteration 2108, loss = 0.16366881\n",
      "Iteration 2109, loss = 0.16361983\n",
      "Iteration 2110, loss = 0.16353867\n",
      "Iteration 2111, loss = 0.16347302\n",
      "Iteration 2112, loss = 0.16333007\n",
      "Iteration 2113, loss = 0.16329646\n",
      "Iteration 2114, loss = 0.16325336\n",
      "Iteration 2115, loss = 0.16314808\n",
      "Iteration 2116, loss = 0.16308966\n",
      "Iteration 2117, loss = 0.16301018\n",
      "Iteration 2118, loss = 0.16291199\n",
      "Iteration 2119, loss = 0.16281017\n",
      "Iteration 2120, loss = 0.16275682\n",
      "Iteration 2121, loss = 0.16269866\n",
      "Iteration 2122, loss = 0.16261985\n",
      "Iteration 2123, loss = 0.16248740\n",
      "Iteration 2124, loss = 0.16247973\n",
      "Iteration 2125, loss = 0.16235454\n",
      "Iteration 2126, loss = 0.16229874\n",
      "Iteration 2127, loss = 0.16219696\n",
      "Iteration 2128, loss = 0.16235645\n",
      "Iteration 2129, loss = 0.16203073\n",
      "Iteration 2130, loss = 0.16198402\n",
      "Iteration 2131, loss = 0.16189453\n",
      "Iteration 2132, loss = 0.16186910\n",
      "Iteration 2133, loss = 0.16186307\n",
      "Iteration 2134, loss = 0.16171972\n",
      "Iteration 2135, loss = 0.16165906\n",
      "Iteration 2136, loss = 0.16150769\n",
      "Iteration 2137, loss = 0.16152846\n",
      "Iteration 2138, loss = 0.16138179\n",
      "Iteration 2139, loss = 0.16128729\n",
      "Iteration 2140, loss = 0.16122488\n",
      "Iteration 2141, loss = 0.16114663\n",
      "Iteration 2142, loss = 0.16106257\n",
      "Iteration 2143, loss = 0.16101952\n",
      "Iteration 2144, loss = 0.16090976\n",
      "Iteration 2145, loss = 0.16082577\n",
      "Iteration 2146, loss = 0.16075149\n",
      "Iteration 2147, loss = 0.16074664\n",
      "Iteration 2148, loss = 0.16068792\n",
      "Iteration 2149, loss = 0.16054096\n",
      "Iteration 2150, loss = 0.16045041\n",
      "Iteration 2151, loss = 0.16035230\n",
      "Iteration 2152, loss = 0.16031433\n",
      "Iteration 2153, loss = 0.16018937\n",
      "Iteration 2154, loss = 0.16012463\n",
      "Iteration 2155, loss = 0.16014435\n",
      "Iteration 2156, loss = 0.15999871\n",
      "Iteration 2157, loss = 0.15993599\n",
      "Iteration 2158, loss = 0.15983163\n",
      "Iteration 2159, loss = 0.15976593\n",
      "Iteration 2160, loss = 0.15969085\n",
      "Iteration 2161, loss = 0.15965045\n",
      "Iteration 2162, loss = 0.15958293\n",
      "Iteration 2163, loss = 0.15954161\n",
      "Iteration 2164, loss = 0.15944437\n",
      "Iteration 2165, loss = 0.15941471\n",
      "Iteration 2166, loss = 0.15929626\n",
      "Iteration 2167, loss = 0.15915722\n",
      "Iteration 2168, loss = 0.15914931\n",
      "Iteration 2169, loss = 0.15913979\n",
      "Iteration 2170, loss = 0.15900749\n",
      "Iteration 2171, loss = 0.15894466\n",
      "Iteration 2172, loss = 0.15882171\n",
      "Iteration 2173, loss = 0.15873825\n",
      "Iteration 2174, loss = 0.15870065\n",
      "Iteration 2175, loss = 0.15856653\n",
      "Iteration 2176, loss = 0.15855162\n",
      "Iteration 2177, loss = 0.15844065\n",
      "Iteration 2178, loss = 0.15847264\n",
      "Iteration 2179, loss = 0.15830778\n",
      "Iteration 2180, loss = 0.15822149\n",
      "Iteration 2181, loss = 0.15812794\n",
      "Iteration 2182, loss = 0.15807198\n",
      "Iteration 2183, loss = 0.15804227\n",
      "Iteration 2184, loss = 0.15792100\n",
      "Iteration 2185, loss = 0.15788917\n",
      "Iteration 2186, loss = 0.15785409\n",
      "Iteration 2187, loss = 0.15771444\n",
      "Iteration 2188, loss = 0.15770430\n",
      "Iteration 2189, loss = 0.15758248\n",
      "Iteration 2190, loss = 0.15745995\n",
      "Iteration 2191, loss = 0.15743099\n",
      "Iteration 2192, loss = 0.15736023\n",
      "Iteration 2193, loss = 0.15726117\n",
      "Iteration 2194, loss = 0.15717121\n",
      "Iteration 2195, loss = 0.15719081\n",
      "Iteration 2196, loss = 0.15703739\n",
      "Iteration 2197, loss = 0.15706748\n",
      "Iteration 2198, loss = 0.15690825\n",
      "Iteration 2199, loss = 0.15684106\n",
      "Iteration 2200, loss = 0.15672916\n",
      "Iteration 2201, loss = 0.15669561\n",
      "Iteration 2202, loss = 0.15665344\n",
      "Iteration 2203, loss = 0.15653774\n",
      "Iteration 2204, loss = 0.15644654\n",
      "Iteration 2205, loss = 0.15637843\n",
      "Iteration 2206, loss = 0.15630046\n",
      "Iteration 2207, loss = 0.15625106\n",
      "Iteration 2208, loss = 0.15615861\n",
      "Iteration 2209, loss = 0.15613705\n",
      "Iteration 2210, loss = 0.15600236\n",
      "Iteration 2211, loss = 0.15602387\n",
      "Iteration 2212, loss = 0.15586505\n",
      "Iteration 2213, loss = 0.15581335\n",
      "Iteration 2214, loss = 0.15577057\n",
      "Iteration 2215, loss = 0.15572323\n",
      "Iteration 2216, loss = 0.15560336\n",
      "Iteration 2217, loss = 0.15548792\n",
      "Iteration 2218, loss = 0.15543105\n",
      "Iteration 2219, loss = 0.15540964\n",
      "Iteration 2220, loss = 0.15528721\n",
      "Iteration 2221, loss = 0.15525947\n",
      "Iteration 2222, loss = 0.15514533\n",
      "Iteration 2223, loss = 0.15513758\n",
      "Iteration 2224, loss = 0.15500566\n",
      "Iteration 2225, loss = 0.15488735\n",
      "Iteration 2226, loss = 0.15482787\n",
      "Iteration 2227, loss = 0.15477133\n",
      "Iteration 2228, loss = 0.15475103\n",
      "Iteration 2229, loss = 0.15463408\n",
      "Iteration 2230, loss = 0.15459952\n",
      "Iteration 2231, loss = 0.15445765\n",
      "Iteration 2232, loss = 0.15447055\n",
      "Iteration 2233, loss = 0.15431708\n",
      "Iteration 2234, loss = 0.15430389\n",
      "Iteration 2235, loss = 0.15423843\n",
      "Iteration 2236, loss = 0.15417851\n",
      "Iteration 2237, loss = 0.15417708\n",
      "Iteration 2238, loss = 0.15404496\n",
      "Iteration 2239, loss = 0.15392127\n",
      "Iteration 2240, loss = 0.15381622\n",
      "Iteration 2241, loss = 0.15375500\n",
      "Iteration 2242, loss = 0.15364980\n",
      "Iteration 2243, loss = 0.15370803\n",
      "Iteration 2244, loss = 0.15352483\n",
      "Iteration 2245, loss = 0.15344571\n",
      "Iteration 2246, loss = 0.15336759\n",
      "Iteration 2247, loss = 0.15331550\n",
      "Iteration 2248, loss = 0.15337285\n",
      "Iteration 2249, loss = 0.15318593\n",
      "Iteration 2250, loss = 0.15313971\n",
      "Iteration 2251, loss = 0.15307027\n",
      "Iteration 2252, loss = 0.15300935\n",
      "Iteration 2253, loss = 0.15291603\n",
      "Iteration 2254, loss = 0.15283447\n",
      "Iteration 2255, loss = 0.15274260\n",
      "Iteration 2256, loss = 0.15273379\n",
      "Iteration 2257, loss = 0.15262116\n",
      "Iteration 2258, loss = 0.15256008\n",
      "Iteration 2259, loss = 0.15243750\n",
      "Iteration 2260, loss = 0.15245679\n",
      "Iteration 2261, loss = 0.15234541\n",
      "Iteration 2262, loss = 0.15225510\n",
      "Iteration 2263, loss = 0.15215529\n",
      "Iteration 2264, loss = 0.15210118\n",
      "Iteration 2265, loss = 0.15207369\n",
      "Iteration 2266, loss = 0.15199975\n",
      "Iteration 2267, loss = 0.15199065\n",
      "Iteration 2268, loss = 0.15193956\n",
      "Iteration 2269, loss = 0.15184714\n",
      "Iteration 2270, loss = 0.15168137\n",
      "Iteration 2271, loss = 0.15160400\n",
      "Iteration 2272, loss = 0.15157773\n",
      "Iteration 2273, loss = 0.15144977\n",
      "Iteration 2274, loss = 0.15140667\n",
      "Iteration 2275, loss = 0.15130156\n",
      "Iteration 2276, loss = 0.15125691\n",
      "Iteration 2277, loss = 0.15119772\n",
      "Iteration 2278, loss = 0.15109434\n",
      "Iteration 2279, loss = 0.15105670\n",
      "Iteration 2280, loss = 0.15097945\n",
      "Iteration 2281, loss = 0.15090799\n",
      "Iteration 2282, loss = 0.15084800\n",
      "Iteration 2283, loss = 0.15078287\n",
      "Iteration 2284, loss = 0.15070982\n",
      "Iteration 2285, loss = 0.15061958\n",
      "Iteration 2286, loss = 0.15054803\n",
      "Iteration 2287, loss = 0.15048610\n",
      "Iteration 2288, loss = 0.15037112\n",
      "Iteration 2289, loss = 0.15038315\n",
      "Iteration 2290, loss = 0.15027607\n",
      "Iteration 2291, loss = 0.15019945\n",
      "Iteration 2292, loss = 0.15016636\n",
      "Iteration 2293, loss = 0.15010005\n",
      "Iteration 2294, loss = 0.15008827\n",
      "Iteration 2295, loss = 0.15000120\n",
      "Iteration 2296, loss = 0.14986961\n",
      "Iteration 2297, loss = 0.14977936\n",
      "Iteration 2298, loss = 0.14970125\n",
      "Iteration 2299, loss = 0.14966287\n",
      "Iteration 2300, loss = 0.14957707\n",
      "Iteration 2301, loss = 0.14945474\n",
      "Iteration 2302, loss = 0.14940848\n",
      "Iteration 2303, loss = 0.14932891\n",
      "Iteration 2304, loss = 0.14927743\n",
      "Iteration 2305, loss = 0.14921489\n",
      "Iteration 2306, loss = 0.14914284\n",
      "Iteration 2307, loss = 0.14909508\n",
      "Iteration 2308, loss = 0.14899973\n",
      "Iteration 2309, loss = 0.14891096\n",
      "Iteration 2310, loss = 0.14887145\n",
      "Iteration 2311, loss = 0.14875185\n",
      "Iteration 2312, loss = 0.14874092\n",
      "Iteration 2313, loss = 0.14862954\n",
      "Iteration 2314, loss = 0.14866494\n",
      "Iteration 2315, loss = 0.14852817\n",
      "Iteration 2316, loss = 0.14841855\n",
      "Iteration 2317, loss = 0.14851076\n",
      "Iteration 2318, loss = 0.14830135\n",
      "Iteration 2319, loss = 0.14828142\n",
      "Iteration 2320, loss = 0.14816350\n",
      "Iteration 2321, loss = 0.14805758\n",
      "Iteration 2322, loss = 0.14798747\n",
      "Iteration 2323, loss = 0.14800398\n",
      "Iteration 2324, loss = 0.14785907\n",
      "Iteration 2325, loss = 0.14775969\n",
      "Iteration 2326, loss = 0.14788198\n",
      "Iteration 2327, loss = 0.14763407\n",
      "Iteration 2328, loss = 0.14758290\n",
      "Iteration 2329, loss = 0.14750947\n",
      "Iteration 2330, loss = 0.14746350\n",
      "Iteration 2331, loss = 0.14736108\n",
      "Iteration 2332, loss = 0.14732530\n",
      "Iteration 2333, loss = 0.14723665\n",
      "Iteration 2334, loss = 0.14711698\n",
      "Iteration 2335, loss = 0.14707795\n",
      "Iteration 2336, loss = 0.14703635\n",
      "Iteration 2337, loss = 0.14705158\n",
      "Iteration 2338, loss = 0.14685653\n",
      "Iteration 2339, loss = 0.14685325\n",
      "Iteration 2340, loss = 0.14682108\n",
      "Iteration 2341, loss = 0.14678501\n",
      "Iteration 2342, loss = 0.14661592\n",
      "Iteration 2343, loss = 0.14651834\n",
      "Iteration 2344, loss = 0.14652927\n",
      "Iteration 2345, loss = 0.14638066\n",
      "Iteration 2346, loss = 0.14635382\n",
      "Iteration 2347, loss = 0.14624565\n",
      "Iteration 2348, loss = 0.14615355\n",
      "Iteration 2349, loss = 0.14612916\n",
      "Iteration 2350, loss = 0.14602124\n",
      "Iteration 2351, loss = 0.14601317\n",
      "Iteration 2352, loss = 0.14599028\n",
      "Iteration 2353, loss = 0.14580037\n",
      "Iteration 2354, loss = 0.14577979\n",
      "Iteration 2355, loss = 0.14569007\n",
      "Iteration 2356, loss = 0.14561418\n",
      "Iteration 2357, loss = 0.14562017\n",
      "Iteration 2358, loss = 0.14549520\n",
      "Iteration 2359, loss = 0.14543708\n",
      "Iteration 2360, loss = 0.14535131\n",
      "Iteration 2361, loss = 0.14543141\n",
      "Iteration 2362, loss = 0.14530727\n",
      "Iteration 2363, loss = 0.14512215\n",
      "Iteration 2364, loss = 0.14510464\n",
      "Iteration 2365, loss = 0.14508842\n",
      "Iteration 2366, loss = 0.14497993\n",
      "Iteration 2367, loss = 0.14489413\n",
      "Iteration 2368, loss = 0.14484996\n",
      "Iteration 2369, loss = 0.14480606\n",
      "Iteration 2370, loss = 0.14465646\n",
      "Iteration 2371, loss = 0.14462329\n",
      "Iteration 2372, loss = 0.14452011\n",
      "Iteration 2373, loss = 0.14445125\n",
      "Iteration 2374, loss = 0.14440338\n",
      "Iteration 2375, loss = 0.14435950\n",
      "Iteration 2376, loss = 0.14425807\n",
      "Iteration 2377, loss = 0.14418455\n",
      "Iteration 2378, loss = 0.14411279\n",
      "Iteration 2379, loss = 0.14402524\n",
      "Iteration 2380, loss = 0.14401742\n",
      "Iteration 2381, loss = 0.14390647\n",
      "Iteration 2382, loss = 0.14387051\n",
      "Iteration 2383, loss = 0.14375825\n",
      "Iteration 2384, loss = 0.14371058\n",
      "Iteration 2385, loss = 0.14369545\n",
      "Iteration 2386, loss = 0.14359854\n",
      "Iteration 2387, loss = 0.14348956\n",
      "Iteration 2388, loss = 0.14340025\n",
      "Iteration 2389, loss = 0.14336575\n",
      "Iteration 2390, loss = 0.14330262\n",
      "Iteration 2391, loss = 0.14330698\n",
      "Iteration 2392, loss = 0.14319038\n",
      "Iteration 2393, loss = 0.14305354\n",
      "Iteration 2394, loss = 0.14304010\n",
      "Iteration 2395, loss = 0.14294726\n",
      "Iteration 2396, loss = 0.14290144\n",
      "Iteration 2397, loss = 0.14281836\n",
      "Iteration 2398, loss = 0.14275787\n",
      "Iteration 2399, loss = 0.14267447\n",
      "Iteration 2400, loss = 0.14259585\n",
      "Iteration 2401, loss = 0.14267631\n",
      "Iteration 2402, loss = 0.14247485\n",
      "Iteration 2403, loss = 0.14242921\n",
      "Iteration 2404, loss = 0.14238932\n",
      "Iteration 2405, loss = 0.14228353\n",
      "Iteration 2406, loss = 0.14219264\n",
      "Iteration 2407, loss = 0.14211965\n",
      "Iteration 2408, loss = 0.14203088\n",
      "Iteration 2409, loss = 0.14196486\n",
      "Iteration 2410, loss = 0.14196852\n",
      "Iteration 2411, loss = 0.14186078\n",
      "Iteration 2412, loss = 0.14186036\n",
      "Iteration 2413, loss = 0.14180846\n",
      "Iteration 2414, loss = 0.14166145\n",
      "Iteration 2415, loss = 0.14161416\n",
      "Iteration 2416, loss = 0.14153688\n",
      "Iteration 2417, loss = 0.14153055\n",
      "Iteration 2418, loss = 0.14137394\n",
      "Iteration 2419, loss = 0.14130043\n",
      "Iteration 2420, loss = 0.14124577\n",
      "Iteration 2421, loss = 0.14113249\n",
      "Iteration 322, loss = 0.35425567\n",
      "Iteration 323, loss = 0.35407790\n",
      "Iteration 324, loss = 0.35391259\n",
      "Iteration 325, loss = 0.35374831\n",
      "Iteration 326, loss = 0.35359009\n",
      "Iteration 327, loss = 0.35341963\n",
      "Iteration 328, loss = 0.35328338\n",
      "Iteration 329, loss = 0.35310145\n",
      "Iteration 330, loss = 0.35295346\n",
      "Iteration 331, loss = 0.35280021\n",
      "Iteration 332, loss = 0.35262272\n",
      "Iteration 333, loss = 0.35248207\n",
      "Iteration 334, loss = 0.35233477\n",
      "Iteration 335, loss = 0.35214687\n",
      "Iteration 336, loss = 0.35199066\n",
      "Iteration 337, loss = 0.35183329\n",
      "Iteration 338, loss = 0.35167381\n",
      "Iteration 339, loss = 0.35156005\n",
      "Iteration 340, loss = 0.35137275\n",
      "Iteration 341, loss = 0.35122392\n",
      "Iteration 342, loss = 0.35106820\n",
      "Iteration 343, loss = 0.35089986\n",
      "Iteration 344, loss = 0.35076951\n",
      "Iteration 345, loss = 0.35060235\n",
      "Iteration 346, loss = 0.35051233\n",
      "Iteration 347, loss = 0.35027985\n",
      "Iteration 348, loss = 0.35018952\n",
      "Iteration 349, loss = 0.34999746\n",
      "Iteration 350, loss = 0.34983468\n",
      "Iteration 351, loss = 0.34971952\n",
      "Iteration 352, loss = 0.34956637\n",
      "Iteration 353, loss = 0.34941534\n",
      "Iteration 354, loss = 0.34923478\n",
      "Iteration 355, loss = 0.34909332\n",
      "Iteration 356, loss = 0.34893261\n",
      "Iteration 357, loss = 0.34879049\n",
      "Iteration 358, loss = 0.34864303\n",
      "Iteration 359, loss = 0.34850527\n",
      "Iteration 360, loss = 0.34834342\n",
      "Iteration 361, loss = 0.34823690\n",
      "Iteration 362, loss = 0.34806724\n",
      "Iteration 363, loss = 0.34789915\n",
      "Iteration 364, loss = 0.34777148\n",
      "Iteration 365, loss = 0.34761315\n",
      "Iteration 366, loss = 0.34749295\n",
      "Iteration 367, loss = 0.34736497\n",
      "Iteration 368, loss = 0.34722359\n",
      "Iteration 369, loss = 0.34711628\n",
      "Iteration 370, loss = 0.34699659\n",
      "Iteration 371, loss = 0.34676189\n",
      "Iteration 372, loss = 0.34664711\n",
      "Iteration 373, loss = 0.34649231\n",
      "Iteration 374, loss = 0.34633983\n",
      "Iteration 375, loss = 0.34619559\n",
      "Iteration 376, loss = 0.34607935\n",
      "Iteration 377, loss = 0.34592475\n",
      "Iteration 378, loss = 0.34576768\n",
      "Iteration 379, loss = 0.34562086\n",
      "Iteration 380, loss = 0.34549965\n",
      "Iteration 381, loss = 0.34534892\n",
      "Iteration 382, loss = 0.34523453\n",
      "Iteration 383, loss = 0.34509099\n",
      "Iteration 384, loss = 0.34492899\n",
      "Iteration 385, loss = 0.34480528\n",
      "Iteration 386, loss = 0.34468544\n",
      "Iteration 387, loss = 0.34455946\n",
      "Iteration 388, loss = 0.34438689\n",
      "Iteration 389, loss = 0.34423722\n",
      "Iteration 390, loss = 0.34414413\n",
      "Iteration 391, loss = 0.34399293\n",
      "Iteration 392, loss = 0.34383349\n",
      "Iteration 393, loss = 0.34371266\n",
      "Iteration 394, loss = 0.34356435\n",
      "Iteration 395, loss = 0.34343500\n",
      "Iteration 396, loss = 0.34327960\n",
      "Iteration 397, loss = 0.34318279\n",
      "Iteration 398, loss = 0.34302640\n",
      "Iteration 399, loss = 0.34291289\n",
      "Iteration 400, loss = 0.34274375\n",
      "Iteration 401, loss = 0.34260944\n",
      "Iteration 402, loss = 0.34247248\n",
      "Iteration 403, loss = 0.34236487\n",
      "Iteration 404, loss = 0.34220524\n",
      "Iteration 405, loss = 0.34211393\n",
      "Iteration 406, loss = 0.34192289\n",
      "Iteration 407, loss = 0.34180730\n",
      "Iteration 408, loss = 0.34165437\n",
      "Iteration 409, loss = 0.34153771\n",
      "Iteration 410, loss = 0.34138232\n",
      "Iteration 411, loss = 0.34127033\n",
      "Iteration 412, loss = 0.34112265\n",
      "Iteration 413, loss = 0.34098712\n",
      "Iteration 414, loss = 0.34085825\n",
      "Iteration 415, loss = 0.34071640\n",
      "Iteration 416, loss = 0.34059107\n",
      "Iteration 417, loss = 0.34047127\n",
      "Iteration 418, loss = 0.34034532\n",
      "Iteration 419, loss = 0.34019193\n",
      "Iteration 420, loss = 0.34007644\n",
      "Iteration 421, loss = 0.33991460\n",
      "Iteration 422, loss = 0.33980028\n",
      "Iteration 423, loss = 0.33966237\n",
      "Iteration 424, loss = 0.33954171\n",
      "Iteration 425, loss = 0.33938515\n",
      "Iteration 426, loss = 0.33927317\n",
      "Iteration 427, loss = 0.33911647\n",
      "Iteration 428, loss = 0.33900748\n",
      "Iteration 429, loss = 0.33887309\n",
      "Iteration 430, loss = 0.33874429\n",
      "Iteration 431, loss = 0.33859505\n",
      "Iteration 432, loss = 0.33848336\n",
      "Iteration 433, loss = 0.33832369\n",
      "Iteration 434, loss = 0.33819477\n",
      "Iteration 435, loss = 0.33806755\n",
      "Iteration 436, loss = 0.33795209\n",
      "Iteration 437, loss = 0.33786814\n",
      "Iteration 438, loss = 0.33768357\n",
      "Iteration 439, loss = 0.33754755\n",
      "Iteration 440, loss = 0.33740353\n",
      "Iteration 441, loss = 0.33731310\n",
      "Iteration 442, loss = 0.33713650\n",
      "Iteration 443, loss = 0.33704997\n",
      "Iteration 444, loss = 0.33688494\n",
      "Iteration 445, loss = 0.33675837\n",
      "Iteration 446, loss = 0.33661710\n",
      "Iteration 447, loss = 0.33648451\n",
      "Iteration 448, loss = 0.33635117\n",
      "Iteration 449, loss = 0.33623732\n",
      "Iteration 450, loss = 0.33610685\n",
      "Iteration 451, loss = 0.33596374\n",
      "Iteration 452, loss = 0.33583800\n",
      "Iteration 453, loss = 0.33573123\n",
      "Iteration 454, loss = 0.33557806\n",
      "Iteration 455, loss = 0.33545568\n",
      "Iteration 456, loss = 0.33533567\n",
      "Iteration 457, loss = 0.33517451\n",
      "Iteration 458, loss = 0.33508857\n",
      "Iteration 459, loss = 0.33491880\n",
      "Iteration 460, loss = 0.33478727\n",
      "Iteration 461, loss = 0.33467474\n",
      "Iteration 462, loss = 0.33454940\n",
      "Iteration 463, loss = 0.33441642\n",
      "Iteration 464, loss = 0.33430086\n",
      "Iteration 465, loss = 0.33415631\n",
      "Iteration 466, loss = 0.33402693\n",
      "Iteration 467, loss = 0.33390548\n",
      "Iteration 468, loss = 0.33381424\n",
      "Iteration 469, loss = 0.33363109\n",
      "Iteration 470, loss = 0.33350973\n",
      "Iteration 471, loss = 0.33338861\n",
      "Iteration 472, loss = 0.33326424\n",
      "Iteration 473, loss = 0.33316821\n",
      "Iteration 474, loss = 0.33301944\n",
      "Iteration 475, loss = 0.33286455\n",
      "Iteration 476, loss = 0.33274553\n",
      "Iteration 477, loss = 0.33261493\n",
      "Iteration 478, loss = 0.33248328\n",
      "Iteration 479, loss = 0.33235471\n",
      "Iteration 480, loss = 0.33221482\n",
      "Iteration 481, loss = 0.33208942\n",
      "Iteration 482, loss = 0.33201081\n",
      "Iteration 483, loss = 0.33186761\n",
      "Iteration 484, loss = 0.33173450\n",
      "Iteration 485, loss = 0.33159546\n",
      "Iteration 486, loss = 0.33146608\n",
      "Iteration 487, loss = 0.33135571\n",
      "Iteration 488, loss = 0.33122113\n",
      "Iteration 489, loss = 0.33108054\n",
      "Iteration 490, loss = 0.33097255\n",
      "Iteration 491, loss = 0.33083616\n",
      "Iteration 492, loss = 0.33075826\n",
      "Iteration 493, loss = 0.33059435\n",
      "Iteration 494, loss = 0.33045023\n",
      "Iteration 495, loss = 0.33032209\n",
      "Iteration 496, loss = 0.33017846\n",
      "Iteration 497, loss = 0.33005615\n",
      "Iteration 498, loss = 0.32993608\n",
      "Iteration 499, loss = 0.32980714\n",
      "Iteration 500, loss = 0.32967774\n",
      "Iteration 501, loss = 0.32957838\n",
      "Iteration 502, loss = 0.32947130\n",
      "Iteration 503, loss = 0.32935958\n",
      "Iteration 504, loss = 0.32924654\n",
      "Iteration 505, loss = 0.32906275\n",
      "Iteration 506, loss = 0.32893627\n",
      "Iteration 507, loss = 0.32881592\n",
      "Iteration 508, loss = 0.32870358\n",
      "Iteration 509, loss = 0.32856902\n",
      "Iteration 510, loss = 0.32845543\n",
      "Iteration 511, loss = 0.32832514\n",
      "Iteration 512, loss = 0.32820445\n",
      "Iteration 513, loss = 0.32807771\n",
      "Iteration 514, loss = 0.32795853\n",
      "Iteration 515, loss = 0.32781323\n",
      "Iteration 516, loss = 0.32773301\n",
      "Iteration 517, loss = 0.32765427\n",
      "Iteration 518, loss = 0.32745627\n",
      "Iteration 519, loss = 0.32735175\n",
      "Iteration 520, loss = 0.32721799\n",
      "Iteration 521, loss = 0.32708734\n",
      "Iteration 522, loss = 0.32700856\n",
      "Iteration 523, loss = 0.32685977\n",
      "Iteration 524, loss = 0.32672572\n",
      "Iteration 525, loss = 0.32660627\n",
      "Iteration 526, loss = 0.32646692\n",
      "Iteration 527, loss = 0.32636448\n",
      "Iteration 528, loss = 0.32628003\n",
      "Iteration 529, loss = 0.32610603\n",
      "Iteration 530, loss = 0.32597808\n",
      "Iteration 531, loss = 0.32585078\n",
      "Iteration 532, loss = 0.32573792\n",
      "Iteration 533, loss = 0.32561134\n",
      "Iteration 534, loss = 0.32549756\n",
      "Iteration 535, loss = 0.32535623\n",
      "Iteration 536, loss = 0.32524122\n",
      "Iteration 537, loss = 0.32511991\n",
      "Iteration 538, loss = 0.32501833\n",
      "Iteration 539, loss = 0.32494675\n",
      "Iteration 540, loss = 0.32476253\n",
      "Iteration 541, loss = 0.32467054\n",
      "Iteration 542, loss = 0.32450340\n",
      "Iteration 543, loss = 0.32439497\n",
      "Iteration 544, loss = 0.32425537\n",
      "Iteration 545, loss = 0.32412533\n",
      "Iteration 546, loss = 0.32400933\n",
      "Iteration 547, loss = 0.32389491\n",
      "Iteration 548, loss = 0.32375159\n",
      "Iteration 549, loss = 0.32361979\n",
      "Iteration 550, loss = 0.32350951\n",
      "Iteration 551, loss = 0.32337252\n",
      "Iteration 552, loss = 0.32326924\n",
      "Iteration 553, loss = 0.32315470\n",
      "Iteration 554, loss = 0.32301481\n",
      "Iteration 555, loss = 0.32288100\n",
      "Iteration 556, loss = 0.32282156\n",
      "Iteration 557, loss = 0.32263702\n",
      "Iteration 558, loss = 0.32251089\n",
      "Iteration 559, loss = 0.32241130\n",
      "Iteration 560, loss = 0.32227725\n",
      "Iteration 561, loss = 0.32214526\n",
      "Iteration 562, loss = 0.32200560\n",
      "Iteration 563, loss = 0.32187922\n",
      "Iteration 564, loss = 0.32177384\n",
      "Iteration 565, loss = 0.32165273\n",
      "Iteration 566, loss = 0.32158332\n",
      "Iteration 567, loss = 0.32145690\n",
      "Iteration 568, loss = 0.32129265\n",
      "Iteration 569, loss = 0.32117794\n",
      "Iteration 570, loss = 0.32103866\n",
      "Iteration 571, loss = 0.32091602\n",
      "Iteration 572, loss = 0.32080035\n",
      "Iteration 573, loss = 0.32069619\n",
      "Iteration 574, loss = 0.32056412\n",
      "Iteration 575, loss = 0.32044469\n",
      "Iteration 576, loss = 0.32032849\n",
      "Iteration 577, loss = 0.32022290\n",
      "Iteration 578, loss = 0.32006600\n",
      "Iteration 579, loss = 0.31995647\n",
      "Iteration 580, loss = 0.31987036\n",
      "Iteration 581, loss = 0.31971340\n",
      "Iteration 582, loss = 0.31962948\n",
      "Iteration 583, loss = 0.31946761\n",
      "Iteration 584, loss = 0.31935951\n",
      "Iteration 585, loss = 0.31923427\n",
      "Iteration 586, loss = 0.31909524\n",
      "Iteration 587, loss = 0.31901139\n",
      "Iteration 588, loss = 0.31887644\n",
      "Iteration 589, loss = 0.31875116\n",
      "Iteration 590, loss = 0.31861636\n",
      "Iteration 591, loss = 0.31849573\n",
      "Iteration 592, loss = 0.31837948\n",
      "Iteration 593, loss = 0.31824494\n",
      "Iteration 594, loss = 0.31812747\n",
      "Iteration 595, loss = 0.31802545\n",
      "Iteration 596, loss = 0.31788327\n",
      "Iteration 597, loss = 0.31778496\n",
      "Iteration 598, loss = 0.31766494\n",
      "Iteration 599, loss = 0.31751162\n",
      "Iteration 600, loss = 0.31739710\n",
      "Iteration 601, loss = 0.31728044\n",
      "Iteration 602, loss = 0.31714115\n",
      "Iteration 603, loss = 0.31701809\n",
      "Iteration 604, loss = 0.31691340\n",
      "Iteration 605, loss = 0.31678608\n",
      "Iteration 606, loss = 0.31665931\n",
      "Iteration 607, loss = 0.31654991\n",
      "Iteration 608, loss = 0.31648692\n",
      "Iteration 609, loss = 0.31631880\n",
      "Iteration 610, loss = 0.31617392\n",
      "Iteration 611, loss = 0.31608145\n",
      "Iteration 612, loss = 0.31594238\n",
      "Iteration 613, loss = 0.31580982\n",
      "Iteration 614, loss = 0.31570669\n",
      "Iteration 615, loss = 0.31556951\n",
      "Iteration 616, loss = 0.31546894\n",
      "Iteration 617, loss = 0.31533576\n",
      "Iteration 618, loss = 0.31524259\n",
      "Iteration 619, loss = 0.31512324\n",
      "Iteration 620, loss = 0.31502154\n",
      "Iteration 621, loss = 0.31486438\n",
      "Iteration 622, loss = 0.31477417\n",
      "Iteration 623, loss = 0.31460248\n",
      "Iteration 624, loss = 0.31448481\n",
      "Iteration 625, loss = 0.31435717\n",
      "Iteration 626, loss = 0.31423955\n",
      "Iteration 627, loss = 0.31413613\n",
      "Iteration 628, loss = 0.31400762\n",
      "Iteration 629, loss = 0.31392542\n",
      "Iteration 630, loss = 0.31376464\n",
      "Iteration 631, loss = 0.31366778\n",
      "Iteration 632, loss = 0.31352241\n",
      "Iteration 633, loss = 0.31342660\n",
      "Iteration 634, loss = 0.31329011\n",
      "Iteration 635, loss = 0.31317074\n",
      "Iteration 636, loss = 0.31304764\n",
      "Iteration 637, loss = 0.31297875\n",
      "Iteration 638, loss = 0.31285437\n",
      "Iteration 639, loss = 0.31268557\n",
      "Iteration 640, loss = 0.31258083\n",
      "Iteration 641, loss = 0.31247854\n",
      "Iteration 642, loss = 0.31233003\n",
      "Iteration 643, loss = 0.31221320\n",
      "Iteration 644, loss = 0.31209306\n",
      "Iteration 645, loss = 0.31202099\n",
      "Iteration 646, loss = 0.31185560\n",
      "Iteration 647, loss = 0.31174002\n",
      "Iteration 648, loss = 0.31162183\n",
      "Iteration 649, loss = 0.31149610\n",
      "Iteration 650, loss = 0.31140034\n",
      "Iteration 651, loss = 0.31125282\n",
      "Iteration 652, loss = 0.31112280\n",
      "Iteration 653, loss = 0.31101123\n",
      "Iteration 654, loss = 0.31090731\n",
      "Iteration 655, loss = 0.31078878\n",
      "Iteration 656, loss = 0.31065800\n",
      "Iteration 657, loss = 0.31063668\n",
      "Iteration 658, loss = 0.31042098\n",
      "Iteration 659, loss = 0.31029897\n",
      "Iteration 660, loss = 0.31018944\n",
      "Iteration 661, loss = 0.31010048\n",
      "Iteration 662, loss = 0.30995620\n",
      "Iteration 663, loss = 0.30982378\n",
      "Iteration 664, loss = 0.30978394\n",
      "Iteration 665, loss = 0.30957601\n",
      "Iteration 666, loss = 0.30953954\n",
      "Iteration 667, loss = 0.30933528\n",
      "Iteration 668, loss = 0.30922994\n",
      "Iteration 669, loss = 0.30912150\n",
      "Iteration 670, loss = 0.30897204\n",
      "Iteration 671, loss = 0.30891200\n",
      "Iteration 672, loss = 0.30876568\n",
      "Iteration 673, loss = 0.30865149\n",
      "Iteration 674, loss = 0.30852476\n",
      "Iteration 675, loss = 0.30841048\n",
      "Iteration 676, loss = 0.30831537\n",
      "Iteration 677, loss = 0.30814142\n",
      "Iteration 678, loss = 0.30805660\n",
      "Iteration 679, loss = 0.30790795\n",
      "Iteration 680, loss = 0.30778817\n",
      "Iteration 681, loss = 0.30766172\n",
      "Iteration 682, loss = 0.30754701\n",
      "Iteration 683, loss = 0.30741379\n",
      "Iteration 684, loss = 0.30731028\n",
      "Iteration 685, loss = 0.30725553\n",
      "Iteration 686, loss = 0.30706696\n",
      "Iteration 687, loss = 0.30694274\n",
      "Iteration 688, loss = 0.30684081\n",
      "Iteration 689, loss = 0.30674985\n",
      "Iteration 690, loss = 0.30660734\n",
      "Iteration 691, loss = 0.30647323\n",
      "Iteration 692, loss = 0.30636780\n",
      "Iteration 693, loss = 0.30623018\n",
      "Iteration 694, loss = 0.30610759\n",
      "Iteration 695, loss = 0.30599449\n",
      "Iteration 696, loss = 0.30587096\n",
      "Iteration 697, loss = 0.30578326\n",
      "Iteration 698, loss = 0.30564904\n",
      "Iteration 699, loss = 0.30550555\n",
      "Iteration 700, loss = 0.30539618\n",
      "Iteration 701, loss = 0.30535714\n",
      "Iteration 702, loss = 0.30517285\n",
      "Iteration 703, loss = 0.30506494\n",
      "Iteration 704, loss = 0.30492904\n",
      "Iteration 705, loss = 0.30481419\n",
      "Iteration 706, loss = 0.30475157\n",
      "Iteration 707, loss = 0.30458412\n",
      "Iteration 708, loss = 0.30444731\n",
      "Iteration 709, loss = 0.30433357\n",
      "Iteration 710, loss = 0.30422875\n",
      "Iteration 711, loss = 0.30409314\n",
      "Iteration 712, loss = 0.30397767\n",
      "Iteration 713, loss = 0.30394134\n",
      "Iteration 714, loss = 0.30376299\n",
      "Iteration 715, loss = 0.30364073\n",
      "Iteration 716, loss = 0.30354149\n",
      "Iteration 717, loss = 0.30342014\n",
      "Iteration 718, loss = 0.30327198\n",
      "Iteration 719, loss = 0.30315376\n",
      "Iteration 720, loss = 0.30303553\n",
      "Iteration 721, loss = 0.30292416\n",
      "Iteration 722, loss = 0.30284437\n",
      "Iteration 723, loss = 0.30270778\n",
      "Iteration 724, loss = 0.30256258\n",
      "Iteration 725, loss = 0.30245234\n",
      "Iteration 726, loss = 0.30235643\n",
      "Iteration 727, loss = 0.30222624\n",
      "Iteration 728, loss = 0.30209815\n",
      "Iteration 729, loss = 0.30199597\n",
      "Iteration 730, loss = 0.30185020\n",
      "Iteration 731, loss = 0.30173949\n",
      "Iteration 732, loss = 0.30164945\n",
      "Iteration 733, loss = 0.30149962\n",
      "Iteration 734, loss = 0.30139585\n",
      "Iteration 735, loss = 0.30126971\n",
      "Iteration 736, loss = 0.30115486\n",
      "Iteration 737, loss = 0.30109071\n",
      "Iteration 738, loss = 0.30092417\n",
      "Iteration 739, loss = 0.30083713\n",
      "Iteration 740, loss = 0.30069270\n",
      "Iteration 741, loss = 0.30060136\n",
      "Iteration 742, loss = 0.30045688\n",
      "Iteration 743, loss = 0.30034961\n",
      "Iteration 744, loss = 0.30021248\n",
      "Iteration 745, loss = 0.30009140\n",
      "Iteration 746, loss = 0.29997810\n",
      "Iteration 747, loss = 0.29986276\n",
      "Iteration 748, loss = 0.29974972\n",
      "Iteration 749, loss = 0.29963097\n",
      "Iteration 750, loss = 0.29952058\n",
      "Iteration 751, loss = 0.29939940\n",
      "Iteration 752, loss = 0.29928523\n",
      "Iteration 753, loss = 0.29917897\n",
      "Iteration 754, loss = 0.29905851\n",
      "Iteration 755, loss = 0.29894950\n",
      "Iteration 756, loss = 0.29883573\n",
      "Iteration 757, loss = 0.29870445\n",
      "Iteration 758, loss = 0.29857195\n",
      "Iteration 759, loss = 0.29845092\n",
      "Iteration 760, loss = 0.29835252\n",
      "Iteration 761, loss = 0.29823584\n",
      "Iteration 762, loss = 0.29816246\n",
      "Iteration 763, loss = 0.29801638\n",
      "Iteration 764, loss = 0.29790747\n",
      "Iteration 765, loss = 0.29776041\n",
      "Iteration 766, loss = 0.29766944\n",
      "Iteration 767, loss = 0.29751130\n",
      "Iteration 768, loss = 0.29743639\n",
      "Iteration 769, loss = 0.29729382\n",
      "Iteration 770, loss = 0.29717426\n",
      "Iteration 771, loss = 0.29711302\n",
      "Iteration 772, loss = 0.29696022\n",
      "Iteration 773, loss = 0.29682763\n",
      "Iteration 774, loss = 0.29670772\n",
      "Iteration 775, loss = 0.29661637\n",
      "Iteration 776, loss = 0.29647257\n",
      "Iteration 777, loss = 0.29638448\n",
      "Iteration 778, loss = 0.29627429\n",
      "Iteration 779, loss = 0.29613031\n",
      "Iteration 780, loss = 0.29602232\n",
      "Iteration 781, loss = 0.29590514\n",
      "Iteration 782, loss = 0.29578321\n",
      "Iteration 783, loss = 0.29566039\n",
      "Iteration 784, loss = 0.29551907\n",
      "Iteration 785, loss = 0.29540247\n",
      "Iteration 786, loss = 0.29533428\n",
      "Iteration 787, loss = 0.29520642\n",
      "Iteration 788, loss = 0.29507046\n",
      "Iteration 789, loss = 0.29498090\n",
      "Iteration 790, loss = 0.29481825\n",
      "Iteration 791, loss = 0.29471665\n",
      "Iteration 792, loss = 0.29459891\n",
      "Iteration 793, loss = 0.29450803\n",
      "Iteration 794, loss = 0.29436680\n",
      "Iteration 795, loss = 0.29426528\n",
      "Iteration 796, loss = 0.29415492\n",
      "Iteration 797, loss = 0.29401939\n",
      "Iteration 798, loss = 0.29392943\n",
      "Iteration 799, loss = 0.29376652\n",
      "Iteration 800, loss = 0.29366461\n",
      "Iteration 801, loss = 0.29355516\n",
      "Iteration 802, loss = 0.29343543\n",
      "Iteration 803, loss = 0.29329595\n",
      "Iteration 804, loss = 0.29319157\n",
      "Iteration 805, loss = 0.29318241\n",
      "Iteration 806, loss = 0.29298147\n",
      "Iteration 807, loss = 0.29287174\n",
      "Iteration 808, loss = 0.29270736\n",
      "Iteration 809, loss = 0.29263783\n",
      "Iteration 810, loss = 0.29248589\n",
      "Iteration 811, loss = 0.29238870\n",
      "Iteration 812, loss = 0.29224953\n",
      "Iteration 813, loss = 0.29218219\n",
      "Iteration 814, loss = 0.29199593\n",
      "Iteration 815, loss = 0.29191120\n",
      "Iteration 816, loss = 0.29177876\n",
      "Iteration 817, loss = 0.29165912\n",
      "Iteration 294, loss = 0.35322268\n",
      "Iteration 295, loss = 0.35302014\n",
      "Iteration 296, loss = 0.35278545\n",
      "Iteration 297, loss = 0.35258765\n",
      "Iteration 298, loss = 0.35235992\n",
      "Iteration 299, loss = 0.35213225\n",
      "Iteration 300, loss = 0.35193513\n",
      "Iteration 301, loss = 0.35172257\n",
      "Iteration 302, loss = 0.35153916\n",
      "Iteration 303, loss = 0.35129548\n",
      "Iteration 304, loss = 0.35108049\n",
      "Iteration 305, loss = 0.35087190\n",
      "Iteration 306, loss = 0.35067866\n",
      "Iteration 307, loss = 0.35047745\n",
      "Iteration 308, loss = 0.35029273\n",
      "Iteration 309, loss = 0.35009252\n",
      "Iteration 310, loss = 0.34987441\n",
      "Iteration 311, loss = 0.34967128\n",
      "Iteration 312, loss = 0.34946216\n",
      "Iteration 313, loss = 0.34928392\n",
      "Iteration 314, loss = 0.34906019\n",
      "Iteration 315, loss = 0.34886873\n",
      "Iteration 316, loss = 0.34867908\n",
      "Iteration 317, loss = 0.34849469\n",
      "Iteration 318, loss = 0.34827363\n",
      "Iteration 319, loss = 0.34810477\n",
      "Iteration 320, loss = 0.34788363\n",
      "Iteration 321, loss = 0.34767717\n",
      "Iteration 322, loss = 0.34750492\n",
      "Iteration 323, loss = 0.34729927\n",
      "Iteration 324, loss = 0.34714563\n",
      "Iteration 325, loss = 0.34695444\n",
      "Iteration 326, loss = 0.34674221\n",
      "Iteration 327, loss = 0.34652237\n",
      "Iteration 328, loss = 0.34638893\n",
      "Iteration 329, loss = 0.34618622\n",
      "Iteration 330, loss = 0.34599598\n",
      "Iteration 331, loss = 0.34578611\n",
      "Iteration 332, loss = 0.34558513\n",
      "Iteration 333, loss = 0.34542607\n",
      "Iteration 334, loss = 0.34524843\n",
      "Iteration 335, loss = 0.34504444\n",
      "Iteration 336, loss = 0.34486696\n",
      "Iteration 337, loss = 0.34467682\n",
      "Iteration 338, loss = 0.34448674\n",
      "Iteration 339, loss = 0.34429993\n",
      "Iteration 340, loss = 0.34411652\n",
      "Iteration 341, loss = 0.34399490\n",
      "Iteration 342, loss = 0.34381506\n",
      "Iteration 343, loss = 0.34360005\n",
      "Iteration 344, loss = 0.34338694\n",
      "Iteration 345, loss = 0.34326300\n",
      "Iteration 346, loss = 0.34302391\n",
      "Iteration 347, loss = 0.34290132\n",
      "Iteration 348, loss = 0.34268289\n",
      "Iteration 349, loss = 0.34251539\n",
      "Iteration 350, loss = 0.34233499\n",
      "Iteration 351, loss = 0.34221175\n",
      "Iteration 352, loss = 0.34197000\n",
      "Iteration 353, loss = 0.34182498\n",
      "Iteration 354, loss = 0.34163647\n",
      "Iteration 355, loss = 0.34147798\n",
      "Iteration 356, loss = 0.34128542\n",
      "Iteration 357, loss = 0.34112393\n",
      "Iteration 358, loss = 0.34094708\n",
      "Iteration 359, loss = 0.34076890\n",
      "Iteration 360, loss = 0.34061531\n",
      "Iteration 361, loss = 0.34043250\n",
      "Iteration 362, loss = 0.34025017\n",
      "Iteration 363, loss = 0.34009487\n",
      "Iteration 364, loss = 0.33991974\n",
      "Iteration 365, loss = 0.33975654\n",
      "Iteration 366, loss = 0.33959252\n",
      "Iteration 367, loss = 0.33943296\n",
      "Iteration 368, loss = 0.33928701\n",
      "Iteration 369, loss = 0.33913052\n",
      "Iteration 370, loss = 0.33895988\n",
      "Iteration 371, loss = 0.33878528\n",
      "Iteration 372, loss = 0.33859448\n",
      "Iteration 373, loss = 0.33844225\n",
      "Iteration 374, loss = 0.33828118\n",
      "Iteration 375, loss = 0.33811316\n",
      "Iteration 376, loss = 0.33793849\n",
      "Iteration 377, loss = 0.33777883\n",
      "Iteration 378, loss = 0.33765196\n",
      "Iteration 379, loss = 0.33747236\n",
      "Iteration 380, loss = 0.33732606\n",
      "Iteration 381, loss = 0.33714394\n",
      "Iteration 382, loss = 0.33700642\n",
      "Iteration 383, loss = 0.33686579\n",
      "Iteration 384, loss = 0.33664987\n",
      "Iteration 385, loss = 0.33648730\n",
      "Iteration 386, loss = 0.33633993\n",
      "Iteration 387, loss = 0.33616115\n",
      "Iteration 388, loss = 0.33599659\n",
      "Iteration 389, loss = 0.33584262\n",
      "Iteration 390, loss = 0.33571782\n",
      "Iteration 391, loss = 0.33553936\n",
      "Iteration 392, loss = 0.33539096\n",
      "Iteration 393, loss = 0.33520697\n",
      "Iteration 394, loss = 0.33503901\n",
      "Iteration 395, loss = 0.33496379\n",
      "Iteration 396, loss = 0.33478288\n",
      "Iteration 397, loss = 0.33458974\n",
      "Iteration 398, loss = 0.33443290\n",
      "Iteration 399, loss = 0.33427145\n",
      "Iteration 400, loss = 0.33416288\n",
      "Iteration 401, loss = 0.33396788\n",
      "Iteration 402, loss = 0.33381665\n",
      "Iteration 403, loss = 0.33368089\n",
      "Iteration 404, loss = 0.33350280\n",
      "Iteration 405, loss = 0.33335250\n",
      "Iteration 406, loss = 0.33320937\n",
      "Iteration 407, loss = 0.33307035\n",
      "Iteration 408, loss = 0.33297011\n",
      "Iteration 409, loss = 0.33278888\n",
      "Iteration 410, loss = 0.33260602\n",
      "Iteration 411, loss = 0.33247356\n",
      "Iteration 412, loss = 0.33235718\n",
      "Iteration 413, loss = 0.33215622\n",
      "Iteration 414, loss = 0.33203406\n",
      "Iteration 415, loss = 0.33188209\n",
      "Iteration 416, loss = 0.33171231\n",
      "Iteration 417, loss = 0.33157272\n",
      "Iteration 418, loss = 0.33146301\n",
      "Iteration 419, loss = 0.33130394\n",
      "Iteration 420, loss = 0.33114992\n",
      "Iteration 421, loss = 0.33098523\n",
      "Iteration 422, loss = 0.33084481\n",
      "Iteration 423, loss = 0.33067744\n",
      "Iteration 424, loss = 0.33057753\n",
      "Iteration 425, loss = 0.33041515\n",
      "Iteration 426, loss = 0.33026858\n",
      "Iteration 427, loss = 0.33013941\n",
      "Iteration 428, loss = 0.32996498\n",
      "Iteration 429, loss = 0.32983475\n",
      "Iteration 430, loss = 0.32968543\n",
      "Iteration 431, loss = 0.32954776\n",
      "Iteration 432, loss = 0.32941989\n",
      "Iteration 433, loss = 0.32926412\n",
      "Iteration 434, loss = 0.32912538\n",
      "Iteration 435, loss = 0.32898349\n",
      "Iteration 436, loss = 0.32882204\n",
      "Iteration 437, loss = 0.32870420\n",
      "Iteration 438, loss = 0.32857514\n",
      "Iteration 439, loss = 0.32842388\n",
      "Iteration 440, loss = 0.32825439\n",
      "Iteration 441, loss = 0.32812337\n",
      "Iteration 442, loss = 0.32799040\n",
      "Iteration 443, loss = 0.32784490\n",
      "Iteration 444, loss = 0.32770099\n",
      "Iteration 445, loss = 0.32756993\n",
      "Iteration 446, loss = 0.32740620\n",
      "Iteration 447, loss = 0.32728093\n",
      "Iteration 448, loss = 0.32714794\n",
      "Iteration 449, loss = 0.32699281\n",
      "Iteration 450, loss = 0.32686330\n",
      "Iteration 451, loss = 0.32672438\n",
      "Iteration 452, loss = 0.32658443\n",
      "Iteration 453, loss = 0.32644640\n",
      "Iteration 454, loss = 0.32633362\n",
      "Iteration 455, loss = 0.32614844\n",
      "Iteration 456, loss = 0.32602124\n",
      "Iteration 457, loss = 0.32589131\n",
      "Iteration 458, loss = 0.32572911\n",
      "Iteration 459, loss = 0.32559500\n",
      "Iteration 460, loss = 0.32546894\n",
      "Iteration 461, loss = 0.32532891\n",
      "Iteration 462, loss = 0.32517588\n",
      "Iteration 463, loss = 0.32506382\n",
      "Iteration 464, loss = 0.32489320\n",
      "Iteration 465, loss = 0.32475968\n",
      "Iteration 466, loss = 0.32463830\n",
      "Iteration 467, loss = 0.32447551\n",
      "Iteration 468, loss = 0.32438660\n",
      "Iteration 469, loss = 0.32423146\n",
      "Iteration 470, loss = 0.32405798\n",
      "Iteration 471, loss = 0.32394857\n",
      "Iteration 472, loss = 0.32382003\n",
      "Iteration 473, loss = 0.32367373\n",
      "Iteration 474, loss = 0.32358233\n",
      "Iteration 475, loss = 0.32338711\n",
      "Iteration 476, loss = 0.32324023\n",
      "Iteration 477, loss = 0.32313767\n",
      "Iteration 478, loss = 0.32298071\n",
      "Iteration 479, loss = 0.32284237\n",
      "Iteration 480, loss = 0.32277744\n",
      "Iteration 481, loss = 0.32255339\n",
      "Iteration 482, loss = 0.32241316\n",
      "Iteration 483, loss = 0.32227760\n",
      "Iteration 484, loss = 0.32217467\n",
      "Iteration 485, loss = 0.32201505\n",
      "Iteration 486, loss = 0.32186833\n",
      "Iteration 487, loss = 0.32178096\n",
      "Iteration 488, loss = 0.32167104\n",
      "Iteration 489, loss = 0.32148416\n",
      "Iteration 490, loss = 0.32134418\n",
      "Iteration 491, loss = 0.32120049\n",
      "Iteration 492, loss = 0.32108400\n",
      "Iteration 493, loss = 0.32095239\n",
      "Iteration 494, loss = 0.32078493\n",
      "Iteration 495, loss = 0.32065739\n",
      "Iteration 496, loss = 0.32053403\n",
      "Iteration 497, loss = 0.32040232\n",
      "Iteration 498, loss = 0.32025376\n",
      "Iteration 499, loss = 0.32012638\n",
      "Iteration 500, loss = 0.31999387\n",
      "Iteration 501, loss = 0.31983587\n",
      "Iteration 502, loss = 0.31970238\n",
      "Iteration 503, loss = 0.31956532\n",
      "Iteration 504, loss = 0.31945622\n",
      "Iteration 505, loss = 0.31934449\n",
      "Iteration 506, loss = 0.31919070\n",
      "Iteration 507, loss = 0.31908502\n",
      "Iteration 508, loss = 0.31889171\n",
      "Iteration 509, loss = 0.31875500\n",
      "Iteration 510, loss = 0.31864291\n",
      "Iteration 511, loss = 0.31848967\n",
      "Iteration 512, loss = 0.31834720\n",
      "Iteration 513, loss = 0.31821580\n",
      "Iteration 514, loss = 0.31810874\n",
      "Iteration 515, loss = 0.31796961\n",
      "Iteration 516, loss = 0.31782943\n",
      "Iteration 517, loss = 0.31769609\n",
      "Iteration 518, loss = 0.31755960\n",
      "Iteration 519, loss = 0.31742904\n",
      "Iteration 520, loss = 0.31735644\n",
      "Iteration 521, loss = 0.31715142\n",
      "Iteration 522, loss = 0.31702348\n",
      "Iteration 523, loss = 0.31688320\n",
      "Iteration 524, loss = 0.31676685\n",
      "Iteration 525, loss = 0.31663609\n",
      "Iteration 526, loss = 0.31653401\n",
      "Iteration 527, loss = 0.31635118\n",
      "Iteration 528, loss = 0.31623155\n",
      "Iteration 529, loss = 0.31610157\n",
      "Iteration 530, loss = 0.31595252\n",
      "Iteration 531, loss = 0.31582083\n",
      "Iteration 532, loss = 0.31570909\n",
      "Iteration 533, loss = 0.31558260\n",
      "Iteration 534, loss = 0.31548050\n",
      "Iteration 535, loss = 0.31530614\n",
      "Iteration 536, loss = 0.31517754\n",
      "Iteration 537, loss = 0.31501058\n",
      "Iteration 538, loss = 0.31487739\n",
      "Iteration 539, loss = 0.31474781\n",
      "Iteration 540, loss = 0.31461400\n",
      "Iteration 541, loss = 0.31448704\n",
      "Iteration 542, loss = 0.31434805\n",
      "Iteration 543, loss = 0.31419599\n",
      "Iteration 544, loss = 0.31407402\n",
      "Iteration 545, loss = 0.31399197\n",
      "Iteration 546, loss = 0.31381701\n",
      "Iteration 547, loss = 0.31371020\n",
      "Iteration 548, loss = 0.31356664\n",
      "Iteration 549, loss = 0.31339564\n",
      "Iteration 550, loss = 0.31327087\n",
      "Iteration 551, loss = 0.31314167\n",
      "Iteration 552, loss = 0.31303953\n",
      "Iteration 553, loss = 0.31286566\n",
      "Iteration 554, loss = 0.31278038\n",
      "Iteration 555, loss = 0.31260517\n",
      "Iteration 556, loss = 0.31247990\n",
      "Iteration 557, loss = 0.31235408\n",
      "Iteration 558, loss = 0.31224667\n",
      "Iteration 559, loss = 0.31207699\n",
      "Iteration 560, loss = 0.31193854\n",
      "Iteration 561, loss = 0.31187960\n",
      "Iteration 562, loss = 0.31169705\n",
      "Iteration 563, loss = 0.31159960\n",
      "Iteration 564, loss = 0.31143150\n",
      "Iteration 565, loss = 0.31129786\n",
      "Iteration 566, loss = 0.31115335\n",
      "Iteration 567, loss = 0.31105125\n",
      "Iteration 568, loss = 0.31089944\n",
      "Iteration 569, loss = 0.31078558\n",
      "Iteration 570, loss = 0.31070485\n",
      "Iteration 571, loss = 0.31051296\n",
      "Iteration 572, loss = 0.31037886\n",
      "Iteration 573, loss = 0.31024580\n",
      "Iteration 574, loss = 0.31012615\n",
      "Iteration 575, loss = 0.30998294\n",
      "Iteration 576, loss = 0.30985135\n",
      "Iteration 577, loss = 0.30974728\n",
      "Iteration 578, loss = 0.30960048\n",
      "Iteration 579, loss = 0.30946629\n",
      "Iteration 580, loss = 0.30934249\n",
      "Iteration 581, loss = 0.30923296\n",
      "Iteration 582, loss = 0.30908778\n",
      "Iteration 583, loss = 0.30900430\n",
      "Iteration 584, loss = 0.30884175\n",
      "Iteration 585, loss = 0.30871978\n",
      "Iteration 586, loss = 0.30862525\n",
      "Iteration 587, loss = 0.30844777\n",
      "Iteration 588, loss = 0.30831188\n",
      "Iteration 589, loss = 0.30818405\n",
      "Iteration 590, loss = 0.30806142\n",
      "Iteration 591, loss = 0.30793968\n",
      "Iteration 592, loss = 0.30786646\n",
      "Iteration 593, loss = 0.30767277\n",
      "Iteration 594, loss = 0.30754469\n",
      "Iteration 595, loss = 0.30740181\n",
      "Iteration 596, loss = 0.30728076\n",
      "Iteration 597, loss = 0.30714584\n",
      "Iteration 598, loss = 0.30702751\n",
      "Iteration 599, loss = 0.30687671\n",
      "Iteration 600, loss = 0.30676283\n",
      "Iteration 601, loss = 0.30665024\n",
      "Iteration 602, loss = 0.30652250\n",
      "Iteration 603, loss = 0.30640598\n",
      "Iteration 604, loss = 0.30624868\n",
      "Iteration 605, loss = 0.30610906\n",
      "Iteration 606, loss = 0.30598568\n",
      "Iteration 607, loss = 0.30594683\n",
      "Iteration 608, loss = 0.30579301\n",
      "Iteration 609, loss = 0.30559346\n",
      "Iteration 610, loss = 0.30547410\n",
      "Iteration 611, loss = 0.30534910\n",
      "Iteration 612, loss = 0.30521542\n",
      "Iteration 613, loss = 0.30507326\n",
      "Iteration 614, loss = 0.30501698\n",
      "Iteration 615, loss = 0.30484312\n",
      "Iteration 616, loss = 0.30470761\n",
      "Iteration 617, loss = 0.30457704\n",
      "Iteration 618, loss = 0.30446031\n",
      "Iteration 619, loss = 0.30431986\n",
      "Iteration 620, loss = 0.30419213\n",
      "Iteration 621, loss = 0.30405815\n",
      "Iteration 622, loss = 0.30394727\n",
      "Iteration 623, loss = 0.30380769\n",
      "Iteration 624, loss = 0.30369690\n",
      "Iteration 625, loss = 0.30356411\n",
      "Iteration 626, loss = 0.30341606\n",
      "Iteration 627, loss = 0.30334435\n",
      "Iteration 628, loss = 0.30316994\n",
      "Iteration 629, loss = 0.30302801\n",
      "Iteration 630, loss = 0.30289388\n",
      "Iteration 631, loss = 0.30281207\n",
      "Iteration 632, loss = 0.30264611\n",
      "Iteration 633, loss = 0.30252982\n",
      "Iteration 634, loss = 0.30240419\n",
      "Iteration 635, loss = 0.30226414\n",
      "Iteration 636, loss = 0.30214268\n",
      "Iteration 637, loss = 0.30203254\n",
      "Iteration 638, loss = 0.30188136\n",
      "Iteration 639, loss = 0.30179546\n",
      "Iteration 640, loss = 0.30164134\n",
      "Iteration 641, loss = 0.30149441\n",
      "Iteration 642, loss = 0.30137199\n",
      "Iteration 643, loss = 0.30129535\n",
      "Iteration 644, loss = 0.30112905\n",
      "Iteration 645, loss = 0.30099651\n",
      "Iteration 646, loss = 0.30083510\n",
      "Iteration 647, loss = 0.30072758\n",
      "Iteration 648, loss = 0.30059873\n",
      "Iteration 649, loss = 0.30047888\n",
      "Iteration 650, loss = 0.30035397\n",
      "Iteration 651, loss = 0.30020845\n",
      "Iteration 652, loss = 0.30011790\n",
      "Iteration 653, loss = 0.29996279\n",
      "Iteration 654, loss = 0.29988147\n",
      "Iteration 655, loss = 0.29969620\n",
      "Iteration 656, loss = 0.29963415\n",
      "Iteration 657, loss = 0.29944370\n",
      "Iteration 658, loss = 0.29933412\n",
      "Iteration 659, loss = 0.29919581\n",
      "Iteration 660, loss = 0.29909808\n",
      "Iteration 661, loss = 0.29895617\n",
      "Iteration 662, loss = 0.29881767\n",
      "Iteration 663, loss = 0.29870403\n",
      "Iteration 664, loss = 0.29858351\n",
      "Iteration 665, loss = 0.29843553\n",
      "Iteration 666, loss = 0.29830494\n",
      "Iteration 667, loss = 0.29818112\n",
      "Iteration 668, loss = 0.29805304\n",
      "Iteration 669, loss = 0.29794088\n",
      "Iteration 670, loss = 0.29781414\n",
      "Iteration 671, loss = 0.29770002\n",
      "Iteration 672, loss = 0.29756401\n",
      "Iteration 673, loss = 0.29741996\n",
      "Iteration 674, loss = 0.29730205\n",
      "Iteration 675, loss = 0.29717871\n",
      "Iteration 676, loss = 0.29705716\n",
      "Iteration 677, loss = 0.29695335\n",
      "Iteration 678, loss = 0.29680374\n",
      "Iteration 679, loss = 0.29668438\n",
      "Iteration 680, loss = 0.29652132\n",
      "Iteration 681, loss = 0.29640644\n",
      "Iteration 682, loss = 0.29630564\n",
      "Iteration 683, loss = 0.29614354\n",
      "Iteration 684, loss = 0.29601893\n",
      "Iteration 685, loss = 0.29589222\n",
      "Iteration 686, loss = 0.29576343\n",
      "Iteration 687, loss = 0.29562435\n",
      "Iteration 688, loss = 0.29551938\n",
      "Iteration 689, loss = 0.29537947\n",
      "Iteration 690, loss = 0.29529842\n",
      "Iteration 691, loss = 0.29512180\n",
      "Iteration 692, loss = 0.29499232\n",
      "Iteration 693, loss = 0.29491655\n",
      "Iteration 694, loss = 0.29474454\n",
      "Iteration 695, loss = 0.29465044\n",
      "Iteration 696, loss = 0.29449842\n",
      "Iteration 697, loss = 0.29435743\n",
      "Iteration 698, loss = 0.29425838\n",
      "Iteration 699, loss = 0.29408662\n",
      "Iteration 700, loss = 0.29396396\n",
      "Iteration 701, loss = 0.29386228\n",
      "Iteration 702, loss = 0.29371417\n",
      "Iteration 703, loss = 0.29357372\n",
      "Iteration 704, loss = 0.29347460\n",
      "Iteration 705, loss = 0.29333400\n",
      "Iteration 706, loss = 0.29323741\n",
      "Iteration 707, loss = 0.29307413\n",
      "Iteration 708, loss = 0.29296597\n",
      "Iteration 709, loss = 0.29283097\n",
      "Iteration 710, loss = 0.29269466\n",
      "Iteration 711, loss = 0.29258339\n",
      "Iteration 712, loss = 0.29244301\n",
      "Iteration 713, loss = 0.29231011\n",
      "Iteration 714, loss = 0.29218878\n",
      "Iteration 715, loss = 0.29209891\n",
      "Iteration 716, loss = 0.29194362\n",
      "Iteration 717, loss = 0.29179895\n",
      "Iteration 718, loss = 0.29169995\n",
      "Iteration 719, loss = 0.29154433\n",
      "Iteration 720, loss = 0.29143591\n",
      "Iteration 721, loss = 0.29129919\n",
      "Iteration 722, loss = 0.29119215\n",
      "Iteration 723, loss = 0.29103674\n",
      "Iteration 724, loss = 0.29091707\n",
      "Iteration 725, loss = 0.29078468\n",
      "Iteration 726, loss = 0.29065438\n",
      "Iteration 727, loss = 0.29053128\n",
      "Iteration 728, loss = 0.29040706\n",
      "Iteration 729, loss = 0.29027126\n",
      "Iteration 730, loss = 0.29015739\n",
      "Iteration 731, loss = 0.29004752\n",
      "Iteration 732, loss = 0.28991068\n",
      "Iteration 733, loss = 0.28980655\n",
      "Iteration 734, loss = 0.28965497\n",
      "Iteration 735, loss = 0.28953820\n",
      "Iteration 736, loss = 0.28940028\n",
      "Iteration 737, loss = 0.28925371\n",
      "Iteration 738, loss = 0.28910978\n",
      "Iteration 739, loss = 0.28898853\n",
      "Iteration 740, loss = 0.28888806\n",
      "Iteration 741, loss = 0.28872665\n",
      "Iteration 742, loss = 0.28861179\n",
      "Iteration 743, loss = 0.28847179\n",
      "Iteration 744, loss = 0.28835426\n",
      "Iteration 745, loss = 0.28821933\n",
      "Iteration 746, loss = 0.28812514\n",
      "Iteration 747, loss = 0.28796753\n",
      "Iteration 748, loss = 0.28786206\n",
      "Iteration 749, loss = 0.28771073\n",
      "Iteration 750, loss = 0.28759802\n",
      "Iteration 751, loss = 0.28744179\n",
      "Iteration 752, loss = 0.28734119\n",
      "Iteration 753, loss = 0.28719558\n",
      "Iteration 754, loss = 0.28705511\n",
      "Iteration 755, loss = 0.28693771\n",
      "Iteration 756, loss = 0.28681981\n",
      "Iteration 757, loss = 0.28667877\n",
      "Iteration 758, loss = 0.28654490\n",
      "Iteration 759, loss = 0.28643014\n",
      "Iteration 760, loss = 0.28631300\n",
      "Iteration 761, loss = 0.28615559\n",
      "Iteration 762, loss = 0.28605809\n",
      "Iteration 763, loss = 0.28590545\n",
      "Iteration 764, loss = 0.28579413\n",
      "Iteration 765, loss = 0.28563481\n",
      "Iteration 766, loss = 0.28554075\n",
      "Iteration 767, loss = 0.28538227\n",
      "Iteration 768, loss = 0.28526620\n",
      "Iteration 769, loss = 0.28511278\n",
      "Iteration 770, loss = 0.28500952\n",
      "Iteration 771, loss = 0.28486725\n",
      "Iteration 772, loss = 0.28472908\n",
      "Iteration 773, loss = 0.28461039\n",
      "Iteration 774, loss = 0.28448473\n",
      "Iteration 775, loss = 0.28434537\n",
      "Iteration 776, loss = 0.28424458\n",
      "Iteration 777, loss = 0.28408511\n",
      "Iteration 778, loss = 0.28399831\n",
      "Iteration 779, loss = 0.28383680\n",
      "Iteration 780, loss = 0.28370885\n",
      "Iteration 781, loss = 0.28356695\n",
      "Iteration 782, loss = 0.28347467\n",
      "Iteration 783, loss = 0.28332870\n",
      "Iteration 784, loss = 0.28324103\n",
      "Iteration 785, loss = 0.28308506\n",
      "Iteration 786, loss = 0.28295025\n",
      "Iteration 787, loss = 0.28281326\n",
      "Iteration 788, loss = 0.28268195\n",
      "Iteration 789, loss = 0.28254980\n",
      "Iteration 1753, loss = 0.20393473\n",
      "Iteration 1754, loss = 0.20387134\n",
      "Iteration 1755, loss = 0.20376372\n",
      "Iteration 1756, loss = 0.20356214\n",
      "Iteration 1757, loss = 0.20353461\n",
      "Iteration 1758, loss = 0.20354767\n",
      "Iteration 1759, loss = 0.20346330\n",
      "Iteration 1760, loss = 0.20315580\n",
      "Iteration 1761, loss = 0.20305376\n",
      "Iteration 1762, loss = 0.20300284\n",
      "Iteration 1763, loss = 0.20288314\n",
      "Iteration 1764, loss = 0.20279119\n",
      "Iteration 1765, loss = 0.20265512\n",
      "Iteration 1766, loss = 0.20267813\n",
      "Iteration 1767, loss = 0.20244641\n",
      "Iteration 1768, loss = 0.20230247\n",
      "Iteration 1769, loss = 0.20222302\n",
      "Iteration 1770, loss = 0.20219251\n",
      "Iteration 1771, loss = 0.20203216\n",
      "Iteration 1772, loss = 0.20200444\n",
      "Iteration 1773, loss = 0.20180157\n",
      "Iteration 1774, loss = 0.20170422\n",
      "Iteration 1775, loss = 0.20162689\n",
      "Iteration 1776, loss = 0.20153408\n",
      "Iteration 1777, loss = 0.20161608\n",
      "Iteration 1778, loss = 0.20127304\n",
      "Iteration 1779, loss = 0.20117009\n",
      "Iteration 1780, loss = 0.20105623\n",
      "Iteration 1781, loss = 0.20114320\n",
      "Iteration 1782, loss = 0.20089780\n",
      "Iteration 1783, loss = 0.20079385\n",
      "Iteration 1784, loss = 0.20063276\n",
      "Iteration 1785, loss = 0.20061852\n",
      "Iteration 1786, loss = 0.20040418\n",
      "Iteration 1787, loss = 0.20048174\n",
      "Iteration 1788, loss = 0.20023260\n",
      "Iteration 1789, loss = 0.20013509\n",
      "Iteration 1790, loss = 0.20006498\n",
      "Iteration 1791, loss = 0.19997792\n",
      "Iteration 1792, loss = 0.19990932\n",
      "Iteration 1793, loss = 0.19972018\n",
      "Iteration 1794, loss = 0.19962749\n",
      "Iteration 1795, loss = 0.19955409\n",
      "Iteration 1796, loss = 0.19941433\n",
      "Iteration 1797, loss = 0.19932095\n",
      "Iteration 1798, loss = 0.19922102\n",
      "Iteration 1799, loss = 0.19908851\n",
      "Iteration 1800, loss = 0.19903889\n",
      "Iteration 1801, loss = 0.19896862\n",
      "Iteration 1802, loss = 0.19875919\n",
      "Iteration 1803, loss = 0.19864760\n",
      "Iteration 1804, loss = 0.19854534\n",
      "Iteration 1805, loss = 0.19844764\n",
      "Iteration 1806, loss = 0.19833520\n",
      "Iteration 1807, loss = 0.19830410\n",
      "Iteration 1808, loss = 0.19815747\n",
      "Iteration 1809, loss = 0.19806365\n",
      "Iteration 1810, loss = 0.19801388\n",
      "Iteration 1811, loss = 0.19785915\n",
      "Iteration 1812, loss = 0.19791286\n",
      "Iteration 1813, loss = 0.19769594\n",
      "Iteration 1814, loss = 0.19748093\n",
      "Iteration 1815, loss = 0.19745863\n",
      "Iteration 1816, loss = 0.19735146\n",
      "Iteration 1817, loss = 0.19724983\n",
      "Iteration 1818, loss = 0.19710850\n",
      "Iteration 1819, loss = 0.19705234\n",
      "Iteration 1820, loss = 0.19688184\n",
      "Iteration 1821, loss = 0.19680552\n",
      "Iteration 1822, loss = 0.19664899\n",
      "Iteration 1823, loss = 0.19660126\n",
      "Iteration 1824, loss = 0.19647078\n",
      "Iteration 1825, loss = 0.19637023\n",
      "Iteration 1826, loss = 0.19628572\n",
      "Iteration 1827, loss = 0.19620929\n",
      "Iteration 1828, loss = 0.19604405\n",
      "Iteration 1829, loss = 0.19591525\n",
      "Iteration 1830, loss = 0.19586571\n",
      "Iteration 1831, loss = 0.19580104\n",
      "Iteration 1832, loss = 0.19563394\n",
      "Iteration 1833, loss = 0.19558004\n",
      "Iteration 1834, loss = 0.19545576\n",
      "Iteration 1835, loss = 0.19531787\n",
      "Iteration 1836, loss = 0.19526005\n",
      "Iteration 1837, loss = 0.19515280\n",
      "Iteration 1838, loss = 0.19502633\n",
      "Iteration 1839, loss = 0.19494601\n",
      "Iteration 1840, loss = 0.19483498\n",
      "Iteration 1841, loss = 0.19477268\n",
      "Iteration 1842, loss = 0.19461857\n",
      "Iteration 1843, loss = 0.19454719\n",
      "Iteration 1844, loss = 0.19457216\n",
      "Iteration 1845, loss = 0.19431204\n",
      "Iteration 1846, loss = 0.19420558\n",
      "Iteration 1847, loss = 0.19411287\n",
      "Iteration 1848, loss = 0.19405304\n",
      "Iteration 1849, loss = 0.19390340\n",
      "Iteration 1850, loss = 0.19384451\n",
      "Iteration 1851, loss = 0.19373471\n",
      "Iteration 1852, loss = 0.19366215\n",
      "Iteration 1853, loss = 0.19355366\n",
      "Iteration 1854, loss = 0.19337937\n",
      "Iteration 1855, loss = 0.19327731\n",
      "Iteration 1856, loss = 0.19315664\n",
      "Iteration 1857, loss = 0.19308925\n",
      "Iteration 1858, loss = 0.19310241\n",
      "Iteration 1859, loss = 0.19290631\n",
      "Iteration 1860, loss = 0.19276006\n",
      "Iteration 1861, loss = 0.19271947\n",
      "Iteration 1862, loss = 0.19262804\n",
      "Iteration 1863, loss = 0.19253184\n",
      "Iteration 1864, loss = 0.19250195\n",
      "Iteration 1865, loss = 0.19234110\n",
      "Iteration 1866, loss = 0.19224164\n",
      "Iteration 1867, loss = 0.19203790\n",
      "Iteration 1868, loss = 0.19201663\n",
      "Iteration 1869, loss = 0.19189455\n",
      "Iteration 1870, loss = 0.19205280\n",
      "Iteration 1871, loss = 0.19178224\n",
      "Iteration 1872, loss = 0.19178434\n",
      "Iteration 1873, loss = 0.19152340\n",
      "Iteration 1874, loss = 0.19135669\n",
      "Iteration 1875, loss = 0.19129185\n",
      "Iteration 1876, loss = 0.19116150\n",
      "Iteration 1877, loss = 0.19108185\n",
      "Iteration 1878, loss = 0.19096908\n",
      "Iteration 1879, loss = 0.19094948\n",
      "Iteration 1880, loss = 0.19073986\n",
      "Iteration 1881, loss = 0.19074907\n",
      "Iteration 1882, loss = 0.19055388\n",
      "Iteration 1883, loss = 0.19044751\n",
      "Iteration 1884, loss = 0.19041769\n",
      "Iteration 1885, loss = 0.19036906\n",
      "Iteration 1886, loss = 0.19018564\n",
      "Iteration 1887, loss = 0.19005402\n",
      "Iteration 1888, loss = 0.19003520\n",
      "Iteration 1889, loss = 0.18989860\n",
      "Iteration 1890, loss = 0.18977683\n",
      "Iteration 1891, loss = 0.18966509\n",
      "Iteration 1892, loss = 0.18955780\n",
      "Iteration 1893, loss = 0.18951677\n",
      "Iteration 1894, loss = 0.18930552\n",
      "Iteration 1895, loss = 0.18924870\n",
      "Iteration 1896, loss = 0.18921466\n",
      "Iteration 1897, loss = 0.18900587\n",
      "Iteration 1898, loss = 0.18892594\n",
      "Iteration 1899, loss = 0.18883800\n",
      "Iteration 1900, loss = 0.18870857\n",
      "Iteration 1901, loss = 0.18874956\n",
      "Iteration 1902, loss = 0.18857381\n",
      "Iteration 1903, loss = 0.18846110\n",
      "Iteration 1904, loss = 0.18836641\n",
      "Iteration 1905, loss = 0.18824343\n",
      "Iteration 1906, loss = 0.18815433\n",
      "Iteration 1907, loss = 0.18805429\n",
      "Iteration 1908, loss = 0.18795444\n",
      "Iteration 1909, loss = 0.18786763\n",
      "Iteration 1910, loss = 0.18783732\n",
      "Iteration 1911, loss = 0.18763410\n",
      "Iteration 1912, loss = 0.18754637\n",
      "Iteration 1913, loss = 0.18744158\n",
      "Iteration 1914, loss = 0.18734153\n",
      "Iteration 1915, loss = 0.18728237\n",
      "Iteration 1916, loss = 0.18717001\n",
      "Iteration 1917, loss = 0.18708669\n",
      "Iteration 1918, loss = 0.18694594\n",
      "Iteration 1919, loss = 0.18691293\n",
      "Iteration 1920, loss = 0.18680982\n",
      "Iteration 1921, loss = 0.18669910\n",
      "Iteration 1922, loss = 0.18666295\n",
      "Iteration 1923, loss = 0.18649434\n",
      "Iteration 1924, loss = 0.18636101\n",
      "Iteration 1925, loss = 0.18626509\n",
      "Iteration 1926, loss = 0.18612704\n",
      "Iteration 1927, loss = 0.18612628\n",
      "Iteration 1928, loss = 0.18600759\n",
      "Iteration 1929, loss = 0.18588309\n",
      "Iteration 1930, loss = 0.18584977\n",
      "Iteration 1931, loss = 0.18572911\n",
      "Iteration 1932, loss = 0.18555599\n",
      "Iteration 1933, loss = 0.18549416\n",
      "Iteration 1934, loss = 0.18535127\n",
      "Iteration 1935, loss = 0.18528388\n",
      "Iteration 1936, loss = 0.18516079\n",
      "Iteration 1937, loss = 0.18520632\n",
      "Iteration 1938, loss = 0.18493484\n",
      "Iteration 1939, loss = 0.18487641\n",
      "Iteration 1940, loss = 0.18484280\n",
      "Iteration 1941, loss = 0.18476237\n",
      "Iteration 1942, loss = 0.18460848\n",
      "Iteration 1943, loss = 0.18451227\n",
      "Iteration 1944, loss = 0.18441294\n",
      "Iteration 1945, loss = 0.18433868\n",
      "Iteration 1946, loss = 0.18423572\n",
      "Iteration 1947, loss = 0.18413284\n",
      "Iteration 1948, loss = 0.18396509\n",
      "Iteration 1949, loss = 0.18394562\n",
      "Iteration 1950, loss = 0.18375508\n",
      "Iteration 1951, loss = 0.18370266\n",
      "Iteration 1952, loss = 0.18355360\n",
      "Iteration 1953, loss = 0.18352746\n",
      "Iteration 1954, loss = 0.18336502\n",
      "Iteration 1955, loss = 0.18350465\n",
      "Iteration 1956, loss = 0.18322311\n",
      "Iteration 1957, loss = 0.18310257\n",
      "Iteration 1958, loss = 0.18300034\n",
      "Iteration 1959, loss = 0.18287530\n",
      "Iteration 1960, loss = 0.18290093\n",
      "Iteration 1961, loss = 0.18266797\n",
      "Iteration 1962, loss = 0.18269408\n",
      "Iteration 1963, loss = 0.18262854\n",
      "Iteration 1964, loss = 0.18237657\n",
      "Iteration 1965, loss = 0.18230777\n",
      "Iteration 1966, loss = 0.18220128\n",
      "Iteration 1967, loss = 0.18213952\n",
      "Iteration 1968, loss = 0.18198229\n",
      "Iteration 1969, loss = 0.18189458\n",
      "Iteration 1970, loss = 0.18176815\n",
      "Iteration 1971, loss = 0.18173675\n",
      "Iteration 1972, loss = 0.18176354\n",
      "Iteration 1973, loss = 0.18153305\n",
      "Iteration 1974, loss = 0.18139843\n",
      "Iteration 1975, loss = 0.18130551\n",
      "Iteration 1976, loss = 0.18123360\n",
      "Iteration 1977, loss = 0.18111416\n",
      "Iteration 1978, loss = 0.18101115\n",
      "Iteration 1979, loss = 0.18090483\n",
      "Iteration 1980, loss = 0.18081016\n",
      "Iteration 1981, loss = 0.18075667\n",
      "Iteration 1982, loss = 0.18068248\n",
      "Iteration 1983, loss = 0.18048491\n",
      "Iteration 1984, loss = 0.18050244\n",
      "Iteration 1985, loss = 0.18028053\n",
      "Iteration 1986, loss = 0.18026851\n",
      "Iteration 1987, loss = 0.18012274\n",
      "Iteration 1988, loss = 0.18000982\n",
      "Iteration 1989, loss = 0.17991667\n",
      "Iteration 1990, loss = 0.17982453\n",
      "Iteration 1991, loss = 0.17974781\n",
      "Iteration 1992, loss = 0.17964401\n",
      "Iteration 1993, loss = 0.17984441\n",
      "Iteration 1994, loss = 0.17941536\n",
      "Iteration 1995, loss = 0.17939731\n",
      "Iteration 1996, loss = 0.17931475\n",
      "Iteration 1997, loss = 0.17915556\n",
      "Iteration 1998, loss = 0.17907207\n",
      "Iteration 1999, loss = 0.17899331\n",
      "Iteration 2000, loss = 0.17896479\n",
      "Iteration 2001, loss = 0.17886558\n",
      "Iteration 2002, loss = 0.17865741\n",
      "Iteration 2003, loss = 0.17860983\n",
      "Iteration 2004, loss = 0.17850396\n",
      "Iteration 2005, loss = 0.17855885\n",
      "Iteration 2006, loss = 0.17827347\n",
      "Iteration 2007, loss = 0.17822422\n",
      "Iteration 2008, loss = 0.17810964\n",
      "Iteration 2009, loss = 0.17796963\n",
      "Iteration 2010, loss = 0.17794659\n",
      "Iteration 2011, loss = 0.17782191\n",
      "Iteration 2012, loss = 0.17778058\n",
      "Iteration 2013, loss = 0.17765170\n",
      "Iteration 2014, loss = 0.17751204\n",
      "Iteration 2015, loss = 0.17740506\n",
      "Iteration 2016, loss = 0.17739730\n",
      "Iteration 2017, loss = 0.17719049\n",
      "Iteration 2018, loss = 0.17715022\n",
      "Iteration 2019, loss = 0.17709591\n",
      "Iteration 2020, loss = 0.17694912\n",
      "Iteration 2021, loss = 0.17681219\n",
      "Iteration 2022, loss = 0.17674662\n",
      "Iteration 2023, loss = 0.17674462\n",
      "Iteration 2024, loss = 0.17654963\n",
      "Iteration 2025, loss = 0.17652680\n",
      "Iteration 2026, loss = 0.17635825\n",
      "Iteration 2027, loss = 0.17625616\n",
      "Iteration 2028, loss = 0.17619798\n",
      "Iteration 2029, loss = 0.17615345\n",
      "Iteration 2030, loss = 0.17597403\n",
      "Iteration 2031, loss = 0.17587153\n",
      "Iteration 2032, loss = 0.17581257\n",
      "Iteration 2033, loss = 0.17578365\n",
      "Iteration 2034, loss = 0.17567132\n",
      "Iteration 2035, loss = 0.17550992\n",
      "Iteration 2036, loss = 0.17542473\n",
      "Iteration 2037, loss = 0.17529570\n",
      "Iteration 2038, loss = 0.17524251\n",
      "Iteration 2039, loss = 0.17514239\n",
      "Iteration 2040, loss = 0.17500280\n",
      "Iteration 2041, loss = 0.17494338\n",
      "Iteration 2042, loss = 0.17484129\n",
      "Iteration 2043, loss = 0.17488714\n",
      "Iteration 2044, loss = 0.17463929\n",
      "Iteration 2045, loss = 0.17451763\n",
      "Iteration 2046, loss = 0.17446168\n",
      "Iteration 2047, loss = 0.17439535\n",
      "Iteration 2048, loss = 0.17442326\n",
      "Iteration 2049, loss = 0.17419946\n",
      "Iteration 2050, loss = 0.17413393\n",
      "Iteration 2051, loss = 0.17406465\n",
      "Iteration 2052, loss = 0.17382801\n",
      "Iteration 2053, loss = 0.17379503\n",
      "Iteration 2054, loss = 0.17366920\n",
      "Iteration 2055, loss = 0.17358062\n",
      "Iteration 2056, loss = 0.17346971\n",
      "Iteration 2057, loss = 0.17346899\n",
      "Iteration 2058, loss = 0.17335516\n",
      "Iteration 2059, loss = 0.17346928\n",
      "Iteration 2060, loss = 0.17313076\n",
      "Iteration 2061, loss = 0.17315274\n",
      "Iteration 2062, loss = 0.17295728\n",
      "Iteration 2063, loss = 0.17285816\n",
      "Iteration 2064, loss = 0.17276241\n",
      "Iteration 2065, loss = 0.17266450\n",
      "Iteration 2066, loss = 0.17255562\n",
      "Iteration 2067, loss = 0.17245296\n",
      "Iteration 2068, loss = 0.17236657\n",
      "Iteration 2069, loss = 0.17224225\n",
      "Iteration 2070, loss = 0.17213799\n",
      "Iteration 2071, loss = 0.17204383\n",
      "Iteration 2072, loss = 0.17196368\n",
      "Iteration 2073, loss = 0.17181315\n",
      "Iteration 2074, loss = 0.17173820\n",
      "Iteration 2075, loss = 0.17167142\n",
      "Iteration 2076, loss = 0.17155324\n",
      "Iteration 2077, loss = 0.17150275\n",
      "Iteration 2078, loss = 0.17139946\n",
      "Iteration 2079, loss = 0.17126916\n",
      "Iteration 2080, loss = 0.17115569\n",
      "Iteration 2081, loss = 0.17104488\n",
      "Iteration 2082, loss = 0.17103293\n",
      "Iteration 2083, loss = 0.17089328\n",
      "Iteration 2084, loss = 0.17092007\n",
      "Iteration 2085, loss = 0.17073568\n",
      "Iteration 2086, loss = 0.17070191\n",
      "Iteration 2087, loss = 0.17054947\n",
      "Iteration 2088, loss = 0.17045541\n",
      "Iteration 2089, loss = 0.17036571\n",
      "Iteration 2090, loss = 0.17024213\n",
      "Iteration 2091, loss = 0.17021390\n",
      "Iteration 2092, loss = 0.17008710\n",
      "Iteration 2093, loss = 0.16995728\n",
      "Iteration 2094, loss = 0.16987475\n",
      "Iteration 2095, loss = 0.16998683\n",
      "Iteration 2096, loss = 0.16991333\n",
      "Iteration 2097, loss = 0.16960782\n",
      "Iteration 2098, loss = 0.16946279\n",
      "Iteration 2099, loss = 0.16936983\n",
      "Iteration 2100, loss = 0.16933434\n",
      "Iteration 2101, loss = 0.16923071\n",
      "Iteration 2102, loss = 0.16911185\n",
      "Iteration 2103, loss = 0.16908119\n",
      "Iteration 2104, loss = 0.16889895\n",
      "Iteration 2105, loss = 0.16882403\n",
      "Iteration 2106, loss = 0.16879469\n",
      "Iteration 2107, loss = 0.16865731\n",
      "Iteration 2108, loss = 0.16853339\n",
      "Iteration 2109, loss = 0.16842200\n",
      "Iteration 2110, loss = 0.16854127\n",
      "Iteration 2111, loss = 0.16826180\n",
      "Iteration 2112, loss = 0.16824712\n",
      "Iteration 2113, loss = 0.16808382\n",
      "Iteration 2114, loss = 0.16794700\n",
      "Iteration 2115, loss = 0.16791288\n",
      "Iteration 2116, loss = 0.16782949\n",
      "Iteration 2117, loss = 0.16769376\n",
      "Iteration 2118, loss = 0.16762342\n",
      "Iteration 2119, loss = 0.16750845\n",
      "Iteration 2120, loss = 0.16746905\n",
      "Iteration 2121, loss = 0.16733283\n",
      "Iteration 2122, loss = 0.16725000\n",
      "Iteration 2123, loss = 0.16723831\n",
      "Iteration 2124, loss = 0.16712894\n",
      "Iteration 2125, loss = 0.16705587\n",
      "Iteration 2126, loss = 0.16686048\n",
      "Iteration 2127, loss = 0.16674727\n",
      "Iteration 2128, loss = 0.16666091\n",
      "Iteration 2129, loss = 0.16666777\n",
      "Iteration 2130, loss = 0.16651828\n",
      "Iteration 2131, loss = 0.16650447\n",
      "Iteration 2132, loss = 0.16645428\n",
      "Iteration 2133, loss = 0.16618687\n",
      "Iteration 2134, loss = 0.16609785\n",
      "Iteration 2135, loss = 0.16603640\n",
      "Iteration 2136, loss = 0.16601880\n",
      "Iteration 2137, loss = 0.16598648\n",
      "Iteration 2138, loss = 0.16579647\n",
      "Iteration 2139, loss = 0.16586979\n",
      "Iteration 2140, loss = 0.16553589\n",
      "Iteration 2141, loss = 0.16549894\n",
      "Iteration 2142, loss = 0.16540512\n",
      "Iteration 2143, loss = 0.16535821\n",
      "Iteration 2144, loss = 0.16543481\n",
      "Iteration 2145, loss = 0.16510357\n",
      "Iteration 2146, loss = 0.16501494\n",
      "Iteration 2147, loss = 0.16489075\n",
      "Iteration 2148, loss = 0.16489075\n",
      "Iteration 2149, loss = 0.16478422\n",
      "Iteration 2150, loss = 0.16465305\n",
      "Iteration 2151, loss = 0.16465351\n",
      "Iteration 2152, loss = 0.16448324\n",
      "Iteration 2153, loss = 0.16438292\n",
      "Iteration 2154, loss = 0.16422799\n",
      "Iteration 2155, loss = 0.16420874\n",
      "Iteration 2156, loss = 0.16407295\n",
      "Iteration 2157, loss = 0.16394442\n",
      "Iteration 2158, loss = 0.16391692\n",
      "Iteration 2159, loss = 0.16378525\n",
      "Iteration 2160, loss = 0.16406050\n",
      "Iteration 2161, loss = 0.16361241\n",
      "Iteration 2162, loss = 0.16354781\n",
      "Iteration 2163, loss = 0.16340813\n",
      "Iteration 2164, loss = 0.16333612\n",
      "Iteration 2165, loss = 0.16338476\n",
      "Iteration 2166, loss = 0.16316031\n",
      "Iteration 2167, loss = 0.16305090\n",
      "Iteration 2168, loss = 0.16299387\n",
      "Iteration 2169, loss = 0.16286059\n",
      "Iteration 2170, loss = 0.16280465\n",
      "Iteration 2171, loss = 0.16267791\n",
      "Iteration 2172, loss = 0.16255359\n",
      "Iteration 2173, loss = 0.16251531\n",
      "Iteration 2174, loss = 0.16246934\n",
      "Iteration 2175, loss = 0.16231149\n",
      "Iteration 2176, loss = 0.16224393\n",
      "Iteration 2177, loss = 0.16214825\n",
      "Iteration 2178, loss = 0.16220650\n",
      "Iteration 2179, loss = 0.16203881\n",
      "Iteration 2180, loss = 0.16184667\n",
      "Iteration 2181, loss = 0.16177817\n",
      "Iteration 2182, loss = 0.16170721\n",
      "Iteration 2183, loss = 0.16156302\n",
      "Iteration 2184, loss = 0.16153212\n",
      "Iteration 2185, loss = 0.16145253\n",
      "Iteration 2186, loss = 0.16129175\n",
      "Iteration 2187, loss = 0.16119993\n",
      "Iteration 2188, loss = 0.16116696\n",
      "Iteration 2189, loss = 0.16115302\n",
      "Iteration 2190, loss = 0.16091903\n",
      "Iteration 2191, loss = 0.16085471\n",
      "Iteration 2192, loss = 0.16084727\n",
      "Iteration 2193, loss = 0.16063845\n",
      "Iteration 2194, loss = 0.16061102\n",
      "Iteration 2195, loss = 0.16065567\n",
      "Iteration 2196, loss = 0.16039821\n",
      "Iteration 2197, loss = 0.16025459\n",
      "Iteration 2198, loss = 0.16024112\n",
      "Iteration 2199, loss = 0.16008144\n",
      "Iteration 2200, loss = 0.16009844\n",
      "Iteration 2201, loss = 0.15996240\n",
      "Iteration 2202, loss = 0.15993306\n",
      "Iteration 2203, loss = 0.15967719\n",
      "Iteration 2204, loss = 0.15965776\n",
      "Iteration 2205, loss = 0.15966822\n",
      "Iteration 2206, loss = 0.15941986\n",
      "Iteration 2207, loss = 0.15935630\n",
      "Iteration 2208, loss = 0.15927265\n",
      "Iteration 2209, loss = 0.15913901\n",
      "Iteration 2210, loss = 0.15931039\n",
      "Iteration 2211, loss = 0.15898115\n",
      "Iteration 2212, loss = 0.15889742\n",
      "Iteration 2213, loss = 0.15881810\n",
      "Iteration 2214, loss = 0.15874789\n",
      "Iteration 2215, loss = 0.15859187\n",
      "Iteration 2216, loss = 0.15857159\n",
      "Iteration 2217, loss = 0.15842527\n",
      "Iteration 2218, loss = 0.15841639\n",
      "Iteration 2219, loss = 0.15823129\n",
      "Iteration 2220, loss = 0.15818371\n",
      "Iteration 2221, loss = 0.15809401\n",
      "Iteration 2222, loss = 0.15799925\n",
      "Iteration 2223, loss = 0.15790272\n",
      "Iteration 2224, loss = 0.15777192\n",
      "Iteration 2225, loss = 0.15769539\n",
      "Iteration 2226, loss = 0.15761001\n",
      "Iteration 2227, loss = 0.15762632\n",
      "Iteration 2228, loss = 0.15745952\n",
      "Iteration 2229, loss = 0.15728508\n",
      "Iteration 2230, loss = 0.15726069\n",
      "Iteration 2231, loss = 0.15713448\n",
      "Iteration 2232, loss = 0.15706467\n",
      "Iteration 2422, loss = 0.14111176\n",
      "Iteration 2423, loss = 0.14101132\n",
      "Iteration 2424, loss = 0.14103389\n",
      "Iteration 2425, loss = 0.14089161\n",
      "Iteration 2426, loss = 0.14084823\n",
      "Iteration 2427, loss = 0.14077873\n",
      "Iteration 2428, loss = 0.14072011\n",
      "Iteration 2429, loss = 0.14063364\n",
      "Iteration 2430, loss = 0.14055240\n",
      "Iteration 2431, loss = 0.14050444\n",
      "Iteration 2432, loss = 0.14038877\n",
      "Iteration 2433, loss = 0.14032431\n",
      "Iteration 2434, loss = 0.14027288\n",
      "Iteration 2435, loss = 0.14025106\n",
      "Iteration 2436, loss = 0.14014886\n",
      "Iteration 2437, loss = 0.14017599\n",
      "Iteration 2438, loss = 0.14020744\n",
      "Iteration 2439, loss = 0.13997076\n",
      "Iteration 2440, loss = 0.13992998\n",
      "Iteration 2441, loss = 0.13981845\n",
      "Iteration 2442, loss = 0.13972768\n",
      "Iteration 2443, loss = 0.13968316\n",
      "Iteration 2444, loss = 0.13971492\n",
      "Iteration 2445, loss = 0.13965368\n",
      "Iteration 2446, loss = 0.13947507\n",
      "Iteration 2447, loss = 0.13962140\n",
      "Iteration 2448, loss = 0.13944066\n",
      "Iteration 2449, loss = 0.13937484\n",
      "Iteration 2450, loss = 0.13917637\n",
      "Iteration 2451, loss = 0.13915939\n",
      "Iteration 2452, loss = 0.13917249\n",
      "Iteration 2453, loss = 0.13900953\n",
      "Iteration 2454, loss = 0.13889385\n",
      "Iteration 2455, loss = 0.13885677\n",
      "Iteration 2456, loss = 0.13882939\n",
      "Iteration 2457, loss = 0.13870901\n",
      "Iteration 2458, loss = 0.13866835\n",
      "Iteration 2459, loss = 0.13856088\n",
      "Iteration 2460, loss = 0.13848558\n",
      "Iteration 2461, loss = 0.13842990\n",
      "Iteration 2462, loss = 0.13843810\n",
      "Iteration 2463, loss = 0.13828645\n",
      "Iteration 2464, loss = 0.13823875\n",
      "Iteration 2465, loss = 0.13816457\n",
      "Iteration 2466, loss = 0.13815755\n",
      "Iteration 2467, loss = 0.13804380\n",
      "Iteration 2468, loss = 0.13794935\n",
      "Iteration 2469, loss = 0.13789352\n",
      "Iteration 2470, loss = 0.13785199\n",
      "Iteration 2471, loss = 0.13775082\n",
      "Iteration 2472, loss = 0.13766432\n",
      "Iteration 2473, loss = 0.13759737\n",
      "Iteration 2474, loss = 0.13754761\n",
      "Iteration 2475, loss = 0.13748175\n",
      "Iteration 2476, loss = 0.13740558\n",
      "Iteration 2477, loss = 0.13734817\n",
      "Iteration 2478, loss = 0.13726156\n",
      "Iteration 2479, loss = 0.13723233\n",
      "Iteration 2480, loss = 0.13716653\n",
      "Iteration 2481, loss = 0.13705793\n",
      "Iteration 2482, loss = 0.13704977\n",
      "Iteration 2483, loss = 0.13694189\n",
      "Iteration 2484, loss = 0.13685473\n",
      "Iteration 2485, loss = 0.13684338\n",
      "Iteration 2486, loss = 0.13678157\n",
      "Iteration 2487, loss = 0.13666802\n",
      "Iteration 2488, loss = 0.13661238\n",
      "Iteration 2489, loss = 0.13658994\n",
      "Iteration 2490, loss = 0.13650598\n",
      "Iteration 2491, loss = 0.13644365\n",
      "Iteration 2492, loss = 0.13629597\n",
      "Iteration 2493, loss = 0.13624602\n",
      "Iteration 2494, loss = 0.13624039\n",
      "Iteration 2495, loss = 0.13611436\n",
      "Iteration 2496, loss = 0.13610976\n",
      "Iteration 2497, loss = 0.13599431\n",
      "Iteration 2498, loss = 0.13595393\n",
      "Iteration 2499, loss = 0.13586731\n",
      "Iteration 2500, loss = 0.13575345\n",
      "Iteration 2501, loss = 0.13571253\n",
      "Iteration 2502, loss = 0.13571886\n",
      "Iteration 2503, loss = 0.13557139\n",
      "Iteration 2504, loss = 0.13550045\n",
      "Iteration 2505, loss = 0.13545745\n",
      "Iteration 2506, loss = 0.13535435\n",
      "Iteration 2507, loss = 0.13529122\n",
      "Iteration 2508, loss = 0.13527623\n",
      "Iteration 2509, loss = 0.13514448\n",
      "Iteration 2510, loss = 0.13512202\n",
      "Iteration 2511, loss = 0.13500925\n",
      "Iteration 2512, loss = 0.13497878\n",
      "Iteration 2513, loss = 0.13495887\n",
      "Iteration 2514, loss = 0.13488247\n",
      "Iteration 2515, loss = 0.13476330\n",
      "Iteration 2516, loss = 0.13469049\n",
      "Iteration 2517, loss = 0.13465704\n",
      "Iteration 2518, loss = 0.13453925\n",
      "Iteration 2519, loss = 0.13451656\n",
      "Iteration 2520, loss = 0.13440609\n",
      "Iteration 2521, loss = 0.13433583\n",
      "Iteration 2522, loss = 0.13431909\n",
      "Iteration 2523, loss = 0.13420912\n",
      "Iteration 2524, loss = 0.13414886\n",
      "Iteration 2525, loss = 0.13407438\n",
      "Iteration 2526, loss = 0.13402761\n",
      "Iteration 2527, loss = 0.13398878\n",
      "Iteration 2528, loss = 0.13387615\n",
      "Iteration 2529, loss = 0.13383136\n",
      "Iteration 2530, loss = 0.13372602\n",
      "Iteration 2531, loss = 0.13369340\n",
      "Iteration 2532, loss = 0.13359704\n",
      "Iteration 2533, loss = 0.13351444\n",
      "Iteration 2534, loss = 0.13345530\n",
      "Iteration 2535, loss = 0.13348344\n",
      "Iteration 2536, loss = 0.13347696\n",
      "Iteration 2537, loss = 0.13328747\n",
      "Iteration 2538, loss = 0.13321900\n",
      "Iteration 2539, loss = 0.13316925\n",
      "Iteration 2540, loss = 0.13309649\n",
      "Iteration 2541, loss = 0.13300010\n",
      "Iteration 2542, loss = 0.13302244\n",
      "Iteration 2543, loss = 0.13285890\n",
      "Iteration 2544, loss = 0.13289767\n",
      "Iteration 2545, loss = 0.13272011\n",
      "Iteration 2546, loss = 0.13281434\n",
      "Iteration 2547, loss = 0.13272466\n",
      "Iteration 2548, loss = 0.13250854\n",
      "Iteration 2549, loss = 0.13246407\n",
      "Iteration 2550, loss = 0.13248594\n",
      "Iteration 2551, loss = 0.13238839\n",
      "Iteration 2552, loss = 0.13227895\n",
      "Iteration 2553, loss = 0.13224653\n",
      "Iteration 2554, loss = 0.13215963\n",
      "Iteration 2555, loss = 0.13205924\n",
      "Iteration 2556, loss = 0.13200339\n",
      "Iteration 2557, loss = 0.13190719\n",
      "Iteration 2558, loss = 0.13183609\n",
      "Iteration 2559, loss = 0.13191283\n",
      "Iteration 2560, loss = 0.13175359\n",
      "Iteration 2561, loss = 0.13188421\n",
      "Iteration 2562, loss = 0.13159389\n",
      "Iteration 2563, loss = 0.13151397\n",
      "Iteration 2564, loss = 0.13147298\n",
      "Iteration 2565, loss = 0.13137263\n",
      "Iteration 2566, loss = 0.13132444\n",
      "Iteration 2567, loss = 0.13126358\n",
      "Iteration 2568, loss = 0.13118186\n",
      "Iteration 2569, loss = 0.13114152\n",
      "Iteration 2570, loss = 0.13104390\n",
      "Iteration 2571, loss = 0.13099241\n",
      "Iteration 2572, loss = 0.13091167\n",
      "Iteration 2573, loss = 0.13081942\n",
      "Iteration 2574, loss = 0.13077822\n",
      "Iteration 2575, loss = 0.13070579\n",
      "Iteration 2576, loss = 0.13064017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74596774\n",
      "Iteration 2, loss = 0.74511162\n",
      "Iteration 3, loss = 0.74369144\n",
      "Iteration 4, loss = 0.74230599\n",
      "Iteration 5, loss = 0.74027010\n",
      "Iteration 6, loss = 0.73828003\n",
      "Iteration 7, loss = 0.73631955\n",
      "Iteration 8, loss = 0.73444706\n",
      "Iteration 9, loss = 0.73242884\n",
      "Iteration 10, loss = 0.73053091\n",
      "Iteration 11, loss = 0.72854212\n",
      "Iteration 12, loss = 0.72665833\n",
      "Iteration 13, loss = 0.72465014\n",
      "Iteration 14, loss = 0.72294448\n",
      "Iteration 15, loss = 0.72110745\n",
      "Iteration 16, loss = 0.71928649\n",
      "Iteration 17, loss = 0.71744136\n",
      "Iteration 18, loss = 0.71568676\n",
      "Iteration 19, loss = 0.71390596\n",
      "Iteration 20, loss = 0.71211068\n",
      "Iteration 21, loss = 0.71030017\n",
      "Iteration 22, loss = 0.70848783\n",
      "Iteration 23, loss = 0.70669666\n",
      "Iteration 24, loss = 0.70488657\n",
      "Iteration 25, loss = 0.70316792\n",
      "Iteration 26, loss = 0.70126376\n",
      "Iteration 27, loss = 0.69947673\n",
      "Iteration 28, loss = 0.69758496\n",
      "Iteration 29, loss = 0.69580757\n",
      "Iteration 30, loss = 0.69390594\n",
      "Iteration 31, loss = 0.69201287\n",
      "Iteration 32, loss = 0.69004597\n",
      "Iteration 33, loss = 0.68814172\n",
      "Iteration 34, loss = 0.68611363\n",
      "Iteration 35, loss = 0.68408254\n",
      "Iteration 36, loss = 0.68200609\n",
      "Iteration 37, loss = 0.67991754\n",
      "Iteration 38, loss = 0.67778737\n",
      "Iteration 39, loss = 0.67555952\n",
      "Iteration 40, loss = 0.67339647\n",
      "Iteration 41, loss = 0.67100697\n",
      "Iteration 42, loss = 0.66874919\n",
      "Iteration 43, loss = 0.66633914\n",
      "Iteration 44, loss = 0.66398553\n",
      "Iteration 45, loss = 0.66146029\n",
      "Iteration 46, loss = 0.65898886\n",
      "Iteration 47, loss = 0.65640925\n",
      "Iteration 48, loss = 0.65384957\n",
      "Iteration 49, loss = 0.65119592\n",
      "Iteration 50, loss = 0.64850466\n",
      "Iteration 51, loss = 0.64572787\n",
      "Iteration 52, loss = 0.64292826\n",
      "Iteration 53, loss = 0.64016116\n",
      "Iteration 54, loss = 0.63724853\n",
      "Iteration 55, loss = 0.63437056\n",
      "Iteration 56, loss = 0.63134634\n",
      "Iteration 57, loss = 0.62839625\n",
      "Iteration 58, loss = 0.62529144\n",
      "Iteration 59, loss = 0.62222704\n",
      "Iteration 60, loss = 0.61912131\n",
      "Iteration 61, loss = 0.61597115\n",
      "Iteration 62, loss = 0.61284738\n",
      "Iteration 63, loss = 0.60967115\n",
      "Iteration 64, loss = 0.60633969\n",
      "Iteration 65, loss = 0.60311224\n",
      "Iteration 66, loss = 0.59990183\n",
      "Iteration 67, loss = 0.59643789\n",
      "Iteration 68, loss = 0.59312033\n",
      "Iteration 69, loss = 0.58972535\n",
      "Iteration 70, loss = 0.58643818\n",
      "Iteration 71, loss = 0.58288246\n",
      "Iteration 72, loss = 0.57945378\n",
      "Iteration 73, loss = 0.57609995\n",
      "Iteration 74, loss = 0.57252429\n",
      "Iteration 75, loss = 0.56917135\n",
      "Iteration 76, loss = 0.56560970\n",
      "Iteration 77, loss = 0.56211145\n",
      "Iteration 78, loss = 0.55854260\n",
      "Iteration 79, loss = 0.55505767\n",
      "Iteration 80, loss = 0.55158925\n",
      "Iteration 81, loss = 0.54802093\n",
      "Iteration 82, loss = 0.54452304\n",
      "Iteration 83, loss = 0.54098160\n",
      "Iteration 84, loss = 0.53749406\n",
      "Iteration 85, loss = 0.53404892\n",
      "Iteration 86, loss = 0.53053559\n",
      "Iteration 87, loss = 0.52708513\n",
      "Iteration 88, loss = 0.52372271\n",
      "Iteration 89, loss = 0.52030285\n",
      "Iteration 90, loss = 0.51685240\n",
      "Iteration 91, loss = 0.51357972\n",
      "Iteration 92, loss = 0.51016278\n",
      "Iteration 93, loss = 0.50699672\n",
      "Iteration 94, loss = 0.50366864\n",
      "Iteration 95, loss = 0.50046914\n",
      "Iteration 96, loss = 0.49732763\n",
      "Iteration 97, loss = 0.49412119\n",
      "Iteration 98, loss = 0.49117381\n",
      "Iteration 99, loss = 0.48824513\n",
      "Iteration 100, loss = 0.48515717\n",
      "Iteration 101, loss = 0.48240752\n",
      "Iteration 102, loss = 0.47952815\n",
      "Iteration 103, loss = 0.47663772\n",
      "Iteration 104, loss = 0.47397466\n",
      "Iteration 105, loss = 0.47122295\n",
      "Iteration 106, loss = 0.46860921\n",
      "Iteration 107, loss = 0.46609256\n",
      "Iteration 108, loss = 0.46359349\n",
      "Iteration 109, loss = 0.46105417\n",
      "Iteration 110, loss = 0.45870893\n",
      "Iteration 111, loss = 0.45641018\n",
      "Iteration 112, loss = 0.45398726\n",
      "Iteration 113, loss = 0.45180997\n",
      "Iteration 114, loss = 0.44959818\n",
      "Iteration 115, loss = 0.44743537\n",
      "Iteration 116, loss = 0.44537840\n",
      "Iteration 117, loss = 0.44335346\n",
      "Iteration 118, loss = 0.44137458\n",
      "Iteration 119, loss = 0.43955941\n",
      "Iteration 120, loss = 0.43765571\n",
      "Iteration 121, loss = 0.43581804\n",
      "Iteration 122, loss = 0.43415305\n",
      "Iteration 123, loss = 0.43233239\n",
      "Iteration 124, loss = 0.43078343\n",
      "Iteration 125, loss = 0.42921367\n",
      "Iteration 126, loss = 0.42758742\n",
      "Iteration 127, loss = 0.42601126\n",
      "Iteration 128, loss = 0.42462660\n",
      "Iteration 129, loss = 0.42313078\n",
      "Iteration 130, loss = 0.42172998\n",
      "Iteration 131, loss = 0.42046505\n",
      "Iteration 132, loss = 0.41913993\n",
      "Iteration 133, loss = 0.41787603\n",
      "Iteration 134, loss = 0.41652350\n",
      "Iteration 135, loss = 0.41545140\n",
      "Iteration 136, loss = 0.41420133\n",
      "Iteration 137, loss = 0.41317213\n",
      "Iteration 138, loss = 0.41199978\n",
      "Iteration 139, loss = 0.41092551\n",
      "Iteration 140, loss = 0.40986676\n",
      "Iteration 141, loss = 0.40892966\n",
      "Iteration 142, loss = 0.40789253\n",
      "Iteration 143, loss = 0.40692539\n",
      "Iteration 144, loss = 0.40595044\n",
      "Iteration 145, loss = 0.40509754\n",
      "Iteration 146, loss = 0.40420237\n",
      "Iteration 147, loss = 0.40334419\n",
      "Iteration 148, loss = 0.40253420\n",
      "Iteration 149, loss = 0.40168573\n",
      "Iteration 150, loss = 0.40085643\n",
      "Iteration 151, loss = 0.40004800\n",
      "Iteration 152, loss = 0.39934203\n",
      "Iteration 153, loss = 0.39861991\n",
      "Iteration 154, loss = 0.39781388\n",
      "Iteration 155, loss = 0.39715725\n",
      "Iteration 156, loss = 0.39642526\n",
      "Iteration 157, loss = 0.39577733\n",
      "Iteration 158, loss = 0.39506966\n",
      "Iteration 159, loss = 0.39442587\n",
      "Iteration 160, loss = 0.39379294\n",
      "Iteration 161, loss = 0.39311428\n",
      "Iteration 162, loss = 0.39256051\n",
      "Iteration 163, loss = 0.39193806\n",
      "Iteration 164, loss = 0.39132106\n",
      "Iteration 165, loss = 0.39074597\n",
      "Iteration 166, loss = 0.39023505\n",
      "Iteration 167, loss = 0.38965114\n",
      "Iteration 168, loss = 0.38905210\n",
      "Iteration 169, loss = 0.38853063\n",
      "Iteration 170, loss = 0.38799511\n",
      "Iteration 171, loss = 0.38753281\n",
      "Iteration 172, loss = 0.38699085\n",
      "Iteration 173, loss = 0.38661880\n",
      "Iteration 174, loss = 0.38590365\n",
      "Iteration 175, loss = 0.38539278\n",
      "Iteration 176, loss = 0.38491946\n",
      "Iteration 177, loss = 0.38440710\n",
      "Iteration 178, loss = 0.38391506\n",
      "Iteration 179, loss = 0.38344165\n",
      "Iteration 180, loss = 0.38299629\n",
      "Iteration 181, loss = 0.38251336\n",
      "Iteration 182, loss = 0.38207007\n",
      "Iteration 183, loss = 0.38161396\n",
      "Iteration 184, loss = 0.38112969\n",
      "Iteration 185, loss = 0.38069517\n",
      "Iteration 186, loss = 0.38022635\n",
      "Iteration 187, loss = 0.37978825\n",
      "Iteration 188, loss = 0.37936746\n",
      "Iteration 189, loss = 0.37894572\n",
      "Iteration 190, loss = 0.37856597\n",
      "Iteration 191, loss = 0.37809654\n",
      "Iteration 192, loss = 0.37776736\n",
      "Iteration 193, loss = 0.37727099\n",
      "Iteration 194, loss = 0.37689502\n",
      "Iteration 195, loss = 0.37649363\n",
      "Iteration 196, loss = 0.37607636\n",
      "Iteration 197, loss = 0.37569283\n",
      "Iteration 198, loss = 0.37531341\n",
      "Iteration 199, loss = 0.37490059\n",
      "Iteration 200, loss = 0.37454821\n",
      "Iteration 201, loss = 0.37417229\n",
      "Iteration 202, loss = 0.37373053\n",
      "Iteration 203, loss = 0.37342837\n",
      "Iteration 204, loss = 0.37301092\n",
      "Iteration 205, loss = 0.37266642\n",
      "Iteration 206, loss = 0.37228294\n",
      "Iteration 207, loss = 0.37191985\n",
      "Iteration 208, loss = 0.37156402\n",
      "Iteration 209, loss = 0.37117295\n",
      "Iteration 210, loss = 0.37083086\n",
      "Iteration 211, loss = 0.37048513\n",
      "Iteration 212, loss = 0.37014391\n",
      "Iteration 213, loss = 0.36982161\n",
      "Iteration 214, loss = 0.36945686\n",
      "Iteration 215, loss = 0.36913881\n",
      "Iteration 216, loss = 0.36878276\n",
      "Iteration 217, loss = 0.36844824\n",
      "Iteration 218, loss = 0.36813912\n",
      "Iteration 219, loss = 0.36776327\n",
      "Iteration 220, loss = 0.36748522\n",
      "Iteration 221, loss = 0.36716185\n",
      "Iteration 222, loss = 0.36679943\n",
      "Iteration 223, loss = 0.36649428\n",
      "Iteration 224, loss = 0.36614003\n",
      "Iteration 225, loss = 0.36585992\n",
      "Iteration 226, loss = 0.36553623\n",
      "Iteration 227, loss = 0.36520119\n",
      "Iteration 228, loss = 0.36497163\n",
      "Iteration 229, loss = 0.36459265\n",
      "Iteration 230, loss = 0.36429755\n",
      "Iteration 231, loss = 0.36397991\n",
      "Iteration 232, loss = 0.36372777\n",
      "Iteration 233, loss = 0.36340060\n",
      "Iteration 234, loss = 0.36312092\n",
      "Iteration 235, loss = 0.36280810\n",
      "Iteration 236, loss = 0.36252157\n",
      "Iteration 237, loss = 0.36221745\n",
      "Iteration 238, loss = 0.36191937\n",
      "Iteration 239, loss = 0.36166851\n",
      "Iteration 240, loss = 0.36135759\n",
      "Iteration 241, loss = 0.36109495\n",
      "Iteration 242, loss = 0.36085429\n",
      "Iteration 243, loss = 0.36049905\n",
      "Iteration 244, loss = 0.36023658\n",
      "Iteration 245, loss = 0.36002449\n",
      "Iteration 246, loss = 0.35972788\n",
      "Iteration 247, loss = 0.35941554\n",
      "Iteration 248, loss = 0.35919232\n",
      "Iteration 249, loss = 0.35895995\n",
      "Iteration 250, loss = 0.35862347\n",
      "Iteration 251, loss = 0.35835211\n",
      "Iteration 252, loss = 0.35809152\n",
      "Iteration 253, loss = 0.35784697\n",
      "Iteration 254, loss = 0.35755855\n",
      "Iteration 255, loss = 0.35734386\n",
      "Iteration 256, loss = 0.35704826\n",
      "Iteration 257, loss = 0.35678777\n",
      "Iteration 258, loss = 0.35655140\n",
      "Iteration 259, loss = 0.35627264\n",
      "Iteration 260, loss = 0.35604546\n",
      "Iteration 261, loss = 0.35580475\n",
      "Iteration 262, loss = 0.35553028\n",
      "Iteration 263, loss = 0.35528466\n",
      "Iteration 264, loss = 0.35503390\n",
      "Iteration 265, loss = 0.35477955\n",
      "Iteration 266, loss = 0.35455635\n",
      "Iteration 267, loss = 0.35437178\n",
      "Iteration 268, loss = 0.35404929\n",
      "Iteration 269, loss = 0.35387473\n",
      "Iteration 270, loss = 0.35360600\n",
      "Iteration 271, loss = 0.35343078\n",
      "Iteration 272, loss = 0.35310040\n",
      "Iteration 273, loss = 0.35282003\n",
      "Iteration 274, loss = 0.35259570\n",
      "Iteration 275, loss = 0.35236772\n",
      "Iteration 276, loss = 0.35213793\n",
      "Iteration 277, loss = 0.35188674\n",
      "Iteration 278, loss = 0.35164253\n",
      "Iteration 279, loss = 0.35149680\n",
      "Iteration 280, loss = 0.35112835\n",
      "Iteration 281, loss = 0.35091190\n",
      "Iteration 282, loss = 0.35067223\n",
      "Iteration 283, loss = 0.35049047\n",
      "Iteration 284, loss = 0.35020503\n",
      "Iteration 285, loss = 0.35000193\n",
      "Iteration 286, loss = 0.34975808\n",
      "Iteration 287, loss = 0.34962802\n",
      "Iteration 288, loss = 0.34931902\n",
      "Iteration 289, loss = 0.34906466\n",
      "Iteration 290, loss = 0.34893028\n",
      "Iteration 291, loss = 0.34866262\n",
      "Iteration 292, loss = 0.34839672\n",
      "Iteration 293, loss = 0.34822545\n",
      "Iteration 294, loss = 0.34797938\n",
      "Iteration 295, loss = 0.34779642\n",
      "Iteration 296, loss = 0.34754512\n",
      "Iteration 297, loss = 0.34736485\n",
      "Iteration 298, loss = 0.34713133\n",
      "Iteration 299, loss = 0.34695237\n",
      "Iteration 300, loss = 0.34669001\n",
      "Iteration 301, loss = 0.34648295\n",
      "Iteration 302, loss = 0.34627359\n",
      "Iteration 303, loss = 0.34607336\n",
      "Iteration 304, loss = 0.34592181\n",
      "Iteration 305, loss = 0.34570140\n",
      "Iteration 306, loss = 0.34545718\n",
      "Iteration 307, loss = 0.34527605\n",
      "Iteration 308, loss = 0.34508860\n",
      "Iteration 309, loss = 0.34492303\n",
      "Iteration 310, loss = 0.34473418\n",
      "Iteration 311, loss = 0.34449979\n",
      "Iteration 312, loss = 0.34433597\n",
      "Iteration 313, loss = 0.34411794\n",
      "Iteration 314, loss = 0.34393782\n",
      "Iteration 315, loss = 0.34374275\n",
      "Iteration 316, loss = 0.34354357\n",
      "Iteration 317, loss = 0.34337556\n",
      "Iteration 318, loss = 0.34321959\n",
      "Iteration 319, loss = 0.34301404\n",
      "Iteration 320, loss = 0.34285215\n",
      "Iteration 321, loss = 0.34265719\n",
      "Iteration 322, loss = 0.34245475\n",
      "Iteration 323, loss = 0.34226805\n",
      "Iteration 324, loss = 0.34208558\n",
      "Iteration 325, loss = 0.34192214\n",
      "Iteration 326, loss = 0.34171793\n",
      "Iteration 327, loss = 0.34154644\n",
      "Iteration 328, loss = 0.34134948\n",
      "Iteration 329, loss = 0.34117130\n",
      "Iteration 330, loss = 0.34101050\n",
      "Iteration 331, loss = 0.34087639\n",
      "Iteration 332, loss = 0.34065582\n",
      "Iteration 333, loss = 0.34052062\n",
      "Iteration 334, loss = 0.34029130\n",
      "Iteration 335, loss = 0.34009958\n",
      "Iteration 336, loss = 0.33993861\n",
      "Iteration 1745, loss = 0.16964647\n",
      "Iteration 1746, loss = 0.16952275\n",
      "Iteration 1747, loss = 0.16937688\n",
      "Iteration 1748, loss = 0.16921884\n",
      "Iteration 1749, loss = 0.16911083\n",
      "Iteration 1750, loss = 0.16901511\n",
      "Iteration 1751, loss = 0.16884617\n",
      "Iteration 1752, loss = 0.16878057\n",
      "Iteration 1753, loss = 0.16863715\n",
      "Iteration 1754, loss = 0.16852087\n",
      "Iteration 1755, loss = 0.16840280\n",
      "Iteration 1756, loss = 0.16844617\n",
      "Iteration 1757, loss = 0.16817827\n",
      "Iteration 1758, loss = 0.16800632\n",
      "Iteration 1759, loss = 0.16791443\n",
      "Iteration 1760, loss = 0.16777786\n",
      "Iteration 1761, loss = 0.16763896\n",
      "Iteration 1762, loss = 0.16755844\n",
      "Iteration 1763, loss = 0.16751061\n",
      "Iteration 1764, loss = 0.16726476\n",
      "Iteration 1765, loss = 0.16715052\n",
      "Iteration 1766, loss = 0.16707961\n",
      "Iteration 1767, loss = 0.16690285\n",
      "Iteration 1768, loss = 0.16681018\n",
      "Iteration 1769, loss = 0.16671003\n",
      "Iteration 1770, loss = 0.16655780\n",
      "Iteration 1771, loss = 0.16641630\n",
      "Iteration 1772, loss = 0.16634780\n",
      "Iteration 1773, loss = 0.16619266\n",
      "Iteration 1774, loss = 0.16606173\n",
      "Iteration 1775, loss = 0.16597572\n",
      "Iteration 1776, loss = 0.16581327\n",
      "Iteration 1777, loss = 0.16583544\n",
      "Iteration 1778, loss = 0.16561307\n",
      "Iteration 1779, loss = 0.16545530\n",
      "Iteration 1780, loss = 0.16541287\n",
      "Iteration 1781, loss = 0.16525473\n",
      "Iteration 1782, loss = 0.16510740\n",
      "Iteration 1783, loss = 0.16504671\n",
      "Iteration 1784, loss = 0.16484058\n",
      "Iteration 1785, loss = 0.16472855\n",
      "Iteration 1786, loss = 0.16473008\n",
      "Iteration 1787, loss = 0.16449698\n",
      "Iteration 1788, loss = 0.16440160\n",
      "Iteration 1789, loss = 0.16428660\n",
      "Iteration 1790, loss = 0.16422404\n",
      "Iteration 1791, loss = 0.16408631\n",
      "Iteration 1792, loss = 0.16409084\n",
      "Iteration 1793, loss = 0.16385805\n",
      "Iteration 1794, loss = 0.16374460\n",
      "Iteration 1795, loss = 0.16354887\n",
      "Iteration 1796, loss = 0.16348576\n",
      "Iteration 1797, loss = 0.16339421\n",
      "Iteration 1798, loss = 0.16322309\n",
      "Iteration 1799, loss = 0.16310797\n",
      "Iteration 1800, loss = 0.16295622\n",
      "Iteration 1801, loss = 0.16286521\n",
      "Iteration 1802, loss = 0.16271609\n",
      "Iteration 1803, loss = 0.16261269\n",
      "Iteration 1804, loss = 0.16248368\n",
      "Iteration 1805, loss = 0.16247332\n",
      "Iteration 1806, loss = 0.16237358\n",
      "Iteration 1807, loss = 0.16216170\n",
      "Iteration 1808, loss = 0.16202090\n",
      "Iteration 1809, loss = 0.16191806\n",
      "Iteration 1810, loss = 0.16179187\n",
      "Iteration 1811, loss = 0.16174248\n",
      "Iteration 1812, loss = 0.16155109\n",
      "Iteration 1813, loss = 0.16141615\n",
      "Iteration 1814, loss = 0.16136410\n",
      "Iteration 1815, loss = 0.16119969\n",
      "Iteration 1816, loss = 0.16105335\n",
      "Iteration 1817, loss = 0.16114396\n",
      "Iteration 1818, loss = 0.16084861\n",
      "Iteration 1819, loss = 0.16078002\n",
      "Iteration 1820, loss = 0.16058041\n",
      "Iteration 1821, loss = 0.16045605\n",
      "Iteration 1822, loss = 0.16037387\n",
      "Iteration 1823, loss = 0.16025265\n",
      "Iteration 1824, loss = 0.16006797\n",
      "Iteration 1825, loss = 0.15998992\n",
      "Iteration 1826, loss = 0.15986119\n",
      "Iteration 1827, loss = 0.15971203\n",
      "Iteration 1828, loss = 0.15959427\n",
      "Iteration 1829, loss = 0.15950054\n",
      "Iteration 1830, loss = 0.15941567\n",
      "Iteration 1831, loss = 0.15929723\n",
      "Iteration 1832, loss = 0.15913105\n",
      "Iteration 1833, loss = 0.15908899\n",
      "Iteration 1834, loss = 0.15892900\n",
      "Iteration 1835, loss = 0.15881373\n",
      "Iteration 1836, loss = 0.15866888\n",
      "Iteration 1837, loss = 0.15860039\n",
      "Iteration 1838, loss = 0.15848473\n",
      "Iteration 1839, loss = 0.15828194\n",
      "Iteration 1840, loss = 0.15817958\n",
      "Iteration 1841, loss = 0.15813669\n",
      "Iteration 1842, loss = 0.15798323\n",
      "Iteration 1843, loss = 0.15784621\n",
      "Iteration 1844, loss = 0.15776168\n",
      "Iteration 1845, loss = 0.15756920\n",
      "Iteration 1846, loss = 0.15751739\n",
      "Iteration 1847, loss = 0.15738701\n",
      "Iteration 1848, loss = 0.15733033\n",
      "Iteration 1849, loss = 0.15716117\n",
      "Iteration 1850, loss = 0.15709523\n",
      "Iteration 1851, loss = 0.15696895\n",
      "Iteration 1852, loss = 0.15678456\n",
      "Iteration 1853, loss = 0.15668132\n",
      "Iteration 1854, loss = 0.15656931\n",
      "Iteration 1855, loss = 0.15647756\n",
      "Iteration 1856, loss = 0.15634701\n",
      "Iteration 1857, loss = 0.15622666\n",
      "Iteration 1858, loss = 0.15609869\n",
      "Iteration 1859, loss = 0.15604806\n",
      "Iteration 1860, loss = 0.15583553\n",
      "Iteration 1861, loss = 0.15581147\n",
      "Iteration 1862, loss = 0.15562251\n",
      "Iteration 1863, loss = 0.15548235\n",
      "Iteration 1864, loss = 0.15539983\n",
      "Iteration 1865, loss = 0.15534243\n",
      "Iteration 1866, loss = 0.15521958\n",
      "Iteration 1867, loss = 0.15504457\n",
      "Iteration 1868, loss = 0.15493084\n",
      "Iteration 1869, loss = 0.15483357\n",
      "Iteration 1870, loss = 0.15477468\n",
      "Iteration 1871, loss = 0.15458316\n",
      "Iteration 1872, loss = 0.15446472\n",
      "Iteration 1873, loss = 0.15437850\n",
      "Iteration 1874, loss = 0.15423819\n",
      "Iteration 1875, loss = 0.15413099\n",
      "Iteration 1876, loss = 0.15406180\n",
      "Iteration 1877, loss = 0.15396998\n",
      "Iteration 1878, loss = 0.15377295\n",
      "Iteration 1879, loss = 0.15369336\n",
      "Iteration 1880, loss = 0.15356669\n",
      "Iteration 1881, loss = 0.15355903\n",
      "Iteration 1882, loss = 0.15332768\n",
      "Iteration 1883, loss = 0.15322652\n",
      "Iteration 1884, loss = 0.15315259\n",
      "Iteration 1885, loss = 0.15300141\n",
      "Iteration 1886, loss = 0.15287004\n",
      "Iteration 1887, loss = 0.15281715\n",
      "Iteration 1888, loss = 0.15269252\n",
      "Iteration 1889, loss = 0.15254137\n",
      "Iteration 1890, loss = 0.15242624\n",
      "Iteration 1891, loss = 0.15228928\n",
      "Iteration 1892, loss = 0.15218562\n",
      "Iteration 1893, loss = 0.15204999\n",
      "Iteration 1894, loss = 0.15194542\n",
      "Iteration 1895, loss = 0.15182106\n",
      "Iteration 1896, loss = 0.15173508\n",
      "Iteration 1897, loss = 0.15159273\n",
      "Iteration 1898, loss = 0.15154542\n",
      "Iteration 1899, loss = 0.15144656\n",
      "Iteration 1900, loss = 0.15134336\n",
      "Iteration 1901, loss = 0.15117845\n",
      "Iteration 1902, loss = 0.15104054\n",
      "Iteration 1903, loss = 0.15098832\n",
      "Iteration 1904, loss = 0.15088146\n",
      "Iteration 1905, loss = 0.15073632\n",
      "Iteration 1906, loss = 0.15076093\n",
      "Iteration 1907, loss = 0.15047145\n",
      "Iteration 1908, loss = 0.15036175\n",
      "Iteration 1909, loss = 0.15030008\n",
      "Iteration 1910, loss = 0.15020712\n",
      "Iteration 1911, loss = 0.15006107\n",
      "Iteration 1912, loss = 0.14995826\n",
      "Iteration 1913, loss = 0.14983288\n",
      "Iteration 1914, loss = 0.14974014\n",
      "Iteration 1915, loss = 0.14959310\n",
      "Iteration 1916, loss = 0.14967730\n",
      "Iteration 1917, loss = 0.14949067\n",
      "Iteration 1918, loss = 0.14926726\n",
      "Iteration 1919, loss = 0.14914926\n",
      "Iteration 1920, loss = 0.14902384\n",
      "Iteration 1921, loss = 0.14894724\n",
      "Iteration 1922, loss = 0.14881511\n",
      "Iteration 1923, loss = 0.14871440\n",
      "Iteration 1924, loss = 0.14864104\n",
      "Iteration 1925, loss = 0.14852169\n",
      "Iteration 1926, loss = 0.14840815\n",
      "Iteration 1927, loss = 0.14838652\n",
      "Iteration 1928, loss = 0.14825571\n",
      "Iteration 1929, loss = 0.14810576\n",
      "Iteration 1930, loss = 0.14797613\n",
      "Iteration 1931, loss = 0.14781027\n",
      "Iteration 1932, loss = 0.14779142\n",
      "Iteration 1933, loss = 0.14769248\n",
      "Iteration 1934, loss = 0.14754601\n",
      "Iteration 1935, loss = 0.14738453\n",
      "Iteration 1936, loss = 0.14728518\n",
      "Iteration 1937, loss = 0.14716372\n",
      "Iteration 1938, loss = 0.14710146\n",
      "Iteration 1939, loss = 0.14699272\n",
      "Iteration 1940, loss = 0.14685399\n",
      "Iteration 1941, loss = 0.14672329\n",
      "Iteration 1942, loss = 0.14662693\n",
      "Iteration 1943, loss = 0.14658002\n",
      "Iteration 1944, loss = 0.14642144\n",
      "Iteration 1945, loss = 0.14630261\n",
      "Iteration 1946, loss = 0.14625146\n",
      "Iteration 1947, loss = 0.14613500\n",
      "Iteration 1948, loss = 0.14597458\n",
      "Iteration 1949, loss = 0.14598106\n",
      "Iteration 1950, loss = 0.14578374\n",
      "Iteration 1951, loss = 0.14579184\n",
      "Iteration 1952, loss = 0.14573703\n",
      "Iteration 1953, loss = 0.14552148\n",
      "Iteration 1954, loss = 0.14539754\n",
      "Iteration 1955, loss = 0.14529264\n",
      "Iteration 1956, loss = 0.14513542\n",
      "Iteration 1957, loss = 0.14504806\n",
      "Iteration 1958, loss = 0.14492577\n",
      "Iteration 1959, loss = 0.14486010\n",
      "Iteration 1960, loss = 0.14470017\n",
      "Iteration 1961, loss = 0.14468485\n",
      "Iteration 1962, loss = 0.14473033\n",
      "Iteration 1963, loss = 0.14450679\n",
      "Iteration 1964, loss = 0.14434086\n",
      "Iteration 1965, loss = 0.14423815\n",
      "Iteration 1966, loss = 0.14407953\n",
      "Iteration 1967, loss = 0.14399651\n",
      "Iteration 1968, loss = 0.14389737\n",
      "Iteration 1969, loss = 0.14374105\n",
      "Iteration 1970, loss = 0.14369429\n",
      "Iteration 1971, loss = 0.14359238\n",
      "Iteration 1972, loss = 0.14351624\n",
      "Iteration 1973, loss = 0.14338040\n",
      "Iteration 1974, loss = 0.14334286\n",
      "Iteration 1975, loss = 0.14317641\n",
      "Iteration 1976, loss = 0.14303001\n",
      "Iteration 1977, loss = 0.14296211\n",
      "Iteration 1978, loss = 0.14286064\n",
      "Iteration 1979, loss = 0.14279924\n",
      "Iteration 1980, loss = 0.14262559\n",
      "Iteration 1981, loss = 0.14258695\n",
      "Iteration 1982, loss = 0.14248459\n",
      "Iteration 1983, loss = 0.14233996\n",
      "Iteration 1984, loss = 0.14232999\n",
      "Iteration 1985, loss = 0.14213230\n",
      "Iteration 1986, loss = 0.14205077\n",
      "Iteration 1987, loss = 0.14195311\n",
      "Iteration 1988, loss = 0.14185929\n",
      "Iteration 1989, loss = 0.14176477\n",
      "Iteration 1990, loss = 0.14163458\n",
      "Iteration 1991, loss = 0.14157667\n",
      "Iteration 1992, loss = 0.14149180\n",
      "Iteration 1993, loss = 0.14132526\n",
      "Iteration 1994, loss = 0.14129419\n",
      "Iteration 1995, loss = 0.14127589\n",
      "Iteration 1996, loss = 0.14114892\n",
      "Iteration 1997, loss = 0.14099207\n",
      "Iteration 1998, loss = 0.14094098\n",
      "Iteration 1999, loss = 0.14073565\n",
      "Iteration 2000, loss = 0.14063562\n",
      "Iteration 2001, loss = 0.14054417\n",
      "Iteration 2002, loss = 0.14053993\n",
      "Iteration 2003, loss = 0.14032315\n",
      "Iteration 2004, loss = 0.14026339\n",
      "Iteration 2005, loss = 0.14012668\n",
      "Iteration 2006, loss = 0.14004763\n",
      "Iteration 2007, loss = 0.13996420\n",
      "Iteration 2008, loss = 0.13986582\n",
      "Iteration 2009, loss = 0.13972452\n",
      "Iteration 2010, loss = 0.13975956\n",
      "Iteration 2011, loss = 0.13960994\n",
      "Iteration 2012, loss = 0.13945511\n",
      "Iteration 2013, loss = 0.13942434\n",
      "Iteration 2014, loss = 0.13924708\n",
      "Iteration 2015, loss = 0.13919081\n",
      "Iteration 2016, loss = 0.13908300\n",
      "Iteration 2017, loss = 0.13901988\n",
      "Iteration 2018, loss = 0.13900964\n",
      "Iteration 2019, loss = 0.13883212\n",
      "Iteration 2020, loss = 0.13869174\n",
      "Iteration 2021, loss = 0.13861147\n",
      "Iteration 2022, loss = 0.13866405\n",
      "Iteration 2023, loss = 0.13844049\n",
      "Iteration 2024, loss = 0.13831583\n",
      "Iteration 2025, loss = 0.13820752\n",
      "Iteration 2026, loss = 0.13811224\n",
      "Iteration 2027, loss = 0.13809960\n",
      "Iteration 2028, loss = 0.13803782\n",
      "Iteration 2029, loss = 0.13780872\n",
      "Iteration 2030, loss = 0.13768694\n",
      "Iteration 2031, loss = 0.13765540\n",
      "Iteration 2032, loss = 0.13760672\n",
      "Iteration 2033, loss = 0.13744106\n",
      "Iteration 2034, loss = 0.13743098\n",
      "Iteration 2035, loss = 0.13732046\n",
      "Iteration 2036, loss = 0.13715530\n",
      "Iteration 2037, loss = 0.13712669\n",
      "Iteration 2038, loss = 0.13695189\n",
      "Iteration 2039, loss = 0.13685563\n",
      "Iteration 2040, loss = 0.13681802\n",
      "Iteration 2041, loss = 0.13667531\n",
      "Iteration 2042, loss = 0.13657550\n",
      "Iteration 2043, loss = 0.13648083\n",
      "Iteration 2044, loss = 0.13644206\n",
      "Iteration 2045, loss = 0.13632406\n",
      "Iteration 2046, loss = 0.13620817\n",
      "Iteration 2047, loss = 0.13610595\n",
      "Iteration 2048, loss = 0.13599906\n",
      "Iteration 2049, loss = 0.13592961\n",
      "Iteration 2050, loss = 0.13580587\n",
      "Iteration 2051, loss = 0.13575282\n",
      "Iteration 2052, loss = 0.13569462\n",
      "Iteration 2053, loss = 0.13550661\n",
      "Iteration 2054, loss = 0.13546393\n",
      "Iteration 2055, loss = 0.13538938\n",
      "Iteration 2056, loss = 0.13527221\n",
      "Iteration 2057, loss = 0.13514069\n",
      "Iteration 2058, loss = 0.13513350\n",
      "Iteration 2059, loss = 0.13496394\n",
      "Iteration 2060, loss = 0.13486106\n",
      "Iteration 2061, loss = 0.13481455\n",
      "Iteration 2062, loss = 0.13468109\n",
      "Iteration 2063, loss = 0.13472194\n",
      "Iteration 2064, loss = 0.13451297\n",
      "Iteration 2065, loss = 0.13449056\n",
      "Iteration 2066, loss = 0.13428504\n",
      "Iteration 2067, loss = 0.13420904\n",
      "Iteration 2068, loss = 0.13410292\n",
      "Iteration 2069, loss = 0.13400573\n",
      "Iteration 2070, loss = 0.13404096\n",
      "Iteration 2071, loss = 0.13386119\n",
      "Iteration 2072, loss = 0.13380525\n",
      "Iteration 2073, loss = 0.13366819\n",
      "Iteration 2074, loss = 0.13357360\n",
      "Iteration 2075, loss = 0.13355859\n",
      "Iteration 2076, loss = 0.13344601\n",
      "Iteration 2077, loss = 0.13337174\n",
      "Iteration 2078, loss = 0.13319680\n",
      "Iteration 2079, loss = 0.13309374\n",
      "Iteration 2080, loss = 0.13300546\n",
      "Iteration 2081, loss = 0.13288154\n",
      "Iteration 2082, loss = 0.13285207\n",
      "Iteration 2083, loss = 0.13272699\n",
      "Iteration 2084, loss = 0.13267269\n",
      "Iteration 2085, loss = 0.13257102\n",
      "Iteration 2086, loss = 0.13263507\n",
      "Iteration 2087, loss = 0.13234519\n",
      "Iteration 2088, loss = 0.13231344\n",
      "Iteration 2089, loss = 0.13223259\n",
      "Iteration 2090, loss = 0.13211724\n",
      "Iteration 2091, loss = 0.13205774\n",
      "Iteration 2092, loss = 0.13198764\n",
      "Iteration 2093, loss = 0.13181967\n",
      "Iteration 2094, loss = 0.13179793\n",
      "Iteration 2095, loss = 0.13166213\n",
      "Iteration 2096, loss = 0.13155766\n",
      "Iteration 2097, loss = 0.13151171\n",
      "Iteration 2098, loss = 0.13137430\n",
      "Iteration 2099, loss = 0.13129419\n",
      "Iteration 2100, loss = 0.13121185\n",
      "Iteration 2101, loss = 0.13118783\n",
      "Iteration 2102, loss = 0.13105426\n",
      "Iteration 2103, loss = 0.13097888\n",
      "Iteration 2104, loss = 0.13086902\n",
      "Iteration 2105, loss = 0.13082121\n",
      "Iteration 2106, loss = 0.13072836\n",
      "Iteration 2107, loss = 0.13059425\n",
      "Iteration 2108, loss = 0.13056963\n",
      "Iteration 2109, loss = 0.13044454\n",
      "Iteration 2110, loss = 0.13032386\n",
      "Iteration 2111, loss = 0.13030161\n",
      "Iteration 2112, loss = 0.13014701\n",
      "Iteration 2113, loss = 0.13005176\n",
      "Iteration 2114, loss = 0.12999669\n",
      "Iteration 2115, loss = 0.13003482\n",
      "Iteration 2116, loss = 0.12986345\n",
      "Iteration 2117, loss = 0.12974625\n",
      "Iteration 2118, loss = 0.12973439\n",
      "Iteration 2119, loss = 0.12955475\n",
      "Iteration 2120, loss = 0.12957061\n",
      "Iteration 2121, loss = 0.12949065\n",
      "Iteration 2122, loss = 0.12932026\n",
      "Iteration 2123, loss = 0.12928803\n",
      "Iteration 2124, loss = 0.12940494\n",
      "Iteration 2125, loss = 0.12900443\n",
      "Iteration 2126, loss = 0.12900954\n",
      "Iteration 2127, loss = 0.12891963\n",
      "Iteration 2128, loss = 0.12879176\n",
      "Iteration 2129, loss = 0.12871094\n",
      "Iteration 2130, loss = 0.12864005\n",
      "Iteration 2131, loss = 0.12852840\n",
      "Iteration 2132, loss = 0.12847211\n",
      "Iteration 2133, loss = 0.12841924\n",
      "Iteration 2134, loss = 0.12833384\n",
      "Iteration 2135, loss = 0.12839407\n",
      "Iteration 2136, loss = 0.12821573\n",
      "Iteration 2137, loss = 0.12810511\n",
      "Iteration 2138, loss = 0.12795917\n",
      "Iteration 2139, loss = 0.12787118\n",
      "Iteration 2140, loss = 0.12780574\n",
      "Iteration 2141, loss = 0.12770450\n",
      "Iteration 2142, loss = 0.12761627\n",
      "Iteration 2143, loss = 0.12752825\n",
      "Iteration 2144, loss = 0.12745344\n",
      "Iteration 2145, loss = 0.12735869\n",
      "Iteration 2146, loss = 0.12727025\n",
      "Iteration 2147, loss = 0.12725438\n",
      "Iteration 2148, loss = 0.12723203\n",
      "Iteration 2149, loss = 0.12705718\n",
      "Iteration 2150, loss = 0.12707704\n",
      "Iteration 2151, loss = 0.12690789\n",
      "Iteration 2152, loss = 0.12686016\n",
      "Iteration 2153, loss = 0.12679173\n",
      "Iteration 2154, loss = 0.12669935\n",
      "Iteration 2155, loss = 0.12662072\n",
      "Iteration 2156, loss = 0.12646426\n",
      "Iteration 2157, loss = 0.12635445\n",
      "Iteration 2158, loss = 0.12636142\n",
      "Iteration 2159, loss = 0.12621615\n",
      "Iteration 2160, loss = 0.12615221\n",
      "Iteration 2161, loss = 0.12607593\n",
      "Iteration 2162, loss = 0.12601178\n",
      "Iteration 2163, loss = 0.12593136\n",
      "Iteration 2164, loss = 0.12581315\n",
      "Iteration 2165, loss = 0.12572817\n",
      "Iteration 2166, loss = 0.12569725\n",
      "Iteration 2167, loss = 0.12565067\n",
      "Iteration 2168, loss = 0.12551775\n",
      "Iteration 2169, loss = 0.12544619\n",
      "Iteration 2170, loss = 0.12538300\n",
      "Iteration 2171, loss = 0.12528949\n",
      "Iteration 2172, loss = 0.12517058\n",
      "Iteration 2173, loss = 0.12525806\n",
      "Iteration 2174, loss = 0.12499991\n",
      "Iteration 2175, loss = 0.12494611\n",
      "Iteration 2176, loss = 0.12484612\n",
      "Iteration 2177, loss = 0.12479601\n",
      "Iteration 2178, loss = 0.12477211\n",
      "Iteration 2179, loss = 0.12460143\n",
      "Iteration 2180, loss = 0.12454160\n",
      "Iteration 2181, loss = 0.12448604\n",
      "Iteration 2182, loss = 0.12441745\n",
      "Iteration 2183, loss = 0.12438536\n",
      "Iteration 2184, loss = 0.12430315\n",
      "Iteration 2185, loss = 0.12415293\n",
      "Iteration 2186, loss = 0.12409675\n",
      "Iteration 2187, loss = 0.12406317\n",
      "Iteration 2188, loss = 0.12395936\n",
      "Iteration 2189, loss = 0.12384727\n",
      "Iteration 2190, loss = 0.12377516\n",
      "Iteration 2191, loss = 0.12369458\n",
      "Iteration 2192, loss = 0.12364859\n",
      "Iteration 2193, loss = 0.12357918\n",
      "Iteration 2194, loss = 0.12343631\n",
      "Iteration 2195, loss = 0.12334469\n",
      "Iteration 2196, loss = 0.12332599\n",
      "Iteration 2197, loss = 0.12326967\n",
      "Iteration 2198, loss = 0.12317775\n",
      "Iteration 2199, loss = 0.12314276\n",
      "Iteration 2200, loss = 0.12297798\n",
      "Iteration 2201, loss = 0.12291643\n",
      "Iteration 2202, loss = 0.12284462\n",
      "Iteration 2203, loss = 0.12275836\n",
      "Iteration 2204, loss = 0.12271509\n",
      "Iteration 2205, loss = 0.12268522\n",
      "Iteration 2206, loss = 0.12253162\n",
      "Iteration 2207, loss = 0.12246017\n",
      "Iteration 2208, loss = 0.12239968\n",
      "Iteration 2209, loss = 0.12230440\n",
      "Iteration 2210, loss = 0.12231144\n",
      "Iteration 2211, loss = 0.12220083\n",
      "Iteration 2212, loss = 0.12212040\n",
      "Iteration 2213, loss = 0.12201797\n",
      "Iteration 2214, loss = 0.12192990\n",
      "Iteration 2215, loss = 0.12185659\n",
      "Iteration 2216, loss = 0.12182731\n",
      "Iteration 2217, loss = 0.12176009\n",
      "Iteration 2218, loss = 0.12163459\n",
      "Iteration 2219, loss = 0.12159257\n",
      "Iteration 2220, loss = 0.12150713\n",
      "Iteration 2221, loss = 0.12144026\n",
      "Iteration 2222, loss = 0.12157255\n",
      "Iteration 2223, loss = 0.12127933\n",
      "Iteration 2224, loss = 0.12121003\n",
      "Iteration 2225, loss = 0.12111191\n",
      "Iteration 2226, loss = 0.12109419\n",
      "Iteration 2227, loss = 0.12108319\n",
      "Iteration 2228, loss = 0.12089917\n",
      "Iteration 2229, loss = 0.12084594\n",
      "Iteration 2230, loss = 0.12080449\n",
      "Iteration 2231, loss = 0.12069492\n",
      "Iteration 2232, loss = 0.12065635\n",
      "Iteration 2233, loss = 0.12055510\n",
      "Iteration 2234, loss = 0.12045510\n",
      "Iteration 2235, loss = 0.12038224\n",
      "Iteration 2236, loss = 0.12032133\n",
      "Iteration 2237, loss = 0.12028423\n",
      "Iteration 2238, loss = 0.12019353\n",
      "Iteration 2239, loss = 0.12011092\n",
      "Iteration 2240, loss = 0.12002934\n",
      "Iteration 2241, loss = 0.11998900\n",
      "Iteration 2242, loss = 0.11994366\n",
      "Iteration 2243, loss = 0.11985355\n",
      "Iteration 2244, loss = 0.11975108\n",
      "Iteration 2245, loss = 0.11975542\n",
      "Iteration 2246, loss = 0.11960092\n",
      "Iteration 2247, loss = 0.11954346\n",
      "Iteration 2248, loss = 0.11948569\n",
      "Iteration 2249, loss = 0.11954937\n",
      "Iteration 2250, loss = 0.11935891\n",
      "Iteration 2251, loss = 0.11927997\n",
      "Iteration 2252, loss = 0.11927362\n",
      "Iteration 2253, loss = 0.11918694\n",
      "Iteration 2254, loss = 0.11906140\n",
      "Iteration 2255, loss = 0.11899800\n",
      "Iteration 2256, loss = 0.11898683\n",
      "Iteration 2257, loss = 0.11885659\n",
      "Iteration 2258, loss = 0.11877994\n",
      "Iteration 2259, loss = 0.11869934\n",
      "Iteration 2260, loss = 0.11868210\n",
      "Iteration 2261, loss = 0.11866808\n",
      "Iteration 2262, loss = 0.11849768\n",
      "Iteration 2263, loss = 0.11841769\n",
      "Iteration 2264, loss = 0.11834926\n",
      "Iteration 2265, loss = 0.11829721\n",
      "Iteration 2266, loss = 0.11819840\n",
      "Iteration 2267, loss = 0.11814130\n",
      "Iteration 2268, loss = 0.11817360\n",
      "Iteration 2269, loss = 0.11803522\n",
      "Iteration 2270, loss = 0.11798047\n",
      "Iteration 2271, loss = 0.11787865\n",
      "Iteration 2272, loss = 0.11780626\n",
      "Iteration 2273, loss = 0.11779321\n",
      "Iteration 2274, loss = 0.11766186\n",
      "Iteration 2275, loss = 0.11761319\n",
      "Iteration 2276, loss = 0.11750653\n",
      "Iteration 2277, loss = 0.11745898\n",
      "Iteration 2278, loss = 0.11744578\n",
      "Iteration 2279, loss = 0.11730994\n",
      "Iteration 2280, loss = 0.11727101\n",
      "Iteration 2281, loss = 0.11727038\n",
      "Iteration 2282, loss = 0.11711955\n",
      "Iteration 2283, loss = 0.11705513\n",
      "Iteration 2284, loss = 0.11704805\n",
      "Iteration 2285, loss = 0.11694327\n",
      "Iteration 2286, loss = 0.11684201\n",
      "Iteration 2287, loss = 0.11680382\n",
      "Iteration 2288, loss = 0.11671193\n",
      "Iteration 2289, loss = 0.11663645\n",
      "Iteration 2290, loss = 0.11658587\n",
      "Iteration 2291, loss = 0.11647805\n",
      "Iteration 2292, loss = 0.11646396\n",
      "Iteration 2293, loss = 0.11641656\n",
      "Iteration 2294, loss = 0.11634418\n",
      "Iteration 2295, loss = 0.11622459\n",
      "Iteration 2296, loss = 0.11615347\n",
      "Iteration 2297, loss = 0.11616764\n",
      "Iteration 2298, loss = 0.11602142\n",
      "Iteration 2299, loss = 0.11598093\n",
      "Iteration 2300, loss = 0.11588415\n",
      "Iteration 2301, loss = 0.11588065\n",
      "Iteration 2302, loss = 0.11580396\n",
      "Iteration 2303, loss = 0.11571039\n",
      "Iteration 2304, loss = 0.11560358\n",
      "Iteration 2305, loss = 0.11556309\n",
      "Iteration 2306, loss = 0.11553174\n",
      "Iteration 2307, loss = 0.11541220\n",
      "Iteration 2308, loss = 0.11537203\n",
      "Iteration 2309, loss = 0.11533550\n",
      "Iteration 2310, loss = 0.11524096\n",
      "Iteration 2311, loss = 0.11517666\n",
      "Iteration 2312, loss = 0.11515862\n",
      "Iteration 2313, loss = 0.11503109\n",
      "Iteration 2314, loss = 0.11498946\n",
      "Iteration 2315, loss = 0.11489336\n",
      "Iteration 2316, loss = 0.11486110\n",
      "Iteration 2317, loss = 0.11477805\n",
      "Iteration 2318, loss = 0.11471769\n",
      "Iteration 2319, loss = 0.11472462\n",
      "Iteration 2320, loss = 0.11459410\n",
      "Iteration 2321, loss = 0.11450965\n",
      "Iteration 2322, loss = 0.11446460\n",
      "Iteration 2323, loss = 0.11440006\n",
      "Iteration 2324, loss = 0.11436915\n",
      "Iteration 2325, loss = 0.11430057\n",
      "Iteration 2326, loss = 0.11426647\n",
      "Iteration 2327, loss = 0.11413980\n",
      "Iteration 2328, loss = 0.11410703\n",
      "Iteration 2329, loss = 0.11410894\n",
      "Iteration 2330, loss = 0.11393244\n",
      "Iteration 2331, loss = 0.11391774\n",
      "Iteration 2332, loss = 0.11390396\n",
      "Iteration 2333, loss = 0.11376381\n",
      "Iteration 2334, loss = 0.11380990\n",
      "Iteration 2335, loss = 0.11363013\n",
      "Iteration 2336, loss = 0.11356543\n",
      "Iteration 2337, loss = 0.11351874\n",
      "Iteration 2338, loss = 0.11353880\n",
      "Iteration 2339, loss = 0.11337911\n",
      "Iteration 2340, loss = 0.11331955\n",
      "Iteration 2341, loss = 0.11331490\n",
      "Iteration 2342, loss = 0.11325796\n",
      "Iteration 2343, loss = 0.11313896\n",
      "Iteration 2344, loss = 0.11309631\n",
      "Iteration 2345, loss = 0.11306289\n",
      "Iteration 2346, loss = 0.11298394\n",
      "Iteration 2347, loss = 0.11291450\n",
      "Iteration 2348, loss = 0.11282007\n",
      "Iteration 2349, loss = 0.11276788\n",
      "Iteration 2350, loss = 0.11268817\n",
      "Iteration 2351, loss = 0.11262647\n",
      "Iteration 2352, loss = 0.11266035\n",
      "Iteration 2353, loss = 0.11253048\n",
      "Iteration 2354, loss = 0.11247772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80282234\n",
      "Iteration 2, loss = 0.79499328\n",
      "Iteration 3, loss = 0.78344212\n",
      "Iteration 4, loss = 0.76962389\n",
      "Iteration 5, loss = 0.75402296\n",
      "Iteration 6, loss = 0.73885161\n",
      "Iteration 7, loss = 0.72271865\n",
      "Iteration 8, loss = 0.70762473\n",
      "Iteration 9, loss = 0.69346319\n",
      "Iteration 10, loss = 0.67954064\n",
      "Iteration 11, loss = 0.66640913\n",
      "Iteration 12, loss = 0.65418168\n",
      "Iteration 13, loss = 0.64329373\n",
      "Iteration 14, loss = 0.63254992\n",
      "Iteration 15, loss = 0.62228549\n",
      "Iteration 16, loss = 0.61343554\n",
      "Iteration 17, loss = 0.60482205\n",
      "Iteration 18, loss = 0.59700641\n",
      "Iteration 19, loss = 0.58926680\n",
      "Iteration 20, loss = 0.58205638\n",
      "Iteration 21, loss = 0.57569365\n",
      "Iteration 22, loss = 0.56925267\n",
      "Iteration 23, loss = 0.56350677\n",
      "Iteration 24, loss = 0.55778396\n",
      "Iteration 25, loss = 0.55231984\n",
      "Iteration 26, loss = 0.54716435\n",
      "Iteration 27, loss = 0.54233222\n",
      "Iteration 28, loss = 0.53781457\n",
      "Iteration 29, loss = 0.53320974\n",
      "Iteration 30, loss = 0.52891936\n",
      "Iteration 31, loss = 0.52467111\n",
      "Iteration 32, loss = 0.52054890\n",
      "Iteration 33, loss = 0.51669721\n",
      "Iteration 34, loss = 0.51297158\n",
      "Iteration 35, loss = 0.50935144\n",
      "Iteration 36, loss = 0.50579932\n",
      "Iteration 37, loss = 0.50238564\n",
      "Iteration 38, loss = 0.49901288\n",
      "Iteration 39, loss = 0.49599173\n",
      "Iteration 40, loss = 0.49278887\n",
      "Iteration 41, loss = 0.48977812\n",
      "Iteration 42, loss = 0.48672610\n",
      "Iteration 43, loss = 0.48393011\n",
      "Iteration 44, loss = 0.48122980\n",
      "Iteration 45, loss = 0.47846800\n",
      "Iteration 46, loss = 0.47581014\n",
      "Iteration 47, loss = 0.47315552\n",
      "Iteration 48, loss = 0.47069551\n",
      "Iteration 49, loss = 0.46823216\n",
      "Iteration 50, loss = 0.46584845\n",
      "Iteration 51, loss = 0.46345399\n",
      "Iteration 52, loss = 0.46121098\n",
      "Iteration 53, loss = 0.45894617\n",
      "Iteration 54, loss = 0.45680338\n",
      "Iteration 55, loss = 0.45467964\n",
      "Iteration 56, loss = 0.45261505\n",
      "Iteration 57, loss = 0.45057636\n",
      "Iteration 58, loss = 0.44857774\n",
      "Iteration 59, loss = 0.44667842\n",
      "Iteration 60, loss = 0.44474278\n",
      "Iteration 61, loss = 0.44291732\n",
      "Iteration 62, loss = 0.44109886\n",
      "Iteration 63, loss = 0.43931808\n",
      "Iteration 64, loss = 0.43760037\n",
      "Iteration 65, loss = 0.43586993\n",
      "Iteration 66, loss = 0.43426297\n",
      "Iteration 67, loss = 0.43262900\n",
      "Iteration 68, loss = 0.43100911\n",
      "Iteration 69, loss = 0.42945459\n",
      "Iteration 70, loss = 0.42792932\n",
      "Iteration 71, loss = 0.42639525\n",
      "Iteration 72, loss = 0.42492340\n",
      "Iteration 73, loss = 0.42350387\n",
      "Iteration 74, loss = 0.42213806\n",
      "Iteration 75, loss = 0.42073671\n",
      "Iteration 76, loss = 0.41939465\n",
      "Iteration 77, loss = 0.41811846\n",
      "Iteration 78, loss = 0.41679152\n",
      "Iteration 79, loss = 0.41551959\n",
      "Iteration 80, loss = 0.41429183\n",
      "Iteration 81, loss = 0.41311760\n",
      "Iteration 82, loss = 0.41193882\n",
      "Iteration 83, loss = 0.41077432\n",
      "Iteration 84, loss = 0.40960188\n",
      "Iteration 85, loss = 0.40851870\n",
      "Iteration 86, loss = 0.40741229\n",
      "Iteration 87, loss = 0.40632655\n",
      "Iteration 88, loss = 0.40529804\n",
      "Iteration 89, loss = 0.40425068\n",
      "Iteration 90, loss = 0.40323891\n",
      "Iteration 91, loss = 0.40222809\n",
      "Iteration 92, loss = 0.40129899\n",
      "Iteration 93, loss = 0.40031812\n",
      "Iteration 94, loss = 0.39935108\n",
      "Iteration 95, loss = 0.39846133\n",
      "Iteration 96, loss = 0.39754270\n",
      "Iteration 97, loss = 0.39669602\n",
      "Iteration 98, loss = 0.39577400\n",
      "Iteration 99, loss = 0.39494302\n",
      "Iteration 100, loss = 0.39408242\n",
      "Iteration 101, loss = 0.39326969\n",
      "Iteration 102, loss = 0.39246618\n",
      "Iteration 103, loss = 0.39165402\n",
      "Iteration 104, loss = 0.39086524\n",
      "Iteration 105, loss = 0.39009561\n",
      "Iteration 106, loss = 0.38937918\n",
      "Iteration 107, loss = 0.38861553\n",
      "Iteration 108, loss = 0.38790554\n",
      "Iteration 109, loss = 0.38717631\n",
      "Iteration 110, loss = 0.38648102\n",
      "Iteration 111, loss = 0.38579486\n",
      "Iteration 112, loss = 0.38508838\n",
      "Iteration 113, loss = 0.38444779\n",
      "Iteration 114, loss = 0.38374090\n",
      "Iteration 115, loss = 0.38314075\n",
      "Iteration 116, loss = 0.38248097\n",
      "Iteration 117, loss = 0.38182370\n",
      "Iteration 118, loss = 0.38126117\n",
      "Iteration 119, loss = 0.38064874\n",
      "Iteration 120, loss = 0.38002580\n",
      "Iteration 121, loss = 0.37944981\n",
      "Iteration 122, loss = 0.37886846\n",
      "Iteration 123, loss = 0.37830712\n",
      "Iteration 124, loss = 0.37773621\n",
      "Iteration 125, loss = 0.37718780\n",
      "Iteration 126, loss = 0.37665430\n",
      "Iteration 127, loss = 0.37608034\n",
      "Iteration 128, loss = 0.37560090\n",
      "Iteration 129, loss = 0.37505952\n",
      "Iteration 130, loss = 0.37455247\n",
      "Iteration 131, loss = 0.37404450\n",
      "Iteration 132, loss = 0.37352396\n",
      "Iteration 133, loss = 0.37309217\n",
      "Iteration 134, loss = 0.37256270\n",
      "Iteration 135, loss = 0.37208033\n",
      "Iteration 136, loss = 0.37160501\n",
      "Iteration 137, loss = 0.37115633\n",
      "Iteration 138, loss = 0.37069894\n",
      "Iteration 139, loss = 0.37025267\n",
      "Iteration 140, loss = 0.36980069\n",
      "Iteration 141, loss = 0.36937377\n",
      "Iteration 142, loss = 0.36893715\n",
      "Iteration 143, loss = 0.36847355\n",
      "Iteration 144, loss = 0.36807755\n",
      "Iteration 145, loss = 0.36764816\n",
      "Iteration 146, loss = 0.36725732\n",
      "Iteration 147, loss = 0.36682875\n",
      "Iteration 148, loss = 0.36642001\n",
      "Iteration 149, loss = 0.36604322\n",
      "Iteration 150, loss = 0.36564732\n",
      "Iteration 151, loss = 0.36525811\n",
      "Iteration 152, loss = 0.36488992\n",
      "Iteration 153, loss = 0.36449205\n",
      "Iteration 154, loss = 0.36412667\n",
      "Iteration 155, loss = 0.36374024\n",
      "Iteration 156, loss = 0.36339741\n",
      "Iteration 157, loss = 0.36303762\n",
      "Iteration 158, loss = 0.36267385\n",
      "Iteration 159, loss = 0.36230758\n",
      "Iteration 160, loss = 0.36197556\n",
      "Iteration 161, loss = 0.36162864\n",
      "Iteration 162, loss = 0.36130716\n",
      "Iteration 163, loss = 0.36095206\n",
      "Iteration 164, loss = 0.36063538\n",
      "Iteration 165, loss = 0.36029281\n",
      "Iteration 166, loss = 0.35998540\n",
      "Iteration 167, loss = 0.35966173\n",
      "Iteration 168, loss = 0.35933791\n",
      "Iteration 169, loss = 0.35902784\n",
      "Iteration 170, loss = 0.35872342\n",
      "Iteration 171, loss = 0.35840242\n",
      "Iteration 172, loss = 0.35810529\n",
      "Iteration 173, loss = 0.35780340\n",
      "Iteration 174, loss = 0.35751279\n",
      "Iteration 175, loss = 0.35721140\n",
      "Iteration 176, loss = 0.35694016\n",
      "Iteration 177, loss = 0.35663848\n",
      "Iteration 178, loss = 0.35633615\n",
      "Iteration 179, loss = 0.35606212\n",
      "Iteration 180, loss = 0.35580150\n",
      "Iteration 181, loss = 0.35552057\n",
      "Iteration 182, loss = 0.35523402\n",
      "Iteration 183, loss = 0.35497046\n",
      "Iteration 184, loss = 0.35472055\n",
      "Iteration 185, loss = 0.35445116\n",
      "Iteration 186, loss = 0.35416343\n",
      "Iteration 187, loss = 0.35389718\n",
      "Iteration 188, loss = 0.35365533\n",
      "Iteration 189, loss = 0.35339277\n",
      "Iteration 190, loss = 0.35314818\n",
      "Iteration 191, loss = 0.35292172\n",
      "Iteration 192, loss = 0.35265305\n",
      "Iteration 193, loss = 0.35239595\n",
      "Iteration 194, loss = 0.35216054\n",
      "Iteration 195, loss = 0.35191281\n",
      "Iteration 196, loss = 0.35169593\n",
      "Iteration 197, loss = 0.35145580\n",
      "Iteration 198, loss = 0.35121700\n",
      "Iteration 199, loss = 0.35097828\n",
      "Iteration 200, loss = 0.35074505\n",
      "Iteration 201, loss = 0.35052830\n",
      "Iteration 202, loss = 0.35028425\n",
      "Iteration 203, loss = 0.35007007\n",
      "Iteration 204, loss = 0.34983964\n",
      "Iteration 205, loss = 0.34961845\n",
      "Iteration 206, loss = 0.34940271\n",
      "Iteration 207, loss = 0.34918215\n",
      "Iteration 208, loss = 0.34897222\n",
      "Iteration 209, loss = 0.34874555\n",
      "Iteration 210, loss = 0.34853667\n",
      "Iteration 211, loss = 0.34832621\n",
      "Iteration 212, loss = 0.34811430\n",
      "Iteration 213, loss = 0.34790631\n",
      "Iteration 214, loss = 0.34770318\n",
      "Iteration 215, loss = 0.34750407\n",
      "Iteration 216, loss = 0.34728910\n",
      "Iteration 217, loss = 0.34708773\n",
      "Iteration 218, loss = 0.34690937\n",
      "Iteration 219, loss = 0.34671015\n",
      "Iteration 220, loss = 0.34650880\n",
      "Iteration 221, loss = 0.34630917\n",
      "Iteration 222, loss = 0.34610817\n",
      "Iteration 223, loss = 0.34594560\n",
      "Iteration 224, loss = 0.34572319\n",
      "Iteration 225, loss = 0.34555025\n",
      "Iteration 226, loss = 0.34536791\n",
      "Iteration 227, loss = 0.34517048\n",
      "Iteration 228, loss = 0.34499383\n",
      "Iteration 229, loss = 0.34479924\n",
      "Iteration 230, loss = 0.34462439\n",
      "Iteration 231, loss = 0.34445427\n",
      "Iteration 232, loss = 0.34426091\n",
      "Iteration 233, loss = 0.34409555\n",
      "Iteration 234, loss = 0.34391043\n",
      "Iteration 235, loss = 0.34373988\n",
      "Iteration 236, loss = 0.34357810\n",
      "Iteration 237, loss = 0.34338675\n",
      "Iteration 238, loss = 0.34321931\n",
      "Iteration 239, loss = 0.34304876\n",
      "Iteration 240, loss = 0.34288286\n",
      "Iteration 241, loss = 0.34270997\n",
      "Iteration 242, loss = 0.34254754\n",
      "Iteration 243, loss = 0.34238290\n",
      "Iteration 244, loss = 0.34221019\n",
      "Iteration 245, loss = 0.34204257\n",
      "Iteration 246, loss = 0.34188128\n",
      "Iteration 247, loss = 0.34172489\n",
      "Iteration 248, loss = 0.34160713\n",
      "Iteration 249, loss = 0.34141464\n",
      "Iteration 250, loss = 0.34124754\n",
      "Iteration 251, loss = 0.34108879\n",
      "Iteration 252, loss = 0.34093264\n",
      "Iteration 253, loss = 0.34078636\n",
      "Iteration 254, loss = 0.34062434\n",
      "Iteration 255, loss = 0.34046192\n",
      "Iteration 256, loss = 0.34032469\n",
      "Iteration 257, loss = 0.34017733\n",
      "Iteration 258, loss = 0.34002560\n",
      "Iteration 259, loss = 0.33987225\n",
      "Iteration 260, loss = 0.33971570\n",
      "Iteration 261, loss = 0.33957558\n",
      "Iteration 262, loss = 0.33942976\n",
      "Iteration 263, loss = 0.33928210\n",
      "Iteration 264, loss = 0.33913842\n",
      "Iteration 265, loss = 0.33898539\n",
      "Iteration 266, loss = 0.33885765\n",
      "Iteration 267, loss = 0.33871128\n",
      "Iteration 268, loss = 0.33858438\n",
      "Iteration 269, loss = 0.33843598\n",
      "Iteration 270, loss = 0.33828724\n",
      "Iteration 271, loss = 0.33816146\n",
      "Iteration 272, loss = 0.33801626\n",
      "Iteration 273, loss = 0.33787793\n",
      "Iteration 274, loss = 0.33774198\n",
      "Iteration 275, loss = 0.33761821\n",
      "Iteration 276, loss = 0.33747641\n",
      "Iteration 277, loss = 0.33733944\n",
      "Iteration 278, loss = 0.33720439\n",
      "Iteration 279, loss = 0.33707738\n",
      "Iteration 280, loss = 0.33694453\n",
      "Iteration 281, loss = 0.33682633\n",
      "Iteration 282, loss = 0.33668021\n",
      "Iteration 283, loss = 0.33656361\n",
      "Iteration 284, loss = 0.33643745\n",
      "Iteration 285, loss = 0.33630547\n",
      "Iteration 286, loss = 0.33617775\n",
      "Iteration 287, loss = 0.33604189\n",
      "Iteration 288, loss = 0.33591331\n",
      "Iteration 289, loss = 0.33579614\n",
      "Iteration 290, loss = 0.33568293\n",
      "Iteration 291, loss = 0.33554668\n",
      "Iteration 292, loss = 0.33544041\n",
      "Iteration 293, loss = 0.33531246\n",
      "Iteration 294, loss = 0.33518543\n",
      "Iteration 295, loss = 0.33506200\n",
      "Iteration 296, loss = 0.33495142\n",
      "Iteration 297, loss = 0.33481920\n",
      "Iteration 298, loss = 0.33470202\n",
      "Iteration 299, loss = 0.33458803\n",
      "Iteration 300, loss = 0.33447126\n",
      "Iteration 301, loss = 0.33434594\n",
      "Iteration 302, loss = 0.33422912\n",
      "Iteration 303, loss = 0.33412070\n",
      "Iteration 304, loss = 0.33400425\n",
      "Iteration 305, loss = 0.33388216\n",
      "Iteration 306, loss = 0.33376734\n",
      "Iteration 307, loss = 0.33365488\n",
      "Iteration 308, loss = 0.33355015\n",
      "Iteration 309, loss = 0.33343841\n",
      "Iteration 310, loss = 0.33332623\n",
      "Iteration 311, loss = 0.33321053\n",
      "Iteration 312, loss = 0.33308842\n",
      "Iteration 313, loss = 0.33298757\n",
      "Iteration 314, loss = 0.33287149\n",
      "Iteration 315, loss = 0.33276324\n",
      "Iteration 316, loss = 0.33264743\n",
      "Iteration 317, loss = 0.33253716\n",
      "Iteration 318, loss = 0.33243547\n",
      "Iteration 319, loss = 0.33232337\n",
      "Iteration 320, loss = 0.33223439\n",
      "Iteration 321, loss = 0.33212582\n",
      "Iteration 322, loss = 0.33201172\n",
      "Iteration 323, loss = 0.33190209\n",
      "Iteration 324, loss = 0.33179954\n",
      "Iteration 325, loss = 0.33170224\n",
      "Iteration 326, loss = 0.33158001\n",
      "Iteration 327, loss = 0.33147554\n",
      "Iteration 328, loss = 0.33137652\n",
      "Iteration 329, loss = 0.33127540\n",
      "Iteration 330, loss = 0.33117179\n",
      "Iteration 331, loss = 0.33107946\n",
      "Iteration 332, loss = 0.33097312\n",
      "Iteration 333, loss = 0.33087380\n",
      "Iteration 334, loss = 0.33077017\n",
      "Iteration 335, loss = 0.33068096\n",
      "Iteration 336, loss = 0.33055950\n",
      "Iteration 337, loss = 0.33047495\n",
      "Iteration 338, loss = 0.33037355\n",
      "Iteration 339, loss = 0.33027696\n",
      "Iteration 340, loss = 0.33017728\n",
      "Iteration 341, loss = 0.33008425\n",
      "Iteration 342, loss = 0.32998714\n",
      "Iteration 343, loss = 0.32988781\n",
      "Iteration 344, loss = 0.32979507\n",
      "Iteration 345, loss = 0.32969827\n",
      "Iteration 346, loss = 0.32959375\n",
      "Iteration 347, loss = 0.32951361\n",
      "Iteration 348, loss = 0.32940400\n",
      "Iteration 349, loss = 0.32930512\n",
      "Iteration 350, loss = 0.32921590\n",
      "Iteration 351, loss = 0.32914542\n",
      "Iteration 352, loss = 0.32903158\n",
      "Iteration 353, loss = 0.32894311\n",
      "Iteration 354, loss = 0.32885009\n",
      "Iteration 355, loss = 0.32874848\n",
      "Iteration 356, loss = 0.32865880\n",
      "Iteration 357, loss = 0.32857567\n",
      "Iteration 358, loss = 0.32847882\n",
      "Iteration 359, loss = 0.32838683\n",
      "Iteration 360, loss = 0.32831266\n",
      "Iteration 361, loss = 0.32821982\n",
      "Iteration 362, loss = 0.32812256\n",
      "Iteration 2274, loss = 0.13967679\n",
      "Iteration 2275, loss = 0.13966341\n",
      "Iteration 2276, loss = 0.13959467\n",
      "Iteration 2277, loss = 0.13943996\n",
      "Iteration 2278, loss = 0.13929388\n",
      "Iteration 2279, loss = 0.13919880\n",
      "Iteration 2280, loss = 0.13907871\n",
      "Iteration 2281, loss = 0.13912429\n",
      "Iteration 2282, loss = 0.13889345\n",
      "Iteration 2283, loss = 0.13888168\n",
      "Iteration 2284, loss = 0.13872046\n",
      "Iteration 2285, loss = 0.13868854\n",
      "Iteration 2286, loss = 0.13854816\n",
      "Iteration 2287, loss = 0.13846395\n",
      "Iteration 2288, loss = 0.13834985\n",
      "Iteration 2289, loss = 0.13828744\n",
      "Iteration 2290, loss = 0.13822748\n",
      "Iteration 2291, loss = 0.13809204\n",
      "Iteration 2292, loss = 0.13800792\n",
      "Iteration 2293, loss = 0.13788326\n",
      "Iteration 2294, loss = 0.13785984\n",
      "Iteration 2295, loss = 0.13771016\n",
      "Iteration 2296, loss = 0.13763528\n",
      "Iteration 2297, loss = 0.13755431\n",
      "Iteration 2298, loss = 0.13747837\n",
      "Iteration 2299, loss = 0.13747032\n",
      "Iteration 2300, loss = 0.13727715\n",
      "Iteration 2301, loss = 0.13722431\n",
      "Iteration 2302, loss = 0.13713399\n",
      "Iteration 2303, loss = 0.13700117\n",
      "Iteration 2304, loss = 0.13690465\n",
      "Iteration 2305, loss = 0.13692491\n",
      "Iteration 2306, loss = 0.13672836\n",
      "Iteration 2307, loss = 0.13670921\n",
      "Iteration 2308, loss = 0.13668186\n",
      "Iteration 2309, loss = 0.13648402\n",
      "Iteration 2310, loss = 0.13637335\n",
      "Iteration 2311, loss = 0.13630277\n",
      "Iteration 2312, loss = 0.13621205\n",
      "Iteration 2313, loss = 0.13615216\n",
      "Iteration 2314, loss = 0.13604323\n",
      "Iteration 2315, loss = 0.13599564\n",
      "Iteration 2316, loss = 0.13586059\n",
      "Iteration 2317, loss = 0.13577306\n",
      "Iteration 2318, loss = 0.13572332\n",
      "Iteration 2319, loss = 0.13563480\n",
      "Iteration 2320, loss = 0.13552222\n",
      "Iteration 2321, loss = 0.13549305\n",
      "Iteration 2322, loss = 0.13536806\n",
      "Iteration 2323, loss = 0.13523777\n",
      "Iteration 2324, loss = 0.13529172\n",
      "Iteration 2325, loss = 0.13515463\n",
      "Iteration 2326, loss = 0.13499894\n",
      "Iteration 2327, loss = 0.13488561\n",
      "Iteration 2328, loss = 0.13493185\n",
      "Iteration 2329, loss = 0.13475838\n",
      "Iteration 2330, loss = 0.13474303\n",
      "Iteration 2331, loss = 0.13472509\n",
      "Iteration 2332, loss = 0.13449254\n",
      "Iteration 2333, loss = 0.13441800\n",
      "Iteration 2334, loss = 0.13436691\n",
      "Iteration 2335, loss = 0.13422419\n",
      "Iteration 2336, loss = 0.13413139\n",
      "Iteration 2337, loss = 0.13405750\n",
      "Iteration 2338, loss = 0.13393685\n",
      "Iteration 2339, loss = 0.13387378\n",
      "Iteration 2340, loss = 0.13376862\n",
      "Iteration 2341, loss = 0.13371550\n",
      "Iteration 2342, loss = 0.13370621\n",
      "Iteration 2343, loss = 0.13351440\n",
      "Iteration 2344, loss = 0.13343212\n",
      "Iteration 2345, loss = 0.13338511\n",
      "Iteration 2346, loss = 0.13334001\n",
      "Iteration 2347, loss = 0.13321218\n",
      "Iteration 2348, loss = 0.13314756\n",
      "Iteration 2349, loss = 0.13301968\n",
      "Iteration 2350, loss = 0.13294676\n",
      "Iteration 2351, loss = 0.13284739\n",
      "Iteration 2352, loss = 0.13279158\n",
      "Iteration 2353, loss = 0.13275748\n",
      "Iteration 2354, loss = 0.13261933\n",
      "Iteration 2355, loss = 0.13262143\n",
      "Iteration 2356, loss = 0.13246591\n",
      "Iteration 2357, loss = 0.13242747\n",
      "Iteration 2358, loss = 0.13227841\n",
      "Iteration 2359, loss = 0.13216053\n",
      "Iteration 2360, loss = 0.13219678\n",
      "Iteration 2361, loss = 0.13205997\n",
      "Iteration 2362, loss = 0.13191001\n",
      "Iteration 2363, loss = 0.13189599\n",
      "Iteration 2364, loss = 0.13185193\n",
      "Iteration 2365, loss = 0.13171666\n",
      "Iteration 2366, loss = 0.13158600\n",
      "Iteration 2367, loss = 0.13154534\n",
      "Iteration 2368, loss = 0.13147205\n",
      "Iteration 2369, loss = 0.13133759\n",
      "Iteration 2370, loss = 0.13125977\n",
      "Iteration 2371, loss = 0.13128139\n",
      "Iteration 2372, loss = 0.13110016\n",
      "Iteration 2373, loss = 0.13099588\n",
      "Iteration 2374, loss = 0.13096222\n",
      "Iteration 2375, loss = 0.13091761\n",
      "Iteration 2376, loss = 0.13077136\n",
      "Iteration 2377, loss = 0.13073250\n",
      "Iteration 2378, loss = 0.13062332\n",
      "Iteration 2379, loss = 0.13058246\n",
      "Iteration 2380, loss = 0.13048101\n",
      "Iteration 2381, loss = 0.13034127\n",
      "Iteration 2382, loss = 0.13029992\n",
      "Iteration 2383, loss = 0.13021144\n",
      "Iteration 2384, loss = 0.13012603\n",
      "Iteration 2385, loss = 0.13006252\n",
      "Iteration 2386, loss = 0.12996101\n",
      "Iteration 2387, loss = 0.12988129\n",
      "Iteration 2388, loss = 0.12983092\n",
      "Iteration 2389, loss = 0.12972765\n",
      "Iteration 2390, loss = 0.12971101\n",
      "Iteration 2391, loss = 0.12954452\n",
      "Iteration 2392, loss = 0.12949180\n",
      "Iteration 2393, loss = 0.12947841\n",
      "Iteration 2394, loss = 0.12934635\n",
      "Iteration 2395, loss = 0.12924199\n",
      "Iteration 2396, loss = 0.12918722\n",
      "Iteration 2397, loss = 0.12916142\n",
      "Iteration 2398, loss = 0.12902006\n",
      "Iteration 2399, loss = 0.12899286\n",
      "Iteration 2400, loss = 0.12892433\n",
      "Iteration 2401, loss = 0.12883787\n",
      "Iteration 2402, loss = 0.12875398\n",
      "Iteration 2403, loss = 0.12865392\n",
      "Iteration 2404, loss = 0.12857388\n",
      "Iteration 2405, loss = 0.12846587\n",
      "Iteration 2406, loss = 0.12840099\n",
      "Iteration 2407, loss = 0.12835395\n",
      "Iteration 2408, loss = 0.12828558\n",
      "Iteration 2409, loss = 0.12832430\n",
      "Iteration 2410, loss = 0.12806552\n",
      "Iteration 2411, loss = 0.12803393\n",
      "Iteration 2412, loss = 0.12800083\n",
      "Iteration 2413, loss = 0.12788441\n",
      "Iteration 2414, loss = 0.12775176\n",
      "Iteration 2415, loss = 0.12769930\n",
      "Iteration 2416, loss = 0.12760930\n",
      "Iteration 2417, loss = 0.12759227\n",
      "Iteration 2418, loss = 0.12743072\n",
      "Iteration 2419, loss = 0.12738846\n",
      "Iteration 2420, loss = 0.12731525\n",
      "Iteration 2421, loss = 0.12722942\n",
      "Iteration 2422, loss = 0.12715696\n",
      "Iteration 2423, loss = 0.12720447\n",
      "Iteration 2424, loss = 0.12700488\n",
      "Iteration 2425, loss = 0.12690555\n",
      "Iteration 2426, loss = 0.12690131\n",
      "Iteration 2427, loss = 0.12687755\n",
      "Iteration 2428, loss = 0.12668598\n",
      "Iteration 2429, loss = 0.12661861\n",
      "Iteration 2430, loss = 0.12652769\n",
      "Iteration 2431, loss = 0.12647430\n",
      "Iteration 2432, loss = 0.12643292\n",
      "Iteration 2433, loss = 0.12638919\n",
      "Iteration 2434, loss = 0.12624180\n",
      "Iteration 2435, loss = 0.12616010\n",
      "Iteration 2436, loss = 0.12608492\n",
      "Iteration 2437, loss = 0.12598830\n",
      "Iteration 2438, loss = 0.12596150\n",
      "Iteration 2439, loss = 0.12585574\n",
      "Iteration 2440, loss = 0.12584343\n",
      "Iteration 2441, loss = 0.12573809\n",
      "Iteration 2442, loss = 0.12565517\n",
      "Iteration 2443, loss = 0.12560313\n",
      "Iteration 2444, loss = 0.12550274\n",
      "Iteration 2445, loss = 0.12545566\n",
      "Iteration 2446, loss = 0.12535794\n",
      "Iteration 2447, loss = 0.12524465\n",
      "Iteration 2448, loss = 0.12554032\n",
      "Iteration 2449, loss = 0.12519321\n",
      "Iteration 2450, loss = 0.12506694\n",
      "Iteration 2451, loss = 0.12511523\n",
      "Iteration 2452, loss = 0.12495926\n",
      "Iteration 2453, loss = 0.12484200\n",
      "Iteration 2454, loss = 0.12475530\n",
      "Iteration 2455, loss = 0.12473326\n",
      "Iteration 2456, loss = 0.12463118\n",
      "Iteration 2457, loss = 0.12467928\n",
      "Iteration 2458, loss = 0.12448084\n",
      "Iteration 2459, loss = 0.12451776\n",
      "Iteration 2460, loss = 0.12429611\n",
      "Iteration 2461, loss = 0.12429886\n",
      "Iteration 2462, loss = 0.12423834\n",
      "Iteration 2463, loss = 0.12415289\n",
      "Iteration 2464, loss = 0.12402737\n",
      "Iteration 2465, loss = 0.12394366\n",
      "Iteration 2466, loss = 0.12390850\n",
      "Iteration 2467, loss = 0.12377630\n",
      "Iteration 2468, loss = 0.12377355\n",
      "Iteration 2469, loss = 0.12375405\n",
      "Iteration 2470, loss = 0.12361409\n",
      "Iteration 2471, loss = 0.12350464\n",
      "Iteration 2472, loss = 0.12355877\n",
      "Iteration 2473, loss = 0.12342471\n",
      "Iteration 2474, loss = 0.12337390\n",
      "Iteration 2475, loss = 0.12327683\n",
      "Iteration 2476, loss = 0.12315351\n",
      "Iteration 2477, loss = 0.12309170\n",
      "Iteration 2478, loss = 0.12302040\n",
      "Iteration 2479, loss = 0.12296235\n",
      "Iteration 2480, loss = 0.12292197\n",
      "Iteration 2481, loss = 0.12279941\n",
      "Iteration 2482, loss = 0.12272033\n",
      "Iteration 2483, loss = 0.12266934\n",
      "Iteration 2484, loss = 0.12259947\n",
      "Iteration 2485, loss = 0.12255890\n",
      "Iteration 2486, loss = 0.12243506\n",
      "Iteration 2487, loss = 0.12241527\n",
      "Iteration 2488, loss = 0.12232032\n",
      "Iteration 2489, loss = 0.12222883\n",
      "Iteration 2490, loss = 0.12215210\n",
      "Iteration 2491, loss = 0.12214186\n",
      "Iteration 2492, loss = 0.12202037\n",
      "Iteration 2493, loss = 0.12195464\n",
      "Iteration 2494, loss = 0.12185490\n",
      "Iteration 2495, loss = 0.12180815\n",
      "Iteration 2496, loss = 0.12178801\n",
      "Iteration 2497, loss = 0.12165759\n",
      "Iteration 2498, loss = 0.12160423\n",
      "Iteration 2499, loss = 0.12153709\n",
      "Iteration 2500, loss = 0.12144979\n",
      "Iteration 2501, loss = 0.12135587\n",
      "Iteration 2502, loss = 0.12133073\n",
      "Iteration 2503, loss = 0.12124412\n",
      "Iteration 2504, loss = 0.12119510\n",
      "Iteration 2505, loss = 0.12112186\n",
      "Iteration 2506, loss = 0.12103689\n",
      "Iteration 2507, loss = 0.12095480\n",
      "Iteration 2508, loss = 0.12095632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86411803\n",
      "Iteration 2, loss = 0.85455572\n",
      "Iteration 3, loss = 0.84054850\n",
      "Iteration 4, loss = 0.82492100\n",
      "Iteration 5, loss = 0.80852274\n",
      "Iteration 6, loss = 0.79225775\n",
      "Iteration 7, loss = 0.77696524\n",
      "Iteration 8, loss = 0.76290682\n",
      "Iteration 9, loss = 0.75048726\n",
      "Iteration 10, loss = 0.73964456\n",
      "Iteration 11, loss = 0.72867718\n",
      "Iteration 12, loss = 0.72000426\n",
      "Iteration 13, loss = 0.71142093\n",
      "Iteration 14, loss = 0.70367549\n",
      "Iteration 15, loss = 0.69679282\n",
      "Iteration 16, loss = 0.69001205\n",
      "Iteration 17, loss = 0.68381523\n",
      "Iteration 18, loss = 0.67810660\n",
      "Iteration 19, loss = 0.67239605\n",
      "Iteration 20, loss = 0.66704533\n",
      "Iteration 21, loss = 0.66203449\n",
      "Iteration 22, loss = 0.65668058\n",
      "Iteration 23, loss = 0.65193236\n",
      "Iteration 24, loss = 0.64701403\n",
      "Iteration 25, loss = 0.64235618\n",
      "Iteration 26, loss = 0.63750735\n",
      "Iteration 27, loss = 0.63283526\n",
      "Iteration 28, loss = 0.62831545\n",
      "Iteration 29, loss = 0.62376357\n",
      "Iteration 30, loss = 0.61917008\n",
      "Iteration 31, loss = 0.61464451\n",
      "Iteration 32, loss = 0.61026815\n",
      "Iteration 33, loss = 0.60577440\n",
      "Iteration 34, loss = 0.60133728\n",
      "Iteration 35, loss = 0.59689141\n",
      "Iteration 36, loss = 0.59246084\n",
      "Iteration 37, loss = 0.58813101\n",
      "Iteration 38, loss = 0.58368264\n",
      "Iteration 39, loss = 0.57929003\n",
      "Iteration 40, loss = 0.57508986\n",
      "Iteration 41, loss = 0.57073255\n",
      "Iteration 42, loss = 0.56648666\n",
      "Iteration 43, loss = 0.56209974\n",
      "Iteration 44, loss = 0.55801352\n",
      "Iteration 45, loss = 0.55375579\n",
      "Iteration 46, loss = 0.54947655\n",
      "Iteration 47, loss = 0.54541527\n",
      "Iteration 48, loss = 0.54135167\n",
      "Iteration 49, loss = 0.53728731\n",
      "Iteration 50, loss = 0.53327652\n",
      "Iteration 51, loss = 0.52930880\n",
      "Iteration 52, loss = 0.52536251\n",
      "Iteration 53, loss = 0.52147110\n",
      "Iteration 54, loss = 0.51772774\n",
      "Iteration 55, loss = 0.51395349\n",
      "Iteration 56, loss = 0.51031278\n",
      "Iteration 57, loss = 0.50666826\n",
      "Iteration 58, loss = 0.50311864\n",
      "Iteration 59, loss = 0.49957518\n",
      "Iteration 60, loss = 0.49615502\n",
      "Iteration 61, loss = 0.49275776\n",
      "Iteration 62, loss = 0.48938809\n",
      "Iteration 63, loss = 0.48624214\n",
      "Iteration 64, loss = 0.48299989\n",
      "Iteration 65, loss = 0.47986891\n",
      "Iteration 66, loss = 0.47692214\n",
      "Iteration 67, loss = 0.47389153\n",
      "Iteration 68, loss = 0.47098588\n",
      "Iteration 69, loss = 0.46823771\n",
      "Iteration 70, loss = 0.46553151\n",
      "Iteration 71, loss = 0.46283594\n",
      "Iteration 72, loss = 0.46033526\n",
      "Iteration 73, loss = 0.45760210\n",
      "Iteration 74, loss = 0.45528627\n",
      "Iteration 75, loss = 0.45279748\n",
      "Iteration 76, loss = 0.45053321\n",
      "Iteration 77, loss = 0.44826379\n",
      "Iteration 78, loss = 0.44594498\n",
      "Iteration 79, loss = 0.44391442\n",
      "Iteration 80, loss = 0.44183798\n",
      "Iteration 81, loss = 0.43982372\n",
      "Iteration 82, loss = 0.43784689\n",
      "Iteration 83, loss = 0.43595254\n",
      "Iteration 84, loss = 0.43414304\n",
      "Iteration 85, loss = 0.43231607\n",
      "Iteration 86, loss = 0.43063363\n",
      "Iteration 87, loss = 0.42895697\n",
      "Iteration 88, loss = 0.42724246\n",
      "Iteration 89, loss = 0.42567782\n",
      "Iteration 90, loss = 0.42408139\n",
      "Iteration 91, loss = 0.42266531\n",
      "Iteration 92, loss = 0.42123928\n",
      "Iteration 93, loss = 0.41981518\n",
      "Iteration 94, loss = 0.41841861\n",
      "Iteration 95, loss = 0.41707953\n",
      "Iteration 96, loss = 0.41587839\n",
      "Iteration 97, loss = 0.41459902\n",
      "Iteration 98, loss = 0.41341689\n",
      "Iteration 99, loss = 0.41226860\n",
      "Iteration 100, loss = 0.41106611\n",
      "Iteration 101, loss = 0.41001555\n",
      "Iteration 102, loss = 0.40897149\n",
      "Iteration 103, loss = 0.40791638\n",
      "Iteration 104, loss = 0.40681850\n",
      "Iteration 105, loss = 0.40593838\n",
      "Iteration 106, loss = 0.40497886\n",
      "Iteration 107, loss = 0.40398975\n",
      "Iteration 108, loss = 0.40307433\n",
      "Iteration 109, loss = 0.40227844\n",
      "Iteration 110, loss = 0.40139736\n",
      "Iteration 111, loss = 0.40053272\n",
      "Iteration 112, loss = 0.39966137\n",
      "Iteration 113, loss = 0.39884513\n",
      "Iteration 114, loss = 0.39805726\n",
      "Iteration 115, loss = 0.39734244\n",
      "Iteration 116, loss = 0.39660889\n",
      "Iteration 117, loss = 0.39589348\n",
      "Iteration 118, loss = 0.39519435\n",
      "Iteration 119, loss = 0.39449295\n",
      "Iteration 120, loss = 0.39377535\n",
      "Iteration 121, loss = 0.39315826\n",
      "Iteration 122, loss = 0.39244097\n",
      "Iteration 123, loss = 0.39184866\n",
      "Iteration 124, loss = 0.39121799\n",
      "Iteration 125, loss = 0.39062916\n",
      "Iteration 126, loss = 0.39003432\n",
      "Iteration 127, loss = 0.38941818\n",
      "Iteration 128, loss = 0.38885033\n",
      "Iteration 129, loss = 0.38827335\n",
      "Iteration 130, loss = 0.38772627\n",
      "Iteration 131, loss = 0.38724886\n",
      "Iteration 132, loss = 0.38663221\n",
      "Iteration 133, loss = 0.38615429\n",
      "Iteration 134, loss = 0.38564574\n",
      "Iteration 135, loss = 0.38512333\n",
      "Iteration 136, loss = 0.38465525\n",
      "Iteration 137, loss = 0.38413554\n",
      "Iteration 138, loss = 0.38367348\n",
      "Iteration 139, loss = 0.38318020\n",
      "Iteration 140, loss = 0.38274489\n",
      "Iteration 141, loss = 0.38229727\n",
      "Iteration 142, loss = 0.38190413\n",
      "Iteration 143, loss = 0.38140618\n",
      "Iteration 144, loss = 0.38094483\n",
      "Iteration 145, loss = 0.38053669\n",
      "Iteration 146, loss = 0.38009889\n",
      "Iteration 147, loss = 0.37969666\n",
      "Iteration 148, loss = 0.37924186\n",
      "Iteration 149, loss = 0.37884798\n",
      "Iteration 150, loss = 0.37843337\n",
      "Iteration 151, loss = 0.37813135\n",
      "Iteration 152, loss = 0.37766982\n",
      "Iteration 153, loss = 0.37728832\n",
      "Iteration 154, loss = 0.37685047\n",
      "Iteration 155, loss = 0.37653770\n",
      "Iteration 156, loss = 0.37611979\n",
      "Iteration 157, loss = 0.37577905\n",
      "Iteration 158, loss = 0.37540291\n",
      "Iteration 159, loss = 0.37504303\n",
      "Iteration 160, loss = 0.37469779\n",
      "Iteration 161, loss = 0.37431973\n",
      "Iteration 162, loss = 0.37394688\n",
      "Iteration 163, loss = 0.37361538\n",
      "Iteration 164, loss = 0.37327773\n",
      "Iteration 165, loss = 0.37294326\n",
      "Iteration 166, loss = 0.37262803\n",
      "Iteration 167, loss = 0.37227691\n",
      "Iteration 168, loss = 0.37193464\n",
      "Iteration 169, loss = 0.37163258\n",
      "Iteration 170, loss = 0.37130717\n",
      "Iteration 171, loss = 0.37099544\n",
      "Iteration 172, loss = 0.37066503\n",
      "Iteration 173, loss = 0.37035177\n",
      "Iteration 174, loss = 0.37004118\n",
      "Iteration 175, loss = 0.36972796\n",
      "Iteration 176, loss = 0.36944709\n",
      "Iteration 177, loss = 0.36914887\n",
      "Iteration 178, loss = 0.36880543\n",
      "Iteration 179, loss = 0.36855067\n",
      "Iteration 180, loss = 0.36823733\n",
      "Iteration 181, loss = 0.36792011\n",
      "Iteration 182, loss = 0.36766414\n",
      "Iteration 183, loss = 0.36735666\n",
      "Iteration 184, loss = 0.36707450\n",
      "Iteration 185, loss = 0.36681321\n",
      "Iteration 186, loss = 0.36651564\n",
      "Iteration 187, loss = 0.36623232\n",
      "Iteration 188, loss = 0.36597367\n",
      "Iteration 189, loss = 0.36565759\n",
      "Iteration 190, loss = 0.36541776\n",
      "Iteration 191, loss = 0.36516705\n",
      "Iteration 192, loss = 0.36487352\n",
      "Iteration 193, loss = 0.36461092\n",
      "Iteration 194, loss = 0.36433153\n",
      "Iteration 195, loss = 0.36408775\n",
      "Iteration 196, loss = 0.36382847\n",
      "Iteration 197, loss = 0.36354874\n",
      "Iteration 198, loss = 0.36329812\n",
      "Iteration 199, loss = 0.36308480\n",
      "Iteration 200, loss = 0.36278151\n",
      "Iteration 201, loss = 0.36257470\n",
      "Iteration 202, loss = 0.36229922\n",
      "Iteration 203, loss = 0.36204141\n",
      "Iteration 204, loss = 0.36180327\n",
      "Iteration 205, loss = 0.36155538\n",
      "Iteration 206, loss = 0.36130074\n",
      "Iteration 207, loss = 0.36109047\n",
      "Iteration 208, loss = 0.36082286\n",
      "Iteration 209, loss = 0.36058616\n",
      "Iteration 210, loss = 0.36033100\n",
      "Iteration 211, loss = 0.36011008\n",
      "Iteration 212, loss = 0.35986468\n",
      "Iteration 213, loss = 0.35961920\n",
      "Iteration 214, loss = 0.35940472\n",
      "Iteration 215, loss = 0.35917032\n",
      "Iteration 216, loss = 0.35892651\n",
      "Iteration 217, loss = 0.35870535\n",
      "Iteration 218, loss = 0.35849528\n",
      "Iteration 219, loss = 0.35825612\n",
      "Iteration 220, loss = 0.35803069\n",
      "Iteration 221, loss = 0.35780356\n",
      "Iteration 222, loss = 0.35758978\n",
      "Iteration 223, loss = 0.35733698\n",
      "Iteration 224, loss = 0.35714128\n",
      "Iteration 225, loss = 0.35693817\n",
      "Iteration 226, loss = 0.35669776\n",
      "Iteration 227, loss = 0.35647316\n",
      "Iteration 228, loss = 0.35625093\n",
      "Iteration 229, loss = 0.35607373\n",
      "Iteration 230, loss = 0.35582402\n",
      "Iteration 231, loss = 0.35564156\n",
      "Iteration 232, loss = 0.35540963\n",
      "Iteration 233, loss = 0.35520479\n",
      "Iteration 234, loss = 0.35500121\n",
      "Iteration 235, loss = 0.35478969\n",
      "Iteration 236, loss = 0.35456209\n",
      "Iteration 237, loss = 0.35436896\n",
      "Iteration 238, loss = 0.35414646\n",
      "Iteration 239, loss = 0.35394405\n",
      "Iteration 240, loss = 0.35379174\n",
      "Iteration 241, loss = 0.35352882\n",
      "Iteration 242, loss = 0.35334374\n",
      "Iteration 243, loss = 0.35312628\n",
      "Iteration 244, loss = 0.35296176\n",
      "Iteration 245, loss = 0.35272411\n",
      "Iteration 246, loss = 0.35251938\n",
      "Iteration 247, loss = 0.35234434\n",
      "Iteration 248, loss = 0.35214454\n",
      "Iteration 249, loss = 0.35194361\n",
      "Iteration 250, loss = 0.35173041\n",
      "Iteration 251, loss = 0.35152024\n",
      "Iteration 252, loss = 0.35135899\n",
      "Iteration 253, loss = 0.35112182\n",
      "Iteration 254, loss = 0.35095935\n",
      "Iteration 2233, loss = 0.15694374\n",
      "Iteration 2234, loss = 0.15700910\n",
      "Iteration 2235, loss = 0.15677464\n",
      "Iteration 2236, loss = 0.15669112\n",
      "Iteration 2237, loss = 0.15666857\n",
      "Iteration 2238, loss = 0.15657669\n",
      "Iteration 2239, loss = 0.15643761\n",
      "Iteration 2240, loss = 0.15640079\n",
      "Iteration 2241, loss = 0.15625855\n",
      "Iteration 2242, loss = 0.15641151\n",
      "Iteration 2243, loss = 0.15606816\n",
      "Iteration 2244, loss = 0.15598641\n",
      "Iteration 2245, loss = 0.15596649\n",
      "Iteration 2246, loss = 0.15588267\n",
      "Iteration 2247, loss = 0.15570187\n",
      "Iteration 2248, loss = 0.15569731\n",
      "Iteration 2249, loss = 0.15559650\n",
      "Iteration 2250, loss = 0.15543588\n",
      "Iteration 2251, loss = 0.15532270\n",
      "Iteration 2252, loss = 0.15527133\n",
      "Iteration 2253, loss = 0.15525569\n",
      "Iteration 2254, loss = 0.15509988\n",
      "Iteration 2255, loss = 0.15497343\n",
      "Iteration 2256, loss = 0.15514976\n",
      "Iteration 2257, loss = 0.15488388\n",
      "Iteration 2258, loss = 0.15472789\n",
      "Iteration 2259, loss = 0.15468525\n",
      "Iteration 2260, loss = 0.15462321\n",
      "Iteration 2261, loss = 0.15451796\n",
      "Iteration 2262, loss = 0.15435422\n",
      "Iteration 2263, loss = 0.15436244\n",
      "Iteration 2264, loss = 0.15430467\n",
      "Iteration 2265, loss = 0.15409776\n",
      "Iteration 2266, loss = 0.15406672\n",
      "Iteration 2267, loss = 0.15412775\n",
      "Iteration 2268, loss = 0.15390621\n",
      "Iteration 2269, loss = 0.15384510\n",
      "Iteration 2270, loss = 0.15366790\n",
      "Iteration 2271, loss = 0.15364837\n",
      "Iteration 2272, loss = 0.15365569\n",
      "Iteration 2273, loss = 0.15346307\n",
      "Iteration 2274, loss = 0.15329682\n",
      "Iteration 2275, loss = 0.15320267\n",
      "Iteration 2276, loss = 0.15315207\n",
      "Iteration 2277, loss = 0.15302153\n",
      "Iteration 2278, loss = 0.15298556\n",
      "Iteration 2279, loss = 0.15286860\n",
      "Iteration 2280, loss = 0.15292207\n",
      "Iteration 2281, loss = 0.15265875\n",
      "Iteration 2282, loss = 0.15262294\n",
      "Iteration 2283, loss = 0.15250737\n",
      "Iteration 2284, loss = 0.15240024\n",
      "Iteration 2285, loss = 0.15229023\n",
      "Iteration 2286, loss = 0.15234893\n",
      "Iteration 2287, loss = 0.15215109\n",
      "Iteration 2288, loss = 0.15206432\n",
      "Iteration 2289, loss = 0.15213801\n",
      "Iteration 2290, loss = 0.15205974\n",
      "Iteration 2291, loss = 0.15174252\n",
      "Iteration 2292, loss = 0.15176281\n",
      "Iteration 2293, loss = 0.15157557\n",
      "Iteration 2294, loss = 0.15151227\n",
      "Iteration 2295, loss = 0.15140544\n",
      "Iteration 2296, loss = 0.15151659\n",
      "Iteration 2297, loss = 0.15144376\n",
      "Iteration 2298, loss = 0.15114671\n",
      "Iteration 2299, loss = 0.15135858\n",
      "Iteration 2300, loss = 0.15096449\n",
      "Iteration 2301, loss = 0.15087166\n",
      "Iteration 2302, loss = 0.15090261\n",
      "Iteration 2303, loss = 0.15069807\n",
      "Iteration 2304, loss = 0.15060712\n",
      "Iteration 2305, loss = 0.15056092\n",
      "Iteration 2306, loss = 0.15042505\n",
      "Iteration 2307, loss = 0.15035438\n",
      "Iteration 2308, loss = 0.15033577\n",
      "Iteration 2309, loss = 0.15016020\n",
      "Iteration 2310, loss = 0.15012131\n",
      "Iteration 2311, loss = 0.15009225\n",
      "Iteration 2312, loss = 0.14998581\n",
      "Iteration 2313, loss = 0.14984922\n",
      "Iteration 2314, loss = 0.14970407\n",
      "Iteration 2315, loss = 0.14974924\n",
      "Iteration 2316, loss = 0.14957963\n",
      "Iteration 2317, loss = 0.14956759\n",
      "Iteration 2318, loss = 0.14948478\n",
      "Iteration 2319, loss = 0.14944383\n",
      "Iteration 2320, loss = 0.14926368\n",
      "Iteration 2321, loss = 0.14920197\n",
      "Iteration 2322, loss = 0.14908457\n",
      "Iteration 2323, loss = 0.14900265\n",
      "Iteration 2324, loss = 0.14885813\n",
      "Iteration 2325, loss = 0.14884713\n",
      "Iteration 2326, loss = 0.14875453\n",
      "Iteration 2327, loss = 0.14873612\n",
      "Iteration 2328, loss = 0.14856210\n",
      "Iteration 2329, loss = 0.14856472\n",
      "Iteration 2330, loss = 0.14845872\n",
      "Iteration 2331, loss = 0.14827301\n",
      "Iteration 2332, loss = 0.14816109\n",
      "Iteration 2333, loss = 0.14831960\n",
      "Iteration 2334, loss = 0.14810904\n",
      "Iteration 2335, loss = 0.14792239\n",
      "Iteration 2336, loss = 0.14786861\n",
      "Iteration 2337, loss = 0.14778026\n",
      "Iteration 2338, loss = 0.14766553\n",
      "Iteration 2339, loss = 0.14758271\n",
      "Iteration 2340, loss = 0.14749287\n",
      "Iteration 2341, loss = 0.14745005\n",
      "Iteration 2342, loss = 0.14730503\n",
      "Iteration 2343, loss = 0.14723150\n",
      "Iteration 2344, loss = 0.14724315\n",
      "Iteration 2345, loss = 0.14703768\n",
      "Iteration 2346, loss = 0.14695803\n",
      "Iteration 2347, loss = 0.14693231\n",
      "Iteration 2348, loss = 0.14683278\n",
      "Iteration 2349, loss = 0.14677566\n",
      "Iteration 2350, loss = 0.14672253\n",
      "Iteration 2351, loss = 0.14661351\n",
      "Iteration 2352, loss = 0.14650170\n",
      "Iteration 2353, loss = 0.14635018\n",
      "Iteration 2354, loss = 0.14647801\n",
      "Iteration 2355, loss = 0.14629803\n",
      "Iteration 2356, loss = 0.14623722\n",
      "Iteration 2357, loss = 0.14605777\n",
      "Iteration 2358, loss = 0.14594104\n",
      "Iteration 2359, loss = 0.14590147\n",
      "Iteration 2360, loss = 0.14580199\n",
      "Iteration 2361, loss = 0.14572174\n",
      "Iteration 2362, loss = 0.14566830\n",
      "Iteration 2363, loss = 0.14549644\n",
      "Iteration 2364, loss = 0.14544401\n",
      "Iteration 2365, loss = 0.14535702\n",
      "Iteration 2366, loss = 0.14525242\n",
      "Iteration 2367, loss = 0.14518315\n",
      "Iteration 2368, loss = 0.14510394\n",
      "Iteration 2369, loss = 0.14505133\n",
      "Iteration 2370, loss = 0.14491705\n",
      "Iteration 2371, loss = 0.14485563\n",
      "Iteration 2372, loss = 0.14478918\n",
      "Iteration 2373, loss = 0.14469387\n",
      "Iteration 2374, loss = 0.14461287\n",
      "Iteration 2375, loss = 0.14454338\n",
      "Iteration 2376, loss = 0.14444776\n",
      "Iteration 2377, loss = 0.14438155\n",
      "Iteration 2378, loss = 0.14424972\n",
      "Iteration 2379, loss = 0.14432921\n",
      "Iteration 2380, loss = 0.14409516\n",
      "Iteration 2381, loss = 0.14411467\n",
      "Iteration 2382, loss = 0.14400971\n",
      "Iteration 2383, loss = 0.14385428\n",
      "Iteration 2384, loss = 0.14380378\n",
      "Iteration 2385, loss = 0.14367532\n",
      "Iteration 2386, loss = 0.14361702\n",
      "Iteration 2387, loss = 0.14357404\n",
      "Iteration 2388, loss = 0.14350565\n",
      "Iteration 2389, loss = 0.14346038\n",
      "Iteration 2390, loss = 0.14326724\n",
      "Iteration 2391, loss = 0.14316664\n",
      "Iteration 2392, loss = 0.14316967\n",
      "Iteration 2393, loss = 0.14303285\n",
      "Iteration 2394, loss = 0.14315464\n",
      "Iteration 2395, loss = 0.14287202\n",
      "Iteration 2396, loss = 0.14286235\n",
      "Iteration 2397, loss = 0.14287481\n",
      "Iteration 2398, loss = 0.14258916\n",
      "Iteration 2399, loss = 0.14253831\n",
      "Iteration 2400, loss = 0.14249019\n",
      "Iteration 2401, loss = 0.14242365\n",
      "Iteration 2402, loss = 0.14228601\n",
      "Iteration 2403, loss = 0.14218757\n",
      "Iteration 2404, loss = 0.14213155\n",
      "Iteration 2405, loss = 0.14211591\n",
      "Iteration 2406, loss = 0.14201521\n",
      "Iteration 2407, loss = 0.14186049\n",
      "Iteration 2408, loss = 0.14176217\n",
      "Iteration 2409, loss = 0.14172458\n",
      "Iteration 2410, loss = 0.14165731\n",
      "Iteration 2411, loss = 0.14156498\n",
      "Iteration 2412, loss = 0.14145712\n",
      "Iteration 2413, loss = 0.14142464\n",
      "Iteration 2414, loss = 0.14134497\n",
      "Iteration 2415, loss = 0.14125976\n",
      "Iteration 2416, loss = 0.14116999\n",
      "Iteration 2417, loss = 0.14104524\n",
      "Iteration 2418, loss = 0.14106384\n",
      "Iteration 2419, loss = 0.14092065\n",
      "Iteration 2420, loss = 0.14081384\n",
      "Iteration 2421, loss = 0.14070901\n",
      "Iteration 2422, loss = 0.14069094\n",
      "Iteration 2423, loss = 0.14067162\n",
      "Iteration 2424, loss = 0.14071907\n",
      "Iteration 2425, loss = 0.14052315\n",
      "Iteration 2426, loss = 0.14032471\n",
      "Iteration 2427, loss = 0.14027807\n",
      "Iteration 2428, loss = 0.14013666\n",
      "Iteration 2429, loss = 0.14006229\n",
      "Iteration 2430, loss = 0.13998517\n",
      "Iteration 2431, loss = 0.13998576\n",
      "Iteration 2432, loss = 0.13999780\n",
      "Iteration 2433, loss = 0.13977235\n",
      "Iteration 2434, loss = 0.13975581\n",
      "Iteration 2435, loss = 0.13959580\n",
      "Iteration 2436, loss = 0.13955488\n",
      "Iteration 2437, loss = 0.13942822\n",
      "Iteration 2438, loss = 0.13951529\n",
      "Iteration 2439, loss = 0.13924884\n",
      "Iteration 2440, loss = 0.13917366\n",
      "Iteration 2441, loss = 0.13912210\n",
      "Iteration 2442, loss = 0.13903381\n",
      "Iteration 2443, loss = 0.13897530\n",
      "Iteration 2444, loss = 0.13895813\n",
      "Iteration 2445, loss = 0.13881060\n",
      "Iteration 2446, loss = 0.13877392\n",
      "Iteration 2447, loss = 0.13864891\n",
      "Iteration 2448, loss = 0.13854883\n",
      "Iteration 2449, loss = 0.13848125\n",
      "Iteration 2450, loss = 0.13845795\n",
      "Iteration 2451, loss = 0.13845289\n",
      "Iteration 2452, loss = 0.13837364\n",
      "Iteration 2453, loss = 0.13812973\n",
      "Iteration 2454, loss = 0.13820311\n",
      "Iteration 2455, loss = 0.13799910\n",
      "Iteration 2456, loss = 0.13796890\n",
      "Iteration 2457, loss = 0.13785653\n",
      "Iteration 2458, loss = 0.13784236\n",
      "Iteration 2459, loss = 0.13765453\n",
      "Iteration 2460, loss = 0.13759095\n",
      "Iteration 2461, loss = 0.13753670\n",
      "Iteration 2462, loss = 0.13742052\n",
      "Iteration 2463, loss = 0.13753088\n",
      "Iteration 2464, loss = 0.13731490\n",
      "Iteration 2465, loss = 0.13740992\n",
      "Iteration 2466, loss = 0.13714177\n",
      "Iteration 2467, loss = 0.13707198\n",
      "Iteration 2468, loss = 0.13695074\n",
      "Iteration 2469, loss = 0.13694113\n",
      "Iteration 2470, loss = 0.13684664\n",
      "Iteration 2471, loss = 0.13681640\n",
      "Iteration 2472, loss = 0.13671970\n",
      "Iteration 2473, loss = 0.13656507\n",
      "Iteration 2474, loss = 0.13651072\n",
      "Iteration 2475, loss = 0.13658762\n",
      "Iteration 2476, loss = 0.13636259\n",
      "Iteration 2477, loss = 0.13626293\n",
      "Iteration 2478, loss = 0.13625266\n",
      "Iteration 2479, loss = 0.13611794\n",
      "Iteration 2480, loss = 0.13607798\n",
      "Iteration 2481, loss = 0.13598241\n",
      "Iteration 2482, loss = 0.13590678\n",
      "Iteration 2483, loss = 0.13605603\n",
      "Iteration 2484, loss = 0.13575613\n",
      "Iteration 2485, loss = 0.13563270\n",
      "Iteration 2486, loss = 0.13564562\n",
      "Iteration 2487, loss = 0.13552992\n",
      "Iteration 2488, loss = 0.13542121\n",
      "Iteration 2489, loss = 0.13539307\n",
      "Iteration 2490, loss = 0.13526013\n",
      "Iteration 2491, loss = 0.13517605\n",
      "Iteration 2492, loss = 0.13511366\n",
      "Iteration 2493, loss = 0.13516507\n",
      "Iteration 2494, loss = 0.13500244\n",
      "Iteration 2495, loss = 0.13506618\n",
      "Iteration 2496, loss = 0.13501615\n",
      "Iteration 2497, loss = 0.13478037\n",
      "Iteration 2498, loss = 0.13467925\n",
      "Iteration 2499, loss = 0.13468463\n",
      "Iteration 2500, loss = 0.13450277\n",
      "Iteration 2501, loss = 0.13445764\n",
      "Iteration 2502, loss = 0.13437554\n",
      "Iteration 2503, loss = 0.13434837\n",
      "Iteration 2504, loss = 0.13426703\n",
      "Iteration 2505, loss = 0.13414019\n",
      "Iteration 2506, loss = 0.13418100\n",
      "Iteration 2507, loss = 0.13405606\n",
      "Iteration 2508, loss = 0.13400860\n",
      "Iteration 2509, loss = 0.13394650\n",
      "Iteration 2510, loss = 0.13384127\n",
      "Iteration 2511, loss = 0.13372799\n",
      "Iteration 2512, loss = 0.13364609\n",
      "Iteration 2513, loss = 0.13360499\n",
      "Iteration 2514, loss = 0.13366712\n",
      "Iteration 2515, loss = 0.13343168\n",
      "Iteration 2516, loss = 0.13337803\n",
      "Iteration 2517, loss = 0.13336322\n",
      "Iteration 2518, loss = 0.13316807\n",
      "Iteration 2519, loss = 0.13315220\n",
      "Iteration 2520, loss = 0.13304715\n",
      "Iteration 2521, loss = 0.13300911\n",
      "Iteration 2522, loss = 0.13299890\n",
      "Iteration 2523, loss = 0.13285054\n",
      "Iteration 2524, loss = 0.13272958\n",
      "Iteration 2525, loss = 0.13269104\n",
      "Iteration 2526, loss = 0.13262515\n",
      "Iteration 2527, loss = 0.13260015\n",
      "Iteration 2528, loss = 0.13244475\n",
      "Iteration 2529, loss = 0.13243160\n",
      "Iteration 2530, loss = 0.13236477\n",
      "Iteration 2531, loss = 0.13221093\n",
      "Iteration 2532, loss = 0.13213617\n",
      "Iteration 2533, loss = 0.13218569\n",
      "Iteration 2534, loss = 0.13200862\n",
      "Iteration 2535, loss = 0.13199906\n",
      "Iteration 2536, loss = 0.13205356\n",
      "Iteration 2537, loss = 0.13177766\n",
      "Iteration 2538, loss = 0.13176889\n",
      "Iteration 2539, loss = 0.13186077\n",
      "Iteration 2540, loss = 0.13167914\n",
      "Iteration 2541, loss = 0.13154600\n",
      "Iteration 2542, loss = 0.13151297\n",
      "Iteration 2543, loss = 0.13144147\n",
      "Iteration 2544, loss = 0.13135503\n",
      "Iteration 2545, loss = 0.13121856\n",
      "Iteration 2546, loss = 0.13119595\n",
      "Iteration 2547, loss = 0.13125843\n",
      "Iteration 2548, loss = 0.13110977\n",
      "Iteration 2549, loss = 0.13104087\n",
      "Iteration 2550, loss = 0.13090066\n",
      "Iteration 2551, loss = 0.13078221\n",
      "Iteration 2552, loss = 0.13072185\n",
      "Iteration 2553, loss = 0.13071172\n",
      "Iteration 2554, loss = 0.13061286\n",
      "Iteration 2555, loss = 0.13051148\n",
      "Iteration 2556, loss = 0.13043961\n",
      "Iteration 2557, loss = 0.13035960\n",
      "Iteration 2558, loss = 0.13047386\n",
      "Iteration 2559, loss = 0.13023340\n",
      "Iteration 2560, loss = 0.13022579\n",
      "Iteration 2561, loss = 0.13014627\n",
      "Iteration 2562, loss = 0.13009385\n",
      "Iteration 2563, loss = 0.13007646\n",
      "Iteration 2564, loss = 0.12999122\n",
      "Iteration 2565, loss = 0.12981342\n",
      "Iteration 2566, loss = 0.12972827\n",
      "Iteration 2567, loss = 0.12965868\n",
      "Iteration 2568, loss = 0.12961315\n",
      "Iteration 2569, loss = 0.12958781\n",
      "Iteration 2570, loss = 0.12944522\n",
      "Iteration 2571, loss = 0.12942333\n",
      "Iteration 2572, loss = 0.12933320\n",
      "Iteration 2573, loss = 0.12926665\n",
      "Iteration 2574, loss = 0.12915299\n",
      "Iteration 2575, loss = 0.12919993\n",
      "Iteration 2576, loss = 0.12918303\n",
      "Iteration 2577, loss = 0.12897611\n",
      "Iteration 2578, loss = 0.12898088\n",
      "Iteration 2579, loss = 0.12882886\n",
      "Iteration 2580, loss = 0.12873318\n",
      "Iteration 2581, loss = 0.12867945\n",
      "Iteration 2582, loss = 0.12860631\n",
      "Iteration 2583, loss = 0.12859428\n",
      "Iteration 2584, loss = 0.12848228\n",
      "Iteration 2585, loss = 0.12844119\n",
      "Iteration 2586, loss = 0.12836337\n",
      "Iteration 2587, loss = 0.12831693\n",
      "Iteration 2588, loss = 0.12818176\n",
      "Iteration 2589, loss = 0.12813590\n",
      "Iteration 2590, loss = 0.12808291\n",
      "Iteration 2591, loss = 0.12808417\n",
      "Iteration 2592, loss = 0.12802164\n",
      "Iteration 2593, loss = 0.12786718\n",
      "Iteration 2594, loss = 0.12775218\n",
      "Iteration 2595, loss = 0.12772667\n",
      "Iteration 2596, loss = 0.12776497\n",
      "Iteration 2597, loss = 0.12755632\n",
      "Iteration 2598, loss = 0.12748573\n",
      "Iteration 2599, loss = 0.12746750\n",
      "Iteration 2600, loss = 0.12737882\n",
      "Iteration 2601, loss = 0.12728980\n",
      "Iteration 2602, loss = 0.12719903\n",
      "Iteration 2603, loss = 0.12714604\n",
      "Iteration 2604, loss = 0.12712245\n",
      "Iteration 2605, loss = 0.12707007\n",
      "Iteration 2606, loss = 0.12697685\n",
      "Iteration 2607, loss = 0.12689878\n",
      "Iteration 2608, loss = 0.12678878\n",
      "Iteration 2609, loss = 0.12679769\n",
      "Iteration 2610, loss = 0.12683855\n",
      "Iteration 2611, loss = 0.12661241\n",
      "Iteration 2612, loss = 0.12659397\n",
      "Iteration 2613, loss = 0.12648997\n",
      "Iteration 2614, loss = 0.12643281\n",
      "Iteration 2615, loss = 0.12631889\n",
      "Iteration 2616, loss = 0.12625435\n",
      "Iteration 2617, loss = 0.12619367\n",
      "Iteration 2618, loss = 0.12625655\n",
      "Iteration 2619, loss = 0.12609035\n",
      "Iteration 2620, loss = 0.12607382\n",
      "Iteration 2621, loss = 0.12593551\n",
      "Iteration 2622, loss = 0.12591983\n",
      "Iteration 2623, loss = 0.12576917\n",
      "Iteration 2624, loss = 0.12578393\n",
      "Iteration 2625, loss = 0.12568741\n",
      "Iteration 2626, loss = 0.12561448\n",
      "Iteration 2627, loss = 0.12552065\n",
      "Iteration 2628, loss = 0.12546328\n",
      "Iteration 2629, loss = 0.12538932\n",
      "Iteration 2630, loss = 0.12539678\n",
      "Iteration 2631, loss = 0.12530903\n",
      "Iteration 2632, loss = 0.12531227\n",
      "Iteration 2633, loss = 0.12531268\n",
      "Iteration 2634, loss = 0.12507133\n",
      "Iteration 2635, loss = 0.12506139\n",
      "Iteration 2636, loss = 0.12506480\n",
      "Iteration 2637, loss = 0.12485167\n",
      "Iteration 2638, loss = 0.12477038\n",
      "Iteration 2639, loss = 0.12496750\n",
      "Iteration 2640, loss = 0.12465139\n",
      "Iteration 2641, loss = 0.12461395\n",
      "Iteration 2642, loss = 0.12454379\n",
      "Iteration 2643, loss = 0.12447283\n",
      "Iteration 2644, loss = 0.12437598\n",
      "Iteration 2645, loss = 0.12438310\n",
      "Iteration 2646, loss = 0.12425873\n",
      "Iteration 2647, loss = 0.12417382\n",
      "Iteration 2648, loss = 0.12419109\n",
      "Iteration 2649, loss = 0.12405281\n",
      "Iteration 2650, loss = 0.12399090\n",
      "Iteration 2651, loss = 0.12409453\n",
      "Iteration 2652, loss = 0.12385498\n",
      "Iteration 2653, loss = 0.12387667\n",
      "Iteration 2654, loss = 0.12390579\n",
      "Iteration 2655, loss = 0.12366065\n",
      "Iteration 2656, loss = 0.12361147\n",
      "Iteration 2657, loss = 0.12352212\n",
      "Iteration 2658, loss = 0.12345245\n",
      "Iteration 2659, loss = 0.12341271\n",
      "Iteration 2660, loss = 0.12332086\n",
      "Iteration 2661, loss = 0.12325752\n",
      "Iteration 2662, loss = 0.12317205\n",
      "Iteration 2663, loss = 0.12312918\n",
      "Iteration 2664, loss = 0.12305102\n",
      "Iteration 2665, loss = 0.12301508\n",
      "Iteration 2666, loss = 0.12299022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74030143\n",
      "Iteration 2, loss = 0.73626507\n",
      "Iteration 3, loss = 0.73029931\n",
      "Iteration 4, loss = 0.72285708\n",
      "Iteration 5, loss = 0.71471752\n",
      "Iteration 6, loss = 0.70583981\n",
      "Iteration 7, loss = 0.69710509\n",
      "Iteration 8, loss = 0.68813532\n",
      "Iteration 9, loss = 0.67928474\n",
      "Iteration 10, loss = 0.67073217\n",
      "Iteration 11, loss = 0.66205241\n",
      "Iteration 12, loss = 0.65391619\n",
      "Iteration 13, loss = 0.64601828\n",
      "Iteration 14, loss = 0.63837322\n",
      "Iteration 15, loss = 0.63095509\n",
      "Iteration 16, loss = 0.62372614\n",
      "Iteration 17, loss = 0.61692684\n",
      "Iteration 18, loss = 0.61036789\n",
      "Iteration 19, loss = 0.60384867\n",
      "Iteration 20, loss = 0.59773962\n",
      "Iteration 21, loss = 0.59175017\n",
      "Iteration 22, loss = 0.58597013\n",
      "Iteration 23, loss = 0.58038818\n",
      "Iteration 24, loss = 0.57497432\n",
      "Iteration 25, loss = 0.56965979\n",
      "Iteration 26, loss = 0.56470532\n",
      "Iteration 27, loss = 0.55975340\n",
      "Iteration 28, loss = 0.55493577\n",
      "Iteration 29, loss = 0.55039015\n",
      "Iteration 30, loss = 0.54579434\n",
      "Iteration 31, loss = 0.54141982\n",
      "Iteration 32, loss = 0.53727060\n",
      "Iteration 33, loss = 0.53311531\n",
      "Iteration 34, loss = 0.52905093\n",
      "Iteration 35, loss = 0.52513141\n",
      "Iteration 36, loss = 0.52133756\n",
      "Iteration 37, loss = 0.51762430\n",
      "Iteration 38, loss = 0.51403037\n",
      "Iteration 39, loss = 0.51055470\n",
      "Iteration 40, loss = 0.50715408\n",
      "Iteration 41, loss = 0.50379983\n",
      "Iteration 42, loss = 0.50046438\n",
      "Iteration 43, loss = 0.49732663\n",
      "Iteration 44, loss = 0.49427115\n",
      "Iteration 45, loss = 0.49123805\n",
      "Iteration 46, loss = 0.48828363\n",
      "Iteration 47, loss = 0.48545700\n",
      "\n",
      "Iteration 73, loss = 0.51796187\n",
      "Iteration 74, loss = 0.51516959\n",
      "Iteration 75, loss = 0.51251621\n",
      "Iteration 76, loss = 0.50978584\n",
      "Iteration 77, loss = 0.50709863\n",
      "Iteration 78, loss = 0.50455368\n",
      "Iteration 79, loss = 0.50205566\n",
      "Iteration 80, loss = 0.49957705\n",
      "Iteration 81, loss = 0.49710213\n",
      "Iteration 82, loss = 0.49481560\n",
      "Iteration 83, loss = 0.49248346\n",
      "Iteration 84, loss = 0.49016411\n",
      "Iteration 85, loss = 0.48796027\n",
      "Iteration 86, loss = 0.48579877\n",
      "Iteration 87, loss = 0.48364512\n",
      "Iteration 88, loss = 0.48162261\n",
      "Iteration 89, loss = 0.47962695\n",
      "Iteration 90, loss = 0.47767402\n",
      "Iteration 91, loss = 0.47583890\n",
      "Iteration 92, loss = 0.47387622\n",
      "Iteration 93, loss = 0.47209582\n",
      "Iteration 94, loss = 0.47040210\n",
      "Iteration 95, loss = 0.46857561\n",
      "Iteration 96, loss = 0.46699930\n",
      "Iteration 97, loss = 0.46529861\n",
      "Iteration 98, loss = 0.46379609\n",
      "Iteration 99, loss = 0.46214748\n",
      "Iteration 100, loss = 0.46064182\n",
      "Iteration 101, loss = 0.45926266\n",
      "Iteration 102, loss = 0.45780372\n",
      "Iteration 103, loss = 0.45641629\n",
      "Iteration 104, loss = 0.45499271\n",
      "Iteration 105, loss = 0.45372942\n",
      "Iteration 106, loss = 0.45245414\n",
      "Iteration 107, loss = 0.45125521\n",
      "Iteration 108, loss = 0.45008638\n",
      "Iteration 109, loss = 0.44883926\n",
      "Iteration 110, loss = 0.44773030\n",
      "Iteration 111, loss = 0.44658679\n",
      "Iteration 112, loss = 0.44549800\n",
      "Iteration 113, loss = 0.44443274\n",
      "Iteration 114, loss = 0.44341670\n",
      "Iteration 115, loss = 0.44244577\n",
      "Iteration 116, loss = 0.44139980\n",
      "Iteration 117, loss = 0.44047300\n",
      "Iteration 118, loss = 0.43953611\n",
      "Iteration 119, loss = 0.43866307\n",
      "Iteration 120, loss = 0.43774789\n",
      "Iteration 121, loss = 0.43690031\n",
      "Iteration 122, loss = 0.43601816\n",
      "Iteration 123, loss = 0.43527014\n",
      "Iteration 124, loss = 0.43441948\n",
      "Iteration 125, loss = 0.43364725\n",
      "Iteration 126, loss = 0.43286666\n",
      "Iteration 127, loss = 0.43211046\n",
      "Iteration 128, loss = 0.43139557\n",
      "Iteration 129, loss = 0.43063638\n",
      "Iteration 130, loss = 0.42996603\n",
      "Iteration 131, loss = 0.42926860\n",
      "Iteration 132, loss = 0.42857850\n",
      "Iteration 133, loss = 0.42792559\n",
      "Iteration 134, loss = 0.42726566\n",
      "Iteration 135, loss = 0.42664093\n",
      "Iteration 136, loss = 0.42600595\n",
      "Iteration 137, loss = 0.42543543\n",
      "Iteration 138, loss = 0.42478023\n",
      "Iteration 139, loss = 0.42419495\n",
      "Iteration 140, loss = 0.42365544\n",
      "Iteration 141, loss = 0.42305792\n",
      "Iteration 142, loss = 0.42251140\n",
      "Iteration 143, loss = 0.42197954\n",
      "Iteration 144, loss = 0.42137981\n",
      "Iteration 145, loss = 0.42085215\n",
      "Iteration 146, loss = 0.42034807\n",
      "Iteration 147, loss = 0.41987378\n",
      "Iteration 148, loss = 0.41932987\n",
      "Iteration 149, loss = 0.41886465\n",
      "Iteration 150, loss = 0.41830294\n",
      "Iteration 151, loss = 0.41782612\n",
      "Iteration 152, loss = 0.41738551\n",
      "Iteration 153, loss = 0.41689634\n",
      "Iteration 154, loss = 0.41640936\n",
      "Iteration 155, loss = 0.41596674\n",
      "Iteration 156, loss = 0.41551751\n",
      "Iteration 157, loss = 0.41510527\n",
      "Iteration 158, loss = 0.41464089\n",
      "Iteration 159, loss = 0.41418974\n",
      "Iteration 160, loss = 0.41380633\n",
      "Iteration 161, loss = 0.41332940\n",
      "Iteration 162, loss = 0.41290607\n",
      "Iteration 163, loss = 0.41250227\n",
      "Iteration 164, loss = 0.41210047\n",
      "Iteration 165, loss = 0.41169479\n",
      "Iteration 166, loss = 0.41128463\n",
      "Iteration 167, loss = 0.41087855\n",
      "Iteration 168, loss = 0.41049389\n",
      "Iteration 169, loss = 0.41010069\n",
      "Iteration 170, loss = 0.40973077\n",
      "Iteration 171, loss = 0.40934085\n",
      "Iteration 172, loss = 0.40900126\n",
      "Iteration 173, loss = 0.40857996\n",
      "Iteration 174, loss = 0.40822743\n",
      "Iteration 175, loss = 0.40787773\n",
      "Iteration 176, loss = 0.40750233\n",
      "Iteration 177, loss = 0.40712808\n",
      "Iteration 178, loss = 0.40678079\n",
      "Iteration 179, loss = 0.40642764\n",
      "Iteration 180, loss = 0.40608949\n",
      "Iteration 181, loss = 0.40574415\n",
      "Iteration 182, loss = 0.40540239\n",
      "Iteration 183, loss = 0.40505224\n",
      "Iteration 184, loss = 0.40470831\n",
      "Iteration 185, loss = 0.40436846\n",
      "Iteration 186, loss = 0.40404759\n",
      "Iteration 187, loss = 0.40371217\n",
      "Iteration 188, loss = 0.40340092\n",
      "Iteration 189, loss = 0.40307285\n",
      "Iteration 190, loss = 0.40277142\n",
      "Iteration 191, loss = 0.40243737\n",
      "Iteration 192, loss = 0.40216063\n",
      "Iteration 193, loss = 0.40180148\n",
      "Iteration 194, loss = 0.40148012\n",
      "Iteration 195, loss = 0.40119941\n",
      "Iteration 196, loss = 0.40087960\n",
      "Iteration 197, loss = 0.40057049\n",
      "Iteration 198, loss = 0.40027584\n",
      "Iteration 199, loss = 0.39997395\n",
      "Iteration 200, loss = 0.39967368\n",
      "Iteration 201, loss = 0.39935562\n",
      "Iteration 202, loss = 0.39908954\n",
      "Iteration 203, loss = 0.39881429\n",
      "Iteration 204, loss = 0.39852457\n",
      "Iteration 205, loss = 0.39828010\n",
      "Iteration 206, loss = 0.39791418\n",
      "Iteration 207, loss = 0.39763549\n",
      "Iteration 208, loss = 0.39736458\n",
      "Iteration 209, loss = 0.39709899\n",
      "Iteration 210, loss = 0.39681346\n",
      "Iteration 211, loss = 0.39653558\n",
      "Iteration 212, loss = 0.39626106\n",
      "Iteration 213, loss = 0.39598207\n",
      "Iteration 214, loss = 0.39572603\n",
      "Iteration 215, loss = 0.39546325\n",
      "Iteration 216, loss = 0.39517720\n",
      "Iteration 217, loss = 0.39491955\n",
      "Iteration 218, loss = 0.39462666\n",
      "Iteration 219, loss = 0.39435113\n",
      "Iteration 220, loss = 0.39409335\n",
      "Iteration 221, loss = 0.39383320\n",
      "Iteration 222, loss = 0.39357637\n",
      "Iteration 223, loss = 0.39332304\n",
      "Iteration 224, loss = 0.39303457\n",
      "Iteration 225, loss = 0.39284403\n",
      "Iteration 226, loss = 0.39253355\n",
      "Iteration 227, loss = 0.39230862\n",
      "Iteration 228, loss = 0.39202993\n",
      "Iteration 229, loss = 0.39177443\n",
      "Iteration 230, loss = 0.39153069\n",
      "Iteration 231, loss = 0.39132410\n",
      "Iteration 232, loss = 0.39104085\n",
      "Iteration 233, loss = 0.39079620\n",
      "Iteration 234, loss = 0.39057590\n",
      "Iteration 235, loss = 0.39032225\n",
      "Iteration 236, loss = 0.39006573\n",
      "Iteration 237, loss = 0.38983538\n",
      "Iteration 238, loss = 0.38959675\n",
      "Iteration 239, loss = 0.38934834\n",
      "Iteration 240, loss = 0.38912643\n",
      "Iteration 241, loss = 0.38889562\n",
      "Iteration 242, loss = 0.38863379\n",
      "Iteration 243, loss = 0.38840951\n",
      "Iteration 244, loss = 0.38826845\n",
      "Iteration 245, loss = 0.38799455\n",
      "Iteration 246, loss = 0.38773443\n",
      "Iteration 247, loss = 0.38754280\n",
      "Iteration 248, loss = 0.38727267\n",
      "Iteration 249, loss = 0.38703761\n",
      "Iteration 250, loss = 0.38683893\n",
      "Iteration 251, loss = 0.38661330\n",
      "Iteration 252, loss = 0.38638593\n",
      "Iteration 253, loss = 0.38614082\n",
      "Iteration 254, loss = 0.38592610\n",
      "Iteration 255, loss = 0.38571834\n",
      "Iteration 256, loss = 0.38549301\n",
      "Iteration 257, loss = 0.38527976\n",
      "Iteration 258, loss = 0.38510243\n",
      "Iteration 259, loss = 0.38491669\n",
      "Iteration 260, loss = 0.38461305\n",
      "Iteration 261, loss = 0.38442427\n",
      "Iteration 262, loss = 0.38420173\n",
      "Iteration 263, loss = 0.38396938\n",
      "Iteration 264, loss = 0.38377255\n",
      "Iteration 265, loss = 0.38356863\n",
      "Iteration 266, loss = 0.38333949\n",
      "Iteration 267, loss = 0.38312048\n",
      "Iteration 268, loss = 0.38292665\n",
      "Iteration 269, loss = 0.38273409\n",
      "Iteration 270, loss = 0.38250962\n",
      "Iteration 271, loss = 0.38229159\n",
      "Iteration 272, loss = 0.38207523\n",
      "Iteration 273, loss = 0.38187009\n",
      "Iteration 274, loss = 0.38166248\n",
      "Iteration 275, loss = 0.38146908\n",
      "Iteration 276, loss = 0.38126035\n",
      "Iteration 277, loss = 0.38104988\n",
      "Iteration 278, loss = 0.38083544\n",
      "Iteration 279, loss = 0.38065566\n",
      "Iteration 280, loss = 0.38043336\n",
      "Iteration 281, loss = 0.38024915\n",
      "Iteration 282, loss = 0.38003174\n",
      "Iteration 283, loss = 0.37982833\n",
      "Iteration 284, loss = 0.37962118\n",
      "Iteration 285, loss = 0.37942130\n",
      "Iteration 286, loss = 0.37921474\n",
      "Iteration 287, loss = 0.37905281\n",
      "Iteration 288, loss = 0.37884383\n",
      "Iteration 289, loss = 0.37863669\n",
      "Iteration 290, loss = 0.37844620\n",
      "Iteration 291, loss = 0.37823935\n",
      "Iteration 292, loss = 0.37805051\n",
      "Iteration 293, loss = 0.37792561\n",
      "Iteration 294, loss = 0.37765255\n",
      "Iteration 295, loss = 0.37746155\n",
      "Iteration 296, loss = 0.37728871\n",
      "Iteration 297, loss = 0.37708914\n",
      "Iteration 298, loss = 0.37687095\n",
      "Iteration 299, loss = 0.37670420\n",
      "Iteration 300, loss = 0.37655188\n",
      "Iteration 301, loss = 0.37637647\n",
      "Iteration 302, loss = 0.37611028\n",
      "Iteration 303, loss = 0.37591633\n",
      "Iteration 304, loss = 0.37573809\n",
      "Iteration 305, loss = 0.37560710\n",
      "Iteration 306, loss = 0.37540943\n",
      "Iteration 307, loss = 0.37518345\n",
      "Iteration 308, loss = 0.37499715\n",
      "Iteration 309, loss = 0.37479480\n",
      "Iteration 310, loss = 0.37462318\n",
      "Iteration 311, loss = 0.37440515\n",
      "Iteration 312, loss = 0.37423338\n",
      "Iteration 313, loss = 0.37407024\n",
      "Iteration 314, loss = 0.37386609\n",
      "Iteration 315, loss = 0.37370520\n",
      "Iteration 316, loss = 0.37350521\n",
      "Iteration 317, loss = 0.37332045\n",
      "Iteration 318, loss = 0.37313463\n",
      "Iteration 319, loss = 0.37296228\n",
      "Iteration 320, loss = 0.37276837\n",
      "Iteration 321, loss = 0.37265527\n",
      "Iteration 322, loss = 0.37243305\n",
      "Iteration 323, loss = 0.37223354\n",
      "Iteration 324, loss = 0.37204013\n",
      "Iteration 325, loss = 0.37187669\n",
      "Iteration 326, loss = 0.37171213\n",
      "Iteration 327, loss = 0.37150178\n",
      "Iteration 328, loss = 0.37135179\n",
      "Iteration 329, loss = 0.37116179\n",
      "Iteration 330, loss = 0.37097808\n",
      "Iteration 331, loss = 0.37086308\n",
      "Iteration 332, loss = 0.37062767\n",
      "Iteration 333, loss = 0.37045620\n",
      "Iteration 334, loss = 0.37026516\n",
      "Iteration 335, loss = 0.37009457\n",
      "Iteration 336, loss = 0.36995756\n",
      "Iteration 337, loss = 0.36978467\n",
      "Iteration 338, loss = 0.36957029\n",
      "Iteration 339, loss = 0.36942178\n",
      "Iteration 340, loss = 0.36923799\n",
      "Iteration 341, loss = 0.36905263\n",
      "Iteration 342, loss = 0.36889617\n",
      "Iteration 343, loss = 0.36872465\n",
      "Iteration 344, loss = 0.36854354\n",
      "Iteration 345, loss = 0.36836016\n",
      "Iteration 346, loss = 0.36820434\n",
      "Iteration 347, loss = 0.36800871\n",
      "Iteration 348, loss = 0.36785268\n",
      "Iteration 349, loss = 0.36767991\n",
      "Iteration 350, loss = 0.36750304\n",
      "Iteration 351, loss = 0.36734383\n",
      "Iteration 352, loss = 0.36717489\n",
      "Iteration 353, loss = 0.36705459\n",
      "Iteration 354, loss = 0.36683273\n",
      "Iteration 355, loss = 0.36666500\n",
      "Iteration 356, loss = 0.36650970\n",
      "Iteration 357, loss = 0.36633979\n",
      "Iteration 358, loss = 0.36617082\n",
      "Iteration 359, loss = 0.36601587\n",
      "Iteration 360, loss = 0.36585561\n",
      "Iteration 361, loss = 0.36568519\n",
      "Iteration 362, loss = 0.36551119\n",
      "Iteration 363, loss = 0.36533465\n",
      "Iteration 364, loss = 0.36515892\n",
      "Iteration 365, loss = 0.36500390\n",
      "Iteration 366, loss = 0.36483866\n",
      "Iteration 367, loss = 0.36465662\n",
      "Iteration 368, loss = 0.36451831\n",
      "Iteration 369, loss = 0.36434398\n",
      "Iteration 370, loss = 0.36419352\n",
      "Iteration 371, loss = 0.36403457\n",
      "Iteration 372, loss = 0.36384030\n",
      "Iteration 373, loss = 0.36369536\n",
      "Iteration 374, loss = 0.36351969\n",
      "Iteration 375, loss = 0.36335978\n",
      "Iteration 376, loss = 0.36320005\n",
      "Iteration 377, loss = 0.36307482\n",
      "Iteration 378, loss = 0.36289876\n",
      "Iteration 379, loss = 0.36272043\n",
      "Iteration 380, loss = 0.36256384\n",
      "Iteration 381, loss = 0.36240046\n",
      "Iteration 382, loss = 0.36226249\n",
      "Iteration 383, loss = 0.36206466\n",
      "Iteration 384, loss = 0.36194050\n",
      "Iteration 385, loss = 0.36174761\n",
      "Iteration 386, loss = 0.36160644\n",
      "Iteration 387, loss = 0.36146366\n",
      "Iteration 388, loss = 0.36128137\n",
      "Iteration 389, loss = 0.36111619\n",
      "Iteration 390, loss = 0.36096697\n",
      "Iteration 391, loss = 0.36082423\n",
      "Iteration 392, loss = 0.36069163\n",
      "Iteration 393, loss = 0.36048423\n",
      "Iteration 394, loss = 0.36032837\n",
      "Iteration 395, loss = 0.36017547\n",
      "Iteration 396, loss = 0.36002011\n",
      "Iteration 397, loss = 0.35987164\n",
      "Iteration 398, loss = 0.35970386\n",
      "Iteration 399, loss = 0.35956420\n",
      "Iteration 400, loss = 0.35945314\n",
      "Iteration 401, loss = 0.35928707\n",
      "Iteration 402, loss = 0.35910052\n",
      "Iteration 403, loss = 0.35896027\n",
      "Iteration 404, loss = 0.35878698\n",
      "Iteration 405, loss = 0.35862152\n",
      "Iteration 406, loss = 0.35850627\n",
      "Iteration 407, loss = 0.35833265\n",
      "Iteration 408, loss = 0.35818498\n",
      "Iteration 409, loss = 0.35803426\n",
      "Iteration 410, loss = 0.35785874\n",
      "Iteration 411, loss = 0.35780707\n",
      "Iteration 412, loss = 0.35757339\n",
      "Iteration 413, loss = 0.35741110\n",
      "Iteration 414, loss = 0.35728189\n",
      "Iteration 415, loss = 0.35710843\n",
      "Iteration 416, loss = 0.35693371\n",
      "Iteration 417, loss = 0.35680653\n",
      "Iteration 418, loss = 0.35663926\n",
      "Iteration 419, loss = 0.35650678\n",
      "Iteration 420, loss = 0.35635566\n",
      "Iteration 421, loss = 0.35627905\n",
      "Iteration 422, loss = 0.35602508\n",
      "Iteration 423, loss = 0.35589989\n",
      "Iteration 424, loss = 0.35577290\n",
      "Iteration 425, loss = 0.35559962\n",
      "Iteration 426, loss = 0.35543317\n",
      "Iteration 427, loss = 0.35529986\n",
      "Iteration 428, loss = 0.35513414\n",
      "Iteration 429, loss = 0.35497400\n",
      "Iteration 430, loss = 0.35482963\n",
      "Iteration 431, loss = 0.35467155\n",
      "Iteration 432, loss = 0.35453480\n",
      "Iteration 433, loss = 0.35436001\n",
      "Iteration 434, loss = 0.35420808\n",
      "Iteration 435, loss = 0.35407217\n",
      "Iteration 436, loss = 0.35393296\n",
      "Iteration 437, loss = 0.35375366\n",
      "Iteration 438, loss = 0.35362467\n",
      "Iteration 439, loss = 0.35349644\n",
      "Iteration 440, loss = 0.35331487\n",
      "Iteration 441, loss = 0.35318418\n",
      "Iteration 442, loss = 0.35300832\n",
      "Iteration 443, loss = 0.35288025\n",
      "Iteration 444, loss = 0.35272058\n",
      "Iteration 445, loss = 0.35256531\n",
      "Iteration 446, loss = 0.35242397\n",
      "Iteration 447, loss = 0.35227224\n",
      "Iteration 448, loss = 0.35213715\n",
      "Iteration 449, loss = 0.35200804\n",
      "Iteration 450, loss = 0.35183003\n",
      "Iteration 451, loss = 0.35168557\n",
      "Iteration 452, loss = 0.35155554\n",
      "Iteration 453, loss = 0.35137534\n",
      "Iteration 454, loss = 0.35123248\n",
      "Iteration 455, loss = 0.35110128\n",
      "Iteration 456, loss = 0.35093003\n",
      "Iteration 457, loss = 0.35080402\n",
      "Iteration 458, loss = 0.35064671\n",
      "Iteration 459, loss = 0.35051156\n",
      "Iteration 460, loss = 0.35038678\n",
      "Iteration 461, loss = 0.35021996\n",
      "Iteration 462, loss = 0.35005897\n",
      "Iteration 463, loss = 0.34989291\n",
      "Iteration 464, loss = 0.34981103\n",
      "Iteration 465, loss = 0.34960529\n",
      "Iteration 466, loss = 0.34945598\n",
      "Iteration 467, loss = 0.34932324\n",
      "Iteration 468, loss = 0.34918247\n",
      "Iteration 469, loss = 0.34903951\n",
      "Iteration 470, loss = 0.34886952\n",
      "Iteration 471, loss = 0.34873273\n",
      "Iteration 472, loss = 0.34859272\n",
      "Iteration 473, loss = 0.34844728\n",
      "Iteration 474, loss = 0.34829569\n",
      "Iteration 475, loss = 0.34816337\n",
      "Iteration 476, loss = 0.34800317\n",
      "Iteration 477, loss = 0.34785942\n",
      "Iteration 478, loss = 0.34773477\n",
      "Iteration 479, loss = 0.34758351\n",
      "Iteration 480, loss = 0.34745157\n",
      "Iteration 481, loss = 0.34733242\n",
      "Iteration 482, loss = 0.34713755\n",
      "Iteration 483, loss = 0.34698925\n",
      "Iteration 484, loss = 0.34686119\n",
      "Iteration 485, loss = 0.34669893\n",
      "Iteration 486, loss = 0.34655866\n",
      "Iteration 487, loss = 0.34643289\n",
      "Iteration 488, loss = 0.34627925\n",
      "Iteration 489, loss = 0.34618790\n",
      "Iteration 490, loss = 0.34599567\n",
      "Iteration 491, loss = 0.34584571\n",
      "Iteration 492, loss = 0.34570500\n",
      "Iteration 493, loss = 0.34556513\n",
      "Iteration 494, loss = 0.34542415\n",
      "Iteration 495, loss = 0.34527853\n",
      "Iteration 496, loss = 0.34514627\n",
      "Iteration 497, loss = 0.34500843\n",
      "Iteration 498, loss = 0.34488173\n",
      "Iteration 499, loss = 0.34472673\n",
      "Iteration 500, loss = 0.34459866\n",
      "Iteration 501, loss = 0.34445806\n",
      "Iteration 502, loss = 0.34429151\n",
      "Iteration 503, loss = 0.34415202\n",
      "Iteration 504, loss = 0.34401640\n",
      "Iteration 505, loss = 0.34389412\n",
      "Iteration 506, loss = 0.34374211\n",
      "Iteration 507, loss = 0.34359918\n",
      "Iteration 508, loss = 0.34348308\n",
      "Iteration 509, loss = 0.34331156\n",
      "Iteration 510, loss = 0.34320167\n",
      "Iteration 511, loss = 0.34311443\n",
      "Iteration 512, loss = 0.34289300\n",
      "Iteration 513, loss = 0.34277518\n",
      "Iteration 514, loss = 0.34265599\n",
      "Iteration 515, loss = 0.34249275\n",
      "Iteration 516, loss = 0.34233826\n",
      "Iteration 517, loss = 0.34222590\n",
      "Iteration 518, loss = 0.34206294\n",
      "Iteration 519, loss = 0.34194559\n",
      "Iteration 520, loss = 0.34178401\n",
      "Iteration 521, loss = 0.34171575\n",
      "Iteration 522, loss = 0.34151435\n",
      "Iteration 523, loss = 0.34136472\n",
      "Iteration 524, loss = 0.34123813\n",
      "Iteration 525, loss = 0.34108789\n",
      "Iteration 526, loss = 0.34095649\n",
      "Iteration 527, loss = 0.34082783\n",
      "Iteration 528, loss = 0.34068425\n",
      "Iteration 529, loss = 0.34052124\n",
      "Iteration 530, loss = 0.34038994\n",
      "Iteration 531, loss = 0.34022876\n",
      "Iteration 532, loss = 0.34009878\n",
      "Iteration 533, loss = 0.33996598\n",
      "Iteration 534, loss = 0.33980636\n",
      "Iteration 535, loss = 0.33967078\n",
      "Iteration 536, loss = 0.33953061\n",
      "Iteration 537, loss = 0.33939022\n",
      "Iteration 538, loss = 0.33926048\n",
      "Iteration 539, loss = 0.33910259\n",
      "Iteration 540, loss = 0.33897583\n",
      "Iteration 541, loss = 0.33883710\n",
      "Iteration 542, loss = 0.33867696\n",
      "Iteration 543, loss = 0.33856297\n",
      "Iteration 544, loss = 0.33843076\n",
      "Iteration 545, loss = 0.33826841\n",
      "Iteration 546, loss = 0.33811775\n",
      "Iteration 547, loss = 0.33798825\n",
      "Iteration 548, loss = 0.33785520\n",
      "Iteration 549, loss = 0.33771400\n",
      "Iteration 550, loss = 0.33756729\n",
      "Iteration 551, loss = 0.33743502\n",
      "Iteration 552, loss = 0.33729339\n",
      "Iteration 553, loss = 0.33717226\n",
      "Iteration 554, loss = 0.33701124\n",
      "Iteration 555, loss = 0.33688184\n",
      "Iteration 556, loss = 0.33676311\n",
      "Iteration 557, loss = 0.33663157\n",
      "Iteration 558, loss = 0.33649093\n",
      "Iteration 559, loss = 0.33634693\n",
      "Iteration 560, loss = 0.33618835\n",
      "Iteration 561, loss = 0.33606736\n",
      "Iteration 562, loss = 0.33591316\n",
      "Iteration 563, loss = 0.33581972\n",
      "Iteration 564, loss = 0.33564963\n",
      "Iteration 565, loss = 0.33552529\n",
      "Iteration 566, loss = 0.33538271\n",
      "Iteration 567, loss = 0.33524141\n",
      "Iteration 568, loss = 0.33519971\n",
      "Iteration 569, loss = 0.33496409\n",
      "Iteration 337, loss = 0.33980104\n",
      "Iteration 338, loss = 0.33959707\n",
      "Iteration 339, loss = 0.33942207\n",
      "Iteration 340, loss = 0.33923284\n",
      "Iteration 341, loss = 0.33908691\n",
      "Iteration 342, loss = 0.33890202\n",
      "Iteration 343, loss = 0.33872486\n",
      "Iteration 344, loss = 0.33855610\n",
      "Iteration 345, loss = 0.33842591\n",
      "Iteration 346, loss = 0.33827040\n",
      "Iteration 347, loss = 0.33803717\n",
      "Iteration 348, loss = 0.33789753\n",
      "Iteration 349, loss = 0.33770315\n",
      "Iteration 350, loss = 0.33754965\n",
      "Iteration 351, loss = 0.33739481\n",
      "Iteration 352, loss = 0.33721758\n",
      "Iteration 353, loss = 0.33710069\n",
      "Iteration 354, loss = 0.33687186\n",
      "Iteration 355, loss = 0.33671272\n",
      "Iteration 356, loss = 0.33666058\n",
      "Iteration 357, loss = 0.33644461\n",
      "Iteration 358, loss = 0.33621048\n",
      "Iteration 359, loss = 0.33607438\n",
      "Iteration 360, loss = 0.33592481\n",
      "Iteration 361, loss = 0.33573865\n",
      "Iteration 362, loss = 0.33555483\n",
      "Iteration 363, loss = 0.33542902\n",
      "Iteration 364, loss = 0.33524139\n",
      "Iteration 365, loss = 0.33508676\n",
      "Iteration 366, loss = 0.33493981\n",
      "Iteration 367, loss = 0.33474144\n",
      "Iteration 368, loss = 0.33461527\n",
      "Iteration 369, loss = 0.33442414\n",
      "Iteration 370, loss = 0.33427490\n",
      "Iteration 371, loss = 0.33410712\n",
      "Iteration 372, loss = 0.33396827\n",
      "Iteration 373, loss = 0.33379294\n",
      "Iteration 374, loss = 0.33363670\n",
      "Iteration 375, loss = 0.33351022\n",
      "Iteration 376, loss = 0.33334935\n",
      "Iteration 377, loss = 0.33319530\n",
      "Iteration 378, loss = 0.33301243\n",
      "Iteration 379, loss = 0.33283163\n",
      "Iteration 380, loss = 0.33271072\n",
      "Iteration 381, loss = 0.33252268\n",
      "Iteration 382, loss = 0.33238835\n",
      "Iteration 383, loss = 0.33228932\n",
      "Iteration 384, loss = 0.33208774\n",
      "Iteration 385, loss = 0.33191377\n",
      "Iteration 386, loss = 0.33177768\n",
      "Iteration 387, loss = 0.33163763\n",
      "Iteration 388, loss = 0.33152297\n",
      "Iteration 389, loss = 0.33131519\n",
      "Iteration 390, loss = 0.33118692\n",
      "Iteration 391, loss = 0.33100960\n",
      "Iteration 392, loss = 0.33088958\n",
      "Iteration 393, loss = 0.33071386\n",
      "Iteration 394, loss = 0.33058105\n",
      "Iteration 395, loss = 0.33038522\n",
      "Iteration 396, loss = 0.33024303\n",
      "Iteration 397, loss = 0.33016115\n",
      "Iteration 398, loss = 0.33000525\n",
      "Iteration 399, loss = 0.32990781\n",
      "Iteration 400, loss = 0.32967763\n",
      "Iteration 401, loss = 0.32952043\n",
      "Iteration 402, loss = 0.32940032\n",
      "Iteration 403, loss = 0.32917907\n",
      "Iteration 404, loss = 0.32905557\n",
      "Iteration 405, loss = 0.32890488\n",
      "Iteration 406, loss = 0.32872561\n",
      "Iteration 407, loss = 0.32856799\n",
      "Iteration 408, loss = 0.32841469\n",
      "Iteration 409, loss = 0.32832752\n",
      "Iteration 410, loss = 0.32816670\n",
      "Iteration 411, loss = 0.32799519\n",
      "Iteration 412, loss = 0.32783031\n",
      "Iteration 413, loss = 0.32767800\n",
      "Iteration 414, loss = 0.32756493\n",
      "Iteration 415, loss = 0.32735068\n",
      "Iteration 416, loss = 0.32721342\n",
      "Iteration 417, loss = 0.32709458\n",
      "Iteration 418, loss = 0.32690519\n",
      "Iteration 419, loss = 0.32674433\n",
      "Iteration 420, loss = 0.32660670\n",
      "Iteration 421, loss = 0.32650249\n",
      "Iteration 422, loss = 0.32632204\n",
      "Iteration 423, loss = 0.32612822\n",
      "Iteration 424, loss = 0.32599123\n",
      "Iteration 425, loss = 0.32583935\n",
      "Iteration 426, loss = 0.32569321\n",
      "Iteration 427, loss = 0.32568010\n",
      "Iteration 428, loss = 0.32540547\n",
      "Iteration 429, loss = 0.32522965\n",
      "Iteration 430, loss = 0.32509648\n",
      "Iteration 431, loss = 0.32497440\n",
      "Iteration 432, loss = 0.32484101\n",
      "Iteration 433, loss = 0.32463385\n",
      "Iteration 434, loss = 0.32450356\n",
      "Iteration 435, loss = 0.32435252\n",
      "Iteration 436, loss = 0.32423743\n",
      "Iteration 437, loss = 0.32411160\n",
      "Iteration 438, loss = 0.32392110\n",
      "Iteration 439, loss = 0.32382563\n",
      "Iteration 440, loss = 0.32363043\n",
      "Iteration 441, loss = 0.32345675\n",
      "Iteration 442, loss = 0.32335560\n",
      "Iteration 443, loss = 0.32317996\n",
      "Iteration 444, loss = 0.32308425\n",
      "Iteration 445, loss = 0.32290897\n",
      "Iteration 446, loss = 0.32275009\n",
      "Iteration 447, loss = 0.32266471\n",
      "Iteration 448, loss = 0.32248644\n",
      "Iteration 449, loss = 0.32230250\n",
      "Iteration 450, loss = 0.32217325\n",
      "Iteration 451, loss = 0.32199135\n",
      "Iteration 452, loss = 0.32184241\n",
      "Iteration 453, loss = 0.32177276\n",
      "Iteration 454, loss = 0.32159241\n",
      "Iteration 455, loss = 0.32141502\n",
      "Iteration 456, loss = 0.32126895\n",
      "Iteration 457, loss = 0.32109821\n",
      "Iteration 458, loss = 0.32096296\n",
      "Iteration 459, loss = 0.32087321\n",
      "Iteration 460, loss = 0.32075833\n",
      "Iteration 461, loss = 0.32058701\n",
      "Iteration 462, loss = 0.32050369\n",
      "Iteration 463, loss = 0.32022329\n",
      "Iteration 464, loss = 0.32009976\n",
      "Iteration 465, loss = 0.31993382\n",
      "Iteration 466, loss = 0.31979520\n",
      "Iteration 467, loss = 0.31964926\n",
      "Iteration 468, loss = 0.31949292\n",
      "Iteration 469, loss = 0.31936863\n",
      "Iteration 470, loss = 0.31919535\n",
      "Iteration 471, loss = 0.31903512\n",
      "Iteration 472, loss = 0.31893679\n",
      "Iteration 473, loss = 0.31887976\n",
      "Iteration 474, loss = 0.31857787\n",
      "Iteration 475, loss = 0.31844625\n",
      "Iteration 476, loss = 0.31831113\n",
      "Iteration 477, loss = 0.31818508\n",
      "Iteration 478, loss = 0.31802355\n",
      "Iteration 479, loss = 0.31787030\n",
      "Iteration 480, loss = 0.31770142\n",
      "Iteration 481, loss = 0.31754651\n",
      "Iteration 482, loss = 0.31742914\n",
      "Iteration 483, loss = 0.31735834\n",
      "Iteration 484, loss = 0.31707910\n",
      "Iteration 485, loss = 0.31698203\n",
      "Iteration 486, loss = 0.31682048\n",
      "Iteration 487, loss = 0.31662541\n",
      "Iteration 488, loss = 0.31649294\n",
      "Iteration 489, loss = 0.31640900\n",
      "Iteration 490, loss = 0.31619796\n",
      "Iteration 491, loss = 0.31603066\n",
      "Iteration 492, loss = 0.31587620\n",
      "Iteration 493, loss = 0.31572941\n",
      "Iteration 494, loss = 0.31557185\n",
      "Iteration 495, loss = 0.31541111\n",
      "Iteration 496, loss = 0.31528559\n",
      "Iteration 497, loss = 0.31514495\n",
      "Iteration 498, loss = 0.31496404\n",
      "Iteration 499, loss = 0.31482841\n",
      "Iteration 500, loss = 0.31468683\n",
      "Iteration 501, loss = 0.31452609\n",
      "Iteration 502, loss = 0.31437715\n",
      "Iteration 503, loss = 0.31428289\n",
      "Iteration 504, loss = 0.31410753\n",
      "Iteration 505, loss = 0.31396916\n",
      "Iteration 506, loss = 0.31379407\n",
      "Iteration 507, loss = 0.31361354\n",
      "Iteration 508, loss = 0.31348465\n",
      "Iteration 509, loss = 0.31333904\n",
      "Iteration 510, loss = 0.31322849\n",
      "Iteration 511, loss = 0.31306288\n",
      "Iteration 512, loss = 0.31290764\n",
      "Iteration 513, loss = 0.31278289\n",
      "Iteration 514, loss = 0.31260951\n",
      "Iteration 515, loss = 0.31246433\n",
      "Iteration 516, loss = 0.31228796\n",
      "Iteration 517, loss = 0.31217725\n",
      "Iteration 518, loss = 0.31202628\n",
      "Iteration 519, loss = 0.31185875\n",
      "Iteration 520, loss = 0.31172446\n",
      "Iteration 521, loss = 0.31156518\n",
      "Iteration 522, loss = 0.31144728\n",
      "Iteration 523, loss = 0.31128297\n",
      "Iteration 524, loss = 0.31109622\n",
      "Iteration 525, loss = 0.31095935\n",
      "Iteration 526, loss = 0.31084087\n",
      "Iteration 527, loss = 0.31067462\n",
      "Iteration 528, loss = 0.31057112\n",
      "Iteration 529, loss = 0.31035476\n",
      "Iteration 530, loss = 0.31018171\n",
      "Iteration 531, loss = 0.31007449\n",
      "Iteration 532, loss = 0.30988674\n",
      "Iteration 533, loss = 0.30974169\n",
      "Iteration 534, loss = 0.30958787\n",
      "Iteration 535, loss = 0.30944335\n",
      "Iteration 536, loss = 0.30928499\n",
      "Iteration 537, loss = 0.30912454\n",
      "Iteration 538, loss = 0.30897420\n",
      "Iteration 539, loss = 0.30888539\n",
      "Iteration 540, loss = 0.30875488\n",
      "Iteration 541, loss = 0.30851370\n",
      "Iteration 542, loss = 0.30837181\n",
      "Iteration 543, loss = 0.30823604\n",
      "Iteration 544, loss = 0.30806870\n",
      "Iteration 545, loss = 0.30797928\n",
      "Iteration 546, loss = 0.30775348\n",
      "Iteration 547, loss = 0.30761572\n",
      "Iteration 548, loss = 0.30747760\n",
      "Iteration 549, loss = 0.30734305\n",
      "Iteration 550, loss = 0.30717601\n",
      "Iteration 551, loss = 0.30705781\n",
      "Iteration 552, loss = 0.30688480\n",
      "Iteration 553, loss = 0.30670649\n",
      "Iteration 554, loss = 0.30658361\n",
      "Iteration 555, loss = 0.30653450\n",
      "Iteration 556, loss = 0.30628720\n",
      "Iteration 557, loss = 0.30612539\n",
      "Iteration 558, loss = 0.30596644\n",
      "Iteration 559, loss = 0.30582269\n",
      "Iteration 560, loss = 0.30571815\n",
      "Iteration 561, loss = 0.30555791\n",
      "Iteration 562, loss = 0.30542238\n",
      "Iteration 563, loss = 0.30526611\n",
      "Iteration 564, loss = 0.30507906\n",
      "Iteration 565, loss = 0.30494141\n",
      "Iteration 566, loss = 0.30479525\n",
      "Iteration 567, loss = 0.30464049\n",
      "Iteration 568, loss = 0.30450415\n",
      "Iteration 569, loss = 0.30435181\n",
      "Iteration 570, loss = 0.30429117\n",
      "Iteration 571, loss = 0.30408418\n",
      "Iteration 572, loss = 0.30396432\n",
      "Iteration 573, loss = 0.30384481\n",
      "Iteration 574, loss = 0.30362061\n",
      "Iteration 575, loss = 0.30352044\n",
      "Iteration 576, loss = 0.30357810\n",
      "Iteration 577, loss = 0.30321087\n",
      "Iteration 578, loss = 0.30306112\n",
      "Iteration 579, loss = 0.30291313\n",
      "Iteration 580, loss = 0.30281974\n",
      "Iteration 581, loss = 0.30264227\n",
      "Iteration 582, loss = 0.30256304\n",
      "Iteration 583, loss = 0.30236827\n",
      "Iteration 584, loss = 0.30218592\n",
      "Iteration 585, loss = 0.30211380\n",
      "Iteration 586, loss = 0.30194258\n",
      "Iteration 587, loss = 0.30178561\n",
      "Iteration 588, loss = 0.30168852\n",
      "Iteration 589, loss = 0.30149626\n",
      "Iteration 590, loss = 0.30134127\n",
      "Iteration 591, loss = 0.30126263\n",
      "Iteration 592, loss = 0.30106759\n",
      "Iteration 593, loss = 0.30092411\n",
      "Iteration 594, loss = 0.30075914\n",
      "Iteration 595, loss = 0.30060587\n",
      "Iteration 596, loss = 0.30058481\n",
      "Iteration 597, loss = 0.30034348\n",
      "Iteration 598, loss = 0.30018366\n",
      "Iteration 599, loss = 0.30003665\n",
      "Iteration 600, loss = 0.29994626\n",
      "Iteration 601, loss = 0.29982182\n",
      "Iteration 602, loss = 0.29961119\n",
      "Iteration 603, loss = 0.29952719\n",
      "Iteration 604, loss = 0.29932711\n",
      "Iteration 605, loss = 0.29920280\n",
      "Iteration 606, loss = 0.29904368\n",
      "Iteration 607, loss = 0.29893077\n",
      "Iteration 608, loss = 0.29877187\n",
      "Iteration 609, loss = 0.29865137\n",
      "Iteration 610, loss = 0.29853477\n",
      "Iteration 611, loss = 0.29853675\n",
      "Iteration 612, loss = 0.29822942\n",
      "Iteration 613, loss = 0.29808401\n",
      "Iteration 614, loss = 0.29792118\n",
      "Iteration 615, loss = 0.29783437\n",
      "Iteration 616, loss = 0.29764090\n",
      "Iteration 617, loss = 0.29750169\n",
      "Iteration 618, loss = 0.29735491\n",
      "Iteration 619, loss = 0.29722443\n",
      "Iteration 620, loss = 0.29729669\n",
      "Iteration 621, loss = 0.29700160\n",
      "Iteration 622, loss = 0.29681392\n",
      "Iteration 623, loss = 0.29666499\n",
      "Iteration 624, loss = 0.29657343\n",
      "Iteration 625, loss = 0.29637042\n",
      "Iteration 626, loss = 0.29625734\n",
      "Iteration 627, loss = 0.29610419\n",
      "Iteration 628, loss = 0.29594059\n",
      "Iteration 629, loss = 0.29581194\n",
      "Iteration 630, loss = 0.29565989\n",
      "Iteration 631, loss = 0.29556422\n",
      "Iteration 632, loss = 0.29544146\n",
      "Iteration 633, loss = 0.29527075\n",
      "Iteration 634, loss = 0.29511902\n",
      "Iteration 635, loss = 0.29495368\n",
      "Iteration 636, loss = 0.29483728\n",
      "Iteration 637, loss = 0.29471190\n",
      "Iteration 638, loss = 0.29454993\n",
      "Iteration 639, loss = 0.29441395\n",
      "Iteration 640, loss = 0.29429405\n",
      "Iteration 641, loss = 0.29416284\n",
      "Iteration 642, loss = 0.29397339\n",
      "Iteration 643, loss = 0.29380418\n",
      "Iteration 644, loss = 0.29368220\n",
      "Iteration 645, loss = 0.29361169\n",
      "Iteration 646, loss = 0.29339322\n",
      "Iteration 647, loss = 0.29329727\n",
      "Iteration 648, loss = 0.29309576\n",
      "Iteration 649, loss = 0.29298383\n",
      "Iteration 650, loss = 0.29284435\n",
      "Iteration 651, loss = 0.29267552\n",
      "Iteration 652, loss = 0.29255105\n",
      "Iteration 653, loss = 0.29240363\n",
      "Iteration 654, loss = 0.29225631\n",
      "Iteration 655, loss = 0.29213138\n",
      "Iteration 656, loss = 0.29196155\n",
      "Iteration 657, loss = 0.29182755\n",
      "Iteration 658, loss = 0.29188017\n",
      "Iteration 659, loss = 0.29155738\n",
      "Iteration 660, loss = 0.29149005\n",
      "Iteration 661, loss = 0.29127404\n",
      "Iteration 662, loss = 0.29112180\n",
      "Iteration 663, loss = 0.29095373\n",
      "Iteration 664, loss = 0.29084142\n",
      "Iteration 665, loss = 0.29068776\n",
      "Iteration 666, loss = 0.29052362\n",
      "Iteration 667, loss = 0.29038636\n",
      "Iteration 668, loss = 0.29033000\n",
      "Iteration 669, loss = 0.29009939\n",
      "Iteration 670, loss = 0.28998786\n",
      "Iteration 671, loss = 0.28984660\n",
      "Iteration 672, loss = 0.28968127\n",
      "Iteration 673, loss = 0.28956139\n",
      "Iteration 674, loss = 0.28942398\n",
      "Iteration 675, loss = 0.28938262\n",
      "Iteration 676, loss = 0.28916035\n",
      "Iteration 677, loss = 0.28906386\n",
      "Iteration 678, loss = 0.28885074\n",
      "Iteration 679, loss = 0.28872477\n",
      "Iteration 680, loss = 0.28857121\n",
      "Iteration 681, loss = 0.28845770\n",
      "Iteration 682, loss = 0.28832437\n",
      "Iteration 683, loss = 0.28817293\n",
      "Iteration 684, loss = 0.28801372\n",
      "Iteration 685, loss = 0.28788760\n",
      "Iteration 686, loss = 0.28774645\n",
      "Iteration 687, loss = 0.28760471\n",
      "Iteration 688, loss = 0.28753697\n",
      "Iteration 689, loss = 0.28734029\n",
      "Iteration 690, loss = 0.28722238\n",
      "Iteration 691, loss = 0.28703550\n",
      "Iteration 692, loss = 0.28708091\n",
      "Iteration 693, loss = 0.28679800\n",
      "Iteration 694, loss = 0.28670450\n",
      "Iteration 695, loss = 0.28653159\n",
      "Iteration 696, loss = 0.28637271\n",
      "Iteration 697, loss = 0.28625267\n",
      "Iteration 698, loss = 0.28608302\n",
      "Iteration 699, loss = 0.28598961\n",
      "Iteration 700, loss = 0.28581995\n",
      "Iteration 701, loss = 0.28574385\n",
      "Iteration 702, loss = 0.28556158\n",
      "Iteration 703, loss = 0.28544764\n",
      "Iteration 704, loss = 0.28527023\n",
      "Iteration 705, loss = 0.28526572\n",
      "Iteration 706, loss = 0.28503567\n",
      "Iteration 707, loss = 0.28488207\n",
      "Iteration 708, loss = 0.28477395\n",
      "Iteration 709, loss = 0.28462673\n",
      "Iteration 710, loss = 0.28448024\n",
      "Iteration 711, loss = 0.28433161\n",
      "Iteration 712, loss = 0.28420613\n",
      "Iteration 713, loss = 0.28403571\n",
      "Iteration 714, loss = 0.28389567\n",
      "Iteration 715, loss = 0.28379340\n",
      "Iteration 716, loss = 0.28363581\n",
      "Iteration 717, loss = 0.28353057\n",
      "Iteration 718, loss = 0.28339538\n",
      "Iteration 719, loss = 0.28324144\n",
      "Iteration 720, loss = 0.28311869\n",
      "Iteration 721, loss = 0.28300782\n",
      "Iteration 722, loss = 0.28285163\n",
      "Iteration 723, loss = 0.28276627\n",
      "Iteration 724, loss = 0.28261613\n",
      "Iteration 725, loss = 0.28250553\n",
      "Iteration 726, loss = 0.28231717\n",
      "Iteration 727, loss = 0.28218441\n",
      "Iteration 728, loss = 0.28207921\n",
      "Iteration 729, loss = 0.28192754\n",
      "Iteration 730, loss = 0.28173760\n",
      "Iteration 731, loss = 0.28168756\n",
      "Iteration 732, loss = 0.28158164\n",
      "Iteration 733, loss = 0.28144710\n",
      "Iteration 734, loss = 0.28121115\n",
      "Iteration 735, loss = 0.28108578\n",
      "Iteration 736, loss = 0.28100269\n",
      "Iteration 737, loss = 0.28081991\n",
      "Iteration 738, loss = 0.28065062\n",
      "Iteration 739, loss = 0.28061998\n",
      "Iteration 740, loss = 0.28041609\n",
      "Iteration 741, loss = 0.28030619\n",
      "Iteration 742, loss = 0.28011455\n",
      "Iteration 743, loss = 0.28002519\n",
      "Iteration 744, loss = 0.27988070\n",
      "Iteration 745, loss = 0.27970956\n",
      "Iteration 746, loss = 0.27967213\n",
      "Iteration 747, loss = 0.27948020\n",
      "Iteration 748, loss = 0.27932481\n",
      "Iteration 749, loss = 0.27922661\n",
      "Iteration 750, loss = 0.27919654\n",
      "Iteration 751, loss = 0.27902375\n",
      "Iteration 752, loss = 0.27884826\n",
      "Iteration 753, loss = 0.27865211\n",
      "Iteration 754, loss = 0.27860813\n",
      "Iteration 755, loss = 0.27839706\n",
      "Iteration 756, loss = 0.27825853\n",
      "Iteration 757, loss = 0.27813655\n",
      "Iteration 758, loss = 0.27798859\n",
      "Iteration 759, loss = 0.27798359\n",
      "Iteration 760, loss = 0.27775210\n",
      "Iteration 761, loss = 0.27762205\n",
      "Iteration 762, loss = 0.27744678\n",
      "Iteration 763, loss = 0.27731654\n",
      "Iteration 764, loss = 0.27720042\n",
      "Iteration 765, loss = 0.27708726\n",
      "Iteration 766, loss = 0.27692966\n",
      "Iteration 767, loss = 0.27679018\n",
      "Iteration 768, loss = 0.27674941\n",
      "Iteration 769, loss = 0.27657547\n",
      "Iteration 770, loss = 0.27639706\n",
      "Iteration 771, loss = 0.27631543\n",
      "Iteration 772, loss = 0.27618394\n",
      "Iteration 773, loss = 0.27601482\n",
      "Iteration 774, loss = 0.27587909\n",
      "Iteration 775, loss = 0.27578041\n",
      "Iteration 776, loss = 0.27566930\n",
      "Iteration 777, loss = 0.27548298\n",
      "Iteration 778, loss = 0.27534656\n",
      "Iteration 779, loss = 0.27522204\n",
      "Iteration 780, loss = 0.27522343\n",
      "Iteration 781, loss = 0.27499232\n",
      "Iteration 782, loss = 0.27481925\n",
      "Iteration 783, loss = 0.27476891\n",
      "Iteration 784, loss = 0.27460786\n",
      "Iteration 785, loss = 0.27444325\n",
      "Iteration 786, loss = 0.27435519\n",
      "Iteration 787, loss = 0.27418209\n",
      "Iteration 788, loss = 0.27404332\n",
      "Iteration 789, loss = 0.27390840\n",
      "Iteration 790, loss = 0.27384130\n",
      "Iteration 791, loss = 0.27366821\n",
      "Iteration 792, loss = 0.27352023\n",
      "Iteration 793, loss = 0.27339855\n",
      "Iteration 794, loss = 0.27327118\n",
      "Iteration 795, loss = 0.27312618\n",
      "Iteration 796, loss = 0.27299242\n",
      "Iteration 797, loss = 0.27290260\n",
      "Iteration 798, loss = 0.27274782\n",
      "Iteration 799, loss = 0.27261704\n",
      "Iteration 800, loss = 0.27244941\n",
      "Iteration 801, loss = 0.27233006\n",
      "Iteration 802, loss = 0.27225124\n",
      "Iteration 803, loss = 0.27206285\n",
      "Iteration 804, loss = 0.27191571\n",
      "Iteration 805, loss = 0.27195951\n",
      "Iteration 806, loss = 0.27167481\n",
      "Iteration 807, loss = 0.27155226\n",
      "Iteration 808, loss = 0.27140955\n",
      "Iteration 809, loss = 0.27132054\n",
      "Iteration 810, loss = 0.27118277\n",
      "Iteration 811, loss = 0.27101728\n",
      "Iteration 812, loss = 0.27087282\n",
      "Iteration 813, loss = 0.27075515\n",
      "Iteration 814, loss = 0.27062122\n",
      "Iteration 815, loss = 0.27050838\n",
      "Iteration 816, loss = 0.27034489\n",
      "Iteration 817, loss = 0.27021737\n",
      "Iteration 818, loss = 0.27008898\n",
      "Iteration 819, loss = 0.26995924\n",
      "Iteration 820, loss = 0.26982339\n",
      "Iteration 821, loss = 0.26986564\n",
      "Iteration 822, loss = 0.26954771\n",
      "Iteration 823, loss = 0.26949564\n",
      "Iteration 824, loss = 0.26930263\n",
      "Iteration 825, loss = 0.26919758\n",
      "Iteration 826, loss = 0.26908372\n",
      "Iteration 827, loss = 0.26887956\n",
      "Iteration 828, loss = 0.26878444\n",
      "Iteration 829, loss = 0.26866103\n",
      "Iteration 830, loss = 0.26851481\n",
      "Iteration 831, loss = 0.26836028\n",
      "Iteration 832, loss = 0.26828867\n",
      "Iteration 363, loss = 0.32802455\n",
      "Iteration 364, loss = 0.32794319\n",
      "Iteration 365, loss = 0.32784623\n",
      "Iteration 366, loss = 0.32777065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78230641\n",
      "Iteration 2, loss = 0.77802367\n",
      "Iteration 3, loss = 0.77147755\n",
      "Iteration 4, loss = 0.76351285\n",
      "Iteration 5, loss = 0.75457018\n",
      "Iteration 6, loss = 0.74512450\n",
      "Iteration 7, loss = 0.73516007\n",
      "Iteration 8, loss = 0.72511121\n",
      "Iteration 9, loss = 0.71499383\n",
      "Iteration 10, loss = 0.70543967\n",
      "Iteration 11, loss = 0.69562825\n",
      "Iteration 12, loss = 0.68615903\n",
      "Iteration 13, loss = 0.67696554\n",
      "Iteration 14, loss = 0.66786674\n",
      "Iteration 15, loss = 0.65942572\n",
      "Iteration 16, loss = 0.65079925\n",
      "Iteration 17, loss = 0.64263102\n",
      "Iteration 18, loss = 0.63496828\n",
      "Iteration 19, loss = 0.62732726\n",
      "Iteration 20, loss = 0.61991587\n",
      "Iteration 21, loss = 0.61290231\n",
      "Iteration 22, loss = 0.60607736\n",
      "Iteration 23, loss = 0.59931861\n",
      "Iteration 24, loss = 0.59290385\n",
      "Iteration 25, loss = 0.58670408\n",
      "Iteration 26, loss = 0.58080734\n",
      "Iteration 27, loss = 0.57486452\n",
      "Iteration 28, loss = 0.56934522\n",
      "Iteration 29, loss = 0.56390590\n",
      "Iteration 30, loss = 0.55859754\n",
      "Iteration 31, loss = 0.55345329\n",
      "Iteration 32, loss = 0.54843546\n",
      "Iteration 33, loss = 0.54369936\n",
      "Iteration 34, loss = 0.53898431\n",
      "Iteration 35, loss = 0.53444507\n",
      "Iteration 36, loss = 0.53006195\n",
      "Iteration 37, loss = 0.52584415\n",
      "Iteration 38, loss = 0.52163071\n",
      "Iteration 39, loss = 0.51765153\n",
      "Iteration 40, loss = 0.51370029\n",
      "Iteration 41, loss = 0.51001895\n",
      "Iteration 42, loss = 0.50630654\n",
      "Iteration 43, loss = 0.50274896\n",
      "Iteration 44, loss = 0.49929594\n",
      "Iteration 45, loss = 0.49594434\n",
      "Iteration 46, loss = 0.49260519\n",
      "Iteration 47, loss = 0.48939781\n",
      "Iteration 48, loss = 0.48631535\n",
      "Iteration 49, loss = 0.48331471\n",
      "Iteration 50, loss = 0.48038864\n",
      "Iteration 51, loss = 0.47748407\n",
      "Iteration 52, loss = 0.47470653\n",
      "Iteration 53, loss = 0.47212144\n",
      "Iteration 54, loss = 0.46942309\n",
      "Iteration 55, loss = 0.46678334\n",
      "Iteration 56, loss = 0.46434253\n",
      "Iteration 57, loss = 0.46185998\n",
      "Iteration 58, loss = 0.45950609\n",
      "Iteration 59, loss = 0.45725331\n",
      "Iteration 60, loss = 0.45496840\n",
      "Iteration 61, loss = 0.45271407\n",
      "Iteration 62, loss = 0.45063981\n",
      "Iteration 63, loss = 0.44855314\n",
      "Iteration 64, loss = 0.44645411\n",
      "Iteration 65, loss = 0.44441753\n",
      "Iteration 66, loss = 0.44255476\n",
      "Iteration 67, loss = 0.44064928\n",
      "Iteration 68, loss = 0.43877379\n",
      "Iteration 69, loss = 0.43693435\n",
      "Iteration 70, loss = 0.43516928\n",
      "Iteration 71, loss = 0.43347347\n",
      "Iteration 72, loss = 0.43173762\n",
      "Iteration 73, loss = 0.43011296\n",
      "Iteration 74, loss = 0.42849628\n",
      "Iteration 75, loss = 0.42697051\n",
      "Iteration 76, loss = 0.42542845\n",
      "Iteration 77, loss = 0.42387270\n",
      "Iteration 78, loss = 0.42240641\n",
      "Iteration 79, loss = 0.42102792\n",
      "Iteration 80, loss = 0.41955177\n",
      "Iteration 81, loss = 0.41818043\n",
      "Iteration 82, loss = 0.41684356\n",
      "Iteration 83, loss = 0.41552699\n",
      "Iteration 84, loss = 0.41425386\n",
      "Iteration 85, loss = 0.41299864\n",
      "Iteration 86, loss = 0.41174801\n",
      "Iteration 87, loss = 0.41055858\n",
      "Iteration 88, loss = 0.40934959\n",
      "Iteration 89, loss = 0.40823491\n",
      "Iteration 90, loss = 0.40704971\n",
      "Iteration 91, loss = 0.40595798\n",
      "Iteration 92, loss = 0.40485869\n",
      "Iteration 93, loss = 0.40380605\n",
      "Iteration 94, loss = 0.40274612\n",
      "Iteration 95, loss = 0.40173254\n",
      "Iteration 96, loss = 0.40071269\n",
      "Iteration 97, loss = 0.39975534\n",
      "Iteration 98, loss = 0.39877014\n",
      "Iteration 99, loss = 0.39785447\n",
      "Iteration 100, loss = 0.39690003\n",
      "Iteration 101, loss = 0.39597860\n",
      "Iteration 102, loss = 0.39509070\n",
      "Iteration 103, loss = 0.39423386\n",
      "Iteration 104, loss = 0.39333678\n",
      "Iteration 105, loss = 0.39250814\n",
      "Iteration 106, loss = 0.39166156\n",
      "Iteration 107, loss = 0.39085644\n",
      "Iteration 108, loss = 0.39004948\n",
      "Iteration 109, loss = 0.38926507\n",
      "Iteration 110, loss = 0.38854060\n",
      "Iteration 111, loss = 0.38776030\n",
      "Iteration 112, loss = 0.38702554\n",
      "Iteration 113, loss = 0.38625777\n",
      "Iteration 114, loss = 0.38555773\n",
      "Iteration 115, loss = 0.38484040\n",
      "Iteration 116, loss = 0.38415367\n",
      "Iteration 117, loss = 0.38347120\n",
      "Iteration 118, loss = 0.38279325\n",
      "Iteration 119, loss = 0.38215193\n",
      "Iteration 120, loss = 0.38144741\n",
      "Iteration 121, loss = 0.38085362\n",
      "Iteration 122, loss = 0.38020140\n",
      "Iteration 123, loss = 0.37960856\n",
      "Iteration 124, loss = 0.37898266\n",
      "Iteration 125, loss = 0.37838938\n",
      "Iteration 126, loss = 0.37778290\n",
      "Iteration 127, loss = 0.37721692\n",
      "Iteration 128, loss = 0.37663741\n",
      "Iteration 129, loss = 0.37606429\n",
      "Iteration 130, loss = 0.37550212\n",
      "Iteration 131, loss = 0.37495119\n",
      "Iteration 132, loss = 0.37440894\n",
      "Iteration 133, loss = 0.37386279\n",
      "Iteration 134, loss = 0.37337703\n",
      "Iteration 135, loss = 0.37285173\n",
      "Iteration 136, loss = 0.37233207\n",
      "Iteration 137, loss = 0.37181975\n",
      "Iteration 138, loss = 0.37133640\n",
      "Iteration 139, loss = 0.37084607\n",
      "Iteration 140, loss = 0.37036798\n",
      "Iteration 141, loss = 0.36986503\n",
      "Iteration 142, loss = 0.36941596\n",
      "Iteration 143, loss = 0.36896753\n",
      "Iteration 144, loss = 0.36850324\n",
      "Iteration 145, loss = 0.36803077\n",
      "Iteration 146, loss = 0.36761043\n",
      "Iteration 147, loss = 0.36716907\n",
      "Iteration 148, loss = 0.36671952\n",
      "Iteration 149, loss = 0.36630797\n",
      "Iteration 150, loss = 0.36589085\n",
      "Iteration 151, loss = 0.36546222\n",
      "Iteration 152, loss = 0.36505520\n",
      "Iteration 153, loss = 0.36463354\n",
      "Iteration 154, loss = 0.36425162\n",
      "Iteration 155, loss = 0.36382951\n",
      "Iteration 156, loss = 0.36345808\n",
      "Iteration 157, loss = 0.36309223\n",
      "Iteration 158, loss = 0.36267944\n",
      "Iteration 159, loss = 0.36232942\n",
      "Iteration 160, loss = 0.36192497\n",
      "Iteration 161, loss = 0.36157195\n",
      "Iteration 162, loss = 0.36120128\n",
      "Iteration 163, loss = 0.36083809\n",
      "Iteration 164, loss = 0.36048720\n",
      "Iteration 165, loss = 0.36012644\n",
      "Iteration 166, loss = 0.35978574\n",
      "Iteration 167, loss = 0.35945057\n",
      "Iteration 168, loss = 0.35911648\n",
      "Iteration 169, loss = 0.35877242\n",
      "Iteration 170, loss = 0.35844633\n",
      "Iteration 171, loss = 0.35810051\n",
      "Iteration 172, loss = 0.35777664\n",
      "Iteration 173, loss = 0.35745225\n",
      "Iteration 174, loss = 0.35714192\n",
      "Iteration 175, loss = 0.35683263\n",
      "Iteration 176, loss = 0.35652254\n",
      "Iteration 177, loss = 0.35620696\n",
      "Iteration 178, loss = 0.35590338\n",
      "Iteration 179, loss = 0.35560282\n",
      "Iteration 180, loss = 0.35528876\n",
      "Iteration 181, loss = 0.35500413\n",
      "Iteration 182, loss = 0.35471756\n",
      "Iteration 183, loss = 0.35441549\n",
      "Iteration 184, loss = 0.35414085\n",
      "Iteration 185, loss = 0.35384735\n",
      "Iteration 186, loss = 0.35356517\n",
      "Iteration 187, loss = 0.35327861\n",
      "Iteration 188, loss = 0.35299666\n",
      "Iteration 189, loss = 0.35271512\n",
      "Iteration 190, loss = 0.35244637\n",
      "Iteration 191, loss = 0.35219731\n",
      "Iteration 192, loss = 0.35193082\n",
      "Iteration 193, loss = 0.35165209\n",
      "Iteration 194, loss = 0.35139073\n",
      "Iteration 195, loss = 0.35113198\n",
      "Iteration 196, loss = 0.35086921\n",
      "Iteration 197, loss = 0.35060264\n",
      "Iteration 198, loss = 0.35036307\n",
      "Iteration 199, loss = 0.35010555\n",
      "Iteration 200, loss = 0.34985724\n",
      "Iteration 201, loss = 0.34961914\n",
      "Iteration 202, loss = 0.34938961\n",
      "Iteration 203, loss = 0.34911940\n",
      "Iteration 204, loss = 0.34889261\n",
      "Iteration 205, loss = 0.34865156\n",
      "Iteration 206, loss = 0.34841304\n",
      "Iteration 207, loss = 0.34817846\n",
      "Iteration 208, loss = 0.34794687\n",
      "Iteration 209, loss = 0.34770847\n",
      "Iteration 210, loss = 0.34748880\n",
      "Iteration 211, loss = 0.34725921\n",
      "Iteration 212, loss = 0.34702167\n",
      "Iteration 213, loss = 0.34681219\n",
      "Iteration 214, loss = 0.34658839\n",
      "Iteration 215, loss = 0.34636086\n",
      "Iteration 216, loss = 0.34615975\n",
      "Iteration 217, loss = 0.34593503\n",
      "Iteration 218, loss = 0.34571135\n",
      "Iteration 219, loss = 0.34550683\n",
      "Iteration 220, loss = 0.34529778\n",
      "Iteration 221, loss = 0.34507131\n",
      "Iteration 222, loss = 0.34487208\n",
      "Iteration 223, loss = 0.34465926\n",
      "Iteration 224, loss = 0.34446712\n",
      "Iteration 225, loss = 0.34425576\n",
      "Iteration 226, loss = 0.34405161\n",
      "Iteration 227, loss = 0.34385571\n",
      "Iteration 228, loss = 0.34365696\n",
      "Iteration 229, loss = 0.34345384\n",
      "Iteration 230, loss = 0.34324815\n",
      "Iteration 231, loss = 0.34306027\n",
      "Iteration 232, loss = 0.34286744\n",
      "Iteration 233, loss = 0.34267882\n",
      "Iteration 234, loss = 0.34247706\n",
      "Iteration 235, loss = 0.34227102\n",
      "Iteration 236, loss = 0.34210187\n",
      "Iteration 237, loss = 0.34191994\n",
      "Iteration 238, loss = 0.34172246\n",
      "Iteration 239, loss = 0.34153104\n",
      "Iteration 240, loss = 0.34134898\n",
      "Iteration 241, loss = 0.34115849\n",
      "Iteration 242, loss = 0.34098586\n",
      "Iteration 243, loss = 0.34081541\n",
      "Iteration 244, loss = 0.34061854\n",
      "Iteration 245, loss = 0.34043327\n",
      "Iteration 246, loss = 0.34026247\n",
      "Iteration 247, loss = 0.34007217\n",
      "Iteration 248, loss = 0.33990401\n",
      "Iteration 249, loss = 0.33972459\n",
      "Iteration 250, loss = 0.33954829\n",
      "Iteration 251, loss = 0.33938630\n",
      "Iteration 252, loss = 0.33920372\n",
      "Iteration 253, loss = 0.33903435\n",
      "Iteration 254, loss = 0.33886948\n",
      "Iteration 255, loss = 0.33868587\n",
      "Iteration 256, loss = 0.33851851\n",
      "Iteration 257, loss = 0.33835081\n",
      "Iteration 258, loss = 0.33818606\n",
      "Iteration 259, loss = 0.33801768\n",
      "Iteration 260, loss = 0.33785190\n",
      "Iteration 261, loss = 0.33769719\n",
      "Iteration 262, loss = 0.33752654\n",
      "Iteration 263, loss = 0.33735475\n",
      "Iteration 264, loss = 0.33720373\n",
      "Iteration 265, loss = 0.33704394\n",
      "Iteration 266, loss = 0.33687059\n",
      "Iteration 267, loss = 0.33671192\n",
      "Iteration 268, loss = 0.33656760\n",
      "Iteration 269, loss = 0.33639903\n",
      "Iteration 270, loss = 0.33623900\n",
      "Iteration 271, loss = 0.33609815\n",
      "Iteration 272, loss = 0.33593621\n",
      "Iteration 273, loss = 0.33578386\n",
      "Iteration 274, loss = 0.33562383\n",
      "Iteration 275, loss = 0.33548023\n",
      "Iteration 276, loss = 0.33532709\n",
      "Iteration 277, loss = 0.33516789\n",
      "Iteration 278, loss = 0.33503599\n",
      "Iteration 279, loss = 0.33485744\n",
      "Iteration 280, loss = 0.33472580\n",
      "Iteration 281, loss = 0.33458307\n",
      "Iteration 282, loss = 0.33443144\n",
      "Iteration 283, loss = 0.33427354\n",
      "Iteration 284, loss = 0.33414258\n",
      "Iteration 285, loss = 0.33401630\n",
      "Iteration 286, loss = 0.33385967\n",
      "Iteration 287, loss = 0.33371152\n",
      "Iteration 288, loss = 0.33356837\n",
      "Iteration 289, loss = 0.33342899\n",
      "Iteration 290, loss = 0.33329246\n",
      "Iteration 291, loss = 0.33314278\n",
      "Iteration 292, loss = 0.33301572\n",
      "Iteration 293, loss = 0.33287088\n",
      "Iteration 294, loss = 0.33273802\n",
      "Iteration 295, loss = 0.33258959\n",
      "Iteration 296, loss = 0.33246357\n",
      "Iteration 297, loss = 0.33232243\n",
      "Iteration 298, loss = 0.33217824\n",
      "Iteration 299, loss = 0.33204942\n",
      "Iteration 300, loss = 0.33191591\n",
      "Iteration 301, loss = 0.33178019\n",
      "Iteration 302, loss = 0.33164543\n",
      "Iteration 303, loss = 0.33150654\n",
      "Iteration 304, loss = 0.33140734\n",
      "Iteration 305, loss = 0.33124905\n",
      "Iteration 306, loss = 0.33112294\n",
      "Iteration 307, loss = 0.33100203\n",
      "Iteration 308, loss = 0.33087532\n",
      "Iteration 309, loss = 0.33073664\n",
      "Iteration 310, loss = 0.33061459\n",
      "Iteration 311, loss = 0.33047503\n",
      "Iteration 312, loss = 0.33035812\n",
      "Iteration 313, loss = 0.33023255\n",
      "Iteration 314, loss = 0.33010509\n",
      "Iteration 315, loss = 0.32997363\n",
      "Iteration 316, loss = 0.32985164\n",
      "Iteration 317, loss = 0.32972645\n",
      "Iteration 318, loss = 0.32961207\n",
      "Iteration 319, loss = 0.32948881\n",
      "Iteration 320, loss = 0.32935899\n",
      "Iteration 321, loss = 0.32924256\n",
      "Iteration 322, loss = 0.32911726\n",
      "Iteration 323, loss = 0.32899696\n",
      "Iteration 324, loss = 0.32887466\n",
      "Iteration 325, loss = 0.32877745\n",
      "Iteration 326, loss = 0.32863926\n",
      "Iteration 327, loss = 0.32852586\n",
      "Iteration 328, loss = 0.32841135\n",
      "Iteration 329, loss = 0.32827947\n",
      "Iteration 330, loss = 0.32818080\n",
      "Iteration 331, loss = 0.32806421\n",
      "Iteration 332, loss = 0.32793378\n",
      "Iteration 333, loss = 0.32782255\n",
      "Iteration 334, loss = 0.32772506\n",
      "Iteration 335, loss = 0.32759887\n",
      "Iteration 336, loss = 0.32748580\n",
      "Iteration 337, loss = 0.32736823\n",
      "Iteration 338, loss = 0.32725478\n",
      "Iteration 339, loss = 0.32714300\n",
      "Iteration 340, loss = 0.32703600\n",
      "Iteration 341, loss = 0.32692010\n",
      "Iteration 342, loss = 0.32680541\n",
      "Iteration 343, loss = 0.32669254\n",
      "Iteration 344, loss = 0.32660481\n",
      "Iteration 345, loss = 0.32648890\n",
      "Iteration 346, loss = 0.32637709\n",
      "Iteration 347, loss = 0.32626811\n",
      "Iteration 348, loss = 0.32615175\n",
      "Iteration 349, loss = 0.32604299\n",
      "Iteration 350, loss = 0.32594120\n",
      "Iteration 351, loss = 0.32582747\n",
      "Iteration 352, loss = 0.32572834\n",
      "Iteration 353, loss = 0.32561670\n",
      "Iteration 354, loss = 0.32551400\n",
      "Iteration 355, loss = 0.32541541\n",
      "Iteration 356, loss = 0.32530592\n",
      "Iteration 357, loss = 0.32521670\n",
      "Iteration 358, loss = 0.32509788\n",
      "Iteration 359, loss = 0.32500017\n",
      "Iteration 360, loss = 0.32488605\n",
      "Iteration 361, loss = 0.32478877\n",
      "Iteration 362, loss = 0.32468190\n",
      "Iteration 363, loss = 0.32457910\n",
      "Iteration 364, loss = 0.32448274\n",
      "Iteration 365, loss = 0.32437861\n",
      "Iteration 366, loss = 0.32427777\n",
      "Iteration 367, loss = 0.32418654\n",
      "Iteration 368, loss = 0.32409189\n",
      "Iteration 369, loss = 0.32398141\n",
      "Iteration 370, loss = 0.32387589\n",
      "Iteration 371, loss = 0.32378215\n",
      "Iteration 372, loss = 0.32368336\n",
      "Iteration 373, loss = 0.32358601\n",
      "Iteration 374, loss = 0.32349129\n",
      "Iteration 375, loss = 0.32339367\n",
      "Iteration 376, loss = 0.32330152\n",
      "Iteration 377, loss = 0.32319534\n",
      "Iteration 378, loss = 0.32310305\n",
      "Iteration 379, loss = 0.32299667\n",
      "Iteration 380, loss = 0.32290975\n",
      "Iteration 381, loss = 0.32281029\n",
      "Iteration 382, loss = 0.32272678\n",
      "Iteration 383, loss = 0.32263548\n",
      "Iteration 384, loss = 0.32253046\n",
      "Iteration 385, loss = 0.32243660\n",
      "Iteration 386, loss = 0.32234711\n",
      "Iteration 387, loss = 0.32225571\n",
      "Iteration 388, loss = 0.32216227\n",
      "Iteration 389, loss = 0.32209526\n",
      "Iteration 390, loss = 0.32197242\n",
      "Iteration 391, loss = 0.32188531\n",
      "Iteration 392, loss = 0.32178410\n",
      "Iteration 393, loss = 0.32170548\n",
      "Iteration 394, loss = 0.32161883\n",
      "Iteration 395, loss = 0.32151312\n",
      "Iteration 396, loss = 0.32143093\n",
      "Iteration 397, loss = 0.32134618\n",
      "Iteration 398, loss = 0.32125674\n",
      "Iteration 399, loss = 0.32117300\n",
      "Iteration 400, loss = 0.32107756\n",
      "Iteration 401, loss = 0.32098851\n",
      "Iteration 402, loss = 0.32089670\n",
      "Iteration 403, loss = 0.32081208\n",
      "Iteration 404, loss = 0.32071871\n",
      "Iteration 405, loss = 0.32063356\n",
      "Iteration 406, loss = 0.32054425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69778819\n",
      "Iteration 2, loss = 0.69573345\n",
      "Iteration 3, loss = 0.69257270\n",
      "Iteration 4, loss = 0.68871208\n",
      "Iteration 5, loss = 0.68443207\n",
      "Iteration 6, loss = 0.67984427\n",
      "Iteration 7, loss = 0.67472689\n",
      "Iteration 8, loss = 0.66983890\n",
      "Iteration 9, loss = 0.66493265\n",
      "Iteration 10, loss = 0.65999814\n",
      "Iteration 11, loss = 0.65505997\n",
      "Iteration 12, loss = 0.65018425\n",
      "Iteration 13, loss = 0.64539968\n",
      "Iteration 14, loss = 0.64073003\n",
      "Iteration 15, loss = 0.63603562\n",
      "Iteration 16, loss = 0.63144349\n",
      "Iteration 17, loss = 0.62698840\n",
      "Iteration 18, loss = 0.62263856\n",
      "Iteration 19, loss = 0.61827439\n",
      "Iteration 20, loss = 0.61405998\n",
      "Iteration 21, loss = 0.60984232\n",
      "Iteration 22, loss = 0.60574179\n",
      "Iteration 23, loss = 0.60160932\n",
      "Iteration 24, loss = 0.59766285\n",
      "Iteration 25, loss = 0.59366787\n",
      "Iteration 26, loss = 0.58976274\n",
      "Iteration 27, loss = 0.58588918\n",
      "Iteration 28, loss = 0.58211346\n",
      "Iteration 29, loss = 0.57834439\n",
      "Iteration 30, loss = 0.57469652\n",
      "Iteration 31, loss = 0.57101251\n",
      "Iteration 32, loss = 0.56742357\n",
      "Iteration 33, loss = 0.56376241\n",
      "Iteration 34, loss = 0.56028104\n",
      "Iteration 35, loss = 0.55674679\n",
      "Iteration 36, loss = 0.55329806\n",
      "Iteration 37, loss = 0.54982592\n",
      "Iteration 38, loss = 0.54641579\n",
      "Iteration 39, loss = 0.54301837\n",
      "Iteration 40, loss = 0.53971803\n",
      "Iteration 41, loss = 0.53638635\n",
      "Iteration 42, loss = 0.53307894\n",
      "Iteration 43, loss = 0.52989441\n",
      "Iteration 44, loss = 0.52671319\n",
      "Iteration 45, loss = 0.52357533\n",
      "Iteration 46, loss = 0.52034143\n",
      "Iteration 47, loss = 0.51733883\n",
      "Iteration 48, loss = 0.51421980\n",
      "Iteration 49, loss = 0.51120780\n",
      "Iteration 50, loss = 0.50835154\n",
      "Iteration 51, loss = 0.50531306\n",
      "Iteration 52, loss = 0.50245292\n",
      "Iteration 53, loss = 0.49962426\n",
      "Iteration 54, loss = 0.49682768\n",
      "Iteration 55, loss = 0.49397938\n",
      "Iteration 56, loss = 0.49127467\n",
      "Iteration 57, loss = 0.48857367\n",
      "Iteration 58, loss = 0.48590481\n",
      "Iteration 59, loss = 0.48325078\n",
      "Iteration 60, loss = 0.48069344\n",
      "Iteration 61, loss = 0.47817112\n",
      "Iteration 62, loss = 0.47562189\n",
      "Iteration 63, loss = 0.47316110\n",
      "Iteration 64, loss = 0.47072169\n",
      "Iteration 65, loss = 0.46825278\n",
      "Iteration 66, loss = 0.46592909\n",
      "Iteration 67, loss = 0.46363904\n",
      "Iteration 68, loss = 0.46137220\n",
      "Iteration 69, loss = 0.45908058\n",
      "Iteration 70, loss = 0.45685293\n",
      "Iteration 71, loss = 0.45472788\n",
      "Iteration 72, loss = 0.45259732\n",
      "Iteration 73, loss = 0.45045313\n",
      "Iteration 74, loss = 0.44843689\n",
      "Iteration 75, loss = 0.44638617\n",
      "Iteration 76, loss = 0.44438576\n",
      "Iteration 77, loss = 0.44241451\n",
      "Iteration 78, loss = 0.44046763\n",
      "Iteration 79, loss = 0.43857927\n",
      "Iteration 80, loss = 0.43671242\n",
      "Iteration 81, loss = 0.43492038\n",
      "Iteration 82, loss = 0.43309692\n",
      "Iteration 83, loss = 0.43140280\n",
      "Iteration 84, loss = 0.42965150\n",
      "Iteration 85, loss = 0.42797816\n",
      "Iteration 86, loss = 0.42621361\n",
      "Iteration 818, loss = 0.29156020\n",
      "Iteration 819, loss = 0.29142829\n",
      "Iteration 820, loss = 0.29130647\n",
      "Iteration 821, loss = 0.29122366\n",
      "Iteration 822, loss = 0.29107886\n",
      "Iteration 823, loss = 0.29097430\n",
      "Iteration 824, loss = 0.29083124\n",
      "Iteration 825, loss = 0.29072896\n",
      "Iteration 826, loss = 0.29059815\n",
      "Iteration 827, loss = 0.29049895\n",
      "Iteration 828, loss = 0.29036256\n",
      "Iteration 829, loss = 0.29028978\n",
      "Iteration 830, loss = 0.29011794\n",
      "Iteration 831, loss = 0.29004805\n",
      "Iteration 832, loss = 0.28991391\n",
      "Iteration 833, loss = 0.28976754\n",
      "Iteration 834, loss = 0.28967697\n",
      "Iteration 835, loss = 0.28959467\n",
      "Iteration 836, loss = 0.28940754\n",
      "Iteration 837, loss = 0.28936727\n",
      "Iteration 838, loss = 0.28918475\n",
      "Iteration 839, loss = 0.28909020\n",
      "Iteration 840, loss = 0.28901751\n",
      "Iteration 841, loss = 0.28884266\n",
      "Iteration 842, loss = 0.28870820\n",
      "Iteration 843, loss = 0.28861827\n",
      "Iteration 844, loss = 0.28846784\n",
      "Iteration 845, loss = 0.28838896\n",
      "Iteration 846, loss = 0.28824000\n",
      "Iteration 847, loss = 0.28814280\n",
      "Iteration 848, loss = 0.28800401\n",
      "Iteration 849, loss = 0.28786719\n",
      "Iteration 850, loss = 0.28780893\n",
      "Iteration 851, loss = 0.28764868\n",
      "Iteration 852, loss = 0.28751527\n",
      "Iteration 853, loss = 0.28741635\n",
      "Iteration 854, loss = 0.28729599\n",
      "Iteration 855, loss = 0.28717561\n",
      "Iteration 856, loss = 0.28705843\n",
      "Iteration 857, loss = 0.28693224\n",
      "Iteration 858, loss = 0.28680968\n",
      "Iteration 859, loss = 0.28671355\n",
      "Iteration 860, loss = 0.28656256\n",
      "Iteration 861, loss = 0.28653275\n",
      "Iteration 862, loss = 0.28631810\n",
      "Iteration 863, loss = 0.28623362\n",
      "Iteration 864, loss = 0.28617085\n",
      "Iteration 865, loss = 0.28603093\n",
      "Iteration 866, loss = 0.28584725\n",
      "Iteration 867, loss = 0.28572870\n",
      "Iteration 868, loss = 0.28561305\n",
      "Iteration 869, loss = 0.28551169\n",
      "Iteration 870, loss = 0.28541814\n",
      "Iteration 871, loss = 0.28527231\n",
      "Iteration 872, loss = 0.28517200\n",
      "Iteration 873, loss = 0.28504128\n",
      "Iteration 874, loss = 0.28490646\n",
      "Iteration 875, loss = 0.28478893\n",
      "Iteration 876, loss = 0.28467419\n",
      "Iteration 877, loss = 0.28456232\n",
      "Iteration 878, loss = 0.28442995\n",
      "Iteration 879, loss = 0.28430985\n",
      "Iteration 880, loss = 0.28422153\n",
      "Iteration 881, loss = 0.28409920\n",
      "Iteration 882, loss = 0.28394722\n",
      "Iteration 883, loss = 0.28383228\n",
      "Iteration 884, loss = 0.28371978\n",
      "Iteration 885, loss = 0.28360447\n",
      "Iteration 886, loss = 0.28350078\n",
      "Iteration 887, loss = 0.28336787\n",
      "Iteration 888, loss = 0.28323980\n",
      "Iteration 889, loss = 0.28313146\n",
      "Iteration 890, loss = 0.28303848\n",
      "Iteration 891, loss = 0.28287991\n",
      "Iteration 892, loss = 0.28277462\n",
      "Iteration 893, loss = 0.28265865\n",
      "Iteration 894, loss = 0.28253179\n",
      "Iteration 895, loss = 0.28243605\n",
      "Iteration 896, loss = 0.28232240\n",
      "Iteration 897, loss = 0.28218079\n",
      "Iteration 898, loss = 0.28206823\n",
      "Iteration 899, loss = 0.28195280\n",
      "Iteration 900, loss = 0.28184708\n",
      "Iteration 901, loss = 0.28170624\n",
      "Iteration 902, loss = 0.28163829\n",
      "Iteration 903, loss = 0.28147442\n",
      "Iteration 904, loss = 0.28138221\n",
      "Iteration 905, loss = 0.28124367\n",
      "Iteration 906, loss = 0.28111819\n",
      "Iteration 907, loss = 0.28101290\n",
      "Iteration 908, loss = 0.28090470\n",
      "Iteration 909, loss = 0.28081230\n",
      "Iteration 910, loss = 0.28066279\n",
      "Iteration 911, loss = 0.28057233\n",
      "Iteration 912, loss = 0.28043162\n",
      "Iteration 913, loss = 0.28030280\n",
      "Iteration 914, loss = 0.28019189\n",
      "Iteration 915, loss = 0.28008943\n",
      "Iteration 916, loss = 0.27994972\n",
      "Iteration 917, loss = 0.27988564\n",
      "Iteration 918, loss = 0.27979507\n",
      "Iteration 919, loss = 0.27960182\n",
      "Iteration 920, loss = 0.27951646\n",
      "Iteration 921, loss = 0.27937962\n",
      "Iteration 922, loss = 0.27929192\n",
      "Iteration 923, loss = 0.27923275\n",
      "Iteration 924, loss = 0.27904671\n",
      "Iteration 925, loss = 0.27892156\n",
      "Iteration 926, loss = 0.27882662\n",
      "Iteration 927, loss = 0.27867902\n",
      "Iteration 928, loss = 0.27855568\n",
      "Iteration 929, loss = 0.27846327\n",
      "Iteration 930, loss = 0.27833975\n",
      "Iteration 931, loss = 0.27824817\n",
      "Iteration 932, loss = 0.27811187\n",
      "Iteration 933, loss = 0.27802429\n",
      "Iteration 934, loss = 0.27787392\n",
      "Iteration 935, loss = 0.27778324\n",
      "Iteration 936, loss = 0.27765143\n",
      "Iteration 937, loss = 0.27753903\n",
      "Iteration 938, loss = 0.27743459\n",
      "Iteration 939, loss = 0.27732677\n",
      "Iteration 940, loss = 0.27717954\n",
      "Iteration 941, loss = 0.27706360\n",
      "Iteration 942, loss = 0.27698052\n",
      "Iteration 943, loss = 0.27685837\n",
      "Iteration 944, loss = 0.27678532\n",
      "Iteration 945, loss = 0.27659275\n",
      "Iteration 946, loss = 0.27651423\n",
      "Iteration 947, loss = 0.27637659\n",
      "Iteration 948, loss = 0.27626700\n",
      "Iteration 949, loss = 0.27615254\n",
      "Iteration 950, loss = 0.27608689\n",
      "Iteration 951, loss = 0.27593809\n",
      "Iteration 952, loss = 0.27579257\n",
      "Iteration 953, loss = 0.27568716\n",
      "Iteration 954, loss = 0.27567195\n",
      "Iteration 955, loss = 0.27549525\n",
      "Iteration 956, loss = 0.27538761\n",
      "Iteration 957, loss = 0.27525274\n",
      "Iteration 958, loss = 0.27514695\n",
      "Iteration 959, loss = 0.27502738\n",
      "Iteration 960, loss = 0.27491864\n",
      "Iteration 961, loss = 0.27477885\n",
      "Iteration 962, loss = 0.27463202\n",
      "Iteration 963, loss = 0.27453377\n",
      "Iteration 964, loss = 0.27440673\n",
      "Iteration 965, loss = 0.27434406\n",
      "Iteration 966, loss = 0.27418260\n",
      "Iteration 967, loss = 0.27411490\n",
      "Iteration 968, loss = 0.27400711\n",
      "Iteration 969, loss = 0.27382657\n",
      "Iteration 970, loss = 0.27373045\n",
      "Iteration 971, loss = 0.27362946\n",
      "Iteration 972, loss = 0.27348584\n",
      "Iteration 973, loss = 0.27336915\n",
      "Iteration 974, loss = 0.27325388\n",
      "Iteration 975, loss = 0.27316677\n",
      "Iteration 976, loss = 0.27306096\n",
      "Iteration 977, loss = 0.27292936\n",
      "Iteration 978, loss = 0.27283854\n",
      "Iteration 979, loss = 0.27269868\n",
      "Iteration 980, loss = 0.27259029\n",
      "Iteration 981, loss = 0.27245223\n",
      "Iteration 982, loss = 0.27234340\n",
      "Iteration 983, loss = 0.27233825\n",
      "Iteration 984, loss = 0.27210723\n",
      "Iteration 985, loss = 0.27205441\n",
      "Iteration 986, loss = 0.27189155\n",
      "Iteration 987, loss = 0.27178393\n",
      "Iteration 988, loss = 0.27165266\n",
      "Iteration 989, loss = 0.27153557\n",
      "Iteration 990, loss = 0.27143987\n",
      "Iteration 991, loss = 0.27129755\n",
      "Iteration 992, loss = 0.27119126\n",
      "Iteration 993, loss = 0.27109537\n",
      "Iteration 994, loss = 0.27097676\n",
      "Iteration 995, loss = 0.27084360\n",
      "Iteration 996, loss = 0.27075504\n",
      "Iteration 997, loss = 0.27067019\n",
      "Iteration 998, loss = 0.27050750\n",
      "Iteration 999, loss = 0.27037933\n",
      "Iteration 1000, loss = 0.27026834\n",
      "Iteration 1001, loss = 0.27024358\n",
      "Iteration 1002, loss = 0.27003060\n",
      "Iteration 1003, loss = 0.27000688\n",
      "Iteration 1004, loss = 0.26989144\n",
      "Iteration 1005, loss = 0.26972824\n",
      "Iteration 1006, loss = 0.26960019\n",
      "Iteration 1007, loss = 0.26945899\n",
      "Iteration 1008, loss = 0.26935267\n",
      "Iteration 1009, loss = 0.26922517\n",
      "Iteration 1010, loss = 0.26915559\n",
      "Iteration 1011, loss = 0.26900215\n",
      "Iteration 1012, loss = 0.26891171\n",
      "Iteration 1013, loss = 0.26877645\n",
      "Iteration 1014, loss = 0.26867434\n",
      "Iteration 1015, loss = 0.26856040\n",
      "Iteration 1016, loss = 0.26848544\n",
      "Iteration 1017, loss = 0.26833024\n",
      "Iteration 1018, loss = 0.26819665\n",
      "Iteration 1019, loss = 0.26811414\n",
      "Iteration 1020, loss = 0.26801345\n",
      "Iteration 1021, loss = 0.26788613\n",
      "Iteration 1022, loss = 0.26774280\n",
      "Iteration 1023, loss = 0.26765001\n",
      "Iteration 1024, loss = 0.26756191\n",
      "Iteration 1025, loss = 0.26742055\n",
      "Iteration 1026, loss = 0.26728123\n",
      "Iteration 1027, loss = 0.26719690\n",
      "Iteration 1028, loss = 0.26706587\n",
      "Iteration 1029, loss = 0.26694317\n",
      "Iteration 1030, loss = 0.26683279\n",
      "Iteration 1031, loss = 0.26671956\n",
      "Iteration 1032, loss = 0.26664122\n",
      "Iteration 1033, loss = 0.26649059\n",
      "Iteration 1034, loss = 0.26650715\n",
      "Iteration 1035, loss = 0.26627856\n",
      "Iteration 1036, loss = 0.26614606\n",
      "Iteration 1037, loss = 0.26604118\n",
      "Iteration 1038, loss = 0.26592607\n",
      "Iteration 1039, loss = 0.26581367\n",
      "Iteration 1040, loss = 0.26574546\n",
      "Iteration 1041, loss = 0.26559916\n",
      "Iteration 1042, loss = 0.26552002\n",
      "Iteration 1043, loss = 0.26540094\n",
      "Iteration 1044, loss = 0.26523605\n",
      "Iteration 1045, loss = 0.26512084\n",
      "Iteration 1046, loss = 0.26504348\n",
      "Iteration 1047, loss = 0.26494514\n",
      "Iteration 1048, loss = 0.26486315\n",
      "Iteration 1049, loss = 0.26469743\n",
      "Iteration 1050, loss = 0.26454462\n",
      "Iteration 1051, loss = 0.26443512\n",
      "Iteration 1052, loss = 0.26439581\n",
      "Iteration 1053, loss = 0.26421153\n",
      "Iteration 1054, loss = 0.26408544\n",
      "Iteration 1055, loss = 0.26407937\n",
      "Iteration 1056, loss = 0.26386751\n",
      "Iteration 1057, loss = 0.26377419\n",
      "Iteration 1058, loss = 0.26366571\n",
      "Iteration 1059, loss = 0.26359691\n",
      "Iteration 1060, loss = 0.26342652\n",
      "Iteration 1061, loss = 0.26334744\n",
      "Iteration 1062, loss = 0.26320860\n",
      "Iteration 1063, loss = 0.26309063\n",
      "Iteration 1064, loss = 0.26296111\n",
      "Iteration 1065, loss = 0.26284357\n",
      "Iteration 1066, loss = 0.26276623\n",
      "Iteration 1067, loss = 0.26263653\n",
      "Iteration 1068, loss = 0.26251563\n",
      "Iteration 1069, loss = 0.26239096\n",
      "Iteration 1070, loss = 0.26231235\n",
      "Iteration 1071, loss = 0.26220186\n",
      "Iteration 1072, loss = 0.26208147\n",
      "Iteration 1073, loss = 0.26204972\n",
      "Iteration 1074, loss = 0.26183464\n",
      "Iteration 1075, loss = 0.26175999\n",
      "Iteration 1076, loss = 0.26163555\n",
      "Iteration 1077, loss = 0.26152032\n",
      "Iteration 1078, loss = 0.26141954\n",
      "Iteration 1079, loss = 0.26131095\n",
      "Iteration 1080, loss = 0.26117440\n",
      "Iteration 1081, loss = 0.26107628\n",
      "Iteration 1082, loss = 0.26095385\n",
      "Iteration 1083, loss = 0.26083826\n",
      "Iteration 1084, loss = 0.26073620\n",
      "Iteration 1085, loss = 0.26068990\n",
      "Iteration 1086, loss = 0.26052279\n",
      "Iteration 1087, loss = 0.26041774\n",
      "Iteration 1088, loss = 0.26028983\n",
      "Iteration 1089, loss = 0.26016702\n",
      "Iteration 1090, loss = 0.26005265\n",
      "Iteration 1091, loss = 0.25993420\n",
      "Iteration 1092, loss = 0.25985353\n",
      "Iteration 1093, loss = 0.25974018\n",
      "Iteration 1094, loss = 0.25961110\n",
      "Iteration 1095, loss = 0.25954565\n",
      "Iteration 1096, loss = 0.25940541\n",
      "Iteration 1097, loss = 0.25928010\n",
      "Iteration 1098, loss = 0.25918395\n",
      "Iteration 1099, loss = 0.25903538\n",
      "Iteration 1100, loss = 0.25895199\n",
      "Iteration 1101, loss = 0.25884335\n",
      "Iteration 1102, loss = 0.25871183\n",
      "Iteration 1103, loss = 0.25859902\n",
      "Iteration 1104, loss = 0.25848693\n",
      "Iteration 1105, loss = 0.25837203\n",
      "Iteration 1106, loss = 0.25828910\n",
      "Iteration 1107, loss = 0.25814048\n",
      "Iteration 1108, loss = 0.25803749\n",
      "Iteration 1109, loss = 0.25794739\n",
      "Iteration 1110, loss = 0.25782540\n",
      "Iteration 1111, loss = 0.25772306\n",
      "Iteration 1112, loss = 0.25756485\n",
      "Iteration 1113, loss = 0.25749497\n",
      "Iteration 1114, loss = 0.25743821\n",
      "Iteration 1115, loss = 0.25724764\n",
      "Iteration 1116, loss = 0.25714823\n",
      "Iteration 1117, loss = 0.25713798\n",
      "Iteration 1118, loss = 0.25705508\n",
      "Iteration 1119, loss = 0.25677892\n",
      "Iteration 1120, loss = 0.25666386\n",
      "Iteration 1121, loss = 0.25658433\n",
      "Iteration 1122, loss = 0.25653802\n",
      "Iteration 1123, loss = 0.25635026\n",
      "Iteration 1124, loss = 0.25626439\n",
      "Iteration 1125, loss = 0.25609972\n",
      "Iteration 1126, loss = 0.25610672\n",
      "Iteration 1127, loss = 0.25588244\n",
      "Iteration 1128, loss = 0.25576977\n",
      "Iteration 1129, loss = 0.25565437\n",
      "Iteration 1130, loss = 0.25553115\n",
      "Iteration 1131, loss = 0.25545553\n",
      "Iteration 1132, loss = 0.25533253\n",
      "Iteration 1133, loss = 0.25519293\n",
      "Iteration 1134, loss = 0.25507318\n",
      "Iteration 1135, loss = 0.25495771\n",
      "Iteration 1136, loss = 0.25486321\n",
      "Iteration 1137, loss = 0.25475987\n",
      "Iteration 1138, loss = 0.25460364\n",
      "Iteration 1139, loss = 0.25450054\n",
      "Iteration 1140, loss = 0.25440144\n",
      "Iteration 1141, loss = 0.25431213\n",
      "Iteration 1142, loss = 0.25416379\n",
      "Iteration 1143, loss = 0.25404467\n",
      "Iteration 1144, loss = 0.25394526\n",
      "Iteration 1145, loss = 0.25383111\n",
      "Iteration 1146, loss = 0.25371659\n",
      "Iteration 1147, loss = 0.25357949\n",
      "Iteration 1148, loss = 0.25349514\n",
      "Iteration 1149, loss = 0.25338876\n",
      "Iteration 1150, loss = 0.25332273\n",
      "Iteration 1151, loss = 0.25324438\n",
      "Iteration 1152, loss = 0.25305444\n",
      "Iteration 1153, loss = 0.25292787\n",
      "Iteration 1154, loss = 0.25280870\n",
      "Iteration 1155, loss = 0.25272571\n",
      "Iteration 1156, loss = 0.25256484\n",
      "Iteration 1157, loss = 0.25248252\n",
      "Iteration 1158, loss = 0.25239839\n",
      "Iteration 1159, loss = 0.25225624\n",
      "Iteration 1160, loss = 0.25216244\n",
      "Iteration 1161, loss = 0.25206123\n",
      "Iteration 1162, loss = 0.25197295\n",
      "Iteration 1163, loss = 0.25178965\n",
      "Iteration 1164, loss = 0.25166346\n",
      "Iteration 1165, loss = 0.25158613\n",
      "Iteration 1166, loss = 0.25143307\n",
      "Iteration 1167, loss = 0.25133762\n",
      "Iteration 1168, loss = 0.25119415\n",
      "Iteration 1169, loss = 0.25110028\n",
      "Iteration 1170, loss = 0.25097875\n",
      "Iteration 1171, loss = 0.25087350\n",
      "Iteration 1172, loss = 0.25075834\n",
      "Iteration 1173, loss = 0.25064256\n",
      "Iteration 1174, loss = 0.25053603\n",
      "Iteration 1175, loss = 0.25041305\n",
      "Iteration 1176, loss = 0.25028178\n",
      "Iteration 1177, loss = 0.25018381\n",
      "Iteration 1178, loss = 0.25010209\n",
      "Iteration 1179, loss = 0.24996603\n",
      "Iteration 1180, loss = 0.24986773\n",
      "Iteration 1181, loss = 0.24974460\n",
      "Iteration 1182, loss = 0.24961931\n",
      "Iteration 1183, loss = 0.24950745\n",
      "Iteration 1184, loss = 0.24940598\n",
      "Iteration 1185, loss = 0.24927722\n",
      "Iteration 1186, loss = 0.24917137\n",
      "Iteration 1187, loss = 0.24906968\n",
      "Iteration 1188, loss = 0.24895580\n",
      "Iteration 1189, loss = 0.24887220\n",
      "Iteration 1190, loss = 0.24872937\n",
      "Iteration 1191, loss = 0.24867237\n",
      "Iteration 1192, loss = 0.24853305\n",
      "Iteration 1193, loss = 0.24839042\n",
      "Iteration 1194, loss = 0.24827315\n",
      "Iteration 1195, loss = 0.24817447\n",
      "Iteration 1196, loss = 0.24803913\n",
      "Iteration 1197, loss = 0.24795210\n",
      "Iteration 1198, loss = 0.24782066\n",
      "Iteration 1199, loss = 0.24772180\n",
      "Iteration 1200, loss = 0.24759934\n",
      "Iteration 1201, loss = 0.24749641\n",
      "Iteration 1202, loss = 0.24736382\n",
      "Iteration 1203, loss = 0.24725402\n",
      "Iteration 1204, loss = 0.24720839\n",
      "Iteration 1205, loss = 0.24706178\n",
      "Iteration 1206, loss = 0.24694567\n",
      "Iteration 1207, loss = 0.24679649\n",
      "Iteration 1208, loss = 0.24670094\n",
      "Iteration 1209, loss = 0.24659415\n",
      "Iteration 1210, loss = 0.24649240\n",
      "Iteration 1211, loss = 0.24638132\n",
      "Iteration 1212, loss = 0.24627969\n",
      "Iteration 1213, loss = 0.24614387\n",
      "Iteration 1214, loss = 0.24606920\n",
      "Iteration 1215, loss = 0.24591113\n",
      "Iteration 1216, loss = 0.24593081\n",
      "Iteration 1217, loss = 0.24571291\n",
      "Iteration 1218, loss = 0.24562183\n",
      "Iteration 1219, loss = 0.24548569\n",
      "Iteration 1220, loss = 0.24538096\n",
      "Iteration 1221, loss = 0.24527166\n",
      "Iteration 1222, loss = 0.24516736\n",
      "Iteration 1223, loss = 0.24502157\n",
      "Iteration 1224, loss = 0.24493740\n",
      "Iteration 1225, loss = 0.24482786\n",
      "Iteration 1226, loss = 0.24470503\n",
      "Iteration 1227, loss = 0.24461784\n",
      "Iteration 1228, loss = 0.24447858\n",
      "Iteration 1229, loss = 0.24439113\n",
      "Iteration 1230, loss = 0.24426210\n",
      "Iteration 1231, loss = 0.24418294\n",
      "Iteration 1232, loss = 0.24406965\n",
      "Iteration 1233, loss = 0.24391253\n",
      "Iteration 1234, loss = 0.24382739\n",
      "Iteration 1235, loss = 0.24379768\n",
      "Iteration 1236, loss = 0.24361108\n",
      "Iteration 1237, loss = 0.24355183\n",
      "Iteration 1238, loss = 0.24337018\n",
      "Iteration 1239, loss = 0.24328288\n",
      "Iteration 1240, loss = 0.24323063\n",
      "Iteration 1241, loss = 0.24304464\n",
      "Iteration 1242, loss = 0.24292736\n",
      "Iteration 1243, loss = 0.24282961\n",
      "Iteration 1244, loss = 0.24274459\n",
      "Iteration 1245, loss = 0.24262384\n",
      "Iteration 1246, loss = 0.24247977\n",
      "Iteration 1247, loss = 0.24242076\n",
      "Iteration 1248, loss = 0.24227353\n",
      "Iteration 1249, loss = 0.24216042\n",
      "Iteration 1250, loss = 0.24204950\n",
      "Iteration 1251, loss = 0.24193834\n",
      "Iteration 1252, loss = 0.24189333\n",
      "Iteration 1253, loss = 0.24172204\n",
      "Iteration 1254, loss = 0.24159333\n",
      "Iteration 1255, loss = 0.24149675\n",
      "Iteration 1256, loss = 0.24147564\n",
      "Iteration 1257, loss = 0.24129695\n",
      "Iteration 1258, loss = 0.24116310\n",
      "Iteration 1259, loss = 0.24106401\n",
      "Iteration 1260, loss = 0.24095007\n",
      "Iteration 1261, loss = 0.24087140\n",
      "Iteration 1262, loss = 0.24070706\n",
      "Iteration 1263, loss = 0.24061738\n",
      "Iteration 1264, loss = 0.24048595\n",
      "Iteration 1265, loss = 0.24036037\n",
      "Iteration 1266, loss = 0.24032347\n",
      "Iteration 1267, loss = 0.24017785\n",
      "Iteration 1268, loss = 0.24006166\n",
      "Iteration 1269, loss = 0.23996869\n",
      "Iteration 1270, loss = 0.23989280\n",
      "Iteration 1271, loss = 0.23971702\n",
      "Iteration 1272, loss = 0.23961308\n",
      "Iteration 1273, loss = 0.23960857\n",
      "Iteration 1274, loss = 0.23940386\n",
      "Iteration 1275, loss = 0.23928298\n",
      "Iteration 1276, loss = 0.23918442\n",
      "Iteration 1277, loss = 0.23906463\n",
      "Iteration 1278, loss = 0.23895480\n",
      "Iteration 1279, loss = 0.23884920\n",
      "Iteration 1280, loss = 0.23876092\n",
      "Iteration 1281, loss = 0.23860881\n",
      "Iteration 1282, loss = 0.23852816\n",
      "Iteration 1283, loss = 0.23841422\n",
      "Iteration 1284, loss = 0.23834105\n",
      "Iteration 1285, loss = 0.23818147\n",
      "Iteration 1286, loss = 0.23812940\n",
      "Iteration 1287, loss = 0.23799045\n",
      "Iteration 1288, loss = 0.23787953\n",
      "Iteration 1289, loss = 0.23777679\n",
      "Iteration 1290, loss = 0.23764826\n",
      "Iteration 1291, loss = 0.23759228\n",
      "Iteration 1292, loss = 0.23743299\n",
      "Iteration 1293, loss = 0.23733735\n",
      "Iteration 1294, loss = 0.23723303\n",
      "Iteration 1295, loss = 0.23709250\n",
      "Iteration 1296, loss = 0.23698780\n",
      "Iteration 1297, loss = 0.23687818\n",
      "Iteration 1298, loss = 0.23676886\n",
      "Iteration 1299, loss = 0.23666158\n",
      "Iteration 1300, loss = 0.23651852\n",
      "Iteration 1301, loss = 0.23649697\n",
      "Iteration 1302, loss = 0.23631508\n",
      "Iteration 1303, loss = 0.23620451\n",
      "Iteration 790, loss = 0.28241323\n",
      "Iteration 791, loss = 0.28231616\n",
      "Iteration 792, loss = 0.28216038\n",
      "Iteration 793, loss = 0.28210121\n",
      "Iteration 794, loss = 0.28191869\n",
      "Iteration 795, loss = 0.28179356\n",
      "Iteration 796, loss = 0.28167704\n",
      "Iteration 797, loss = 0.28153473\n",
      "Iteration 798, loss = 0.28141454\n",
      "Iteration 799, loss = 0.28126935\n",
      "Iteration 800, loss = 0.28117334\n",
      "Iteration 801, loss = 0.28101501\n",
      "Iteration 802, loss = 0.28090859\n",
      "Iteration 803, loss = 0.28074664\n",
      "Iteration 804, loss = 0.28064273\n",
      "Iteration 805, loss = 0.28055904\n",
      "Iteration 806, loss = 0.28035134\n",
      "Iteration 807, loss = 0.28028234\n",
      "Iteration 808, loss = 0.28019898\n",
      "Iteration 809, loss = 0.27999712\n",
      "Iteration 810, loss = 0.27988528\n",
      "Iteration 811, loss = 0.27973167\n",
      "Iteration 812, loss = 0.27958747\n",
      "Iteration 813, loss = 0.27953964\n",
      "Iteration 814, loss = 0.27934099\n",
      "Iteration 815, loss = 0.27923769\n",
      "Iteration 816, loss = 0.27916161\n",
      "Iteration 817, loss = 0.27897654\n",
      "Iteration 818, loss = 0.27884506\n",
      "Iteration 819, loss = 0.27870759\n",
      "Iteration 820, loss = 0.27861871\n",
      "Iteration 821, loss = 0.27844542\n",
      "Iteration 822, loss = 0.27834405\n",
      "Iteration 823, loss = 0.27821787\n",
      "Iteration 824, loss = 0.27806412\n",
      "Iteration 825, loss = 0.27795253\n",
      "Iteration 826, loss = 0.27782240\n",
      "Iteration 827, loss = 0.27774249\n",
      "Iteration 828, loss = 0.27758618\n",
      "Iteration 829, loss = 0.27742624\n",
      "Iteration 830, loss = 0.27730611\n",
      "Iteration 831, loss = 0.27719514\n",
      "Iteration 832, loss = 0.27704123\n",
      "Iteration 833, loss = 0.27700389\n",
      "Iteration 834, loss = 0.27681022\n",
      "Iteration 835, loss = 0.27671942\n",
      "Iteration 836, loss = 0.27653770\n",
      "Iteration 837, loss = 0.27641628\n",
      "Iteration 838, loss = 0.27631145\n",
      "Iteration 839, loss = 0.27619550\n",
      "Iteration 840, loss = 0.27603064\n",
      "Iteration 841, loss = 0.27589489\n",
      "Iteration 842, loss = 0.27579710\n",
      "Iteration 843, loss = 0.27566691\n",
      "Iteration 844, loss = 0.27555583\n",
      "Iteration 845, loss = 0.27542009\n",
      "Iteration 846, loss = 0.27528531\n",
      "Iteration 847, loss = 0.27517491\n",
      "Iteration 848, loss = 0.27503993\n",
      "Iteration 849, loss = 0.27492772\n",
      "Iteration 850, loss = 0.27479190\n",
      "Iteration 851, loss = 0.27464879\n",
      "Iteration 852, loss = 0.27453744\n",
      "Iteration 853, loss = 0.27443760\n",
      "Iteration 854, loss = 0.27431503\n",
      "Iteration 855, loss = 0.27419134\n",
      "Iteration 856, loss = 0.27404634\n",
      "Iteration 857, loss = 0.27390914\n",
      "Iteration 858, loss = 0.27379866\n",
      "Iteration 859, loss = 0.27366419\n",
      "Iteration 860, loss = 0.27357397\n",
      "Iteration 861, loss = 0.27345046\n",
      "Iteration 862, loss = 0.27331938\n",
      "Iteration 863, loss = 0.27318251\n",
      "Iteration 864, loss = 0.27306459\n",
      "Iteration 865, loss = 0.27293200\n",
      "Iteration 866, loss = 0.27283861\n",
      "Iteration 867, loss = 0.27267222\n",
      "Iteration 868, loss = 0.27255411\n",
      "Iteration 869, loss = 0.27243121\n",
      "Iteration 870, loss = 0.27231474\n",
      "Iteration 871, loss = 0.27218352\n",
      "Iteration 872, loss = 0.27206158\n",
      "Iteration 873, loss = 0.27195644\n",
      "Iteration 874, loss = 0.27180553\n",
      "Iteration 875, loss = 0.27168124\n",
      "Iteration 876, loss = 0.27156735\n",
      "Iteration 877, loss = 0.27144033\n",
      "Iteration 878, loss = 0.27132001\n",
      "Iteration 879, loss = 0.27122331\n",
      "Iteration 880, loss = 0.27108540\n",
      "Iteration 881, loss = 0.27095855\n",
      "Iteration 882, loss = 0.27081307\n",
      "Iteration 883, loss = 0.27074971\n",
      "Iteration 884, loss = 0.27056652\n",
      "Iteration 885, loss = 0.27043009\n",
      "Iteration 886, loss = 0.27033303\n",
      "Iteration 887, loss = 0.27020646\n",
      "Iteration 888, loss = 0.27005251\n",
      "Iteration 889, loss = 0.26993548\n",
      "Iteration 890, loss = 0.26982024\n",
      "Iteration 891, loss = 0.26969789\n",
      "Iteration 892, loss = 0.26959345\n",
      "Iteration 893, loss = 0.26944381\n",
      "Iteration 894, loss = 0.26935264\n",
      "Iteration 895, loss = 0.26923606\n",
      "Iteration 896, loss = 0.26906296\n",
      "Iteration 897, loss = 0.26894470\n",
      "Iteration 898, loss = 0.26883840\n",
      "Iteration 899, loss = 0.26869129\n",
      "Iteration 900, loss = 0.26857444\n",
      "Iteration 901, loss = 0.26845655\n",
      "Iteration 902, loss = 0.26835056\n",
      "Iteration 903, loss = 0.26819839\n",
      "Iteration 904, loss = 0.26807195\n",
      "Iteration 905, loss = 0.26800534\n",
      "Iteration 906, loss = 0.26782771\n",
      "Iteration 907, loss = 0.26772377\n",
      "Iteration 908, loss = 0.26759912\n",
      "Iteration 909, loss = 0.26749281\n",
      "Iteration 910, loss = 0.26732563\n",
      "Iteration 911, loss = 0.26720573\n",
      "Iteration 912, loss = 0.26707750\n",
      "Iteration 913, loss = 0.26699385\n",
      "Iteration 914, loss = 0.26682797\n",
      "Iteration 915, loss = 0.26670934\n",
      "Iteration 916, loss = 0.26656194\n",
      "Iteration 917, loss = 0.26645154\n",
      "Iteration 918, loss = 0.26635536\n",
      "Iteration 919, loss = 0.26622961\n",
      "Iteration 920, loss = 0.26610706\n",
      "Iteration 921, loss = 0.26596978\n",
      "Iteration 922, loss = 0.26583174\n",
      "Iteration 923, loss = 0.26577091\n",
      "Iteration 924, loss = 0.26561094\n",
      "Iteration 925, loss = 0.26546801\n",
      "Iteration 926, loss = 0.26534173\n",
      "Iteration 927, loss = 0.26520634\n",
      "Iteration 928, loss = 0.26513970\n",
      "Iteration 929, loss = 0.26496714\n",
      "Iteration 930, loss = 0.26488176\n",
      "Iteration 931, loss = 0.26470942\n",
      "Iteration 932, loss = 0.26458195\n",
      "Iteration 933, loss = 0.26446595\n",
      "Iteration 934, loss = 0.26437382\n",
      "Iteration 935, loss = 0.26421953\n",
      "Iteration 936, loss = 0.26416138\n",
      "Iteration 937, loss = 0.26400534\n",
      "Iteration 938, loss = 0.26393118\n",
      "Iteration 939, loss = 0.26373442\n",
      "Iteration 940, loss = 0.26365582\n",
      "Iteration 941, loss = 0.26355824\n",
      "Iteration 942, loss = 0.26336611\n",
      "Iteration 943, loss = 0.26323672\n",
      "Iteration 944, loss = 0.26311765\n",
      "Iteration 945, loss = 0.26299674\n",
      "Iteration 946, loss = 0.26286281\n",
      "Iteration 947, loss = 0.26277671\n",
      "Iteration 948, loss = 0.26263500\n",
      "Iteration 949, loss = 0.26250785\n",
      "Iteration 950, loss = 0.26238293\n",
      "Iteration 951, loss = 0.26225491\n",
      "Iteration 952, loss = 0.26218529\n",
      "Iteration 953, loss = 0.26201409\n",
      "Iteration 954, loss = 0.26189392\n",
      "Iteration 955, loss = 0.26175884\n",
      "Iteration 956, loss = 0.26167654\n",
      "Iteration 957, loss = 0.26151290\n",
      "Iteration 958, loss = 0.26141747\n",
      "Iteration 959, loss = 0.26126782\n",
      "Iteration 960, loss = 0.26115358\n",
      "Iteration 961, loss = 0.26101914\n",
      "Iteration 962, loss = 0.26091971\n",
      "Iteration 963, loss = 0.26079819\n",
      "Iteration 964, loss = 0.26065876\n",
      "Iteration 965, loss = 0.26054525\n",
      "Iteration 966, loss = 0.26040216\n",
      "Iteration 967, loss = 0.26028221\n",
      "Iteration 968, loss = 0.26017564\n",
      "Iteration 969, loss = 0.26011376\n",
      "Iteration 970, loss = 0.25994168\n",
      "Iteration 971, loss = 0.25979792\n",
      "Iteration 972, loss = 0.25970034\n",
      "Iteration 973, loss = 0.25959682\n",
      "Iteration 974, loss = 0.25946674\n",
      "Iteration 975, loss = 0.25932278\n",
      "Iteration 976, loss = 0.25918339\n",
      "Iteration 977, loss = 0.25906728\n",
      "Iteration 978, loss = 0.25898252\n",
      "Iteration 979, loss = 0.25883444\n",
      "Iteration 980, loss = 0.25871715\n",
      "Iteration 981, loss = 0.25859431\n",
      "Iteration 982, loss = 0.25845491\n",
      "Iteration 983, loss = 0.25832710\n",
      "Iteration 984, loss = 0.25824741\n",
      "Iteration 985, loss = 0.25808695\n",
      "Iteration 986, loss = 0.25802163\n",
      "Iteration 987, loss = 0.25785969\n",
      "Iteration 988, loss = 0.25775562\n",
      "Iteration 989, loss = 0.25772145\n",
      "Iteration 990, loss = 0.25751815\n",
      "Iteration 991, loss = 0.25736727\n",
      "Iteration 992, loss = 0.25727017\n",
      "Iteration 993, loss = 0.25716057\n",
      "Iteration 994, loss = 0.25697740\n",
      "Iteration 995, loss = 0.25686252\n",
      "Iteration 996, loss = 0.25676294\n",
      "Iteration 997, loss = 0.25665855\n",
      "Iteration 998, loss = 0.25651036\n",
      "Iteration 999, loss = 0.25638349\n",
      "Iteration 1000, loss = 0.25624762\n",
      "Iteration 1001, loss = 0.25617189\n",
      "Iteration 1002, loss = 0.25602632\n",
      "Iteration 1003, loss = 0.25588987\n",
      "Iteration 1004, loss = 0.25583488\n",
      "Iteration 1005, loss = 0.25566616\n",
      "Iteration 1006, loss = 0.25554397\n",
      "Iteration 1007, loss = 0.25540435\n",
      "Iteration 1008, loss = 0.25528744\n",
      "Iteration 1009, loss = 0.25515872\n",
      "Iteration 1010, loss = 0.25510203\n",
      "Iteration 1011, loss = 0.25496835\n",
      "Iteration 1012, loss = 0.25481099\n",
      "Iteration 1013, loss = 0.25467640\n",
      "Iteration 1014, loss = 0.25456893\n",
      "Iteration 1015, loss = 0.25445573\n",
      "Iteration 1016, loss = 0.25434991\n",
      "Iteration 1017, loss = 0.25421605\n",
      "Iteration 1018, loss = 0.25422750\n",
      "Iteration 1019, loss = 0.25399439\n",
      "Iteration 1020, loss = 0.25383880\n",
      "Iteration 1021, loss = 0.25377855\n",
      "Iteration 1022, loss = 0.25358989\n",
      "Iteration 1023, loss = 0.25347122\n",
      "Iteration 1024, loss = 0.25337677\n",
      "Iteration 1025, loss = 0.25327758\n",
      "Iteration 1026, loss = 0.25313239\n",
      "Iteration 1027, loss = 0.25298760\n",
      "Iteration 1028, loss = 0.25296376\n",
      "Iteration 1029, loss = 0.25277110\n",
      "Iteration 1030, loss = 0.25265789\n",
      "Iteration 1031, loss = 0.25250719\n",
      "Iteration 1032, loss = 0.25241374\n",
      "Iteration 1033, loss = 0.25227262\n",
      "Iteration 1034, loss = 0.25219256\n",
      "Iteration 1035, loss = 0.25204586\n",
      "Iteration 1036, loss = 0.25195325\n",
      "Iteration 1037, loss = 0.25179517\n",
      "Iteration 1038, loss = 0.25192018\n",
      "Iteration 1039, loss = 0.25157016\n",
      "Iteration 1040, loss = 0.25144229\n",
      "Iteration 1041, loss = 0.25131952\n",
      "Iteration 1042, loss = 0.25121588\n",
      "Iteration 1043, loss = 0.25108033\n",
      "Iteration 1044, loss = 0.25095240\n",
      "Iteration 1045, loss = 0.25085233\n",
      "Iteration 1046, loss = 0.25076026\n",
      "Iteration 1047, loss = 0.25060111\n",
      "Iteration 1048, loss = 0.25047648\n",
      "Iteration 1049, loss = 0.25041466\n",
      "Iteration 1050, loss = 0.25025101\n",
      "Iteration 1051, loss = 0.25014506\n",
      "Iteration 1052, loss = 0.24999396\n",
      "Iteration 1053, loss = 0.24988260\n",
      "Iteration 1054, loss = 0.24980476\n",
      "Iteration 1055, loss = 0.24967483\n",
      "Iteration 1056, loss = 0.24951938\n",
      "Iteration 1057, loss = 0.24939067\n",
      "Iteration 1058, loss = 0.24929058\n",
      "Iteration 1059, loss = 0.24915752\n",
      "Iteration 1060, loss = 0.24904669\n",
      "Iteration 1061, loss = 0.24898872\n",
      "Iteration 1062, loss = 0.24876817\n",
      "Iteration 1063, loss = 0.24866148\n",
      "Iteration 1064, loss = 0.24855837\n",
      "Iteration 1065, loss = 0.24842122\n",
      "Iteration 1066, loss = 0.24832126\n",
      "Iteration 1067, loss = 0.24818985\n",
      "Iteration 1068, loss = 0.24807139\n",
      "Iteration 1069, loss = 0.24792387\n",
      "Iteration 1070, loss = 0.24781992\n",
      "Iteration 1071, loss = 0.24767536\n",
      "Iteration 1072, loss = 0.24756636\n",
      "Iteration 1073, loss = 0.24743332\n",
      "Iteration 1074, loss = 0.24732074\n",
      "Iteration 1075, loss = 0.24719336\n",
      "Iteration 1076, loss = 0.24705046\n",
      "Iteration 1077, loss = 0.24695580\n",
      "Iteration 1078, loss = 0.24682042\n",
      "Iteration 1079, loss = 0.24671448\n",
      "Iteration 1080, loss = 0.24658024\n",
      "Iteration 1081, loss = 0.24645467\n",
      "Iteration 1082, loss = 0.24641863\n",
      "Iteration 1083, loss = 0.24624182\n",
      "Iteration 1084, loss = 0.24608693\n",
      "Iteration 1085, loss = 0.24596933\n",
      "Iteration 1086, loss = 0.24584953\n",
      "Iteration 1087, loss = 0.24581128\n",
      "Iteration 1088, loss = 0.24573213\n",
      "Iteration 1089, loss = 0.24553723\n",
      "Iteration 1090, loss = 0.24540057\n",
      "Iteration 1091, loss = 0.24524023\n",
      "Iteration 1092, loss = 0.24515486\n",
      "Iteration 1093, loss = 0.24503606\n",
      "Iteration 1094, loss = 0.24492210\n",
      "Iteration 1095, loss = 0.24480130\n",
      "Iteration 1096, loss = 0.24463601\n",
      "Iteration 1097, loss = 0.24451221\n",
      "Iteration 1098, loss = 0.24449005\n",
      "Iteration 1099, loss = 0.24427927\n",
      "Iteration 1100, loss = 0.24419707\n",
      "Iteration 1101, loss = 0.24406970\n",
      "Iteration 1102, loss = 0.24392534\n",
      "Iteration 1103, loss = 0.24383467\n",
      "Iteration 1104, loss = 0.24367818\n",
      "Iteration 1105, loss = 0.24356966\n",
      "Iteration 1106, loss = 0.24344563\n",
      "Iteration 1107, loss = 0.24332012\n",
      "Iteration 1108, loss = 0.24320108\n",
      "Iteration 1109, loss = 0.24308002\n",
      "Iteration 1110, loss = 0.24299354\n",
      "Iteration 1111, loss = 0.24283802\n",
      "Iteration 1112, loss = 0.24273766\n",
      "Iteration 1113, loss = 0.24260863\n",
      "Iteration 1114, loss = 0.24246134\n",
      "Iteration 1115, loss = 0.24237059\n",
      "Iteration 1116, loss = 0.24230822\n",
      "Iteration 1117, loss = 0.24214252\n",
      "Iteration 1118, loss = 0.24203891\n",
      "Iteration 1119, loss = 0.24186346\n",
      "Iteration 1120, loss = 0.24174302\n",
      "Iteration 1121, loss = 0.24166678\n",
      "Iteration 1122, loss = 0.24151849\n",
      "Iteration 1123, loss = 0.24138836\n",
      "Iteration 1124, loss = 0.24134642\n",
      "Iteration 1125, loss = 0.24113509\n",
      "Iteration 1126, loss = 0.24101382\n",
      "Iteration 1127, loss = 0.24094598\n",
      "Iteration 1128, loss = 0.24077725\n",
      "Iteration 1129, loss = 0.24066825\n",
      "Iteration 1130, loss = 0.24053371\n",
      "Iteration 1131, loss = 0.24041915\n",
      "Iteration 1132, loss = 0.24029818\n",
      "Iteration 1133, loss = 0.24018081\n",
      "Iteration 1134, loss = 0.24006216\n",
      "Iteration 1135, loss = 0.23993203\n",
      "Iteration 1136, loss = 0.23982629\n",
      "Iteration 1137, loss = 0.23971855\n",
      "Iteration 1138, loss = 0.23959747\n",
      "Iteration 1139, loss = 0.23945915\n",
      "Iteration 1140, loss = 0.23937686\n",
      "Iteration 1141, loss = 0.23921159\n",
      "Iteration 1142, loss = 0.23909039\n",
      "Iteration 1143, loss = 0.23899241\n",
      "Iteration 1144, loss = 0.23882839\n",
      "Iteration 1145, loss = 0.23872720\n",
      "Iteration 1146, loss = 0.23859475\n",
      "Iteration 1147, loss = 0.23846011\n",
      "Iteration 1148, loss = 0.23834913\n",
      "Iteration 1149, loss = 0.23820685\n",
      "Iteration 1150, loss = 0.23808349\n",
      "Iteration 1151, loss = 0.23798643\n",
      "Iteration 1152, loss = 0.23785955\n",
      "Iteration 1153, loss = 0.23771135\n",
      "Iteration 1154, loss = 0.23759962\n",
      "Iteration 1155, loss = 0.23751445\n",
      "Iteration 1156, loss = 0.23737365\n",
      "Iteration 1157, loss = 0.23738282\n",
      "Iteration 1158, loss = 0.23714823\n",
      "Iteration 1159, loss = 0.23699677\n",
      "Iteration 1160, loss = 0.23689391\n",
      "Iteration 1161, loss = 0.23675592\n",
      "Iteration 1162, loss = 0.23661591\n",
      "Iteration 1163, loss = 0.23652455\n",
      "Iteration 1164, loss = 0.23639641\n",
      "Iteration 1165, loss = 0.23624017\n",
      "Iteration 1166, loss = 0.23613515\n",
      "Iteration 1167, loss = 0.23600395\n",
      "Iteration 1168, loss = 0.23589377\n",
      "Iteration 1169, loss = 0.23577600\n",
      "Iteration 1170, loss = 0.23565135\n",
      "Iteration 1171, loss = 0.23555147\n",
      "Iteration 1172, loss = 0.23540890\n",
      "Iteration 1173, loss = 0.23528914\n",
      "Iteration 1174, loss = 0.23520611\n",
      "Iteration 1175, loss = 0.23503811\n",
      "Iteration 1176, loss = 0.23493647\n",
      "Iteration 1177, loss = 0.23479598\n",
      "Iteration 1178, loss = 0.23474884\n",
      "Iteration 1179, loss = 0.23454411\n",
      "Iteration 1180, loss = 0.23442739\n",
      "Iteration 1181, loss = 0.23429566\n",
      "Iteration 1182, loss = 0.23417494\n",
      "Iteration 1183, loss = 0.23404001\n",
      "Iteration 1184, loss = 0.23399498\n",
      "Iteration 1185, loss = 0.23380541\n",
      "Iteration 1186, loss = 0.23368414\n",
      "Iteration 1187, loss = 0.23360388\n",
      "Iteration 1188, loss = 0.23345026\n",
      "Iteration 1189, loss = 0.23333064\n",
      "Iteration 1190, loss = 0.23319478\n",
      "Iteration 1191, loss = 0.23315565\n",
      "Iteration 1192, loss = 0.23300543\n",
      "Iteration 1193, loss = 0.23288627\n",
      "Iteration 1194, loss = 0.23271160\n",
      "Iteration 1195, loss = 0.23258017\n",
      "Iteration 1196, loss = 0.23247981\n",
      "Iteration 1197, loss = 0.23236911\n",
      "Iteration 1198, loss = 0.23222680\n",
      "Iteration 1199, loss = 0.23213503\n",
      "Iteration 1200, loss = 0.23200201\n",
      "Iteration 1201, loss = 0.23188944\n",
      "Iteration 1202, loss = 0.23171656\n",
      "Iteration 1203, loss = 0.23162241\n",
      "Iteration 1204, loss = 0.23149833\n",
      "Iteration 1205, loss = 0.23136827\n",
      "Iteration 1206, loss = 0.23125125\n",
      "Iteration 1207, loss = 0.23117838\n",
      "Iteration 1208, loss = 0.23104900\n",
      "Iteration 1209, loss = 0.23088922\n",
      "Iteration 1210, loss = 0.23080077\n",
      "Iteration 1211, loss = 0.23064958\n",
      "Iteration 1212, loss = 0.23051221\n",
      "Iteration 1213, loss = 0.23042786\n",
      "Iteration 1214, loss = 0.23025328\n",
      "Iteration 1215, loss = 0.23016304\n",
      "Iteration 1216, loss = 0.23002992\n",
      "Iteration 1217, loss = 0.22993648\n",
      "Iteration 1218, loss = 0.22987981\n",
      "Iteration 1219, loss = 0.22972072\n",
      "Iteration 1220, loss = 0.22957501\n",
      "Iteration 1221, loss = 0.22945093\n",
      "Iteration 1222, loss = 0.22929773\n",
      "Iteration 1223, loss = 0.22923587\n",
      "Iteration 1224, loss = 0.22906391\n",
      "Iteration 1225, loss = 0.22894887\n",
      "Iteration 1226, loss = 0.22886005\n",
      "Iteration 1227, loss = 0.22872168\n",
      "Iteration 1228, loss = 0.22859326\n",
      "Iteration 1229, loss = 0.22851445\n",
      "Iteration 1230, loss = 0.22836761\n",
      "Iteration 1231, loss = 0.22825967\n",
      "Iteration 1232, loss = 0.22809988\n",
      "Iteration 1233, loss = 0.22799903\n",
      "Iteration 1234, loss = 0.22789759\n",
      "Iteration 1235, loss = 0.22773329\n",
      "Iteration 1236, loss = 0.22768139\n",
      "Iteration 1237, loss = 0.22753710\n",
      "Iteration 1238, loss = 0.22740216\n",
      "Iteration 1239, loss = 0.22726201\n",
      "Iteration 1240, loss = 0.22714028\n",
      "Iteration 1241, loss = 0.22704812\n",
      "Iteration 1242, loss = 0.22689900\n",
      "Iteration 1243, loss = 0.22679569\n",
      "Iteration 1244, loss = 0.22668189\n",
      "Iteration 1245, loss = 0.22656780\n",
      "Iteration 1246, loss = 0.22644808\n",
      "Iteration 1247, loss = 0.22636724\n",
      "Iteration 1248, loss = 0.22620239\n",
      "Iteration 1249, loss = 0.22610608\n",
      "Iteration 1250, loss = 0.22600716\n",
      "Iteration 1251, loss = 0.22586905\n",
      "Iteration 1252, loss = 0.22572703\n",
      "Iteration 1253, loss = 0.22561460\n",
      "Iteration 1254, loss = 0.22551524\n",
      "Iteration 1255, loss = 0.22540722\n",
      "Iteration 1256, loss = 0.22524771\n",
      "Iteration 1257, loss = 0.22514520\n",
      "Iteration 1258, loss = 0.22506027\n",
      "Iteration 1259, loss = 0.22491033\n",
      "Iteration 1260, loss = 0.22482585\n",
      "Iteration 1261, loss = 0.22468856\n",
      "Iteration 1262, loss = 0.22459553\n",
      "Iteration 1263, loss = 0.22442229\n",
      "Iteration 1264, loss = 0.22435669\n",
      "Iteration 1265, loss = 0.22419167\n",
      "Iteration 1266, loss = 0.22409114\n",
      "Iteration 1267, loss = 0.22397249\n",
      "Iteration 1268, loss = 0.22383873\n",
      "Iteration 1269, loss = 0.22375518\n",
      "Iteration 1270, loss = 0.22360052\n",
      "Iteration 1271, loss = 0.22348347\n",
      "Iteration 1272, loss = 0.22336373\n",
      "Iteration 1273, loss = 0.22329751\n",
      "Iteration 1274, loss = 0.22312239\n",
      "Iteration 1275, loss = 0.22303538\n",
      "Iteration 1276, loss = 0.22294022\n",
      "Iteration 749, loss = 0.33591537\n",
      "Iteration 750, loss = 0.33573280\n",
      "Iteration 751, loss = 0.33561748\n",
      "Iteration 752, loss = 0.33546599\n",
      "Iteration 753, loss = 0.33534766\n",
      "Iteration 754, loss = 0.33523387\n",
      "Iteration 755, loss = 0.33509347\n",
      "Iteration 756, loss = 0.33498856\n",
      "Iteration 757, loss = 0.33487281\n",
      "Iteration 758, loss = 0.33472044\n",
      "Iteration 759, loss = 0.33457452\n",
      "Iteration 760, loss = 0.33445386\n",
      "Iteration 761, loss = 0.33434578\n",
      "Iteration 762, loss = 0.33419863\n",
      "Iteration 763, loss = 0.33407559\n",
      "Iteration 764, loss = 0.33395410\n",
      "Iteration 765, loss = 0.33385151\n",
      "Iteration 766, loss = 0.33367644\n",
      "Iteration 767, loss = 0.33357728\n",
      "Iteration 768, loss = 0.33345366\n",
      "Iteration 769, loss = 0.33331001\n",
      "Iteration 770, loss = 0.33326131\n",
      "Iteration 771, loss = 0.33305035\n",
      "Iteration 772, loss = 0.33291355\n",
      "Iteration 773, loss = 0.33279851\n",
      "Iteration 774, loss = 0.33264525\n",
      "Iteration 775, loss = 0.33252675\n",
      "Iteration 776, loss = 0.33239990\n",
      "Iteration 777, loss = 0.33226233\n",
      "Iteration 778, loss = 0.33220851\n",
      "Iteration 779, loss = 0.33201583\n",
      "Iteration 780, loss = 0.33187549\n",
      "Iteration 781, loss = 0.33174861\n",
      "Iteration 782, loss = 0.33163618\n",
      "Iteration 783, loss = 0.33148803\n",
      "Iteration 784, loss = 0.33139173\n",
      "Iteration 785, loss = 0.33123920\n",
      "Iteration 786, loss = 0.33114433\n",
      "Iteration 787, loss = 0.33100886\n",
      "Iteration 788, loss = 0.33083661\n",
      "Iteration 789, loss = 0.33074090\n",
      "Iteration 790, loss = 0.33062352\n",
      "Iteration 791, loss = 0.33048351\n",
      "Iteration 792, loss = 0.33033595\n",
      "Iteration 793, loss = 0.33019589\n",
      "Iteration 794, loss = 0.33005874\n",
      "Iteration 795, loss = 0.32993566\n",
      "Iteration 796, loss = 0.32983093\n",
      "Iteration 797, loss = 0.32972152\n",
      "Iteration 798, loss = 0.32953688\n",
      "Iteration 799, loss = 0.32943295\n",
      "Iteration 800, loss = 0.32930986\n",
      "Iteration 801, loss = 0.32918953\n",
      "Iteration 802, loss = 0.32906691\n",
      "Iteration 803, loss = 0.32894096\n",
      "Iteration 804, loss = 0.32875584\n",
      "Iteration 805, loss = 0.32863598\n",
      "Iteration 806, loss = 0.32851978\n",
      "Iteration 807, loss = 0.32839606\n",
      "Iteration 808, loss = 0.32824312\n",
      "Iteration 809, loss = 0.32810734\n",
      "Iteration 810, loss = 0.32798915\n",
      "Iteration 811, loss = 0.32787896\n",
      "Iteration 812, loss = 0.32778638\n",
      "Iteration 813, loss = 0.32761635\n",
      "Iteration 814, loss = 0.32749052\n",
      "Iteration 815, loss = 0.32741890\n",
      "Iteration 816, loss = 0.32724461\n",
      "Iteration 817, loss = 0.32709597\n",
      "Iteration 818, loss = 0.32698212\n",
      "Iteration 819, loss = 0.32682856\n",
      "Iteration 820, loss = 0.32672427\n",
      "Iteration 821, loss = 0.32656740\n",
      "Iteration 822, loss = 0.32644649\n",
      "Iteration 823, loss = 0.32633977\n",
      "Iteration 824, loss = 0.32620317\n",
      "Iteration 825, loss = 0.32606925\n",
      "Iteration 826, loss = 0.32594293\n",
      "Iteration 827, loss = 0.32581589\n",
      "Iteration 828, loss = 0.32568014\n",
      "Iteration 829, loss = 0.32553808\n",
      "Iteration 830, loss = 0.32545940\n",
      "Iteration 831, loss = 0.32528078\n",
      "Iteration 832, loss = 0.32514874\n",
      "Iteration 833, loss = 0.32501258\n",
      "Iteration 834, loss = 0.32488745\n",
      "Iteration 835, loss = 0.32473876\n",
      "Iteration 836, loss = 0.32462292\n",
      "Iteration 837, loss = 0.32455792\n",
      "Iteration 838, loss = 0.32438153\n",
      "Iteration 839, loss = 0.32424731\n",
      "Iteration 840, loss = 0.32409145\n",
      "Iteration 841, loss = 0.32396742\n",
      "Iteration 842, loss = 0.32383287\n",
      "Iteration 843, loss = 0.32370931\n",
      "Iteration 844, loss = 0.32360284\n",
      "Iteration 845, loss = 0.32346007\n",
      "Iteration 846, loss = 0.32332008\n",
      "Iteration 847, loss = 0.32317548\n",
      "Iteration 848, loss = 0.32305115\n",
      "Iteration 849, loss = 0.32293674\n",
      "Iteration 850, loss = 0.32278234\n",
      "Iteration 851, loss = 0.32265999\n",
      "Iteration 852, loss = 0.32252574\n",
      "Iteration 853, loss = 0.32239160\n",
      "Iteration 854, loss = 0.32229906\n",
      "Iteration 855, loss = 0.32214645\n",
      "Iteration 856, loss = 0.32199682\n",
      "Iteration 857, loss = 0.32185558\n",
      "Iteration 858, loss = 0.32175190\n",
      "Iteration 859, loss = 0.32161286\n",
      "Iteration 860, loss = 0.32145095\n",
      "Iteration 861, loss = 0.32133279\n",
      "Iteration 862, loss = 0.32121271\n",
      "Iteration 863, loss = 0.32107232\n",
      "Iteration 864, loss = 0.32094012\n",
      "Iteration 865, loss = 0.32083116\n",
      "Iteration 866, loss = 0.32066178\n",
      "Iteration 867, loss = 0.32053999\n",
      "Iteration 868, loss = 0.32046978\n",
      "Iteration 869, loss = 0.32028690\n",
      "Iteration 870, loss = 0.32014399\n",
      "Iteration 871, loss = 0.32001673\n",
      "Iteration 872, loss = 0.31990224\n",
      "Iteration 873, loss = 0.31979057\n",
      "Iteration 874, loss = 0.31962288\n",
      "Iteration 875, loss = 0.31953321\n",
      "Iteration 876, loss = 0.31934770\n",
      "Iteration 877, loss = 0.31923844\n",
      "Iteration 878, loss = 0.31912878\n",
      "Iteration 879, loss = 0.31895264\n",
      "Iteration 880, loss = 0.31881423\n",
      "Iteration 881, loss = 0.31872595\n",
      "Iteration 882, loss = 0.31856572\n",
      "Iteration 883, loss = 0.31846622\n",
      "Iteration 884, loss = 0.31831846\n",
      "Iteration 885, loss = 0.31817560\n",
      "Iteration 886, loss = 0.31808718\n",
      "Iteration 887, loss = 0.31789978\n",
      "Iteration 888, loss = 0.31780950\n",
      "Iteration 889, loss = 0.31765405\n",
      "Iteration 890, loss = 0.31753006\n",
      "Iteration 891, loss = 0.31740774\n",
      "Iteration 892, loss = 0.31723132\n",
      "Iteration 893, loss = 0.31725172\n",
      "Iteration 894, loss = 0.31697694\n",
      "Iteration 895, loss = 0.31683163\n",
      "Iteration 896, loss = 0.31675527\n",
      "Iteration 897, loss = 0.31657901\n",
      "Iteration 898, loss = 0.31653330\n",
      "Iteration 899, loss = 0.31641132\n",
      "Iteration 900, loss = 0.31630171\n",
      "Iteration 901, loss = 0.31606227\n",
      "Iteration 902, loss = 0.31598382\n",
      "Iteration 903, loss = 0.31579583\n",
      "Iteration 904, loss = 0.31566387\n",
      "Iteration 905, loss = 0.31553730\n",
      "Iteration 906, loss = 0.31544049\n",
      "Iteration 907, loss = 0.31536833\n",
      "Iteration 908, loss = 0.31512653\n",
      "Iteration 909, loss = 0.31505047\n",
      "Iteration 910, loss = 0.31486042\n",
      "Iteration 911, loss = 0.31476094\n",
      "Iteration 912, loss = 0.31461607\n",
      "Iteration 913, loss = 0.31446108\n",
      "Iteration 914, loss = 0.31436128\n",
      "Iteration 915, loss = 0.31421275\n",
      "Iteration 916, loss = 0.31406648\n",
      "Iteration 917, loss = 0.31396987\n",
      "Iteration 918, loss = 0.31385601\n",
      "Iteration 919, loss = 0.31368117\n",
      "Iteration 920, loss = 0.31353780\n",
      "Iteration 921, loss = 0.31340242\n",
      "Iteration 922, loss = 0.31334095\n",
      "Iteration 923, loss = 0.31314553\n",
      "Iteration 924, loss = 0.31301636\n",
      "Iteration 925, loss = 0.31293590\n",
      "Iteration 926, loss = 0.31279278\n",
      "Iteration 927, loss = 0.31262862\n",
      "Iteration 928, loss = 0.31247648\n",
      "Iteration 929, loss = 0.31238413\n",
      "Iteration 930, loss = 0.31222105\n",
      "Iteration 931, loss = 0.31211391\n",
      "Iteration 932, loss = 0.31194313\n",
      "Iteration 933, loss = 0.31183500\n",
      "Iteration 934, loss = 0.31169665\n",
      "Iteration 935, loss = 0.31155086\n",
      "Iteration 936, loss = 0.31144557\n",
      "Iteration 937, loss = 0.31132045\n",
      "Iteration 938, loss = 0.31113822\n",
      "Iteration 939, loss = 0.31102706\n",
      "Iteration 940, loss = 0.31091160\n",
      "Iteration 941, loss = 0.31077565\n",
      "Iteration 942, loss = 0.31063856\n",
      "Iteration 943, loss = 0.31050151\n",
      "Iteration 944, loss = 0.31034769\n",
      "Iteration 945, loss = 0.31020509\n",
      "Iteration 946, loss = 0.31009482\n",
      "Iteration 947, loss = 0.30999562\n",
      "Iteration 948, loss = 0.30986155\n",
      "Iteration 949, loss = 0.30969920\n",
      "Iteration 950, loss = 0.30955486\n",
      "Iteration 951, loss = 0.30943813\n",
      "Iteration 952, loss = 0.30929047\n",
      "Iteration 953, loss = 0.30919966\n",
      "Iteration 954, loss = 0.30906345\n",
      "Iteration 955, loss = 0.30890766\n",
      "Iteration 956, loss = 0.30876926\n",
      "Iteration 957, loss = 0.30862441\n",
      "Iteration 958, loss = 0.30849925\n",
      "Iteration 959, loss = 0.30835881\n",
      "Iteration 960, loss = 0.30823717\n",
      "Iteration 961, loss = 0.30809437\n",
      "Iteration 962, loss = 0.30799117\n",
      "Iteration 963, loss = 0.30783354\n",
      "Iteration 964, loss = 0.30770932\n",
      "Iteration 965, loss = 0.30755116\n",
      "Iteration 966, loss = 0.30743545\n",
      "Iteration 967, loss = 0.30733401\n",
      "Iteration 968, loss = 0.30716878\n",
      "Iteration 969, loss = 0.30702919\n",
      "Iteration 970, loss = 0.30697942\n",
      "Iteration 971, loss = 0.30677095\n",
      "Iteration 972, loss = 0.30667065\n",
      "Iteration 973, loss = 0.30649325\n",
      "Iteration 974, loss = 0.30637147\n",
      "Iteration 975, loss = 0.30624891\n",
      "Iteration 976, loss = 0.30615351\n",
      "Iteration 977, loss = 0.30597825\n",
      "Iteration 978, loss = 0.30584464\n",
      "Iteration 979, loss = 0.30569317\n",
      "Iteration 980, loss = 0.30555891\n",
      "Iteration 981, loss = 0.30544731\n",
      "Iteration 982, loss = 0.30534437\n",
      "Iteration 983, loss = 0.30521429\n",
      "Iteration 984, loss = 0.30507907\n",
      "Iteration 985, loss = 0.30492653\n",
      "Iteration 986, loss = 0.30481958\n",
      "Iteration 987, loss = 0.30465535\n",
      "Iteration 988, loss = 0.30454858\n",
      "Iteration 989, loss = 0.30443527\n",
      "Iteration 990, loss = 0.30426728\n",
      "Iteration 991, loss = 0.30412091\n",
      "Iteration 992, loss = 0.30402989\n",
      "Iteration 993, loss = 0.30386692\n",
      "Iteration 994, loss = 0.30381193\n",
      "Iteration 995, loss = 0.30359714\n",
      "Iteration 996, loss = 0.30344492\n",
      "Iteration 997, loss = 0.30334769\n",
      "Iteration 998, loss = 0.30318619\n",
      "Iteration 999, loss = 0.30304628\n",
      "Iteration 1000, loss = 0.30296375\n",
      "Iteration 1001, loss = 0.30279650\n",
      "Iteration 1002, loss = 0.30265770\n",
      "Iteration 1003, loss = 0.30252152\n",
      "Iteration 1004, loss = 0.30241632\n",
      "Iteration 1005, loss = 0.30225880\n",
      "Iteration 1006, loss = 0.30215444\n",
      "Iteration 1007, loss = 0.30201776\n",
      "Iteration 1008, loss = 0.30185871\n",
      "Iteration 1009, loss = 0.30170981\n",
      "Iteration 1010, loss = 0.30159688\n",
      "Iteration 1011, loss = 0.30157754\n",
      "Iteration 1012, loss = 0.30131800\n",
      "Iteration 1013, loss = 0.30124656\n",
      "Iteration 1014, loss = 0.30104663\n",
      "Iteration 1015, loss = 0.30092545\n",
      "Iteration 1016, loss = 0.30081486\n",
      "Iteration 1017, loss = 0.30066769\n",
      "Iteration 1018, loss = 0.30056206\n",
      "Iteration 1019, loss = 0.30040298\n",
      "Iteration 1020, loss = 0.30025096\n",
      "Iteration 1021, loss = 0.30012449\n",
      "Iteration 1022, loss = 0.30004184\n",
      "Iteration 1023, loss = 0.29999642\n",
      "Iteration 1024, loss = 0.29971986\n",
      "Iteration 1025, loss = 0.29958855\n",
      "Iteration 1026, loss = 0.29947520\n",
      "Iteration 1027, loss = 0.29932124\n",
      "Iteration 1028, loss = 0.29924859\n",
      "Iteration 1029, loss = 0.29903819\n",
      "Iteration 1030, loss = 0.29892373\n",
      "Iteration 1031, loss = 0.29879461\n",
      "Iteration 1032, loss = 0.29865095\n",
      "Iteration 1033, loss = 0.29869698\n",
      "Iteration 1034, loss = 0.29838981\n",
      "Iteration 1035, loss = 0.29826515\n",
      "Iteration 1036, loss = 0.29810704\n",
      "Iteration 1037, loss = 0.29802542\n",
      "Iteration 1038, loss = 0.29783404\n",
      "Iteration 1039, loss = 0.29769193\n",
      "Iteration 1040, loss = 0.29760328\n",
      "Iteration 1041, loss = 0.29748646\n",
      "Iteration 1042, loss = 0.29732143\n",
      "Iteration 1043, loss = 0.29728408\n",
      "Iteration 1044, loss = 0.29702431\n",
      "Iteration 1045, loss = 0.29688563\n",
      "Iteration 1046, loss = 0.29675185\n",
      "Iteration 1047, loss = 0.29660772\n",
      "Iteration 1048, loss = 0.29652877\n",
      "Iteration 1049, loss = 0.29633492\n",
      "Iteration 1050, loss = 0.29628038\n",
      "Iteration 1051, loss = 0.29608184\n",
      "Iteration 1052, loss = 0.29602076\n",
      "Iteration 1053, loss = 0.29583590\n",
      "Iteration 1054, loss = 0.29566604\n",
      "Iteration 1055, loss = 0.29555710\n",
      "Iteration 1056, loss = 0.29540445\n",
      "Iteration 1057, loss = 0.29532532\n",
      "Iteration 1058, loss = 0.29513890\n",
      "Iteration 1059, loss = 0.29499458\n",
      "Iteration 1060, loss = 0.29485524\n",
      "Iteration 1061, loss = 0.29475630\n",
      "Iteration 1062, loss = 0.29458665\n",
      "Iteration 1063, loss = 0.29447484\n",
      "Iteration 1064, loss = 0.29435156\n",
      "Iteration 1065, loss = 0.29425580\n",
      "Iteration 1066, loss = 0.29407765\n",
      "Iteration 1067, loss = 0.29391856\n",
      "Iteration 1068, loss = 0.29379417\n",
      "Iteration 1069, loss = 0.29368682\n",
      "Iteration 1070, loss = 0.29352605\n",
      "Iteration 1071, loss = 0.29342265\n",
      "Iteration 1072, loss = 0.29322939\n",
      "Iteration 1073, loss = 0.29313888\n",
      "Iteration 1074, loss = 0.29294986\n",
      "Iteration 1075, loss = 0.29282471\n",
      "Iteration 1076, loss = 0.29271217\n",
      "Iteration 1077, loss = 0.29267432\n",
      "Iteration 1078, loss = 0.29248121\n",
      "Iteration 1079, loss = 0.29231245\n",
      "Iteration 1080, loss = 0.29216513\n",
      "Iteration 1081, loss = 0.29210888\n",
      "Iteration 1082, loss = 0.29190978\n",
      "Iteration 1083, loss = 0.29175130\n",
      "Iteration 1084, loss = 0.29164750\n",
      "Iteration 1085, loss = 0.29150751\n",
      "Iteration 1086, loss = 0.29134957\n",
      "Iteration 1087, loss = 0.29126974\n",
      "Iteration 1088, loss = 0.29113564\n",
      "Iteration 1089, loss = 0.29095499\n",
      "Iteration 1090, loss = 0.29087545\n",
      "Iteration 1091, loss = 0.29069660\n",
      "Iteration 1092, loss = 0.29060340\n",
      "Iteration 1093, loss = 0.29044194\n",
      "Iteration 1094, loss = 0.29027387\n",
      "Iteration 1095, loss = 0.29013130\n",
      "Iteration 1096, loss = 0.29004273\n",
      "Iteration 1097, loss = 0.28989774\n",
      "Iteration 1098, loss = 0.28972759\n",
      "Iteration 1099, loss = 0.28961127\n",
      "Iteration 1100, loss = 0.28946763\n",
      "Iteration 1101, loss = 0.28932374\n",
      "Iteration 1102, loss = 0.28917184\n",
      "Iteration 1103, loss = 0.28902506\n",
      "Iteration 1104, loss = 0.28890414\n",
      "Iteration 1105, loss = 0.28883844\n",
      "Iteration 1106, loss = 0.28865353\n",
      "Iteration 1107, loss = 0.28845805\n",
      "Iteration 1108, loss = 0.28836486\n",
      "Iteration 1109, loss = 0.28823707\n",
      "Iteration 1110, loss = 0.28809549\n",
      "Iteration 1111, loss = 0.28801149\n",
      "Iteration 1112, loss = 0.28782190\n",
      "Iteration 1113, loss = 0.28765967\n",
      "Iteration 1114, loss = 0.28754628\n",
      "Iteration 1115, loss = 0.28737703\n",
      "Iteration 1116, loss = 0.28724878\n",
      "Iteration 1117, loss = 0.28711921\n",
      "Iteration 1118, loss = 0.28699790\n",
      "Iteration 1119, loss = 0.28687901\n",
      "Iteration 1120, loss = 0.28670767\n",
      "Iteration 1121, loss = 0.28656104\n",
      "Iteration 1122, loss = 0.28658958\n",
      "Iteration 1123, loss = 0.28632263\n",
      "Iteration 1124, loss = 0.28616515\n",
      "Iteration 1125, loss = 0.28607963\n",
      "Iteration 1126, loss = 0.28589452\n",
      "Iteration 1127, loss = 0.28575531\n",
      "Iteration 1128, loss = 0.28573839\n",
      "Iteration 1129, loss = 0.28550345\n",
      "Iteration 1130, loss = 0.28535217\n",
      "Iteration 1131, loss = 0.28520474\n",
      "Iteration 1132, loss = 0.28510141\n",
      "Iteration 1133, loss = 0.28494123\n",
      "Iteration 1134, loss = 0.28486682\n",
      "Iteration 1135, loss = 0.28469691\n",
      "Iteration 1136, loss = 0.28457361\n",
      "Iteration 1137, loss = 0.28439961\n",
      "Iteration 1138, loss = 0.28428410\n",
      "Iteration 1139, loss = 0.28413236\n",
      "Iteration 1140, loss = 0.28401537\n",
      "Iteration 1141, loss = 0.28394358\n",
      "Iteration 1142, loss = 0.28370985\n",
      "Iteration 1143, loss = 0.28363561\n",
      "Iteration 1144, loss = 0.28349131\n",
      "Iteration 1145, loss = 0.28331199\n",
      "Iteration 1146, loss = 0.28323690\n",
      "Iteration 1147, loss = 0.28304669\n",
      "Iteration 1148, loss = 0.28289424\n",
      "Iteration 1149, loss = 0.28275589\n",
      "Iteration 1150, loss = 0.28263431\n",
      "Iteration 1151, loss = 0.28248097\n",
      "Iteration 1152, loss = 0.28237281\n",
      "Iteration 1153, loss = 0.28222215\n",
      "Iteration 1154, loss = 0.28224760\n",
      "Iteration 1155, loss = 0.28198164\n",
      "Iteration 1156, loss = 0.28181409\n",
      "Iteration 1157, loss = 0.28169649\n",
      "Iteration 1158, loss = 0.28154071\n",
      "Iteration 1159, loss = 0.28138420\n",
      "Iteration 1160, loss = 0.28126724\n",
      "Iteration 1161, loss = 0.28113891\n",
      "Iteration 1162, loss = 0.28100444\n",
      "Iteration 1163, loss = 0.28091641\n",
      "Iteration 1164, loss = 0.28072224\n",
      "Iteration 1165, loss = 0.28059510\n",
      "Iteration 1166, loss = 0.28044994\n",
      "Iteration 1167, loss = 0.28040084\n",
      "Iteration 1168, loss = 0.28017665\n",
      "Iteration 1169, loss = 0.28005310\n",
      "Iteration 1170, loss = 0.27996740\n",
      "Iteration 1171, loss = 0.27976530\n",
      "Iteration 1172, loss = 0.27966038\n",
      "Iteration 1173, loss = 0.27949616\n",
      "Iteration 1174, loss = 0.27936434\n",
      "Iteration 1175, loss = 0.27921267\n",
      "Iteration 1176, loss = 0.27913510\n",
      "Iteration 1177, loss = 0.27892941\n",
      "Iteration 1178, loss = 0.27879344\n",
      "Iteration 1179, loss = 0.27867827\n",
      "Iteration 1180, loss = 0.27858519\n",
      "Iteration 1181, loss = 0.27850040\n",
      "Iteration 1182, loss = 0.27828298\n",
      "Iteration 1183, loss = 0.27816921\n",
      "Iteration 1184, loss = 0.27801114\n",
      "Iteration 1185, loss = 0.27788788\n",
      "Iteration 1186, loss = 0.27771535\n",
      "Iteration 1187, loss = 0.27757425\n",
      "Iteration 1188, loss = 0.27743629\n",
      "Iteration 1189, loss = 0.27732328\n",
      "Iteration 1190, loss = 0.27717936\n",
      "Iteration 1191, loss = 0.27704432\n",
      "Iteration 1192, loss = 0.27696273\n",
      "Iteration 1193, loss = 0.27677736\n",
      "Iteration 1194, loss = 0.27663105\n",
      "Iteration 1195, loss = 0.27651639\n",
      "Iteration 1196, loss = 0.27641040\n",
      "Iteration 1197, loss = 0.27628896\n",
      "Iteration 1198, loss = 0.27610079\n",
      "Iteration 1199, loss = 0.27597574\n",
      "Iteration 1200, loss = 0.27584507\n",
      "Iteration 1201, loss = 0.27568595\n",
      "Iteration 1202, loss = 0.27556084\n",
      "Iteration 1203, loss = 0.27541911\n",
      "Iteration 1204, loss = 0.27529613\n",
      "Iteration 1205, loss = 0.27517037\n",
      "Iteration 1206, loss = 0.27501383\n",
      "Iteration 1207, loss = 0.27489549\n",
      "Iteration 1208, loss = 0.27474446\n",
      "Iteration 1209, loss = 0.27459433\n",
      "Iteration 1210, loss = 0.27446421\n",
      "Iteration 1211, loss = 0.27443844\n",
      "Iteration 1212, loss = 0.27423145\n",
      "Iteration 1213, loss = 0.27408706\n",
      "Iteration 1214, loss = 0.27394957\n",
      "Iteration 1215, loss = 0.27388807\n",
      "Iteration 1216, loss = 0.27363925\n",
      "Iteration 1217, loss = 0.27357656\n",
      "Iteration 1218, loss = 0.27339839\n",
      "Iteration 1219, loss = 0.27324825\n",
      "Iteration 1220, loss = 0.27313152\n",
      "Iteration 1221, loss = 0.27296277\n",
      "Iteration 1222, loss = 0.27289945\n",
      "Iteration 1223, loss = 0.27269631\n",
      "Iteration 1224, loss = 0.27257752\n",
      "Iteration 1225, loss = 0.27246115\n",
      "Iteration 1226, loss = 0.27232281\n",
      "Iteration 1227, loss = 0.27221783\n",
      "Iteration 1228, loss = 0.27201718\n",
      "Iteration 1229, loss = 0.27189541\n",
      "Iteration 1230, loss = 0.27177508\n",
      "Iteration 1231, loss = 0.27164198\n",
      "Iteration 1232, loss = 0.27149175\n",
      "Iteration 1233, loss = 0.27138149\n",
      "Iteration 1234, loss = 0.27121230\n",
      "Iteration 1235, loss = 0.27109080\n",
      "Iteration 1236, loss = 0.27095172\n",
      "Iteration 1237, loss = 0.27081817\n",
      "Iteration 48, loss = 0.48255093\n",
      "Iteration 49, loss = 0.47984284\n",
      "Iteration 50, loss = 0.47718033\n",
      "Iteration 51, loss = 0.47455011\n",
      "Iteration 52, loss = 0.47195188\n",
      "Iteration 53, loss = 0.46952772\n",
      "Iteration 54, loss = 0.46700519\n",
      "Iteration 55, loss = 0.46464553\n",
      "Iteration 56, loss = 0.46224077\n",
      "Iteration 57, loss = 0.46003326\n",
      "Iteration 58, loss = 0.45777645\n",
      "Iteration 59, loss = 0.45558341\n",
      "Iteration 60, loss = 0.45341017\n",
      "Iteration 61, loss = 0.45138236\n",
      "Iteration 62, loss = 0.44934726\n",
      "Iteration 63, loss = 0.44735358\n",
      "Iteration 64, loss = 0.44537929\n",
      "Iteration 65, loss = 0.44342821\n",
      "Iteration 66, loss = 0.44158255\n",
      "Iteration 67, loss = 0.43969401\n",
      "Iteration 68, loss = 0.43794175\n",
      "Iteration 69, loss = 0.43617885\n",
      "Iteration 70, loss = 0.43446086\n",
      "Iteration 71, loss = 0.43275277\n",
      "Iteration 72, loss = 0.43113232\n",
      "Iteration 73, loss = 0.42948466\n",
      "Iteration 74, loss = 0.42786354\n",
      "Iteration 75, loss = 0.42637649\n",
      "Iteration 76, loss = 0.42483248\n",
      "Iteration 77, loss = 0.42340055\n",
      "Iteration 78, loss = 0.42184077\n",
      "Iteration 79, loss = 0.42043892\n",
      "Iteration 80, loss = 0.41908445\n",
      "Iteration 81, loss = 0.41765682\n",
      "Iteration 82, loss = 0.41632192\n",
      "Iteration 83, loss = 0.41499408\n",
      "Iteration 84, loss = 0.41370015\n",
      "Iteration 85, loss = 0.41239669\n",
      "Iteration 86, loss = 0.41112336\n",
      "Iteration 87, loss = 0.40993525\n",
      "Iteration 88, loss = 0.40870750\n",
      "Iteration 89, loss = 0.40757550\n",
      "Iteration 90, loss = 0.40638678\n",
      "Iteration 91, loss = 0.40527701\n",
      "Iteration 92, loss = 0.40410640\n",
      "Iteration 93, loss = 0.40302764\n",
      "Iteration 94, loss = 0.40197657\n",
      "Iteration 95, loss = 0.40091568\n",
      "Iteration 96, loss = 0.39986670\n",
      "Iteration 97, loss = 0.39886952\n",
      "Iteration 98, loss = 0.39786831\n",
      "Iteration 99, loss = 0.39685564\n",
      "Iteration 100, loss = 0.39592402\n",
      "Iteration 101, loss = 0.39494208\n",
      "Iteration 102, loss = 0.39404113\n",
      "Iteration 103, loss = 0.39309801\n",
      "Iteration 104, loss = 0.39223452\n",
      "Iteration 105, loss = 0.39132161\n",
      "Iteration 106, loss = 0.39044626\n",
      "Iteration 107, loss = 0.38960787\n",
      "Iteration 108, loss = 0.38877032\n",
      "Iteration 109, loss = 0.38799902\n",
      "Iteration 110, loss = 0.38712633\n",
      "Iteration 111, loss = 0.38629727\n",
      "Iteration 112, loss = 0.38551556\n",
      "Iteration 113, loss = 0.38479617\n",
      "Iteration 114, loss = 0.38398609\n",
      "Iteration 115, loss = 0.38323036\n",
      "Iteration 116, loss = 0.38252096\n",
      "Iteration 117, loss = 0.38177272\n",
      "Iteration 118, loss = 0.38109568\n",
      "Iteration 119, loss = 0.38036159\n",
      "Iteration 120, loss = 0.37969725\n",
      "Iteration 121, loss = 0.37901174\n",
      "Iteration 122, loss = 0.37834785\n",
      "Iteration 123, loss = 0.37767411\n",
      "Iteration 124, loss = 0.37701355\n",
      "Iteration 125, loss = 0.37636073\n",
      "Iteration 126, loss = 0.37576566\n",
      "Iteration 127, loss = 0.37509613\n",
      "Iteration 128, loss = 0.37450010\n",
      "Iteration 129, loss = 0.37389603\n",
      "Iteration 130, loss = 0.37332394\n",
      "Iteration 131, loss = 0.37274394\n",
      "Iteration 132, loss = 0.37217015\n",
      "Iteration 133, loss = 0.37156314\n",
      "Iteration 134, loss = 0.37103287\n",
      "Iteration 135, loss = 0.37045692\n",
      "Iteration 136, loss = 0.36990667\n",
      "Iteration 137, loss = 0.36936503\n",
      "Iteration 138, loss = 0.36884356\n",
      "Iteration 139, loss = 0.36833018\n",
      "Iteration 140, loss = 0.36781653\n",
      "Iteration 141, loss = 0.36729360\n",
      "Iteration 142, loss = 0.36678723\n",
      "Iteration 143, loss = 0.36629426\n",
      "Iteration 144, loss = 0.36582148\n",
      "Iteration 145, loss = 0.36533094\n",
      "Iteration 146, loss = 0.36485468\n",
      "Iteration 147, loss = 0.36434404\n",
      "Iteration 148, loss = 0.36391788\n",
      "Iteration 149, loss = 0.36346077\n",
      "Iteration 150, loss = 0.36300191\n",
      "Iteration 151, loss = 0.36255143\n",
      "Iteration 152, loss = 0.36211283\n",
      "Iteration 153, loss = 0.36170853\n",
      "Iteration 154, loss = 0.36125394\n",
      "Iteration 155, loss = 0.36083731\n",
      "Iteration 156, loss = 0.36040898\n",
      "Iteration 157, loss = 0.36001256\n",
      "Iteration 158, loss = 0.35960138\n",
      "Iteration 159, loss = 0.35917682\n",
      "Iteration 160, loss = 0.35878548\n",
      "Iteration 161, loss = 0.35841115\n",
      "Iteration 162, loss = 0.35800263\n",
      "Iteration 163, loss = 0.35764456\n",
      "Iteration 164, loss = 0.35726228\n",
      "Iteration 165, loss = 0.35687211\n",
      "Iteration 166, loss = 0.35649359\n",
      "Iteration 167, loss = 0.35615276\n",
      "Iteration 168, loss = 0.35577417\n",
      "Iteration 169, loss = 0.35542609\n",
      "Iteration 170, loss = 0.35507545\n",
      "Iteration 171, loss = 0.35472621\n",
      "Iteration 172, loss = 0.35438549\n",
      "Iteration 173, loss = 0.35403453\n",
      "Iteration 174, loss = 0.35368864\n",
      "Iteration 175, loss = 0.35337222\n",
      "Iteration 176, loss = 0.35302525\n",
      "Iteration 177, loss = 0.35270490\n",
      "Iteration 178, loss = 0.35239848\n",
      "Iteration 179, loss = 0.35205540\n",
      "Iteration 180, loss = 0.35176079\n",
      "Iteration 181, loss = 0.35143065\n",
      "Iteration 182, loss = 0.35112359\n",
      "Iteration 183, loss = 0.35080590\n",
      "Iteration 184, loss = 0.35050251\n",
      "Iteration 185, loss = 0.35021804\n",
      "Iteration 186, loss = 0.34991253\n",
      "Iteration 187, loss = 0.34963529\n",
      "Iteration 188, loss = 0.34933264\n",
      "Iteration 189, loss = 0.34905978\n",
      "Iteration 190, loss = 0.34875793\n",
      "Iteration 191, loss = 0.34847760\n",
      "Iteration 192, loss = 0.34821152\n",
      "Iteration 193, loss = 0.34792146\n",
      "Iteration 194, loss = 0.34763825\n",
      "Iteration 195, loss = 0.34736647\n",
      "Iteration 196, loss = 0.34711033\n",
      "Iteration 197, loss = 0.34682919\n",
      "Iteration 198, loss = 0.34658082\n",
      "Iteration 199, loss = 0.34632439\n",
      "Iteration 200, loss = 0.34605279\n",
      "Iteration 201, loss = 0.34580402\n",
      "Iteration 202, loss = 0.34554136\n",
      "Iteration 203, loss = 0.34530155\n",
      "Iteration 204, loss = 0.34503087\n",
      "Iteration 205, loss = 0.34479784\n",
      "Iteration 206, loss = 0.34454954\n",
      "Iteration 207, loss = 0.34431110\n",
      "Iteration 208, loss = 0.34406651\n",
      "Iteration 209, loss = 0.34382191\n",
      "Iteration 210, loss = 0.34359143\n",
      "Iteration 211, loss = 0.34334043\n",
      "Iteration 212, loss = 0.34312705\n",
      "Iteration 213, loss = 0.34290225\n",
      "Iteration 214, loss = 0.34266155\n",
      "Iteration 215, loss = 0.34242982\n",
      "Iteration 216, loss = 0.34220722\n",
      "Iteration 217, loss = 0.34198264\n",
      "Iteration 218, loss = 0.34176295\n",
      "Iteration 219, loss = 0.34155008\n",
      "Iteration 220, loss = 0.34131814\n",
      "Iteration 221, loss = 0.34112049\n",
      "Iteration 222, loss = 0.34090337\n",
      "Iteration 223, loss = 0.34071530\n",
      "Iteration 224, loss = 0.34047644\n",
      "Iteration 225, loss = 0.34025941\n",
      "Iteration 226, loss = 0.34006400\n",
      "Iteration 227, loss = 0.33985383\n",
      "Iteration 228, loss = 0.33964507\n",
      "Iteration 229, loss = 0.33945109\n",
      "Iteration 230, loss = 0.33925012\n",
      "Iteration 231, loss = 0.33905198\n",
      "Iteration 232, loss = 0.33883758\n",
      "Iteration 233, loss = 0.33865689\n",
      "Iteration 234, loss = 0.33844597\n",
      "Iteration 235, loss = 0.33825800\n",
      "Iteration 236, loss = 0.33808079\n",
      "Iteration 237, loss = 0.33789084\n",
      "Iteration 238, loss = 0.33768949\n",
      "Iteration 239, loss = 0.33749257\n",
      "Iteration 240, loss = 0.33731067\n",
      "Iteration 241, loss = 0.33712962\n",
      "Iteration 242, loss = 0.33694903\n",
      "Iteration 243, loss = 0.33676775\n",
      "Iteration 244, loss = 0.33658890\n",
      "Iteration 245, loss = 0.33641537\n",
      "Iteration 246, loss = 0.33620855\n",
      "Iteration 247, loss = 0.33604781\n",
      "Iteration 248, loss = 0.33586971\n",
      "Iteration 249, loss = 0.33569041\n",
      "Iteration 250, loss = 0.33551783\n",
      "Iteration 251, loss = 0.33535548\n",
      "Iteration 252, loss = 0.33517296\n",
      "Iteration 253, loss = 0.33501819\n",
      "Iteration 254, loss = 0.33483022\n",
      "Iteration 255, loss = 0.33466982\n",
      "Iteration 256, loss = 0.33451241\n",
      "Iteration 257, loss = 0.33432580\n",
      "Iteration 258, loss = 0.33417214\n",
      "Iteration 259, loss = 0.33400975\n",
      "Iteration 260, loss = 0.33383431\n",
      "Iteration 261, loss = 0.33368102\n",
      "Iteration 262, loss = 0.33351675\n",
      "Iteration 263, loss = 0.33336410\n",
      "Iteration 264, loss = 0.33319708\n",
      "Iteration 265, loss = 0.33304475\n",
      "Iteration 266, loss = 0.33288703\n",
      "Iteration 267, loss = 0.33274112\n",
      "Iteration 268, loss = 0.33257008\n",
      "Iteration 269, loss = 0.33244038\n",
      "Iteration 270, loss = 0.33226048\n",
      "Iteration 271, loss = 0.33211100\n",
      "Iteration 272, loss = 0.33196848\n",
      "Iteration 273, loss = 0.33180922\n",
      "Iteration 274, loss = 0.33166361\n",
      "Iteration 275, loss = 0.33152445\n",
      "Iteration 276, loss = 0.33136188\n",
      "Iteration 277, loss = 0.33121525\n",
      "Iteration 278, loss = 0.33108359\n",
      "Iteration 279, loss = 0.33092650\n",
      "Iteration 280, loss = 0.33078159\n",
      "Iteration 281, loss = 0.33064550\n",
      "Iteration 282, loss = 0.33049700\n",
      "Iteration 283, loss = 0.33036054\n",
      "Iteration 284, loss = 0.33021777\n",
      "Iteration 285, loss = 0.33007249\n",
      "Iteration 286, loss = 0.32992861\n",
      "Iteration 287, loss = 0.32980148\n",
      "Iteration 288, loss = 0.32966493\n",
      "Iteration 289, loss = 0.32951861\n",
      "Iteration 290, loss = 0.32937850\n",
      "Iteration 291, loss = 0.32924265\n",
      "Iteration 292, loss = 0.32911876\n",
      "Iteration 293, loss = 0.32898284\n",
      "Iteration 294, loss = 0.32884438\n",
      "Iteration 295, loss = 0.32871053\n",
      "Iteration 296, loss = 0.32857711\n",
      "Iteration 297, loss = 0.32844508\n",
      "Iteration 298, loss = 0.32832739\n",
      "Iteration 299, loss = 0.32819483\n",
      "Iteration 300, loss = 0.32805707\n",
      "Iteration 301, loss = 0.32793150\n",
      "Iteration 302, loss = 0.32779942\n",
      "Iteration 303, loss = 0.32767631\n",
      "Iteration 304, loss = 0.32755005\n",
      "Iteration 305, loss = 0.32742876\n",
      "Iteration 306, loss = 0.32729593\n",
      "Iteration 307, loss = 0.32717193\n",
      "Iteration 308, loss = 0.32704884\n",
      "Iteration 309, loss = 0.32692623\n",
      "Iteration 310, loss = 0.32680602\n",
      "Iteration 311, loss = 0.32668112\n",
      "Iteration 312, loss = 0.32656051\n",
      "Iteration 313, loss = 0.32643285\n",
      "Iteration 314, loss = 0.32632532\n",
      "Iteration 315, loss = 0.32618921\n",
      "Iteration 316, loss = 0.32609081\n",
      "Iteration 317, loss = 0.32594829\n",
      "Iteration 318, loss = 0.32584032\n",
      "Iteration 319, loss = 0.32572232\n",
      "Iteration 320, loss = 0.32560637\n",
      "Iteration 321, loss = 0.32548468\n",
      "Iteration 322, loss = 0.32536251\n",
      "Iteration 323, loss = 0.32525141\n",
      "Iteration 324, loss = 0.32513528\n",
      "Iteration 325, loss = 0.32501146\n",
      "Iteration 326, loss = 0.32490040\n",
      "Iteration 327, loss = 0.32478974\n",
      "Iteration 328, loss = 0.32467811\n",
      "Iteration 329, loss = 0.32457155\n",
      "Iteration 330, loss = 0.32445009\n",
      "Iteration 331, loss = 0.32435217\n",
      "Iteration 332, loss = 0.32421944\n",
      "Iteration 333, loss = 0.32411792\n",
      "Iteration 334, loss = 0.32400566\n",
      "Iteration 335, loss = 0.32389159\n",
      "Iteration 336, loss = 0.32378008\n",
      "Iteration 337, loss = 0.32367434\n",
      "Iteration 338, loss = 0.32356850\n",
      "Iteration 339, loss = 0.32345585\n",
      "Iteration 340, loss = 0.32334507\n",
      "Iteration 341, loss = 0.32323778\n",
      "Iteration 342, loss = 0.32313983\n",
      "Iteration 343, loss = 0.32302837\n",
      "Iteration 344, loss = 0.32294070\n",
      "Iteration 345, loss = 0.32282749\n",
      "Iteration 346, loss = 0.32270803\n",
      "Iteration 347, loss = 0.32260784\n",
      "Iteration 348, loss = 0.32250759\n",
      "Iteration 349, loss = 0.32240676\n",
      "Iteration 350, loss = 0.32230454\n",
      "Iteration 351, loss = 0.32219164\n",
      "Iteration 352, loss = 0.32210010\n",
      "Iteration 353, loss = 0.32200104\n",
      "Iteration 354, loss = 0.32189289\n",
      "Iteration 355, loss = 0.32179417\n",
      "Iteration 356, loss = 0.32168592\n",
      "Iteration 357, loss = 0.32159920\n",
      "Iteration 358, loss = 0.32149242\n",
      "Iteration 359, loss = 0.32138681\n",
      "Iteration 360, loss = 0.32129673\n",
      "Iteration 361, loss = 0.32119312\n",
      "Iteration 362, loss = 0.32109988\n",
      "Iteration 363, loss = 0.32099890\n",
      "Iteration 364, loss = 0.32090543\n",
      "Iteration 365, loss = 0.32080053\n",
      "Iteration 366, loss = 0.32071369\n",
      "Iteration 367, loss = 0.32061055\n",
      "Iteration 368, loss = 0.32053067\n",
      "Iteration 369, loss = 0.32042645\n",
      "Iteration 370, loss = 0.32033332\n",
      "Iteration 371, loss = 0.32022437\n",
      "Iteration 372, loss = 0.32013649\n",
      "Iteration 373, loss = 0.32004208\n",
      "Iteration 374, loss = 0.31995040\n",
      "Iteration 375, loss = 0.31986835\n",
      "Iteration 376, loss = 0.31975903\n",
      "Iteration 377, loss = 0.31966624\n",
      "Iteration 378, loss = 0.31958324\n",
      "Iteration 379, loss = 0.31948509\n",
      "Iteration 380, loss = 0.31939884\n",
      "Iteration 381, loss = 0.31930751\n",
      "Iteration 382, loss = 0.31921466\n",
      "Iteration 383, loss = 0.31912592\n",
      "Iteration 384, loss = 0.31902893\n",
      "Iteration 385, loss = 0.31893790\n",
      "Iteration 386, loss = 0.31885324\n",
      "Iteration 387, loss = 0.31876465\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70868025\n",
      "Iteration 2, loss = 0.70599165\n",
      "Iteration 3, loss = 0.70187834\n",
      "Iteration 4, loss = 0.69680927\n",
      "Iteration 5, loss = 0.69099706\n",
      "Iteration 6, loss = 0.68488374\n",
      "Iteration 7, loss = 0.67833372\n",
      "Iteration 8, loss = 0.67184373\n",
      "Iteration 9, loss = 0.66527639\n",
      "Iteration 10, loss = 0.65838837\n",
      "Iteration 11, loss = 0.65192967\n",
      "Iteration 12, loss = 0.64527205\n",
      "Iteration 13, loss = 0.63888899\n",
      "Iteration 14, loss = 0.63259115\n",
      "Iteration 15, loss = 0.62642464\n",
      "Iteration 16, loss = 0.62015398\n",
      "Iteration 17, loss = 0.61418200\n",
      "Iteration 18, loss = 0.60857130\n",
      "Iteration 19, loss = 0.60281714\n",
      "Iteration 20, loss = 0.59725983\n",
      "Iteration 21, loss = 0.59175439\n",
      "Iteration 22, loss = 0.58645985\n",
      "Iteration 23, loss = 0.58133346\n",
      "Iteration 24, loss = 0.57616219\n",
      "Iteration 25, loss = 0.57120135\n",
      "Iteration 26, loss = 0.56621876\n",
      "Iteration 27, loss = 0.56138235\n",
      "Iteration 28, loss = 0.55666634\n",
      "Iteration 29, loss = 0.55214830\n",
      "Iteration 30, loss = 0.54758695\n",
      "Iteration 31, loss = 0.54318815\n",
      "Iteration 32, loss = 0.53886414\n",
      "Iteration 33, loss = 0.53453350\n",
      "Iteration 34, loss = 0.53034037\n",
      "Iteration 35, loss = 0.52639431\n",
      "Iteration 36, loss = 0.52237513\n",
      "Iteration 37, loss = 0.51843766\n",
      "Iteration 38, loss = 0.51466378\n",
      "Iteration 39, loss = 0.51083071\n",
      "Iteration 40, loss = 0.50720871\n",
      "Iteration 41, loss = 0.50362474\n",
      "Iteration 42, loss = 0.50020780\n",
      "Iteration 43, loss = 0.49670359\n",
      "Iteration 44, loss = 0.49339388\n",
      "Iteration 45, loss = 0.49015610\n",
      "Iteration 46, loss = 0.48685623\n",
      "Iteration 47, loss = 0.48375399\n",
      "Iteration 48, loss = 0.48065069\n",
      "Iteration 49, loss = 0.47768048\n",
      "Iteration 50, loss = 0.47475305\n",
      "Iteration 51, loss = 0.47185791\n",
      "Iteration 52, loss = 0.46904575\n",
      "Iteration 53, loss = 0.46639139\n",
      "Iteration 54, loss = 0.46365013\n",
      "Iteration 55, loss = 0.46106828\n",
      "Iteration 56, loss = 0.45845934\n",
      "Iteration 57, loss = 0.45593705\n",
      "Iteration 58, loss = 0.45359794\n",
      "Iteration 59, loss = 0.45121710\n",
      "Iteration 60, loss = 0.44887685\n",
      "Iteration 61, loss = 0.44659737\n",
      "Iteration 62, loss = 0.44433749\n",
      "Iteration 63, loss = 0.44215581\n",
      "Iteration 64, loss = 0.44002537\n",
      "Iteration 65, loss = 0.43792708\n",
      "Iteration 66, loss = 0.43595025\n",
      "Iteration 67, loss = 0.43393312\n",
      "Iteration 68, loss = 0.43209857\n",
      "Iteration 69, loss = 0.43016861\n",
      "Iteration 70, loss = 0.42833074\n",
      "Iteration 71, loss = 0.42659784\n",
      "Iteration 72, loss = 0.42476624\n",
      "Iteration 73, loss = 0.42306733\n",
      "Iteration 74, loss = 0.42146751\n",
      "Iteration 75, loss = 0.41981274\n",
      "Iteration 76, loss = 0.41821011\n",
      "Iteration 77, loss = 0.41670825\n",
      "Iteration 78, loss = 0.41517621\n",
      "Iteration 79, loss = 0.41366908\n",
      "Iteration 80, loss = 0.41226504\n",
      "Iteration 81, loss = 0.41084974\n",
      "Iteration 82, loss = 0.40946878\n",
      "Iteration 83, loss = 0.40814027\n",
      "Iteration 84, loss = 0.40681380\n",
      "Iteration 85, loss = 0.40556609\n",
      "Iteration 86, loss = 0.40430055\n",
      "Iteration 87, loss = 0.40309851\n",
      "Iteration 88, loss = 0.40191600\n",
      "Iteration 89, loss = 0.40071040\n",
      "Iteration 90, loss = 0.39954758\n",
      "Iteration 91, loss = 0.39843484\n",
      "Iteration 92, loss = 0.39734373\n",
      "Iteration 93, loss = 0.39624473\n",
      "Iteration 94, loss = 0.39523864\n",
      "Iteration 95, loss = 0.39420448\n",
      "Iteration 96, loss = 0.39321407\n",
      "Iteration 97, loss = 0.39221651\n",
      "Iteration 98, loss = 0.39124859\n",
      "Iteration 99, loss = 0.39031499\n",
      "Iteration 100, loss = 0.38940582\n",
      "Iteration 101, loss = 0.38850013\n",
      "Iteration 102, loss = 0.38757698\n",
      "Iteration 103, loss = 0.38671091\n",
      "Iteration 104, loss = 0.38589063\n",
      "Iteration 105, loss = 0.38502492\n",
      "Iteration 106, loss = 0.38425785\n",
      "Iteration 107, loss = 0.38341517\n",
      "Iteration 108, loss = 0.38260928\n",
      "Iteration 109, loss = 0.38187567\n",
      "Iteration 110, loss = 0.38112507\n",
      "Iteration 111, loss = 0.38044673\n",
      "Iteration 112, loss = 0.37965440\n",
      "Iteration 113, loss = 0.37898405\n",
      "Iteration 114, loss = 0.37825075\n",
      "Iteration 115, loss = 0.37754135\n",
      "Iteration 116, loss = 0.37686533\n",
      "Iteration 117, loss = 0.37620416\n",
      "Iteration 118, loss = 0.37555409\n",
      "Iteration 119, loss = 0.37488574\n",
      "Iteration 120, loss = 0.37425363\n",
      "Iteration 121, loss = 0.37364408\n",
      "Iteration 122, loss = 0.37302591\n",
      "Iteration 123, loss = 0.37243500\n",
      "Iteration 124, loss = 0.37186554\n",
      "Iteration 125, loss = 0.37125944\n",
      "Iteration 126, loss = 0.37070314\n",
      "Iteration 127, loss = 0.37017630\n",
      "Iteration 128, loss = 0.36956554\n",
      "Iteration 129, loss = 0.36902195\n",
      "Iteration 130, loss = 0.36850595\n",
      "Iteration 131, loss = 0.36793540\n",
      "Iteration 132, loss = 0.36742716\n",
      "Iteration 133, loss = 0.36696176\n",
      "Iteration 134, loss = 0.36638953\n",
      "Iteration 135, loss = 0.36590313\n",
      "Iteration 136, loss = 0.36543163\n",
      "Iteration 137, loss = 0.36493571\n",
      "Iteration 138, loss = 0.36445376\n",
      "Iteration 139, loss = 0.36399372\n",
      "Iteration 140, loss = 0.36352417\n",
      "Iteration 141, loss = 0.36304976\n",
      "Iteration 142, loss = 0.36259671\n",
      "Iteration 143, loss = 0.36215621\n",
      "Iteration 144, loss = 0.36172310\n",
      "Iteration 145, loss = 0.36128080\n",
      "Iteration 146, loss = 0.36084281\n",
      "Iteration 147, loss = 0.36039350\n",
      "Iteration 148, loss = 0.35997347\n",
      "Iteration 149, loss = 0.35957080\n",
      "Iteration 150, loss = 0.35917107\n",
      "Iteration 151, loss = 0.35875941\n",
      "Iteration 152, loss = 0.35834567\n",
      "Iteration 153, loss = 0.35794133\n",
      "Iteration 154, loss = 0.35754879\n",
      "Iteration 155, loss = 0.35715899\n",
      "Iteration 156, loss = 0.35676088\n",
      "Iteration 157, loss = 0.35637813\n",
      "Iteration 833, loss = 0.26811974\n",
      "Iteration 834, loss = 0.26797057\n",
      "Iteration 835, loss = 0.26781169\n",
      "Iteration 836, loss = 0.26773968\n",
      "Iteration 837, loss = 0.26755116\n",
      "Iteration 838, loss = 0.26741831\n",
      "Iteration 839, loss = 0.26727783\n",
      "Iteration 840, loss = 0.26717645\n",
      "Iteration 841, loss = 0.26701680\n",
      "Iteration 842, loss = 0.26685926\n",
      "Iteration 843, loss = 0.26680556\n",
      "Iteration 844, loss = 0.26659664\n",
      "Iteration 845, loss = 0.26646325\n",
      "Iteration 846, loss = 0.26637654\n",
      "Iteration 847, loss = 0.26625801\n",
      "Iteration 848, loss = 0.26620954\n",
      "Iteration 849, loss = 0.26593846\n",
      "Iteration 850, loss = 0.26582657\n",
      "Iteration 851, loss = 0.26570799\n",
      "Iteration 852, loss = 0.26557503\n",
      "Iteration 853, loss = 0.26542334\n",
      "Iteration 854, loss = 0.26532186\n",
      "Iteration 855, loss = 0.26515953\n",
      "Iteration 856, loss = 0.26500017\n",
      "Iteration 857, loss = 0.26488766\n",
      "Iteration 858, loss = 0.26484401\n",
      "Iteration 859, loss = 0.26467935\n",
      "Iteration 860, loss = 0.26444363\n",
      "Iteration 861, loss = 0.26438950\n",
      "Iteration 862, loss = 0.26417990\n",
      "Iteration 863, loss = 0.26402934\n",
      "Iteration 864, loss = 0.26393015\n",
      "Iteration 865, loss = 0.26380092\n",
      "Iteration 866, loss = 0.26373379\n",
      "Iteration 867, loss = 0.26355002\n",
      "Iteration 868, loss = 0.26332490\n",
      "Iteration 869, loss = 0.26323380\n",
      "Iteration 870, loss = 0.26307297\n",
      "Iteration 871, loss = 0.26295455\n",
      "Iteration 872, loss = 0.26278735\n",
      "Iteration 873, loss = 0.26267482\n",
      "Iteration 874, loss = 0.26261136\n",
      "Iteration 875, loss = 0.26238380\n",
      "Iteration 876, loss = 0.26226368\n",
      "Iteration 877, loss = 0.26217833\n",
      "Iteration 878, loss = 0.26195478\n",
      "Iteration 879, loss = 0.26185165\n",
      "Iteration 880, loss = 0.26180355\n",
      "Iteration 881, loss = 0.26156102\n",
      "Iteration 882, loss = 0.26149143\n",
      "Iteration 883, loss = 0.26126532\n",
      "Iteration 884, loss = 0.26119740\n",
      "Iteration 885, loss = 0.26097752\n",
      "Iteration 886, loss = 0.26084951\n",
      "Iteration 887, loss = 0.26072771\n",
      "Iteration 888, loss = 0.26059458\n",
      "Iteration 889, loss = 0.26042460\n",
      "Iteration 890, loss = 0.26029551\n",
      "Iteration 891, loss = 0.26014828\n",
      "Iteration 892, loss = 0.26000544\n",
      "Iteration 893, loss = 0.25995847\n",
      "Iteration 894, loss = 0.25973258\n",
      "Iteration 895, loss = 0.25959246\n",
      "Iteration 896, loss = 0.25940394\n",
      "Iteration 897, loss = 0.25926428\n",
      "Iteration 898, loss = 0.25917486\n",
      "Iteration 899, loss = 0.25905172\n",
      "Iteration 900, loss = 0.25885136\n",
      "Iteration 901, loss = 0.25870119\n",
      "Iteration 902, loss = 0.25868678\n",
      "Iteration 903, loss = 0.25841858\n",
      "Iteration 904, loss = 0.25827457\n",
      "Iteration 905, loss = 0.25813040\n",
      "Iteration 906, loss = 0.25801765\n",
      "Iteration 907, loss = 0.25785506\n",
      "Iteration 908, loss = 0.25772747\n",
      "Iteration 909, loss = 0.25756318\n",
      "Iteration 910, loss = 0.25744806\n",
      "Iteration 911, loss = 0.25730732\n",
      "Iteration 912, loss = 0.25714586\n",
      "Iteration 913, loss = 0.25703003\n",
      "Iteration 914, loss = 0.25689651\n",
      "Iteration 915, loss = 0.25669103\n",
      "Iteration 916, loss = 0.25659178\n",
      "Iteration 917, loss = 0.25640510\n",
      "Iteration 918, loss = 0.25631655\n",
      "Iteration 919, loss = 0.25614863\n",
      "Iteration 920, loss = 0.25609186\n",
      "Iteration 921, loss = 0.25588761\n",
      "Iteration 922, loss = 0.25566822\n",
      "Iteration 923, loss = 0.25554724\n",
      "Iteration 924, loss = 0.25537731\n",
      "Iteration 925, loss = 0.25526676\n",
      "Iteration 926, loss = 0.25518388\n",
      "Iteration 927, loss = 0.25495046\n",
      "Iteration 928, loss = 0.25487208\n",
      "Iteration 929, loss = 0.25462866\n",
      "Iteration 930, loss = 0.25451465\n",
      "Iteration 931, loss = 0.25438521\n",
      "Iteration 932, loss = 0.25424720\n",
      "Iteration 933, loss = 0.25411735\n",
      "Iteration 934, loss = 0.25392042\n",
      "Iteration 935, loss = 0.25381838\n",
      "Iteration 936, loss = 0.25366402\n",
      "Iteration 937, loss = 0.25354997\n",
      "Iteration 938, loss = 0.25336066\n",
      "Iteration 939, loss = 0.25323217\n",
      "Iteration 940, loss = 0.25308853\n",
      "Iteration 941, loss = 0.25288789\n",
      "Iteration 942, loss = 0.25281981\n",
      "Iteration 943, loss = 0.25264598\n",
      "Iteration 944, loss = 0.25248100\n",
      "Iteration 945, loss = 0.25227287\n",
      "Iteration 946, loss = 0.25213032\n",
      "Iteration 947, loss = 0.25199174\n",
      "Iteration 948, loss = 0.25200766\n",
      "Iteration 949, loss = 0.25186804\n",
      "Iteration 950, loss = 0.25155508\n",
      "Iteration 951, loss = 0.25139922\n",
      "Iteration 952, loss = 0.25130571\n",
      "Iteration 953, loss = 0.25108833\n",
      "Iteration 954, loss = 0.25100722\n",
      "Iteration 955, loss = 0.25078520\n",
      "Iteration 956, loss = 0.25062764\n",
      "Iteration 957, loss = 0.25050032\n",
      "Iteration 958, loss = 0.25032727\n",
      "Iteration 959, loss = 0.25045282\n",
      "Iteration 960, loss = 0.25001000\n",
      "Iteration 961, loss = 0.24989482\n",
      "Iteration 962, loss = 0.24985883\n",
      "Iteration 963, loss = 0.24970753\n",
      "Iteration 964, loss = 0.24939510\n",
      "Iteration 965, loss = 0.24931197\n",
      "Iteration 966, loss = 0.24914686\n",
      "Iteration 967, loss = 0.24893123\n",
      "Iteration 968, loss = 0.24881930\n",
      "Iteration 969, loss = 0.24865087\n",
      "Iteration 970, loss = 0.24849475\n",
      "Iteration 971, loss = 0.24833714\n",
      "Iteration 972, loss = 0.24820718\n",
      "Iteration 973, loss = 0.24798143\n",
      "Iteration 974, loss = 0.24781954\n",
      "Iteration 975, loss = 0.24767849\n",
      "Iteration 976, loss = 0.24755278\n",
      "Iteration 977, loss = 0.24736663\n",
      "Iteration 978, loss = 0.24727837\n",
      "Iteration 979, loss = 0.24704072\n",
      "Iteration 980, loss = 0.24686239\n",
      "Iteration 981, loss = 0.24670409\n",
      "Iteration 982, loss = 0.24655442\n",
      "Iteration 983, loss = 0.24638950\n",
      "Iteration 984, loss = 0.24626787\n",
      "Iteration 985, loss = 0.24614083\n",
      "Iteration 986, loss = 0.24592593\n",
      "Iteration 987, loss = 0.24582375\n",
      "Iteration 988, loss = 0.24563505\n",
      "Iteration 989, loss = 0.24556196\n",
      "Iteration 990, loss = 0.24531942\n",
      "Iteration 991, loss = 0.24514838\n",
      "Iteration 992, loss = 0.24505816\n",
      "Iteration 993, loss = 0.24490223\n",
      "Iteration 994, loss = 0.24470834\n",
      "Iteration 995, loss = 0.24459748\n",
      "Iteration 996, loss = 0.24451633\n",
      "Iteration 997, loss = 0.24421122\n",
      "Iteration 998, loss = 0.24414119\n",
      "Iteration 999, loss = 0.24393233\n",
      "Iteration 1000, loss = 0.24385569\n",
      "Iteration 1001, loss = 0.24363651\n",
      "Iteration 1002, loss = 0.24346332\n",
      "Iteration 1003, loss = 0.24332301\n",
      "Iteration 1004, loss = 0.24318467\n",
      "Iteration 1005, loss = 0.24302232\n",
      "Iteration 1006, loss = 0.24281591\n",
      "Iteration 1007, loss = 0.24263848\n",
      "Iteration 1008, loss = 0.24251808\n",
      "Iteration 1009, loss = 0.24241912\n",
      "Iteration 1010, loss = 0.24219553\n",
      "Iteration 1011, loss = 0.24200631\n",
      "Iteration 1012, loss = 0.24187499\n",
      "Iteration 1013, loss = 0.24175433\n",
      "Iteration 1014, loss = 0.24167641\n",
      "Iteration 1015, loss = 0.24143748\n",
      "Iteration 1016, loss = 0.24121450\n",
      "Iteration 1017, loss = 0.24108092\n",
      "Iteration 1018, loss = 0.24105021\n",
      "Iteration 1019, loss = 0.24073658\n",
      "Iteration 1020, loss = 0.24058968\n",
      "Iteration 1021, loss = 0.24041513\n",
      "Iteration 1022, loss = 0.24026037\n",
      "Iteration 1023, loss = 0.24008582\n",
      "Iteration 1024, loss = 0.23993648\n",
      "Iteration 1025, loss = 0.23980275\n",
      "Iteration 1026, loss = 0.23962692\n",
      "Iteration 1027, loss = 0.23947778\n",
      "Iteration 1028, loss = 0.23928071\n",
      "Iteration 1029, loss = 0.23913789\n",
      "Iteration 1030, loss = 0.23897659\n",
      "Iteration 1031, loss = 0.23888404\n",
      "Iteration 1032, loss = 0.23863152\n",
      "Iteration 1033, loss = 0.23850998\n",
      "Iteration 1034, loss = 0.23837213\n",
      "Iteration 1035, loss = 0.23820815\n",
      "Iteration 1036, loss = 0.23797731\n",
      "Iteration 1037, loss = 0.23780821\n",
      "Iteration 1038, loss = 0.23769780\n",
      "Iteration 1039, loss = 0.23750734\n",
      "Iteration 1040, loss = 0.23736156\n",
      "Iteration 1041, loss = 0.23729088\n",
      "Iteration 1042, loss = 0.23702114\n",
      "Iteration 1043, loss = 0.23693848\n",
      "Iteration 1044, loss = 0.23672609\n",
      "Iteration 1045, loss = 0.23650339\n",
      "Iteration 1046, loss = 0.23633837\n",
      "Iteration 1047, loss = 0.23627369\n",
      "Iteration 1048, loss = 0.23605478\n",
      "Iteration 1049, loss = 0.23593643\n",
      "Iteration 1050, loss = 0.23572168\n",
      "Iteration 1051, loss = 0.23557266\n",
      "Iteration 1052, loss = 0.23544603\n",
      "Iteration 1053, loss = 0.23527537\n",
      "Iteration 1054, loss = 0.23507366\n",
      "Iteration 1055, loss = 0.23492356\n",
      "Iteration 1056, loss = 0.23474141\n",
      "Iteration 1057, loss = 0.23457535\n",
      "Iteration 1058, loss = 0.23444052\n",
      "Iteration 1059, loss = 0.23426523\n",
      "Iteration 1060, loss = 0.23421808\n",
      "Iteration 1061, loss = 0.23391093\n",
      "Iteration 1062, loss = 0.23373775\n",
      "Iteration 1063, loss = 0.23359493\n",
      "Iteration 1064, loss = 0.23342973\n",
      "Iteration 1065, loss = 0.23335967\n",
      "Iteration 1066, loss = 0.23314850\n",
      "Iteration 1067, loss = 0.23295167\n",
      "Iteration 1068, loss = 0.23273501\n",
      "Iteration 1069, loss = 0.23277792\n",
      "Iteration 1070, loss = 0.23239683\n",
      "Iteration 1071, loss = 0.23224218\n",
      "Iteration 1072, loss = 0.23207575\n",
      "Iteration 1073, loss = 0.23196475\n",
      "Iteration 1074, loss = 0.23190670\n",
      "Iteration 1075, loss = 0.23165742\n",
      "Iteration 1076, loss = 0.23139139\n",
      "Iteration 1077, loss = 0.23126474\n",
      "Iteration 1078, loss = 0.23112253\n",
      "Iteration 1079, loss = 0.23091688\n",
      "Iteration 1080, loss = 0.23074964\n",
      "Iteration 1081, loss = 0.23053698\n",
      "Iteration 1082, loss = 0.23044044\n",
      "Iteration 1083, loss = 0.23026111\n",
      "Iteration 1084, loss = 0.23010044\n",
      "Iteration 1085, loss = 0.22995040\n",
      "Iteration 1086, loss = 0.22979636\n",
      "Iteration 1087, loss = 0.22960391\n",
      "Iteration 1088, loss = 0.22957549\n",
      "Iteration 1089, loss = 0.22923384\n",
      "Iteration 1090, loss = 0.22908998\n",
      "Iteration 1091, loss = 0.22892707\n",
      "Iteration 1092, loss = 0.22880957\n",
      "Iteration 1093, loss = 0.22861892\n",
      "Iteration 1094, loss = 0.22845579\n",
      "Iteration 1095, loss = 0.22825898\n",
      "Iteration 1096, loss = 0.22807644\n",
      "Iteration 1097, loss = 0.22791722\n",
      "Iteration 1098, loss = 0.22773232\n",
      "Iteration 1099, loss = 0.22758322\n",
      "Iteration 1100, loss = 0.22754534\n",
      "Iteration 1101, loss = 0.22721278\n",
      "Iteration 1102, loss = 0.22704396\n",
      "Iteration 1103, loss = 0.22686617\n",
      "Iteration 1104, loss = 0.22666437\n",
      "Iteration 1105, loss = 0.22655568\n",
      "Iteration 1106, loss = 0.22649898\n",
      "Iteration 1107, loss = 0.22618760\n",
      "Iteration 1108, loss = 0.22598874\n",
      "Iteration 1109, loss = 0.22591526\n",
      "Iteration 1110, loss = 0.22564179\n",
      "Iteration 1111, loss = 0.22563063\n",
      "Iteration 1112, loss = 0.22550454\n",
      "Iteration 1113, loss = 0.22516257\n",
      "Iteration 1114, loss = 0.22497795\n",
      "Iteration 1115, loss = 0.22486565\n",
      "Iteration 1116, loss = 0.22461801\n",
      "Iteration 1117, loss = 0.22449420\n",
      "Iteration 1118, loss = 0.22432915\n",
      "Iteration 1119, loss = 0.22416854\n",
      "Iteration 1120, loss = 0.22402758\n",
      "Iteration 1121, loss = 0.22379859\n",
      "Iteration 1122, loss = 0.22367369\n",
      "Iteration 1123, loss = 0.22344437\n",
      "Iteration 1124, loss = 0.22331029\n",
      "Iteration 1125, loss = 0.22306098\n",
      "Iteration 1126, loss = 0.22291204\n",
      "Iteration 1127, loss = 0.22275337\n",
      "Iteration 1128, loss = 0.22252637\n",
      "Iteration 1129, loss = 0.22239213\n",
      "Iteration 1130, loss = 0.22219407\n",
      "Iteration 1131, loss = 0.22200883\n",
      "Iteration 1132, loss = 0.22200461\n",
      "Iteration 1133, loss = 0.22177178\n",
      "Iteration 1134, loss = 0.22151900\n",
      "Iteration 1135, loss = 0.22135383\n",
      "Iteration 1136, loss = 0.22112224\n",
      "Iteration 1137, loss = 0.22097998\n",
      "Iteration 1138, loss = 0.22081065\n",
      "Iteration 1139, loss = 0.22059820\n",
      "Iteration 1140, loss = 0.22045240\n",
      "Iteration 1141, loss = 0.22024750\n",
      "Iteration 1142, loss = 0.22011954\n",
      "Iteration 1143, loss = 0.21988380\n",
      "Iteration 1144, loss = 0.21980029\n",
      "Iteration 1145, loss = 0.21955170\n",
      "Iteration 1146, loss = 0.21941747\n",
      "Iteration 1147, loss = 0.21929225\n",
      "Iteration 1148, loss = 0.21903375\n",
      "Iteration 1149, loss = 0.21915871\n",
      "Iteration 1150, loss = 0.21880422\n",
      "Iteration 1151, loss = 0.21849842\n",
      "Iteration 1152, loss = 0.21833711\n",
      "Iteration 1153, loss = 0.21813170\n",
      "Iteration 1154, loss = 0.21800436\n",
      "Iteration 1155, loss = 0.21787215\n",
      "Iteration 1156, loss = 0.21758971\n",
      "Iteration 1157, loss = 0.21743386\n",
      "Iteration 1158, loss = 0.21748476\n",
      "Iteration 1159, loss = 0.21716039\n",
      "Iteration 1160, loss = 0.21695182\n",
      "Iteration 1161, loss = 0.21685084\n",
      "Iteration 1162, loss = 0.21664706\n",
      "Iteration 1163, loss = 0.21648464\n",
      "Iteration 1164, loss = 0.21619866\n",
      "Iteration 1165, loss = 0.21601966\n",
      "Iteration 1166, loss = 0.21582454\n",
      "Iteration 1167, loss = 0.21566201\n",
      "Iteration 1168, loss = 0.21555088\n",
      "Iteration 1169, loss = 0.21530222\n",
      "Iteration 1170, loss = 0.21517172\n",
      "Iteration 1171, loss = 0.21506079\n",
      "Iteration 1172, loss = 0.21484017\n",
      "Iteration 1173, loss = 0.21471524\n",
      "Iteration 1174, loss = 0.21451856\n",
      "Iteration 1175, loss = 0.21428392\n",
      "Iteration 1176, loss = 0.21413174\n",
      "Iteration 1177, loss = 0.21399233\n",
      "Iteration 1178, loss = 0.21395334\n",
      "Iteration 1179, loss = 0.21361203\n",
      "Iteration 1180, loss = 0.21350967\n",
      "Iteration 1181, loss = 0.21327413\n",
      "Iteration 1182, loss = 0.21337966\n",
      "Iteration 1183, loss = 0.21290820\n",
      "Iteration 1184, loss = 0.21272571\n",
      "Iteration 1185, loss = 0.21255044\n",
      "Iteration 1186, loss = 0.21237112\n",
      "Iteration 1187, loss = 0.21223598\n",
      "Iteration 1188, loss = 0.21210575\n",
      "Iteration 1189, loss = 0.21190885\n",
      "Iteration 1190, loss = 0.21172150\n",
      "Iteration 1191, loss = 0.21154418\n",
      "Iteration 1192, loss = 0.21148482\n",
      "Iteration 1193, loss = 0.21119211\n",
      "Iteration 1194, loss = 0.21106555\n",
      "Iteration 1195, loss = 0.21089900\n",
      "Iteration 1196, loss = 0.21076362\n",
      "Iteration 1197, loss = 0.21056549\n",
      "Iteration 1198, loss = 0.21047450\n",
      "Iteration 1199, loss = 0.21028280\n",
      "Iteration 1200, loss = 0.21002703\n",
      "Iteration 1201, loss = 0.20989151\n",
      "Iteration 1202, loss = 0.20974365\n",
      "Iteration 1203, loss = 0.20952078\n",
      "Iteration 1204, loss = 0.20932072\n",
      "Iteration 1205, loss = 0.20918182\n",
      "Iteration 1206, loss = 0.20903576\n",
      "Iteration 1207, loss = 0.20884485\n",
      "Iteration 1208, loss = 0.20909757\n",
      "Iteration 1209, loss = 0.20860815\n",
      "Iteration 1210, loss = 0.20836990\n",
      "Iteration 1211, loss = 0.20818268\n",
      "Iteration 1212, loss = 0.20806216\n",
      "Iteration 1213, loss = 0.20791518\n",
      "Iteration 1214, loss = 0.20774775\n",
      "Iteration 1215, loss = 0.20757221\n",
      "Iteration 1216, loss = 0.20742746\n",
      "Iteration 1217, loss = 0.20724151\n",
      "Iteration 1218, loss = 0.20707419\n",
      "Iteration 1219, loss = 0.20686974\n",
      "Iteration 1220, loss = 0.20672492\n",
      "Iteration 1221, loss = 0.20680622\n",
      "Iteration 1222, loss = 0.20640072\n",
      "Iteration 1223, loss = 0.20622926\n",
      "Iteration 1224, loss = 0.20602706\n",
      "Iteration 1225, loss = 0.20590587\n",
      "Iteration 1226, loss = 0.20575597\n",
      "Iteration 1227, loss = 0.20562429\n",
      "Iteration 1228, loss = 0.20536720\n",
      "Iteration 1229, loss = 0.20531558\n",
      "Iteration 1230, loss = 0.20502266\n",
      "Iteration 1231, loss = 0.20489969\n",
      "Iteration 1232, loss = 0.20469088\n",
      "Iteration 1233, loss = 0.20459860\n",
      "Iteration 1234, loss = 0.20445828\n",
      "Iteration 1235, loss = 0.20422088\n",
      "Iteration 1236, loss = 0.20400394\n",
      "Iteration 1237, loss = 0.20388986\n",
      "Iteration 1238, loss = 0.20398900\n",
      "Iteration 1239, loss = 0.20352802\n",
      "Iteration 1240, loss = 0.20335505\n",
      "Iteration 1241, loss = 0.20336494\n",
      "Iteration 1242, loss = 0.20302826\n",
      "Iteration 1243, loss = 0.20283338\n",
      "Iteration 1244, loss = 0.20270245\n",
      "Iteration 1245, loss = 0.20249014\n",
      "Iteration 1246, loss = 0.20234840\n",
      "Iteration 1247, loss = 0.20220182\n",
      "Iteration 1248, loss = 0.20206505\n",
      "Iteration 1249, loss = 0.20184038\n",
      "Iteration 1250, loss = 0.20169981\n",
      "Iteration 1251, loss = 0.20146605\n",
      "Iteration 1252, loss = 0.20132546\n",
      "Iteration 1253, loss = 0.20124324\n",
      "Iteration 1254, loss = 0.20102168\n",
      "Iteration 1255, loss = 0.20103108\n",
      "Iteration 1256, loss = 0.20068245\n",
      "Iteration 1257, loss = 0.20054793\n",
      "Iteration 1258, loss = 0.20034990\n",
      "Iteration 1259, loss = 0.20017658\n",
      "Iteration 1260, loss = 0.20001951\n",
      "Iteration 1261, loss = 0.19988806\n",
      "Iteration 1262, loss = 0.19966424\n",
      "Iteration 1263, loss = 0.19948802\n",
      "Iteration 1264, loss = 0.19936993\n",
      "Iteration 1265, loss = 0.19925455\n",
      "Iteration 1266, loss = 0.19920840\n",
      "Iteration 1267, loss = 0.19886647\n",
      "Iteration 1268, loss = 0.19858580\n",
      "Iteration 1269, loss = 0.19846013\n",
      "Iteration 1270, loss = 0.19831160\n",
      "Iteration 1271, loss = 0.19808067\n",
      "Iteration 1272, loss = 0.19808599\n",
      "Iteration 1273, loss = 0.19788826\n",
      "Iteration 1274, loss = 0.19752509\n",
      "Iteration 1275, loss = 0.19744303\n",
      "Iteration 1276, loss = 0.19735911\n",
      "Iteration 1277, loss = 0.19713487\n",
      "Iteration 1278, loss = 0.19701169\n",
      "Iteration 1279, loss = 0.19681884\n",
      "Iteration 1280, loss = 0.19657393\n",
      "Iteration 1281, loss = 0.19652646\n",
      "Iteration 1282, loss = 0.19640434\n",
      "Iteration 1283, loss = 0.19618988\n",
      "Iteration 1284, loss = 0.19588966\n",
      "Iteration 1285, loss = 0.19574308\n",
      "Iteration 1286, loss = 0.19550600\n",
      "Iteration 1287, loss = 0.19545741\n",
      "Iteration 1288, loss = 0.19544739\n",
      "Iteration 1289, loss = 0.19508216\n",
      "Iteration 1290, loss = 0.19487684\n",
      "Iteration 1291, loss = 0.19472664\n",
      "Iteration 1292, loss = 0.19447220\n",
      "Iteration 1293, loss = 0.19441449\n",
      "Iteration 1294, loss = 0.19419321\n",
      "Iteration 1295, loss = 0.19411541\n",
      "Iteration 1296, loss = 0.19438195\n",
      "Iteration 1297, loss = 0.19375504\n",
      "Iteration 1298, loss = 0.19368524\n",
      "Iteration 1299, loss = 0.19337937\n",
      "Iteration 1300, loss = 0.19338725\n",
      "Iteration 1301, loss = 0.19310192\n",
      "Iteration 1302, loss = 0.19308402\n",
      "Iteration 1303, loss = 0.19261171\n",
      "Iteration 1304, loss = 0.19243660\n",
      "Iteration 1305, loss = 0.19230158\n",
      "Iteration 1306, loss = 0.19211577\n",
      "Iteration 1307, loss = 0.19210824\n",
      "Iteration 1308, loss = 0.19178283\n",
      "Iteration 1309, loss = 0.19177808\n",
      "Iteration 1310, loss = 0.19169598\n",
      "Iteration 1311, loss = 0.19134393\n",
      "Iteration 1312, loss = 0.19112378\n",
      "Iteration 1313, loss = 0.19100704\n",
      "Iteration 1314, loss = 0.19078744\n",
      "Iteration 1315, loss = 0.19071651\n",
      "Iteration 1316, loss = 0.19047202\n",
      "Iteration 1317, loss = 0.19026233\n",
      "Iteration 87, loss = 0.42467035\n",
      "Iteration 88, loss = 0.42312884\n",
      "Iteration 89, loss = 0.42146961\n",
      "Iteration 90, loss = 0.42002244\n",
      "Iteration 91, loss = 0.41845808\n",
      "Iteration 92, loss = 0.41697583\n",
      "Iteration 93, loss = 0.41557942\n",
      "Iteration 94, loss = 0.41411263\n",
      "Iteration 95, loss = 0.41278381\n",
      "Iteration 96, loss = 0.41146681\n",
      "Iteration 97, loss = 0.41009001\n",
      "Iteration 98, loss = 0.40884114\n",
      "Iteration 99, loss = 0.40754226\n",
      "Iteration 100, loss = 0.40635408\n",
      "Iteration 101, loss = 0.40511398\n",
      "Iteration 102, loss = 0.40385563\n",
      "Iteration 103, loss = 0.40266082\n",
      "Iteration 104, loss = 0.40154416\n",
      "Iteration 105, loss = 0.40042886\n",
      "Iteration 106, loss = 0.39933373\n",
      "Iteration 107, loss = 0.39825731\n",
      "Iteration 108, loss = 0.39717549\n",
      "Iteration 109, loss = 0.39615245\n",
      "Iteration 110, loss = 0.39511751\n",
      "Iteration 111, loss = 0.39409529\n",
      "Iteration 112, loss = 0.39315092\n",
      "Iteration 113, loss = 0.39218969\n",
      "Iteration 114, loss = 0.39123891\n",
      "Iteration 115, loss = 0.39030179\n",
      "Iteration 116, loss = 0.38946166\n",
      "Iteration 117, loss = 0.38858124\n",
      "Iteration 118, loss = 0.38768612\n",
      "Iteration 119, loss = 0.38682484\n",
      "Iteration 120, loss = 0.38594221\n",
      "Iteration 121, loss = 0.38515985\n",
      "Iteration 122, loss = 0.38434121\n",
      "Iteration 123, loss = 0.38357229\n",
      "Iteration 124, loss = 0.38278746\n",
      "Iteration 125, loss = 0.38207419\n",
      "Iteration 126, loss = 0.38130931\n",
      "Iteration 127, loss = 0.38058026\n",
      "Iteration 128, loss = 0.37985774\n",
      "Iteration 129, loss = 0.37915344\n",
      "Iteration 130, loss = 0.37847835\n",
      "Iteration 131, loss = 0.37780501\n",
      "Iteration 132, loss = 0.37715569\n",
      "Iteration 133, loss = 0.37644227\n",
      "Iteration 134, loss = 0.37585433\n",
      "Iteration 135, loss = 0.37520281\n",
      "Iteration 136, loss = 0.37456108\n",
      "Iteration 137, loss = 0.37397681\n",
      "Iteration 138, loss = 0.37341533\n",
      "Iteration 139, loss = 0.37278439\n",
      "Iteration 140, loss = 0.37224373\n",
      "Iteration 141, loss = 0.37170461\n",
      "Iteration 142, loss = 0.37110331\n",
      "Iteration 143, loss = 0.37054184\n",
      "Iteration 144, loss = 0.37003404\n",
      "Iteration 145, loss = 0.36950082\n",
      "Iteration 146, loss = 0.36894561\n",
      "Iteration 147, loss = 0.36843495\n",
      "Iteration 148, loss = 0.36790769\n",
      "Iteration 149, loss = 0.36744650\n",
      "Iteration 150, loss = 0.36692777\n",
      "Iteration 151, loss = 0.36648331\n",
      "Iteration 152, loss = 0.36596149\n",
      "Iteration 153, loss = 0.36552308\n",
      "Iteration 154, loss = 0.36502872\n",
      "Iteration 155, loss = 0.36458551\n",
      "Iteration 156, loss = 0.36412203\n",
      "Iteration 157, loss = 0.36370667\n",
      "Iteration 158, loss = 0.36323760\n",
      "Iteration 159, loss = 0.36283431\n",
      "Iteration 160, loss = 0.36241838\n",
      "Iteration 161, loss = 0.36198769\n",
      "Iteration 162, loss = 0.36155972\n",
      "Iteration 163, loss = 0.36121110\n",
      "Iteration 164, loss = 0.36077834\n",
      "Iteration 165, loss = 0.36037419\n",
      "Iteration 166, loss = 0.36000674\n",
      "Iteration 167, loss = 0.35959604\n",
      "Iteration 168, loss = 0.35923600\n",
      "Iteration 169, loss = 0.35887810\n",
      "Iteration 170, loss = 0.35850041\n",
      "Iteration 171, loss = 0.35809982\n",
      "Iteration 172, loss = 0.35774399\n",
      "Iteration 173, loss = 0.35739970\n",
      "Iteration 174, loss = 0.35704650\n",
      "Iteration 175, loss = 0.35669401\n",
      "Iteration 176, loss = 0.35638951\n",
      "Iteration 177, loss = 0.35602776\n",
      "Iteration 178, loss = 0.35568158\n",
      "Iteration 179, loss = 0.35534518\n",
      "Iteration 180, loss = 0.35500852\n",
      "Iteration 181, loss = 0.35470383\n",
      "Iteration 182, loss = 0.35440301\n",
      "Iteration 183, loss = 0.35405935\n",
      "Iteration 184, loss = 0.35373026\n",
      "Iteration 185, loss = 0.35341462\n",
      "Iteration 186, loss = 0.35311627\n",
      "Iteration 187, loss = 0.35280778\n",
      "Iteration 188, loss = 0.35250517\n",
      "Iteration 189, loss = 0.35222546\n",
      "Iteration 190, loss = 0.35191480\n",
      "Iteration 191, loss = 0.35166180\n",
      "Iteration 192, loss = 0.35136858\n",
      "Iteration 193, loss = 0.35108351\n",
      "Iteration 194, loss = 0.35079529\n",
      "Iteration 195, loss = 0.35050127\n",
      "Iteration 196, loss = 0.35022092\n",
      "Iteration 197, loss = 0.34995049\n",
      "Iteration 198, loss = 0.34967312\n",
      "Iteration 199, loss = 0.34941535\n",
      "Iteration 200, loss = 0.34916349\n",
      "Iteration 201, loss = 0.34888448\n",
      "Iteration 202, loss = 0.34862304\n",
      "Iteration 203, loss = 0.34839113\n",
      "Iteration 204, loss = 0.34812938\n",
      "Iteration 205, loss = 0.34787336\n",
      "Iteration 206, loss = 0.34760617\n",
      "Iteration 207, loss = 0.34735100\n",
      "Iteration 208, loss = 0.34712940\n",
      "Iteration 209, loss = 0.34688758\n",
      "Iteration 210, loss = 0.34664069\n",
      "Iteration 211, loss = 0.34638113\n",
      "Iteration 212, loss = 0.34615393\n",
      "Iteration 213, loss = 0.34591694\n",
      "Iteration 214, loss = 0.34571962\n",
      "Iteration 215, loss = 0.34544881\n",
      "Iteration 216, loss = 0.34520910\n",
      "Iteration 217, loss = 0.34499428\n",
      "Iteration 218, loss = 0.34475924\n",
      "Iteration 219, loss = 0.34452334\n",
      "Iteration 220, loss = 0.34431135\n",
      "Iteration 221, loss = 0.34409287\n",
      "Iteration 222, loss = 0.34385108\n",
      "Iteration 223, loss = 0.34363186\n",
      "Iteration 224, loss = 0.34342596\n",
      "Iteration 225, loss = 0.34321705\n",
      "Iteration 226, loss = 0.34300081\n",
      "Iteration 227, loss = 0.34278365\n",
      "Iteration 228, loss = 0.34257793\n",
      "Iteration 229, loss = 0.34237939\n",
      "Iteration 230, loss = 0.34215889\n",
      "Iteration 231, loss = 0.34194747\n",
      "Iteration 232, loss = 0.34175641\n",
      "Iteration 233, loss = 0.34153331\n",
      "Iteration 234, loss = 0.34133097\n",
      "Iteration 235, loss = 0.34113767\n",
      "Iteration 236, loss = 0.34098191\n",
      "Iteration 237, loss = 0.34075041\n",
      "Iteration 238, loss = 0.34053303\n",
      "Iteration 239, loss = 0.34036348\n",
      "Iteration 240, loss = 0.34015810\n",
      "Iteration 241, loss = 0.33994965\n",
      "Iteration 242, loss = 0.33976613\n",
      "Iteration 243, loss = 0.33957884\n",
      "Iteration 244, loss = 0.33938588\n",
      "Iteration 245, loss = 0.33922677\n",
      "Iteration 246, loss = 0.33901418\n",
      "Iteration 247, loss = 0.33884507\n",
      "Iteration 248, loss = 0.33868700\n",
      "Iteration 249, loss = 0.33850685\n",
      "Iteration 250, loss = 0.33829869\n",
      "Iteration 251, loss = 0.33814549\n",
      "Iteration 252, loss = 0.33795229\n",
      "Iteration 253, loss = 0.33779265\n",
      "Iteration 254, loss = 0.33760694\n",
      "Iteration 255, loss = 0.33742515\n",
      "Iteration 256, loss = 0.33725298\n",
      "Iteration 257, loss = 0.33708109\n",
      "Iteration 258, loss = 0.33693289\n",
      "Iteration 259, loss = 0.33673731\n",
      "Iteration 260, loss = 0.33655729\n",
      "Iteration 261, loss = 0.33641579\n",
      "Iteration 262, loss = 0.33624845\n",
      "Iteration 263, loss = 0.33607592\n",
      "Iteration 264, loss = 0.33590559\n",
      "Iteration 265, loss = 0.33573981\n",
      "Iteration 266, loss = 0.33559475\n",
      "Iteration 267, loss = 0.33541467\n",
      "Iteration 268, loss = 0.33526448\n",
      "Iteration 269, loss = 0.33509728\n",
      "Iteration 270, loss = 0.33497798\n",
      "Iteration 271, loss = 0.33479339\n",
      "Iteration 272, loss = 0.33463299\n",
      "Iteration 273, loss = 0.33447644\n",
      "Iteration 274, loss = 0.33432729\n",
      "Iteration 275, loss = 0.33416506\n",
      "Iteration 276, loss = 0.33401448\n",
      "Iteration 277, loss = 0.33387780\n",
      "Iteration 278, loss = 0.33370482\n",
      "Iteration 279, loss = 0.33355649\n",
      "Iteration 280, loss = 0.33342173\n",
      "Iteration 281, loss = 0.33329183\n",
      "Iteration 282, loss = 0.33310890\n",
      "Iteration 283, loss = 0.33297517\n",
      "Iteration 284, loss = 0.33282307\n",
      "Iteration 285, loss = 0.33267893\n",
      "Iteration 286, loss = 0.33252668\n",
      "Iteration 287, loss = 0.33240182\n",
      "Iteration 288, loss = 0.33224279\n",
      "Iteration 289, loss = 0.33210694\n",
      "Iteration 290, loss = 0.33195485\n",
      "Iteration 291, loss = 0.33181995\n",
      "Iteration 292, loss = 0.33168232\n",
      "Iteration 293, loss = 0.33154991\n",
      "Iteration 294, loss = 0.33140443\n",
      "Iteration 295, loss = 0.33127854\n",
      "Iteration 296, loss = 0.33111615\n",
      "Iteration 297, loss = 0.33100274\n",
      "Iteration 298, loss = 0.33085837\n",
      "Iteration 299, loss = 0.33073817\n",
      "Iteration 300, loss = 0.33059048\n",
      "Iteration 301, loss = 0.33044493\n",
      "Iteration 302, loss = 0.33031849\n",
      "Iteration 303, loss = 0.33019657\n",
      "Iteration 304, loss = 0.33006254\n",
      "Iteration 305, loss = 0.32993576\n",
      "Iteration 306, loss = 0.32979261\n",
      "Iteration 307, loss = 0.32969406\n",
      "Iteration 308, loss = 0.32954649\n",
      "Iteration 309, loss = 0.32940762\n",
      "Iteration 310, loss = 0.32930663\n",
      "Iteration 311, loss = 0.32915680\n",
      "Iteration 312, loss = 0.32902323\n",
      "Iteration 313, loss = 0.32889976\n",
      "Iteration 314, loss = 0.32876614\n",
      "Iteration 315, loss = 0.32866536\n",
      "Iteration 316, loss = 0.32851075\n",
      "Iteration 317, loss = 0.32840752\n",
      "Iteration 318, loss = 0.32826797\n",
      "Iteration 319, loss = 0.32813326\n",
      "Iteration 320, loss = 0.32801026\n",
      "Iteration 321, loss = 0.32791929\n",
      "Iteration 322, loss = 0.32778140\n",
      "Iteration 323, loss = 0.32764293\n",
      "Iteration 324, loss = 0.32752722\n",
      "Iteration 325, loss = 0.32739539\n",
      "Iteration 326, loss = 0.32730923\n",
      "Iteration 327, loss = 0.32715580\n",
      "Iteration 328, loss = 0.32703657\n",
      "Iteration 329, loss = 0.32693542\n",
      "Iteration 330, loss = 0.32679193\n",
      "Iteration 331, loss = 0.32667535\n",
      "Iteration 332, loss = 0.32656254\n",
      "Iteration 333, loss = 0.32643803\n",
      "Iteration 334, loss = 0.32632560\n",
      "Iteration 335, loss = 0.32619761\n",
      "Iteration 336, loss = 0.32611493\n",
      "Iteration 337, loss = 0.32595814\n",
      "Iteration 338, loss = 0.32584060\n",
      "Iteration 339, loss = 0.32572694\n",
      "Iteration 340, loss = 0.32562629\n",
      "Iteration 341, loss = 0.32550360\n",
      "Iteration 342, loss = 0.32538729\n",
      "Iteration 343, loss = 0.32527608\n",
      "Iteration 344, loss = 0.32514633\n",
      "Iteration 345, loss = 0.32504804\n",
      "Iteration 346, loss = 0.32493823\n",
      "Iteration 347, loss = 0.32481518\n",
      "Iteration 348, loss = 0.32471331\n",
      "Iteration 349, loss = 0.32459045\n",
      "Iteration 350, loss = 0.32452949\n",
      "Iteration 351, loss = 0.32436556\n",
      "Iteration 352, loss = 0.32427843\n",
      "Iteration 353, loss = 0.32419727\n",
      "Iteration 354, loss = 0.32406964\n",
      "Iteration 355, loss = 0.32395812\n",
      "Iteration 356, loss = 0.32383353\n",
      "Iteration 357, loss = 0.32372523\n",
      "Iteration 358, loss = 0.32361269\n",
      "Iteration 359, loss = 0.32350322\n",
      "Iteration 360, loss = 0.32338295\n",
      "Iteration 361, loss = 0.32328570\n",
      "Iteration 362, loss = 0.32319215\n",
      "Iteration 363, loss = 0.32307938\n",
      "Iteration 364, loss = 0.32295832\n",
      "Iteration 365, loss = 0.32286138\n",
      "Iteration 366, loss = 0.32275484\n",
      "Iteration 367, loss = 0.32264808\n",
      "Iteration 368, loss = 0.32253554\n",
      "Iteration 369, loss = 0.32242579\n",
      "Iteration 370, loss = 0.32234075\n",
      "Iteration 371, loss = 0.32227522\n",
      "Iteration 372, loss = 0.32212629\n",
      "Iteration 373, loss = 0.32205747\n",
      "Iteration 374, loss = 0.32192332\n",
      "Iteration 375, loss = 0.32180235\n",
      "Iteration 376, loss = 0.32170940\n",
      "Iteration 377, loss = 0.32160799\n",
      "Iteration 378, loss = 0.32150332\n",
      "Iteration 379, loss = 0.32140002\n",
      "Iteration 380, loss = 0.32129312\n",
      "Iteration 381, loss = 0.32120589\n",
      "Iteration 382, loss = 0.32107440\n",
      "Iteration 383, loss = 0.32098978\n",
      "Iteration 384, loss = 0.32087755\n",
      "Iteration 385, loss = 0.32076964\n",
      "Iteration 386, loss = 0.32067513\n",
      "Iteration 387, loss = 0.32056638\n",
      "Iteration 388, loss = 0.32047306\n",
      "Iteration 389, loss = 0.32038074\n",
      "Iteration 390, loss = 0.32030653\n",
      "Iteration 391, loss = 0.32017644\n",
      "Iteration 392, loss = 0.32008978\n",
      "Iteration 393, loss = 0.31997432\n",
      "Iteration 394, loss = 0.31991468\n",
      "Iteration 395, loss = 0.31975800\n",
      "Iteration 396, loss = 0.31966673\n",
      "Iteration 397, loss = 0.31957333\n",
      "Iteration 398, loss = 0.31947669\n",
      "Iteration 399, loss = 0.31942939\n",
      "Iteration 400, loss = 0.31929152\n",
      "Iteration 401, loss = 0.31920383\n",
      "Iteration 402, loss = 0.31910505\n",
      "Iteration 403, loss = 0.31899394\n",
      "Iteration 404, loss = 0.31891455\n",
      "Iteration 405, loss = 0.31880140\n",
      "Iteration 406, loss = 0.31871695\n",
      "Iteration 407, loss = 0.31859938\n",
      "Iteration 408, loss = 0.31850491\n",
      "Iteration 409, loss = 0.31846497\n",
      "Iteration 410, loss = 0.31832399\n",
      "Iteration 411, loss = 0.31823557\n",
      "Iteration 412, loss = 0.31814920\n",
      "Iteration 413, loss = 0.31803640\n",
      "Iteration 414, loss = 0.31794244\n",
      "Iteration 415, loss = 0.31784593\n",
      "Iteration 416, loss = 0.31775721\n",
      "Iteration 417, loss = 0.31765719\n",
      "Iteration 418, loss = 0.31756285\n",
      "Iteration 419, loss = 0.31747626\n",
      "Iteration 420, loss = 0.31739289\n",
      "Iteration 421, loss = 0.31731263\n",
      "Iteration 422, loss = 0.31719463\n",
      "Iteration 423, loss = 0.31710762\n",
      "Iteration 424, loss = 0.31703649\n",
      "Iteration 425, loss = 0.31692174\n",
      "Iteration 426, loss = 0.31684253\n",
      "Iteration 427, loss = 0.31673608\n",
      "Iteration 428, loss = 0.31666251\n",
      "Iteration 429, loss = 0.31655553\n",
      "Iteration 430, loss = 0.31646786\n",
      "Iteration 431, loss = 0.31638153\n",
      "Iteration 432, loss = 0.31628634\n",
      "Iteration 433, loss = 0.31620987\n",
      "Iteration 434, loss = 0.31610063\n",
      "Iteration 435, loss = 0.31601082\n",
      "Iteration 436, loss = 0.31592805\n",
      "Iteration 437, loss = 0.31583542\n",
      "Iteration 438, loss = 0.31575795\n",
      "Iteration 439, loss = 0.31565587\n",
      "Iteration 440, loss = 0.31557239\n",
      "Iteration 441, loss = 0.31548767\n",
      "Iteration 442, loss = 0.31539972\n",
      "Iteration 443, loss = 0.31529892\n",
      "Iteration 444, loss = 0.31523612\n",
      "Iteration 445, loss = 0.31513251\n",
      "Iteration 446, loss = 0.31506127\n",
      "Iteration 447, loss = 0.31497191\n",
      "Iteration 448, loss = 0.31489362\n",
      "Iteration 449, loss = 0.31479721\n",
      "Iteration 450, loss = 0.31470023\n",
      "Iteration 451, loss = 0.31462419\n",
      "Iteration 452, loss = 0.31453396\n",
      "Iteration 453, loss = 0.31443567\n",
      "Iteration 454, loss = 0.31435695\n",
      "Iteration 455, loss = 0.31426614\n",
      "Iteration 456, loss = 0.31419929\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77954674\n",
      "Iteration 2, loss = 0.77660257\n",
      "Iteration 3, loss = 0.77207296\n",
      "Iteration 4, loss = 0.76656036\n",
      "Iteration 5, loss = 0.76031709\n",
      "Iteration 6, loss = 0.75352073\n",
      "Iteration 7, loss = 0.74651647\n",
      "Iteration 8, loss = 0.73957212\n",
      "Iteration 9, loss = 0.73235912\n",
      "Iteration 10, loss = 0.72533795\n",
      "Iteration 11, loss = 0.71827259\n",
      "Iteration 12, loss = 0.71142149\n",
      "Iteration 13, loss = 0.70462231\n",
      "Iteration 14, loss = 0.69808570\n",
      "Iteration 15, loss = 0.69161512\n",
      "Iteration 16, loss = 0.68513839\n",
      "Iteration 17, loss = 0.67911173\n",
      "Iteration 18, loss = 0.67300931\n",
      "Iteration 19, loss = 0.66693167\n",
      "Iteration 20, loss = 0.66133930\n",
      "Iteration 21, loss = 0.65557336\n",
      "Iteration 22, loss = 0.64997631\n",
      "Iteration 23, loss = 0.64454803\n",
      "Iteration 24, loss = 0.63924672\n",
      "Iteration 25, loss = 0.63388684\n",
      "Iteration 26, loss = 0.62870459\n",
      "Iteration 27, loss = 0.62365840\n",
      "Iteration 28, loss = 0.61877383\n",
      "Iteration 29, loss = 0.61385344\n",
      "Iteration 30, loss = 0.60908497\n",
      "Iteration 31, loss = 0.60432760\n",
      "Iteration 32, loss = 0.59970924\n",
      "Iteration 33, loss = 0.59517553\n",
      "Iteration 34, loss = 0.59067383\n",
      "Iteration 35, loss = 0.58624274\n",
      "Iteration 36, loss = 0.58193016\n",
      "Iteration 37, loss = 0.57759549\n",
      "Iteration 38, loss = 0.57338183\n",
      "Iteration 39, loss = 0.56939106\n",
      "Iteration 40, loss = 0.56523411\n",
      "Iteration 41, loss = 0.56121402\n",
      "Iteration 42, loss = 0.55734441\n",
      "Iteration 43, loss = 0.55344536\n",
      "Iteration 44, loss = 0.54967203\n",
      "Iteration 45, loss = 0.54595425\n",
      "Iteration 46, loss = 0.54221587\n",
      "Iteration 47, loss = 0.53870245\n",
      "Iteration 48, loss = 0.53517350\n",
      "Iteration 49, loss = 0.53169853\n",
      "Iteration 50, loss = 0.52832322\n",
      "Iteration 51, loss = 0.52493807\n",
      "Iteration 52, loss = 0.52176460\n",
      "Iteration 53, loss = 0.51852688\n",
      "Iteration 54, loss = 0.51531963\n",
      "Iteration 55, loss = 0.51235250\n",
      "Iteration 56, loss = 0.50918539\n",
      "Iteration 57, loss = 0.50622440\n",
      "Iteration 58, loss = 0.50334673\n",
      "Iteration 59, loss = 0.50050545\n",
      "Iteration 60, loss = 0.49774289\n",
      "Iteration 61, loss = 0.49503832\n",
      "Iteration 62, loss = 0.49233537\n",
      "Iteration 63, loss = 0.48966807\n",
      "Iteration 64, loss = 0.48719809\n",
      "Iteration 65, loss = 0.48467810\n",
      "Iteration 66, loss = 0.48217821\n",
      "Iteration 67, loss = 0.47974214\n",
      "Iteration 68, loss = 0.47744091\n",
      "Iteration 69, loss = 0.47521930\n",
      "Iteration 70, loss = 0.47297883\n",
      "Iteration 71, loss = 0.47078718\n",
      "Iteration 72, loss = 0.46863714\n",
      "Iteration 73, loss = 0.46648650\n",
      "Iteration 74, loss = 0.46448983\n",
      "Iteration 75, loss = 0.46248902\n",
      "Iteration 76, loss = 0.46053645\n",
      "Iteration 77, loss = 0.45858276\n",
      "Iteration 78, loss = 0.45675672\n",
      "Iteration 79, loss = 0.45493708\n",
      "Iteration 80, loss = 0.45319453\n",
      "Iteration 81, loss = 0.45139731\n",
      "Iteration 82, loss = 0.44974532\n",
      "Iteration 83, loss = 0.44807997\n",
      "Iteration 84, loss = 0.44647163\n",
      "Iteration 85, loss = 0.44489872\n",
      "Iteration 86, loss = 0.44343412\n",
      "Iteration 87, loss = 0.44188548\n",
      "Iteration 88, loss = 0.44042143\n",
      "Iteration 89, loss = 0.43895713\n",
      "Iteration 90, loss = 0.43760384\n",
      "Iteration 91, loss = 0.43627753\n",
      "Iteration 92, loss = 0.43488639\n",
      "Iteration 93, loss = 0.43356080\n",
      "Iteration 94, loss = 0.43235239\n",
      "Iteration 95, loss = 0.43108319\n",
      "Iteration 96, loss = 0.42985805\n",
      "Iteration 97, loss = 0.42868512\n",
      "Iteration 98, loss = 0.42752047\n",
      "Iteration 99, loss = 0.42640306\n",
      "Iteration 100, loss = 0.42521211\n",
      "Iteration 101, loss = 0.42417927\n",
      "Iteration 102, loss = 0.42308958\n",
      "Iteration 103, loss = 0.42202579\n",
      "Iteration 104, loss = 0.42102699\n",
      "Iteration 105, loss = 0.42009950\n",
      "Iteration 106, loss = 0.41901658\n",
      "Iteration 107, loss = 0.41810635\n",
      "Iteration 108, loss = 0.41714472\n",
      "Iteration 109, loss = 0.41620470\n",
      "Iteration 110, loss = 0.41530494\n",
      "Iteration 111, loss = 0.41442365\n",
      "Iteration 112, loss = 0.41354133\n",
      "Iteration 113, loss = 0.41266958\n",
      "Iteration 114, loss = 0.41185745\n",
      "Iteration 115, loss = 0.41106972\n",
      "Iteration 116, loss = 0.41023028\n",
      "Iteration 117, loss = 0.40949922\n",
      "Iteration 118, loss = 0.40868732\n",
      "Iteration 119, loss = 0.40789806\n",
      "Iteration 120, loss = 0.40718952\n",
      "Iteration 121, loss = 0.40642739\n",
      "Iteration 122, loss = 0.40570005\n",
      "Iteration 123, loss = 0.40499736\n",
      "Iteration 124, loss = 0.40430695\n",
      "Iteration 125, loss = 0.40361268\n",
      "Iteration 126, loss = 0.40294199\n",
      "Iteration 255, loss = 0.35074918\n",
      "Iteration 256, loss = 0.35056461\n",
      "Iteration 257, loss = 0.35035726\n",
      "Iteration 258, loss = 0.35016750\n",
      "Iteration 259, loss = 0.35002104\n",
      "Iteration 260, loss = 0.34978904\n",
      "Iteration 261, loss = 0.34958975\n",
      "Iteration 262, loss = 0.34940495\n",
      "Iteration 263, loss = 0.34921735\n",
      "Iteration 264, loss = 0.34902612\n",
      "Iteration 265, loss = 0.34884769\n",
      "Iteration 266, loss = 0.34866105\n",
      "Iteration 267, loss = 0.34850332\n",
      "Iteration 268, loss = 0.34828007\n",
      "Iteration 269, loss = 0.34811294\n",
      "Iteration 270, loss = 0.34790790\n",
      "Iteration 271, loss = 0.34773677\n",
      "Iteration 272, loss = 0.34756843\n",
      "Iteration 273, loss = 0.34739705\n",
      "Iteration 274, loss = 0.34719626\n",
      "Iteration 275, loss = 0.34702338\n",
      "Iteration 276, loss = 0.34683076\n",
      "Iteration 277, loss = 0.34666320\n",
      "Iteration 278, loss = 0.34647759\n",
      "Iteration 279, loss = 0.34631164\n",
      "Iteration 280, loss = 0.34612305\n",
      "Iteration 281, loss = 0.34594328\n",
      "Iteration 282, loss = 0.34577654\n",
      "Iteration 283, loss = 0.34559155\n",
      "Iteration 284, loss = 0.34543113\n",
      "Iteration 285, loss = 0.34523452\n",
      "Iteration 286, loss = 0.34506170\n",
      "Iteration 287, loss = 0.34489671\n",
      "Iteration 288, loss = 0.34473325\n",
      "Iteration 289, loss = 0.34454873\n",
      "Iteration 290, loss = 0.34437884\n",
      "Iteration 291, loss = 0.34420310\n",
      "Iteration 292, loss = 0.34403150\n",
      "Iteration 293, loss = 0.34386277\n",
      "Iteration 294, loss = 0.34371505\n",
      "Iteration 295, loss = 0.34351921\n",
      "Iteration 296, loss = 0.34335112\n",
      "Iteration 297, loss = 0.34320327\n",
      "Iteration 298, loss = 0.34302796\n",
      "Iteration 299, loss = 0.34292315\n",
      "Iteration 300, loss = 0.34269592\n",
      "Iteration 301, loss = 0.34254553\n",
      "Iteration 302, loss = 0.34238038\n",
      "Iteration 303, loss = 0.34222014\n",
      "Iteration 304, loss = 0.34203023\n",
      "Iteration 305, loss = 0.34189179\n",
      "Iteration 306, loss = 0.34172857\n",
      "Iteration 307, loss = 0.34155104\n",
      "Iteration 308, loss = 0.34146605\n",
      "Iteration 309, loss = 0.34123963\n",
      "Iteration 310, loss = 0.34109199\n",
      "Iteration 311, loss = 0.34092111\n",
      "Iteration 312, loss = 0.34074888\n",
      "Iteration 313, loss = 0.34062250\n",
      "Iteration 314, loss = 0.34045850\n",
      "Iteration 315, loss = 0.34030017\n",
      "Iteration 316, loss = 0.34013235\n",
      "Iteration 317, loss = 0.33996867\n",
      "Iteration 318, loss = 0.33983539\n",
      "Iteration 319, loss = 0.33967743\n",
      "Iteration 320, loss = 0.33951813\n",
      "Iteration 321, loss = 0.33935286\n",
      "Iteration 322, loss = 0.33920134\n",
      "Iteration 323, loss = 0.33902684\n",
      "Iteration 324, loss = 0.33890119\n",
      "Iteration 325, loss = 0.33872842\n",
      "Iteration 326, loss = 0.33857234\n",
      "Iteration 327, loss = 0.33841031\n",
      "Iteration 328, loss = 0.33826007\n",
      "Iteration 329, loss = 0.33813432\n",
      "Iteration 330, loss = 0.33797250\n",
      "Iteration 331, loss = 0.33780880\n",
      "Iteration 332, loss = 0.33763151\n",
      "Iteration 333, loss = 0.33751561\n",
      "Iteration 334, loss = 0.33733222\n",
      "Iteration 335, loss = 0.33716654\n",
      "Iteration 336, loss = 0.33703181\n",
      "Iteration 337, loss = 0.33689090\n",
      "Iteration 338, loss = 0.33677657\n",
      "Iteration 339, loss = 0.33662846\n",
      "Iteration 340, loss = 0.33644309\n",
      "Iteration 341, loss = 0.33627534\n",
      "Iteration 342, loss = 0.33613774\n",
      "Iteration 343, loss = 0.33598590\n",
      "Iteration 344, loss = 0.33583728\n",
      "Iteration 345, loss = 0.33567055\n",
      "Iteration 346, loss = 0.33552901\n",
      "Iteration 347, loss = 0.33537131\n",
      "Iteration 348, loss = 0.33524005\n",
      "Iteration 349, loss = 0.33509044\n",
      "Iteration 350, loss = 0.33494809\n",
      "Iteration 351, loss = 0.33481486\n",
      "Iteration 352, loss = 0.33466666\n",
      "Iteration 353, loss = 0.33452786\n",
      "Iteration 354, loss = 0.33435852\n",
      "Iteration 355, loss = 0.33421990\n",
      "Iteration 356, loss = 0.33407928\n",
      "Iteration 357, loss = 0.33395686\n",
      "Iteration 358, loss = 0.33378517\n",
      "Iteration 359, loss = 0.33365976\n",
      "Iteration 360, loss = 0.33353460\n",
      "Iteration 361, loss = 0.33335796\n",
      "Iteration 362, loss = 0.33321068\n",
      "Iteration 363, loss = 0.33307641\n",
      "Iteration 364, loss = 0.33293263\n",
      "Iteration 365, loss = 0.33277788\n",
      "Iteration 366, loss = 0.33263620\n",
      "Iteration 367, loss = 0.33249308\n",
      "Iteration 368, loss = 0.33235747\n",
      "Iteration 369, loss = 0.33221876\n",
      "Iteration 370, loss = 0.33207170\n",
      "Iteration 371, loss = 0.33196418\n",
      "Iteration 372, loss = 0.33180072\n",
      "Iteration 373, loss = 0.33164102\n",
      "Iteration 374, loss = 0.33149597\n",
      "Iteration 375, loss = 0.33143229\n",
      "Iteration 376, loss = 0.33122009\n",
      "Iteration 377, loss = 0.33106940\n",
      "Iteration 378, loss = 0.33092790\n",
      "Iteration 379, loss = 0.33081402\n",
      "Iteration 380, loss = 0.33065109\n",
      "Iteration 381, loss = 0.33050317\n",
      "Iteration 382, loss = 0.33036450\n",
      "Iteration 383, loss = 0.33027814\n",
      "Iteration 384, loss = 0.33008313\n",
      "Iteration 385, loss = 0.32997169\n",
      "Iteration 386, loss = 0.32979836\n",
      "Iteration 387, loss = 0.32964949\n",
      "Iteration 388, loss = 0.32952134\n",
      "Iteration 389, loss = 0.32939094\n",
      "Iteration 390, loss = 0.32923709\n",
      "Iteration 391, loss = 0.32909509\n",
      "Iteration 392, loss = 0.32896733\n",
      "Iteration 393, loss = 0.32881535\n",
      "Iteration 394, loss = 0.32866836\n",
      "Iteration 395, loss = 0.32852370\n",
      "Iteration 396, loss = 0.32839438\n",
      "Iteration 397, loss = 0.32824976\n",
      "Iteration 398, loss = 0.32811920\n",
      "Iteration 399, loss = 0.32796609\n",
      "Iteration 400, loss = 0.32784202\n",
      "Iteration 401, loss = 0.32770651\n",
      "Iteration 402, loss = 0.32754382\n",
      "Iteration 403, loss = 0.32740656\n",
      "Iteration 404, loss = 0.32727069\n",
      "Iteration 405, loss = 0.32712299\n",
      "Iteration 406, loss = 0.32698638\n",
      "Iteration 407, loss = 0.32683605\n",
      "Iteration 408, loss = 0.32670322\n",
      "Iteration 409, loss = 0.32655620\n",
      "Iteration 410, loss = 0.32644232\n",
      "Iteration 411, loss = 0.32630847\n",
      "Iteration 412, loss = 0.32616307\n",
      "Iteration 413, loss = 0.32600639\n",
      "Iteration 414, loss = 0.32588946\n",
      "Iteration 415, loss = 0.32572339\n",
      "Iteration 416, loss = 0.32558894\n",
      "Iteration 417, loss = 0.32545641\n",
      "Iteration 418, loss = 0.32537568\n",
      "Iteration 419, loss = 0.32516446\n",
      "Iteration 420, loss = 0.32503149\n",
      "Iteration 421, loss = 0.32490784\n",
      "Iteration 422, loss = 0.32473745\n",
      "Iteration 423, loss = 0.32461277\n",
      "Iteration 424, loss = 0.32448806\n",
      "Iteration 425, loss = 0.32434465\n",
      "Iteration 426, loss = 0.32420950\n",
      "Iteration 427, loss = 0.32405906\n",
      "Iteration 428, loss = 0.32390637\n",
      "Iteration 429, loss = 0.32377631\n",
      "Iteration 430, loss = 0.32363193\n",
      "Iteration 431, loss = 0.32347891\n",
      "Iteration 432, loss = 0.32336469\n",
      "Iteration 433, loss = 0.32323983\n",
      "Iteration 434, loss = 0.32307285\n",
      "Iteration 435, loss = 0.32295853\n",
      "Iteration 436, loss = 0.32278264\n",
      "Iteration 437, loss = 0.32266886\n",
      "Iteration 438, loss = 0.32249679\n",
      "Iteration 439, loss = 0.32236431\n",
      "Iteration 440, loss = 0.32224710\n",
      "Iteration 441, loss = 0.32208916\n",
      "Iteration 442, loss = 0.32198315\n",
      "Iteration 443, loss = 0.32182686\n",
      "Iteration 444, loss = 0.32167675\n",
      "Iteration 445, loss = 0.32157613\n",
      "Iteration 446, loss = 0.32142395\n",
      "Iteration 447, loss = 0.32129089\n",
      "Iteration 448, loss = 0.32113350\n",
      "Iteration 449, loss = 0.32097688\n",
      "Iteration 450, loss = 0.32087097\n",
      "Iteration 451, loss = 0.32073921\n",
      "Iteration 452, loss = 0.32063922\n",
      "Iteration 453, loss = 0.32046283\n",
      "Iteration 454, loss = 0.32030276\n",
      "Iteration 455, loss = 0.32016639\n",
      "Iteration 456, loss = 0.32003011\n",
      "Iteration 457, loss = 0.31989546\n",
      "Iteration 458, loss = 0.31985653\n",
      "Iteration 459, loss = 0.31962608\n",
      "Iteration 460, loss = 0.31949525\n",
      "Iteration 461, loss = 0.31935254\n",
      "Iteration 462, loss = 0.31922127\n",
      "Iteration 463, loss = 0.31907559\n",
      "Iteration 464, loss = 0.31895931\n",
      "Iteration 465, loss = 0.31880124\n",
      "Iteration 466, loss = 0.31867196\n",
      "Iteration 467, loss = 0.31854070\n",
      "Iteration 468, loss = 0.31841378\n",
      "Iteration 469, loss = 0.31826433\n",
      "Iteration 470, loss = 0.31814913\n",
      "Iteration 471, loss = 0.31800534\n",
      "Iteration 472, loss = 0.31791505\n",
      "Iteration 473, loss = 0.31773373\n",
      "Iteration 474, loss = 0.31758111\n",
      "Iteration 475, loss = 0.31745252\n",
      "Iteration 476, loss = 0.31734530\n",
      "Iteration 477, loss = 0.31716874\n",
      "Iteration 478, loss = 0.31707568\n",
      "Iteration 479, loss = 0.31692775\n",
      "Iteration 480, loss = 0.31676820\n",
      "Iteration 481, loss = 0.31664617\n",
      "Iteration 482, loss = 0.31651321\n",
      "Iteration 483, loss = 0.31641011\n",
      "Iteration 484, loss = 0.31624340\n",
      "Iteration 485, loss = 0.31613717\n",
      "Iteration 486, loss = 0.31602522\n",
      "Iteration 487, loss = 0.31584520\n",
      "Iteration 488, loss = 0.31585910\n",
      "Iteration 489, loss = 0.31560534\n",
      "Iteration 490, loss = 0.31544278\n",
      "Iteration 491, loss = 0.31535743\n",
      "Iteration 492, loss = 0.31517567\n",
      "Iteration 493, loss = 0.31504415\n",
      "Iteration 494, loss = 0.31492702\n",
      "Iteration 495, loss = 0.31478857\n",
      "Iteration 496, loss = 0.31465556\n",
      "Iteration 497, loss = 0.31452526\n",
      "Iteration 498, loss = 0.31453770\n",
      "Iteration 499, loss = 0.31425426\n",
      "Iteration 500, loss = 0.31413518\n",
      "Iteration 501, loss = 0.31399830\n",
      "Iteration 502, loss = 0.31386352\n",
      "Iteration 503, loss = 0.31374822\n",
      "Iteration 504, loss = 0.31358643\n",
      "Iteration 505, loss = 0.31348263\n",
      "Iteration 506, loss = 0.31333793\n",
      "Iteration 507, loss = 0.31320108\n",
      "Iteration 508, loss = 0.31307659\n",
      "Iteration 509, loss = 0.31295483\n",
      "Iteration 510, loss = 0.31279361\n",
      "Iteration 511, loss = 0.31267702\n",
      "Iteration 512, loss = 0.31253947\n",
      "Iteration 513, loss = 0.31243783\n",
      "Iteration 514, loss = 0.31232868\n",
      "Iteration 515, loss = 0.31214825\n",
      "Iteration 516, loss = 0.31201824\n",
      "Iteration 517, loss = 0.31189467\n",
      "Iteration 518, loss = 0.31174954\n",
      "Iteration 519, loss = 0.31161553\n",
      "Iteration 520, loss = 0.31150034\n",
      "Iteration 521, loss = 0.31136700\n",
      "Iteration 522, loss = 0.31125177\n",
      "Iteration 523, loss = 0.31109780\n",
      "Iteration 524, loss = 0.31098065\n",
      "Iteration 525, loss = 0.31084212\n",
      "Iteration 526, loss = 0.31070884\n",
      "Iteration 527, loss = 0.31057635\n",
      "Iteration 528, loss = 0.31044373\n",
      "Iteration 529, loss = 0.31031706\n",
      "Iteration 530, loss = 0.31018196\n",
      "Iteration 531, loss = 0.31004890\n",
      "Iteration 532, loss = 0.30992663\n",
      "Iteration 533, loss = 0.30977851\n",
      "Iteration 534, loss = 0.30967062\n",
      "Iteration 535, loss = 0.30953037\n",
      "Iteration 536, loss = 0.30940899\n",
      "Iteration 537, loss = 0.30926187\n",
      "Iteration 538, loss = 0.30913897\n",
      "Iteration 539, loss = 0.30903711\n",
      "Iteration 540, loss = 0.30886504\n",
      "Iteration 541, loss = 0.30879963\n",
      "Iteration 542, loss = 0.30866082\n",
      "Iteration 543, loss = 0.30848600\n",
      "Iteration 544, loss = 0.30837754\n",
      "Iteration 545, loss = 0.30823632\n",
      "Iteration 546, loss = 0.30810488\n",
      "Iteration 547, loss = 0.30797290\n",
      "Iteration 548, loss = 0.30786603\n",
      "Iteration 549, loss = 0.30773384\n",
      "Iteration 550, loss = 0.30757935\n",
      "Iteration 551, loss = 0.30746133\n",
      "Iteration 552, loss = 0.30734902\n",
      "Iteration 553, loss = 0.30724869\n",
      "Iteration 554, loss = 0.30711671\n",
      "Iteration 555, loss = 0.30696086\n",
      "Iteration 556, loss = 0.30680260\n",
      "Iteration 557, loss = 0.30667233\n",
      "Iteration 558, loss = 0.30658352\n",
      "Iteration 559, loss = 0.30642503\n",
      "Iteration 560, loss = 0.30631471\n",
      "Iteration 561, loss = 0.30616399\n",
      "Iteration 562, loss = 0.30602585\n",
      "Iteration 563, loss = 0.30592567\n",
      "Iteration 564, loss = 0.30582407\n",
      "Iteration 565, loss = 0.30563473\n",
      "Iteration 566, loss = 0.30549587\n",
      "Iteration 567, loss = 0.30538388\n",
      "Iteration 568, loss = 0.30527496\n",
      "Iteration 569, loss = 0.30511371\n",
      "Iteration 570, loss = 0.30501591\n",
      "Iteration 571, loss = 0.30494238\n",
      "Iteration 572, loss = 0.30475092\n",
      "Iteration 573, loss = 0.30462078\n",
      "Iteration 574, loss = 0.30448236\n",
      "Iteration 575, loss = 0.30435627\n",
      "Iteration 576, loss = 0.30427075\n",
      "Iteration 577, loss = 0.30408756\n",
      "Iteration 578, loss = 0.30396104\n",
      "Iteration 579, loss = 0.30383739\n",
      "Iteration 580, loss = 0.30370721\n",
      "Iteration 581, loss = 0.30358210\n",
      "Iteration 582, loss = 0.30343019\n",
      "Iteration 583, loss = 0.30334395\n",
      "Iteration 584, loss = 0.30317684\n",
      "Iteration 585, loss = 0.30304599\n",
      "Iteration 586, loss = 0.30295564\n",
      "Iteration 587, loss = 0.30281123\n",
      "Iteration 588, loss = 0.30266881\n",
      "Iteration 589, loss = 0.30255855\n",
      "Iteration 590, loss = 0.30239915\n",
      "Iteration 591, loss = 0.30229046\n",
      "Iteration 592, loss = 0.30220994\n",
      "Iteration 593, loss = 0.30205501\n",
      "Iteration 594, loss = 0.30192063\n",
      "Iteration 595, loss = 0.30177456\n",
      "Iteration 596, loss = 0.30169424\n",
      "Iteration 597, loss = 0.30149916\n",
      "Iteration 598, loss = 0.30136433\n",
      "Iteration 599, loss = 0.30126078\n",
      "Iteration 600, loss = 0.30115520\n",
      "Iteration 601, loss = 0.30099932\n",
      "Iteration 602, loss = 0.30089229\n",
      "Iteration 603, loss = 0.30074524\n",
      "Iteration 604, loss = 0.30061711\n",
      "Iteration 605, loss = 0.30049878\n",
      "Iteration 606, loss = 0.30035634\n",
      "Iteration 607, loss = 0.30022599\n",
      "Iteration 608, loss = 0.30012447\n",
      "Iteration 609, loss = 0.29996237\n",
      "Iteration 610, loss = 0.29984921\n",
      "Iteration 611, loss = 0.29973814\n",
      "Iteration 612, loss = 0.29959509\n",
      "Iteration 613, loss = 0.29945136\n",
      "Iteration 614, loss = 0.29933581\n",
      "Iteration 615, loss = 0.29919264\n",
      "Iteration 616, loss = 0.29909111\n",
      "Iteration 617, loss = 0.29892900\n",
      "Iteration 618, loss = 0.29882599\n",
      "Iteration 619, loss = 0.29867783\n",
      "Iteration 620, loss = 0.29854965\n",
      "Iteration 621, loss = 0.29843470\n",
      "Iteration 622, loss = 0.29830161\n",
      "Iteration 623, loss = 0.29815327\n",
      "Iteration 624, loss = 0.29804247\n",
      "Iteration 625, loss = 0.29789991\n",
      "Iteration 626, loss = 0.29778481\n",
      "Iteration 627, loss = 0.29764930\n",
      "Iteration 628, loss = 0.29751895\n",
      "Iteration 629, loss = 0.29741572\n",
      "Iteration 630, loss = 0.29727107\n",
      "Iteration 631, loss = 0.29720409\n",
      "Iteration 632, loss = 0.29701645\n",
      "Iteration 633, loss = 0.29696738\n",
      "Iteration 634, loss = 0.29675928\n",
      "Iteration 635, loss = 0.29669525\n",
      "Iteration 636, loss = 0.29651464\n",
      "Iteration 637, loss = 0.29637682\n",
      "Iteration 638, loss = 0.29626914\n",
      "Iteration 639, loss = 0.29614852\n",
      "Iteration 640, loss = 0.29602416\n",
      "Iteration 641, loss = 0.29588446\n",
      "Iteration 642, loss = 0.29577363\n",
      "Iteration 643, loss = 0.29564856\n",
      "Iteration 644, loss = 0.29552190\n",
      "Iteration 645, loss = 0.29540315\n",
      "Iteration 646, loss = 0.29528226\n",
      "Iteration 647, loss = 0.29514524\n",
      "Iteration 648, loss = 0.29501012\n",
      "Iteration 649, loss = 0.29490341\n",
      "Iteration 650, loss = 0.29475428\n",
      "Iteration 651, loss = 0.29463119\n",
      "Iteration 652, loss = 0.29450123\n",
      "Iteration 653, loss = 0.29436993\n",
      "Iteration 654, loss = 0.29427239\n",
      "Iteration 655, loss = 0.29415736\n",
      "Iteration 656, loss = 0.29402325\n",
      "Iteration 657, loss = 0.29394774\n",
      "Iteration 658, loss = 0.29377385\n",
      "Iteration 659, loss = 0.29365663\n",
      "Iteration 660, loss = 0.29351440\n",
      "Iteration 661, loss = 0.29338941\n",
      "Iteration 662, loss = 0.29328774\n",
      "Iteration 663, loss = 0.29315587\n",
      "Iteration 664, loss = 0.29302281\n",
      "Iteration 665, loss = 0.29290442\n",
      "Iteration 666, loss = 0.29279823\n",
      "Iteration 667, loss = 0.29266333\n",
      "Iteration 668, loss = 0.29255157\n",
      "Iteration 669, loss = 0.29243267\n",
      "Iteration 670, loss = 0.29229370\n",
      "Iteration 671, loss = 0.29218236\n",
      "Iteration 672, loss = 0.29205144\n",
      "Iteration 673, loss = 0.29194000\n",
      "Iteration 674, loss = 0.29183265\n",
      "Iteration 675, loss = 0.29167398\n",
      "Iteration 676, loss = 0.29157903\n",
      "Iteration 677, loss = 0.29146662\n",
      "Iteration 678, loss = 0.29133486\n",
      "Iteration 679, loss = 0.29119713\n",
      "Iteration 680, loss = 0.29108577\n",
      "Iteration 681, loss = 0.29094450\n",
      "Iteration 682, loss = 0.29083626\n",
      "Iteration 683, loss = 0.29068862\n",
      "Iteration 684, loss = 0.29057944\n",
      "Iteration 685, loss = 0.29045871\n",
      "Iteration 686, loss = 0.29033683\n",
      "Iteration 687, loss = 0.29021033\n",
      "Iteration 688, loss = 0.29009926\n",
      "Iteration 689, loss = 0.28996067\n",
      "Iteration 690, loss = 0.28984616\n",
      "Iteration 691, loss = 0.28975458\n",
      "Iteration 692, loss = 0.28960544\n",
      "Iteration 693, loss = 0.28951616\n",
      "Iteration 694, loss = 0.28935919\n",
      "Iteration 695, loss = 0.28924179\n",
      "Iteration 696, loss = 0.28909945\n",
      "Iteration 697, loss = 0.28898512\n",
      "Iteration 698, loss = 0.28885509\n",
      "Iteration 699, loss = 0.28874030\n",
      "Iteration 700, loss = 0.28861293\n",
      "Iteration 701, loss = 0.28848073\n",
      "Iteration 702, loss = 0.28836416\n",
      "Iteration 703, loss = 0.28824194\n",
      "Iteration 704, loss = 0.28813522\n",
      "Iteration 705, loss = 0.28799802\n",
      "Iteration 706, loss = 0.28786917\n",
      "Iteration 707, loss = 0.28775952\n",
      "Iteration 708, loss = 0.28763982\n",
      "Iteration 709, loss = 0.28750160\n",
      "Iteration 710, loss = 0.28736382\n",
      "Iteration 711, loss = 0.28724600\n",
      "Iteration 712, loss = 0.28713230\n",
      "Iteration 713, loss = 0.28702867\n",
      "Iteration 714, loss = 0.28687822\n",
      "Iteration 715, loss = 0.28675731\n",
      "Iteration 716, loss = 0.28664895\n",
      "Iteration 717, loss = 0.28650720\n",
      "Iteration 718, loss = 0.28638873\n",
      "Iteration 719, loss = 0.28624883\n",
      "Iteration 720, loss = 0.28612907\n",
      "Iteration 721, loss = 0.28598736\n",
      "Iteration 722, loss = 0.28588843\n",
      "Iteration 723, loss = 0.28575324\n",
      "Iteration 724, loss = 0.28563233\n",
      "Iteration 725, loss = 0.28550103\n",
      "Iteration 726, loss = 0.28540898\n",
      "Iteration 727, loss = 0.28527131\n",
      "Iteration 728, loss = 0.28511413\n",
      "Iteration 729, loss = 0.28503149\n",
      "Iteration 730, loss = 0.28488384\n",
      "Iteration 731, loss = 0.28478017\n",
      "Iteration 732, loss = 0.28465482\n",
      "Iteration 733, loss = 0.28448775\n",
      "Iteration 734, loss = 0.28436203\n",
      "Iteration 735, loss = 0.28425969\n",
      "Iteration 736, loss = 0.28412627\n",
      "Iteration 737, loss = 0.28400272\n",
      "Iteration 738, loss = 0.28387730\n",
      "Iteration 739, loss = 0.28377007\n",
      "Iteration 740, loss = 0.28363929\n",
      "Iteration 741, loss = 0.28350639\n",
      "Iteration 742, loss = 0.28339156\n",
      "Iteration 743, loss = 0.28324407\n",
      "Iteration 744, loss = 0.28312078\n",
      "Iteration 745, loss = 0.28299961\n",
      "Iteration 746, loss = 0.28288855\n",
      "Iteration 747, loss = 0.28276259\n",
      "Iteration 748, loss = 0.28262090\n",
      "Iteration 749, loss = 0.28249982\n",
      "Iteration 750, loss = 0.28236408\n",
      "Iteration 1277, loss = 0.22277683\n",
      "Iteration 1278, loss = 0.22266629\n",
      "Iteration 1279, loss = 0.22254209\n",
      "Iteration 1280, loss = 0.22241966\n",
      "Iteration 1281, loss = 0.22232194\n",
      "Iteration 1282, loss = 0.22215917\n",
      "Iteration 1283, loss = 0.22204933\n",
      "Iteration 1284, loss = 0.22193156\n",
      "Iteration 1285, loss = 0.22182032\n",
      "Iteration 1286, loss = 0.22174940\n",
      "Iteration 1287, loss = 0.22157512\n",
      "Iteration 1288, loss = 0.22147384\n",
      "Iteration 1289, loss = 0.22133128\n",
      "Iteration 1290, loss = 0.22124563\n",
      "Iteration 1291, loss = 0.22110531\n",
      "Iteration 1292, loss = 0.22099777\n",
      "Iteration 1293, loss = 0.22088172\n",
      "Iteration 1294, loss = 0.22078029\n",
      "Iteration 1295, loss = 0.22066248\n",
      "Iteration 1296, loss = 0.22051249\n",
      "Iteration 1297, loss = 0.22042086\n",
      "Iteration 1298, loss = 0.22028197\n",
      "Iteration 1299, loss = 0.22017046\n",
      "Iteration 1300, loss = 0.22008365\n",
      "Iteration 1301, loss = 0.21994564\n",
      "Iteration 1302, loss = 0.21979517\n",
      "Iteration 1303, loss = 0.21971638\n",
      "Iteration 1304, loss = 0.21960440\n",
      "Iteration 1305, loss = 0.21947500\n",
      "Iteration 1306, loss = 0.21935599\n",
      "Iteration 1307, loss = 0.21933210\n",
      "Iteration 1308, loss = 0.21913319\n",
      "Iteration 1309, loss = 0.21898211\n",
      "Iteration 1310, loss = 0.21888563\n",
      "Iteration 1311, loss = 0.21876963\n",
      "Iteration 1312, loss = 0.21868520\n",
      "Iteration 1313, loss = 0.21855619\n",
      "Iteration 1314, loss = 0.21843460\n",
      "Iteration 1315, loss = 0.21829351\n",
      "Iteration 1316, loss = 0.21819895\n",
      "Iteration 1317, loss = 0.21806340\n",
      "Iteration 1318, loss = 0.21798120\n",
      "Iteration 1319, loss = 0.21787190\n",
      "Iteration 1320, loss = 0.21773630\n",
      "Iteration 1321, loss = 0.21758656\n",
      "Iteration 1322, loss = 0.21749137\n",
      "Iteration 1323, loss = 0.21736730\n",
      "Iteration 1324, loss = 0.21722067\n",
      "Iteration 1325, loss = 0.21719578\n",
      "Iteration 1326, loss = 0.21699951\n",
      "Iteration 1327, loss = 0.21691308\n",
      "Iteration 1328, loss = 0.21677053\n",
      "Iteration 1329, loss = 0.21673269\n",
      "Iteration 1330, loss = 0.21654799\n",
      "Iteration 1331, loss = 0.21641697\n",
      "Iteration 1332, loss = 0.21630830\n",
      "Iteration 1333, loss = 0.21619271\n",
      "Iteration 1334, loss = 0.21610034\n",
      "Iteration 1335, loss = 0.21596495\n",
      "Iteration 1336, loss = 0.21588202\n",
      "Iteration 1337, loss = 0.21571311\n",
      "Iteration 1338, loss = 0.21565951\n",
      "Iteration 1339, loss = 0.21551508\n",
      "Iteration 1340, loss = 0.21541393\n",
      "Iteration 1341, loss = 0.21528757\n",
      "Iteration 1342, loss = 0.21516079\n",
      "Iteration 1343, loss = 0.21504362\n",
      "Iteration 1344, loss = 0.21490845\n",
      "Iteration 1345, loss = 0.21480791\n",
      "Iteration 1346, loss = 0.21471088\n",
      "Iteration 1347, loss = 0.21459699\n",
      "Iteration 1348, loss = 0.21443582\n",
      "Iteration 1349, loss = 0.21432826\n",
      "Iteration 1350, loss = 0.21421201\n",
      "Iteration 1351, loss = 0.21410396\n",
      "Iteration 1352, loss = 0.21397467\n",
      "Iteration 1353, loss = 0.21385761\n",
      "Iteration 1354, loss = 0.21379045\n",
      "Iteration 1355, loss = 0.21365932\n",
      "Iteration 1356, loss = 0.21350932\n",
      "Iteration 1357, loss = 0.21342348\n",
      "Iteration 1358, loss = 0.21330175\n",
      "Iteration 1359, loss = 0.21317288\n",
      "Iteration 1360, loss = 0.21306218\n",
      "Iteration 1361, loss = 0.21295271\n",
      "Iteration 1362, loss = 0.21282894\n",
      "Iteration 1363, loss = 0.21274341\n",
      "Iteration 1364, loss = 0.21258747\n",
      "Iteration 1365, loss = 0.21251659\n",
      "Iteration 1366, loss = 0.21238163\n",
      "Iteration 1367, loss = 0.21232809\n",
      "Iteration 1368, loss = 0.21222319\n",
      "Iteration 1369, loss = 0.21201319\n",
      "Iteration 1370, loss = 0.21190850\n",
      "Iteration 1371, loss = 0.21178830\n",
      "Iteration 1372, loss = 0.21167059\n",
      "Iteration 1373, loss = 0.21156982\n",
      "Iteration 1374, loss = 0.21144183\n",
      "Iteration 1375, loss = 0.21136889\n",
      "Iteration 1376, loss = 0.21122142\n",
      "Iteration 1377, loss = 0.21111815\n",
      "Iteration 1378, loss = 0.21101346\n",
      "Iteration 1379, loss = 0.21089311\n",
      "Iteration 1380, loss = 0.21081206\n",
      "Iteration 1381, loss = 0.21069165\n",
      "Iteration 1382, loss = 0.21057892\n",
      "Iteration 1383, loss = 0.21041814\n",
      "Iteration 1384, loss = 0.21032247\n",
      "Iteration 1385, loss = 0.21021858\n",
      "Iteration 1386, loss = 0.21011815\n",
      "Iteration 1387, loss = 0.20996951\n",
      "Iteration 1388, loss = 0.20983528\n",
      "Iteration 1389, loss = 0.20976176\n",
      "Iteration 1390, loss = 0.20972829\n",
      "Iteration 1391, loss = 0.20952288\n",
      "Iteration 1392, loss = 0.20943664\n",
      "Iteration 1393, loss = 0.20931798\n",
      "Iteration 1394, loss = 0.20918976\n",
      "Iteration 1395, loss = 0.20909793\n",
      "Iteration 1396, loss = 0.20897192\n",
      "Iteration 1397, loss = 0.20893056\n",
      "Iteration 1398, loss = 0.20873500\n",
      "Iteration 1399, loss = 0.20869452\n",
      "Iteration 1400, loss = 0.20852905\n",
      "Iteration 1401, loss = 0.20840895\n",
      "Iteration 1402, loss = 0.20829056\n",
      "Iteration 1403, loss = 0.20817804\n",
      "Iteration 1404, loss = 0.20805622\n",
      "Iteration 1405, loss = 0.20794362\n",
      "Iteration 1406, loss = 0.20783004\n",
      "Iteration 1407, loss = 0.20775364\n",
      "Iteration 1408, loss = 0.20763315\n",
      "Iteration 1409, loss = 0.20751234\n",
      "Iteration 1410, loss = 0.20738019\n",
      "Iteration 1411, loss = 0.20725177\n",
      "Iteration 1412, loss = 0.20715210\n",
      "Iteration 1413, loss = 0.20703023\n",
      "Iteration 1414, loss = 0.20693046\n",
      "Iteration 1415, loss = 0.20691397\n",
      "Iteration 1416, loss = 0.20668178\n",
      "Iteration 1417, loss = 0.20662700\n",
      "Iteration 1418, loss = 0.20657358\n",
      "Iteration 1419, loss = 0.20635910\n",
      "Iteration 1420, loss = 0.20627297\n",
      "Iteration 1421, loss = 0.20611282\n",
      "Iteration 1422, loss = 0.20602138\n",
      "Iteration 1423, loss = 0.20589773\n",
      "Iteration 1424, loss = 0.20588406\n",
      "Iteration 1425, loss = 0.20570327\n",
      "Iteration 1426, loss = 0.20556018\n",
      "Iteration 1427, loss = 0.20543602\n",
      "Iteration 1428, loss = 0.20536529\n",
      "Iteration 1429, loss = 0.20525187\n",
      "Iteration 1430, loss = 0.20509307\n",
      "Iteration 1431, loss = 0.20498735\n",
      "Iteration 1432, loss = 0.20490830\n",
      "Iteration 1433, loss = 0.20477645\n",
      "Iteration 1434, loss = 0.20468039\n",
      "Iteration 1435, loss = 0.20455555\n",
      "Iteration 1436, loss = 0.20446593\n",
      "Iteration 1437, loss = 0.20430479\n",
      "Iteration 1438, loss = 0.20424429\n",
      "Iteration 1439, loss = 0.20412018\n",
      "Iteration 1440, loss = 0.20399349\n",
      "Iteration 1441, loss = 0.20391533\n",
      "Iteration 1442, loss = 0.20375439\n",
      "Iteration 1443, loss = 0.20368494\n",
      "Iteration 1444, loss = 0.20359424\n",
      "Iteration 1445, loss = 0.20343690\n",
      "Iteration 1446, loss = 0.20335987\n",
      "Iteration 1447, loss = 0.20320909\n",
      "Iteration 1448, loss = 0.20310969\n",
      "Iteration 1449, loss = 0.20299013\n",
      "Iteration 1450, loss = 0.20286550\n",
      "Iteration 1451, loss = 0.20278110\n",
      "Iteration 1452, loss = 0.20276468\n",
      "Iteration 1453, loss = 0.20253804\n",
      "Iteration 1454, loss = 0.20244560\n",
      "Iteration 1455, loss = 0.20233183\n",
      "Iteration 1456, loss = 0.20221969\n",
      "Iteration 1457, loss = 0.20210003\n",
      "Iteration 1458, loss = 0.20203384\n",
      "Iteration 1459, loss = 0.20191308\n",
      "Iteration 1460, loss = 0.20175242\n",
      "Iteration 1461, loss = 0.20165366\n",
      "Iteration 1462, loss = 0.20156053\n",
      "Iteration 1463, loss = 0.20141796\n",
      "Iteration 1464, loss = 0.20134664\n",
      "Iteration 1465, loss = 0.20118644\n",
      "Iteration 1466, loss = 0.20109255\n",
      "Iteration 1467, loss = 0.20102100\n",
      "Iteration 1468, loss = 0.20085735\n",
      "Iteration 1469, loss = 0.20078285\n",
      "Iteration 1470, loss = 0.20063009\n",
      "Iteration 1471, loss = 0.20052130\n",
      "Iteration 1472, loss = 0.20043020\n",
      "Iteration 1473, loss = 0.20038410\n",
      "Iteration 1474, loss = 0.20019398\n",
      "Iteration 1475, loss = 0.20005243\n",
      "Iteration 1476, loss = 0.19996701\n",
      "Iteration 1477, loss = 0.19987713\n",
      "Iteration 1478, loss = 0.19972124\n",
      "Iteration 1479, loss = 0.19963898\n",
      "Iteration 1480, loss = 0.19951887\n",
      "Iteration 1481, loss = 0.19941527\n",
      "Iteration 1482, loss = 0.19931003\n",
      "Iteration 1483, loss = 0.19920567\n",
      "Iteration 1484, loss = 0.19908105\n",
      "Iteration 1485, loss = 0.19906847\n",
      "Iteration 1486, loss = 0.19885014\n",
      "Iteration 1487, loss = 0.19879002\n",
      "Iteration 1488, loss = 0.19864334\n",
      "Iteration 1489, loss = 0.19856290\n",
      "Iteration 1490, loss = 0.19842516\n",
      "Iteration 1491, loss = 0.19832334\n",
      "Iteration 1492, loss = 0.19824370\n",
      "Iteration 1493, loss = 0.19810414\n",
      "Iteration 1494, loss = 0.19797531\n",
      "Iteration 1495, loss = 0.19803856\n",
      "Iteration 1496, loss = 0.19777854\n",
      "Iteration 1497, loss = 0.19770241\n",
      "Iteration 1498, loss = 0.19758264\n",
      "Iteration 1499, loss = 0.19748703\n",
      "Iteration 1500, loss = 0.19734328\n",
      "Iteration 1501, loss = 0.19726128\n",
      "Iteration 1502, loss = 0.19718685\n",
      "Iteration 1503, loss = 0.19709996\n",
      "Iteration 1504, loss = 0.19697558\n",
      "Iteration 1505, loss = 0.19682301\n",
      "Iteration 1506, loss = 0.19673877\n",
      "Iteration 1507, loss = 0.19659699\n",
      "Iteration 1508, loss = 0.19650891\n",
      "Iteration 1509, loss = 0.19642403\n",
      "Iteration 1510, loss = 0.19634298\n",
      "Iteration 1511, loss = 0.19621006\n",
      "Iteration 1512, loss = 0.19608559\n",
      "Iteration 1513, loss = 0.19600368\n",
      "Iteration 1514, loss = 0.19592291\n",
      "Iteration 1515, loss = 0.19576472\n",
      "Iteration 1516, loss = 0.19565521\n",
      "Iteration 1517, loss = 0.19560681\n",
      "Iteration 1518, loss = 0.19548934\n",
      "Iteration 1519, loss = 0.19544303\n",
      "Iteration 1520, loss = 0.19526226\n",
      "Iteration 1521, loss = 0.19514910\n",
      "Iteration 1522, loss = 0.19505159\n",
      "Iteration 1523, loss = 0.19492980\n",
      "Iteration 1524, loss = 0.19485181\n",
      "Iteration 1525, loss = 0.19475896\n",
      "Iteration 1526, loss = 0.19461498\n",
      "Iteration 1527, loss = 0.19452604\n",
      "Iteration 1528, loss = 0.19443438\n",
      "Iteration 1529, loss = 0.19442163\n",
      "Iteration 1530, loss = 0.19418853\n",
      "Iteration 1531, loss = 0.19420020\n",
      "Iteration 1532, loss = 0.19401953\n",
      "Iteration 1533, loss = 0.19403322\n",
      "Iteration 1534, loss = 0.19379311\n",
      "Iteration 1535, loss = 0.19370644\n",
      "Iteration 1536, loss = 0.19367583\n",
      "Iteration 1537, loss = 0.19352792\n",
      "Iteration 1538, loss = 0.19337838\n",
      "Iteration 1539, loss = 0.19328412\n",
      "Iteration 1540, loss = 0.19318136\n",
      "Iteration 1541, loss = 0.19306604\n",
      "Iteration 1542, loss = 0.19299511\n",
      "Iteration 1543, loss = 0.19286301\n",
      "Iteration 1544, loss = 0.19274305\n",
      "Iteration 1545, loss = 0.19267347\n",
      "Iteration 1546, loss = 0.19257300\n",
      "Iteration 1547, loss = 0.19246856\n",
      "Iteration 1548, loss = 0.19235333\n",
      "Iteration 1549, loss = 0.19223743\n",
      "Iteration 1550, loss = 0.19218491\n",
      "Iteration 1551, loss = 0.19213889\n",
      "Iteration 1552, loss = 0.19196805\n",
      "Iteration 1553, loss = 0.19186508\n",
      "Iteration 1554, loss = 0.19177613\n",
      "Iteration 1555, loss = 0.19166509\n",
      "Iteration 1556, loss = 0.19156144\n",
      "Iteration 1557, loss = 0.19145577\n",
      "Iteration 1558, loss = 0.19133365\n",
      "Iteration 1559, loss = 0.19123708\n",
      "Iteration 1560, loss = 0.19121997\n",
      "Iteration 1561, loss = 0.19111896\n",
      "Iteration 1562, loss = 0.19094329\n",
      "Iteration 1563, loss = 0.19085636\n",
      "Iteration 1564, loss = 0.19070882\n",
      "Iteration 1565, loss = 0.19063019\n",
      "Iteration 1566, loss = 0.19070626\n",
      "Iteration 1567, loss = 0.19049778\n",
      "Iteration 1568, loss = 0.19039816\n",
      "Iteration 1569, loss = 0.19023554\n",
      "Iteration 1570, loss = 0.19014528\n",
      "Iteration 1571, loss = 0.19006011\n",
      "Iteration 1572, loss = 0.18993564\n",
      "Iteration 1573, loss = 0.18987558\n",
      "Iteration 1574, loss = 0.18974234\n",
      "Iteration 1575, loss = 0.18964070\n",
      "Iteration 1576, loss = 0.18951945\n",
      "Iteration 1577, loss = 0.18949613\n",
      "Iteration 1578, loss = 0.18938086\n",
      "Iteration 1579, loss = 0.18922486\n",
      "Iteration 1580, loss = 0.18916546\n",
      "Iteration 1581, loss = 0.18903087\n",
      "Iteration 1582, loss = 0.18895500\n",
      "Iteration 1583, loss = 0.18883075\n",
      "Iteration 1584, loss = 0.18869698\n",
      "Iteration 1585, loss = 0.18861897\n",
      "Iteration 1586, loss = 0.18856020\n",
      "Iteration 1587, loss = 0.18842256\n",
      "Iteration 1588, loss = 0.18829127\n",
      "Iteration 1589, loss = 0.18824698\n",
      "Iteration 1590, loss = 0.18812115\n",
      "Iteration 1591, loss = 0.18807527\n",
      "Iteration 1592, loss = 0.18789001\n",
      "Iteration 1593, loss = 0.18779114\n",
      "Iteration 1594, loss = 0.18773610\n",
      "Iteration 1595, loss = 0.18762664\n",
      "Iteration 1596, loss = 0.18750262\n",
      "Iteration 1597, loss = 0.18742694\n",
      "Iteration 1598, loss = 0.18727519\n",
      "Iteration 1599, loss = 0.18719928\n",
      "Iteration 1600, loss = 0.18710465\n",
      "Iteration 1601, loss = 0.18697445\n",
      "Iteration 1602, loss = 0.18688740\n",
      "Iteration 1603, loss = 0.18686767\n",
      "Iteration 1604, loss = 0.18668753\n",
      "Iteration 1605, loss = 0.18661310\n",
      "Iteration 1606, loss = 0.18650817\n",
      "Iteration 1607, loss = 0.18641367\n",
      "Iteration 1608, loss = 0.18630355\n",
      "Iteration 1609, loss = 0.18623000\n",
      "Iteration 1610, loss = 0.18612498\n",
      "Iteration 1611, loss = 0.18604859\n",
      "Iteration 1612, loss = 0.18592720\n",
      "Iteration 1613, loss = 0.18577799\n",
      "Iteration 1614, loss = 0.18569090\n",
      "Iteration 1615, loss = 0.18563188\n",
      "Iteration 1616, loss = 0.18558539\n",
      "Iteration 1617, loss = 0.18540971\n",
      "Iteration 1618, loss = 0.18536476\n",
      "Iteration 1619, loss = 0.18519585\n",
      "Iteration 1620, loss = 0.18511001\n",
      "Iteration 1621, loss = 0.18499690\n",
      "Iteration 1622, loss = 0.18493304\n",
      "Iteration 1623, loss = 0.18479355\n",
      "Iteration 1624, loss = 0.18472477\n",
      "Iteration 1625, loss = 0.18464065\n",
      "Iteration 1626, loss = 0.18454222\n",
      "Iteration 1627, loss = 0.18447035\n",
      "Iteration 1628, loss = 0.18429756\n",
      "Iteration 1629, loss = 0.18420133\n",
      "Iteration 1630, loss = 0.18417833\n",
      "Iteration 1631, loss = 0.18399410\n",
      "Iteration 1632, loss = 0.18390246\n",
      "Iteration 1633, loss = 0.18380746\n",
      "Iteration 1634, loss = 0.18380574\n",
      "Iteration 1635, loss = 0.18373451\n",
      "Iteration 1636, loss = 0.18354797\n",
      "Iteration 1637, loss = 0.18343054\n",
      "Iteration 1638, loss = 0.18332680\n",
      "Iteration 1639, loss = 0.18322590\n",
      "Iteration 1640, loss = 0.18316443\n",
      "Iteration 1641, loss = 0.18302383\n",
      "Iteration 1642, loss = 0.18305389\n",
      "Iteration 1643, loss = 0.18280374\n",
      "Iteration 1644, loss = 0.18278519\n",
      "Iteration 1645, loss = 0.18268530\n",
      "Iteration 1646, loss = 0.18252496\n",
      "Iteration 1647, loss = 0.18244436\n",
      "Iteration 1648, loss = 0.18239431\n",
      "Iteration 1649, loss = 0.18233323\n",
      "Iteration 1650, loss = 0.18216309\n",
      "Iteration 1651, loss = 0.18201606\n",
      "Iteration 1652, loss = 0.18192598\n",
      "Iteration 1653, loss = 0.18183030\n",
      "Iteration 1654, loss = 0.18179370\n",
      "Iteration 1655, loss = 0.18165604\n",
      "Iteration 1656, loss = 0.18152558\n",
      "Iteration 1657, loss = 0.18149597\n",
      "Iteration 1658, loss = 0.18140407\n",
      "Iteration 1659, loss = 0.18125067\n",
      "Iteration 1660, loss = 0.18121742\n",
      "Iteration 1661, loss = 0.18108933\n",
      "Iteration 1662, loss = 0.18094611\n",
      "Iteration 1663, loss = 0.18085688\n",
      "Iteration 1664, loss = 0.18077159\n",
      "Iteration 1665, loss = 0.18070949\n",
      "Iteration 1666, loss = 0.18058736\n",
      "Iteration 1667, loss = 0.18053177\n",
      "Iteration 1668, loss = 0.18040660\n",
      "Iteration 1669, loss = 0.18034650\n",
      "Iteration 1670, loss = 0.18017683\n",
      "Iteration 1671, loss = 0.18018497\n",
      "Iteration 1672, loss = 0.18001449\n",
      "Iteration 1673, loss = 0.17989062\n",
      "Iteration 1674, loss = 0.17979971\n",
      "Iteration 1675, loss = 0.17970850\n",
      "Iteration 1676, loss = 0.17960156\n",
      "Iteration 1677, loss = 0.17949026\n",
      "Iteration 1678, loss = 0.17940611\n",
      "Iteration 1679, loss = 0.17928555\n",
      "Iteration 1680, loss = 0.17928045\n",
      "Iteration 1681, loss = 0.17919831\n",
      "Iteration 1682, loss = 0.17900597\n",
      "Iteration 1683, loss = 0.17892849\n",
      "Iteration 1684, loss = 0.17882960\n",
      "Iteration 1685, loss = 0.17875504\n",
      "Iteration 1686, loss = 0.17861854\n",
      "Iteration 1687, loss = 0.17859982\n",
      "Iteration 1688, loss = 0.17860135\n",
      "Iteration 1689, loss = 0.17838218\n",
      "Iteration 1690, loss = 0.17826436\n",
      "Iteration 1691, loss = 0.17815199\n",
      "Iteration 1692, loss = 0.17815135\n",
      "Iteration 1693, loss = 0.17797978\n",
      "Iteration 1694, loss = 0.17784747\n",
      "Iteration 1695, loss = 0.17775704\n",
      "Iteration 1696, loss = 0.17764883\n",
      "Iteration 1697, loss = 0.17757062\n",
      "Iteration 1698, loss = 0.17750270\n",
      "Iteration 1699, loss = 0.17745658\n",
      "Iteration 1700, loss = 0.17729754\n",
      "Iteration 1701, loss = 0.17728137\n",
      "Iteration 1702, loss = 0.17709967\n",
      "Iteration 1703, loss = 0.17697478\n",
      "Iteration 1704, loss = 0.17687866\n",
      "Iteration 1705, loss = 0.17677721\n",
      "Iteration 1706, loss = 0.17684003\n",
      "Iteration 1707, loss = 0.17662300\n",
      "Iteration 1708, loss = 0.17657883\n",
      "Iteration 1709, loss = 0.17642454\n",
      "Iteration 1710, loss = 0.17631670\n",
      "Iteration 1711, loss = 0.17628594\n",
      "Iteration 1712, loss = 0.17614773\n",
      "Iteration 1713, loss = 0.17603027\n",
      "Iteration 1714, loss = 0.17598215\n",
      "Iteration 1715, loss = 0.17583964\n",
      "Iteration 1716, loss = 0.17576718\n",
      "Iteration 1717, loss = 0.17568190\n",
      "Iteration 1718, loss = 0.17555885\n",
      "Iteration 1719, loss = 0.17559287\n",
      "Iteration 1720, loss = 0.17544136\n",
      "Iteration 1721, loss = 0.17530606\n",
      "Iteration 1722, loss = 0.17517701\n",
      "Iteration 1723, loss = 0.17523804\n",
      "Iteration 1724, loss = 0.17503296\n",
      "Iteration 1725, loss = 0.17491954\n",
      "Iteration 1726, loss = 0.17486721\n",
      "Iteration 1727, loss = 0.17477961\n",
      "Iteration 1728, loss = 0.17464377\n",
      "Iteration 1729, loss = 0.17451406\n",
      "Iteration 1730, loss = 0.17444846\n",
      "Iteration 1731, loss = 0.17439407\n",
      "Iteration 1732, loss = 0.17426572\n",
      "Iteration 1733, loss = 0.17415530\n",
      "Iteration 1734, loss = 0.17412670\n",
      "Iteration 1735, loss = 0.17397093\n",
      "Iteration 1736, loss = 0.17386396\n",
      "Iteration 1737, loss = 0.17375260\n",
      "Iteration 1738, loss = 0.17378021\n",
      "Iteration 1739, loss = 0.17359862\n",
      "Iteration 1740, loss = 0.17352371\n",
      "Iteration 1741, loss = 0.17339368\n",
      "Iteration 1742, loss = 0.17335402\n",
      "Iteration 1743, loss = 0.17324952\n",
      "Iteration 1744, loss = 0.17314742\n",
      "Iteration 1745, loss = 0.17304152\n",
      "Iteration 1746, loss = 0.17299907\n",
      "Iteration 1747, loss = 0.17291033\n",
      "Iteration 1748, loss = 0.17274826\n",
      "Iteration 1749, loss = 0.17266534\n",
      "Iteration 1750, loss = 0.17261871\n",
      "Iteration 1751, loss = 0.17249612\n",
      "Iteration 1752, loss = 0.17240417\n",
      "Iteration 1753, loss = 0.17234807\n",
      "Iteration 1754, loss = 0.17234293\n",
      "Iteration 1755, loss = 0.17215844\n",
      "Iteration 1756, loss = 0.17205969\n",
      "Iteration 1304, loss = 0.23611682\n",
      "Iteration 1305, loss = 0.23600943\n",
      "Iteration 1306, loss = 0.23589375\n",
      "Iteration 1307, loss = 0.23581449\n",
      "Iteration 1308, loss = 0.23565837\n",
      "Iteration 1309, loss = 0.23554357\n",
      "Iteration 1310, loss = 0.23547125\n",
      "Iteration 1311, loss = 0.23533381\n",
      "Iteration 1312, loss = 0.23528220\n",
      "Iteration 1313, loss = 0.23509800\n",
      "Iteration 1314, loss = 0.23500771\n",
      "Iteration 1315, loss = 0.23488333\n",
      "Iteration 1316, loss = 0.23476783\n",
      "Iteration 1317, loss = 0.23476078\n",
      "Iteration 1318, loss = 0.23454928\n",
      "Iteration 1319, loss = 0.23446820\n",
      "Iteration 1320, loss = 0.23436562\n",
      "Iteration 1321, loss = 0.23425713\n",
      "Iteration 1322, loss = 0.23414947\n",
      "Iteration 1323, loss = 0.23404873\n",
      "Iteration 1324, loss = 0.23388191\n",
      "Iteration 1325, loss = 0.23381560\n",
      "Iteration 1326, loss = 0.23375030\n",
      "Iteration 1327, loss = 0.23357304\n",
      "Iteration 1328, loss = 0.23348045\n",
      "Iteration 1329, loss = 0.23337245\n",
      "Iteration 1330, loss = 0.23325728\n",
      "Iteration 1331, loss = 0.23311474\n",
      "Iteration 1332, loss = 0.23303526\n",
      "Iteration 1333, loss = 0.23293004\n",
      "Iteration 1334, loss = 0.23280114\n",
      "Iteration 1335, loss = 0.23271743\n",
      "Iteration 1336, loss = 0.23259564\n",
      "Iteration 1337, loss = 0.23252467\n",
      "Iteration 1338, loss = 0.23238090\n",
      "Iteration 1339, loss = 0.23229238\n",
      "Iteration 1340, loss = 0.23217932\n",
      "Iteration 1341, loss = 0.23214780\n",
      "Iteration 1342, loss = 0.23194030\n",
      "Iteration 1343, loss = 0.23186022\n",
      "Iteration 1344, loss = 0.23174047\n",
      "Iteration 1345, loss = 0.23163080\n",
      "Iteration 1346, loss = 0.23149941\n",
      "Iteration 1347, loss = 0.23141800\n",
      "Iteration 1348, loss = 0.23131679\n",
      "Iteration 1349, loss = 0.23124126\n",
      "Iteration 1350, loss = 0.23110747\n",
      "Iteration 1351, loss = 0.23103601\n",
      "Iteration 1352, loss = 0.23089260\n",
      "Iteration 1353, loss = 0.23076155\n",
      "Iteration 1354, loss = 0.23068641\n",
      "Iteration 1355, loss = 0.23053655\n",
      "Iteration 1356, loss = 0.23042561\n",
      "Iteration 1357, loss = 0.23033869\n",
      "Iteration 1358, loss = 0.23025022\n",
      "Iteration 1359, loss = 0.23010357\n",
      "Iteration 1360, loss = 0.23000644\n",
      "Iteration 1361, loss = 0.22991243\n",
      "Iteration 1362, loss = 0.22978671\n",
      "Iteration 1363, loss = 0.22967192\n",
      "Iteration 1364, loss = 0.22958211\n",
      "Iteration 1365, loss = 0.22949579\n",
      "Iteration 1366, loss = 0.22939434\n",
      "Iteration 1367, loss = 0.22935283\n",
      "Iteration 1368, loss = 0.22915218\n",
      "Iteration 1369, loss = 0.22904363\n",
      "Iteration 1370, loss = 0.22892150\n",
      "Iteration 1371, loss = 0.22889184\n",
      "Iteration 1372, loss = 0.22871039\n",
      "Iteration 1373, loss = 0.22863845\n",
      "Iteration 1374, loss = 0.22848251\n",
      "Iteration 1375, loss = 0.22840495\n",
      "Iteration 1376, loss = 0.22833785\n",
      "Iteration 1377, loss = 0.22817335\n",
      "Iteration 1378, loss = 0.22811465\n",
      "Iteration 1379, loss = 0.22801256\n",
      "Iteration 1380, loss = 0.22790629\n",
      "Iteration 1381, loss = 0.22773367\n",
      "Iteration 1382, loss = 0.22765104\n",
      "Iteration 1383, loss = 0.22756641\n",
      "Iteration 1384, loss = 0.22744322\n",
      "Iteration 1385, loss = 0.22735532\n",
      "Iteration 1386, loss = 0.22722028\n",
      "Iteration 1387, loss = 0.22709018\n",
      "Iteration 1388, loss = 0.22701702\n",
      "Iteration 1389, loss = 0.22694971\n",
      "Iteration 1390, loss = 0.22679721\n",
      "Iteration 1391, loss = 0.22669889\n",
      "Iteration 1392, loss = 0.22663449\n",
      "Iteration 1393, loss = 0.22647298\n",
      "Iteration 1394, loss = 0.22642753\n",
      "Iteration 1395, loss = 0.22628227\n",
      "Iteration 1396, loss = 0.22617426\n",
      "Iteration 1397, loss = 0.22605157\n",
      "Iteration 1398, loss = 0.22592791\n",
      "Iteration 1399, loss = 0.22588109\n",
      "Iteration 1400, loss = 0.22572291\n",
      "Iteration 1401, loss = 0.22566456\n",
      "Iteration 1402, loss = 0.22553961\n",
      "Iteration 1403, loss = 0.22539797\n",
      "Iteration 1404, loss = 0.22529896\n",
      "Iteration 1405, loss = 0.22521355\n",
      "Iteration 1406, loss = 0.22514580\n",
      "Iteration 1407, loss = 0.22498737\n",
      "Iteration 1408, loss = 0.22489150\n",
      "Iteration 1409, loss = 0.22481814\n",
      "Iteration 1410, loss = 0.22468423\n",
      "Iteration 1411, loss = 0.22460667\n",
      "Iteration 1412, loss = 0.22445082\n",
      "Iteration 1413, loss = 0.22433890\n",
      "Iteration 1414, loss = 0.22425003\n",
      "Iteration 1415, loss = 0.22412507\n",
      "Iteration 1416, loss = 0.22403191\n",
      "Iteration 1417, loss = 0.22390559\n",
      "Iteration 1418, loss = 0.22385286\n",
      "Iteration 1419, loss = 0.22372612\n",
      "Iteration 1420, loss = 0.22363283\n",
      "Iteration 1421, loss = 0.22351081\n",
      "Iteration 1422, loss = 0.22339171\n",
      "Iteration 1423, loss = 0.22328095\n",
      "Iteration 1424, loss = 0.22325065\n",
      "Iteration 1425, loss = 0.22307569\n",
      "Iteration 1426, loss = 0.22298353\n",
      "Iteration 1427, loss = 0.22285887\n",
      "Iteration 1428, loss = 0.22274498\n",
      "Iteration 1429, loss = 0.22274300\n",
      "Iteration 1430, loss = 0.22259013\n",
      "Iteration 1431, loss = 0.22256653\n",
      "Iteration 1432, loss = 0.22231397\n",
      "Iteration 1433, loss = 0.22229638\n",
      "Iteration 1434, loss = 0.22210848\n",
      "Iteration 1435, loss = 0.22201713\n",
      "Iteration 1436, loss = 0.22190707\n",
      "Iteration 1437, loss = 0.22178829\n",
      "Iteration 1438, loss = 0.22180055\n",
      "Iteration 1439, loss = 0.22171091\n",
      "Iteration 1440, loss = 0.22150236\n",
      "Iteration 1441, loss = 0.22135950\n",
      "Iteration 1442, loss = 0.22124244\n",
      "Iteration 1443, loss = 0.22114925\n",
      "Iteration 1444, loss = 0.22104040\n",
      "Iteration 1445, loss = 0.22101278\n",
      "Iteration 1446, loss = 0.22088911\n",
      "Iteration 1447, loss = 0.22073688\n",
      "Iteration 1448, loss = 0.22063945\n",
      "Iteration 1449, loss = 0.22055086\n",
      "Iteration 1450, loss = 0.22042120\n",
      "Iteration 1451, loss = 0.22031374\n",
      "Iteration 1452, loss = 0.22019192\n",
      "Iteration 1453, loss = 0.22012317\n",
      "Iteration 1454, loss = 0.21999295\n",
      "Iteration 1455, loss = 0.21992916\n",
      "Iteration 1456, loss = 0.21979153\n",
      "Iteration 1457, loss = 0.21969588\n",
      "Iteration 1458, loss = 0.21965205\n",
      "Iteration 1459, loss = 0.21956454\n",
      "Iteration 1460, loss = 0.21935773\n",
      "Iteration 1461, loss = 0.21926817\n",
      "Iteration 1462, loss = 0.21913821\n",
      "Iteration 1463, loss = 0.21906159\n",
      "Iteration 1464, loss = 0.21893399\n",
      "Iteration 1465, loss = 0.21886672\n",
      "Iteration 1466, loss = 0.21874354\n",
      "Iteration 1467, loss = 0.21866045\n",
      "Iteration 1468, loss = 0.21852743\n",
      "Iteration 1469, loss = 0.21845473\n",
      "Iteration 1470, loss = 0.21840356\n",
      "Iteration 1471, loss = 0.21823044\n",
      "Iteration 1472, loss = 0.21813330\n",
      "Iteration 1473, loss = 0.21812124\n",
      "Iteration 1474, loss = 0.21788738\n",
      "Iteration 1475, loss = 0.21776164\n",
      "Iteration 1476, loss = 0.21766217\n",
      "Iteration 1477, loss = 0.21758477\n",
      "Iteration 1478, loss = 0.21750160\n",
      "Iteration 1479, loss = 0.21740379\n",
      "Iteration 1480, loss = 0.21725957\n",
      "Iteration 1481, loss = 0.21714391\n",
      "Iteration 1482, loss = 0.21709021\n",
      "Iteration 1483, loss = 0.21697172\n",
      "Iteration 1484, loss = 0.21683303\n",
      "Iteration 1485, loss = 0.21672223\n",
      "Iteration 1486, loss = 0.21664278\n",
      "Iteration 1487, loss = 0.21651507\n",
      "Iteration 1488, loss = 0.21639195\n",
      "Iteration 1489, loss = 0.21644033\n",
      "Iteration 1490, loss = 0.21627508\n",
      "Iteration 1491, loss = 0.21614361\n",
      "Iteration 1492, loss = 0.21599994\n",
      "Iteration 1493, loss = 0.21594672\n",
      "Iteration 1494, loss = 0.21578993\n",
      "Iteration 1495, loss = 0.21573088\n",
      "Iteration 1496, loss = 0.21560988\n",
      "Iteration 1497, loss = 0.21551389\n",
      "Iteration 1498, loss = 0.21539440\n",
      "Iteration 1499, loss = 0.21531979\n",
      "Iteration 1500, loss = 0.21513457\n",
      "Iteration 1501, loss = 0.21506192\n",
      "Iteration 1502, loss = 0.21492428\n",
      "Iteration 1503, loss = 0.21482146\n",
      "Iteration 1504, loss = 0.21474986\n",
      "Iteration 1505, loss = 0.21463918\n",
      "Iteration 1506, loss = 0.21453239\n",
      "Iteration 1507, loss = 0.21439355\n",
      "Iteration 1508, loss = 0.21432621\n",
      "Iteration 1509, loss = 0.21423466\n",
      "Iteration 1510, loss = 0.21408947\n",
      "Iteration 1511, loss = 0.21401845\n",
      "Iteration 1512, loss = 0.21389905\n",
      "Iteration 1513, loss = 0.21378520\n",
      "Iteration 1514, loss = 0.21369764\n",
      "Iteration 1515, loss = 0.21357490\n",
      "Iteration 1516, loss = 0.21349840\n",
      "Iteration 1517, loss = 0.21356283\n",
      "Iteration 1518, loss = 0.21340808\n",
      "Iteration 1519, loss = 0.21318218\n",
      "Iteration 1520, loss = 0.21306287\n",
      "Iteration 1521, loss = 0.21296674\n",
      "Iteration 1522, loss = 0.21284498\n",
      "Iteration 1523, loss = 0.21276347\n",
      "Iteration 1524, loss = 0.21269983\n",
      "Iteration 1525, loss = 0.21255110\n",
      "Iteration 1526, loss = 0.21243049\n",
      "Iteration 1527, loss = 0.21231524\n",
      "Iteration 1528, loss = 0.21221075\n",
      "Iteration 1529, loss = 0.21211922\n",
      "Iteration 1530, loss = 0.21201047\n",
      "Iteration 1531, loss = 0.21191478\n",
      "Iteration 1532, loss = 0.21179884\n",
      "Iteration 1533, loss = 0.21168862\n",
      "Iteration 1534, loss = 0.21158329\n",
      "Iteration 1535, loss = 0.21153003\n",
      "Iteration 1536, loss = 0.21146664\n",
      "Iteration 1537, loss = 0.21128708\n",
      "Iteration 1538, loss = 0.21118976\n",
      "Iteration 1539, loss = 0.21107437\n",
      "Iteration 1540, loss = 0.21106508\n",
      "Iteration 1541, loss = 0.21106815\n",
      "Iteration 1542, loss = 0.21076947\n",
      "Iteration 1543, loss = 0.21072425\n",
      "Iteration 1544, loss = 0.21060831\n",
      "Iteration 1545, loss = 0.21055287\n",
      "Iteration 1546, loss = 0.21033475\n",
      "Iteration 1547, loss = 0.21026493\n",
      "Iteration 1548, loss = 0.21015127\n",
      "Iteration 1549, loss = 0.21003015\n",
      "Iteration 1550, loss = 0.20998824\n",
      "Iteration 1551, loss = 0.20983415\n",
      "Iteration 1552, loss = 0.20979416\n",
      "Iteration 1553, loss = 0.20964080\n",
      "Iteration 1554, loss = 0.20954692\n",
      "Iteration 1555, loss = 0.20943692\n",
      "Iteration 1556, loss = 0.20933192\n",
      "Iteration 1557, loss = 0.20924456\n",
      "Iteration 1558, loss = 0.20913898\n",
      "Iteration 1559, loss = 0.20903104\n",
      "Iteration 1560, loss = 0.20892637\n",
      "Iteration 1561, loss = 0.20881178\n",
      "Iteration 1562, loss = 0.20870119\n",
      "Iteration 1563, loss = 0.20862548\n",
      "Iteration 1564, loss = 0.20851786\n",
      "Iteration 1565, loss = 0.20839158\n",
      "Iteration 1566, loss = 0.20830720\n",
      "Iteration 1567, loss = 0.20818201\n",
      "Iteration 1568, loss = 0.20812533\n",
      "Iteration 1569, loss = 0.20800444\n",
      "Iteration 1570, loss = 0.20793985\n",
      "Iteration 1571, loss = 0.20782497\n",
      "Iteration 1572, loss = 0.20768144\n",
      "Iteration 1573, loss = 0.20767779\n",
      "Iteration 1574, loss = 0.20747519\n",
      "Iteration 1575, loss = 0.20740796\n",
      "Iteration 1576, loss = 0.20731261\n",
      "Iteration 1577, loss = 0.20718940\n",
      "Iteration 1578, loss = 0.20709015\n",
      "Iteration 1579, loss = 0.20695934\n",
      "Iteration 1580, loss = 0.20689878\n",
      "Iteration 1581, loss = 0.20681884\n",
      "Iteration 1582, loss = 0.20666475\n",
      "Iteration 1583, loss = 0.20658872\n",
      "Iteration 1584, loss = 0.20650100\n",
      "Iteration 1585, loss = 0.20634698\n",
      "Iteration 1586, loss = 0.20634285\n",
      "Iteration 1587, loss = 0.20618973\n",
      "Iteration 1588, loss = 0.20612461\n",
      "Iteration 1589, loss = 0.20617981\n",
      "Iteration 1590, loss = 0.20587507\n",
      "Iteration 1591, loss = 0.20580553\n",
      "Iteration 1592, loss = 0.20567341\n",
      "Iteration 1593, loss = 0.20556225\n",
      "Iteration 1594, loss = 0.20548372\n",
      "Iteration 1595, loss = 0.20534478\n",
      "Iteration 1596, loss = 0.20524963\n",
      "Iteration 1597, loss = 0.20518040\n",
      "Iteration 1598, loss = 0.20511937\n",
      "Iteration 1599, loss = 0.20494032\n",
      "Iteration 1600, loss = 0.20489203\n",
      "Iteration 1601, loss = 0.20474664\n",
      "Iteration 1602, loss = 0.20463712\n",
      "Iteration 1603, loss = 0.20460976\n",
      "Iteration 1604, loss = 0.20447805\n",
      "Iteration 1605, loss = 0.20431420\n",
      "Iteration 1606, loss = 0.20422320\n",
      "Iteration 1607, loss = 0.20414469\n",
      "Iteration 1608, loss = 0.20404529\n",
      "Iteration 1609, loss = 0.20392367\n",
      "Iteration 1610, loss = 0.20381377\n",
      "Iteration 1611, loss = 0.20379074\n",
      "Iteration 1612, loss = 0.20360043\n",
      "Iteration 1613, loss = 0.20350358\n",
      "Iteration 1614, loss = 0.20339107\n",
      "Iteration 1615, loss = 0.20335287\n",
      "Iteration 1616, loss = 0.20321966\n",
      "Iteration 1617, loss = 0.20309123\n",
      "Iteration 1618, loss = 0.20298608\n",
      "Iteration 1619, loss = 0.20297691\n",
      "Iteration 1620, loss = 0.20282220\n",
      "Iteration 1621, loss = 0.20270987\n",
      "Iteration 1622, loss = 0.20265257\n",
      "Iteration 1623, loss = 0.20251319\n",
      "Iteration 1624, loss = 0.20242156\n",
      "Iteration 1625, loss = 0.20246734\n",
      "Iteration 1626, loss = 0.20222985\n",
      "Iteration 1627, loss = 0.20213094\n",
      "Iteration 1628, loss = 0.20198157\n",
      "Iteration 1629, loss = 0.20187981\n",
      "Iteration 1630, loss = 0.20182578\n",
      "Iteration 1631, loss = 0.20170126\n",
      "Iteration 1632, loss = 0.20158753\n",
      "Iteration 1633, loss = 0.20149566\n",
      "Iteration 1634, loss = 0.20139182\n",
      "Iteration 1635, loss = 0.20131784\n",
      "Iteration 1636, loss = 0.20118340\n",
      "Iteration 1637, loss = 0.20109417\n",
      "Iteration 1638, loss = 0.20096689\n",
      "Iteration 1639, loss = 0.20093798\n",
      "Iteration 1640, loss = 0.20083284\n",
      "Iteration 1641, loss = 0.20072773\n",
      "Iteration 1642, loss = 0.20059635\n",
      "Iteration 1643, loss = 0.20055447\n",
      "Iteration 1644, loss = 0.20040637\n",
      "Iteration 1645, loss = 0.20028213\n",
      "Iteration 1646, loss = 0.20024249\n",
      "Iteration 1647, loss = 0.20008459\n",
      "Iteration 1648, loss = 0.20002479\n",
      "Iteration 1649, loss = 0.19988578\n",
      "Iteration 1650, loss = 0.19982551\n",
      "Iteration 1651, loss = 0.19968454\n",
      "Iteration 1652, loss = 0.19958219\n",
      "Iteration 1653, loss = 0.19946573\n",
      "Iteration 1654, loss = 0.19940666\n",
      "Iteration 1655, loss = 0.19928586\n",
      "Iteration 1656, loss = 0.19915933\n",
      "Iteration 1657, loss = 0.19911714\n",
      "Iteration 1658, loss = 0.19899773\n",
      "Iteration 1659, loss = 0.19889517\n",
      "Iteration 1660, loss = 0.19877924\n",
      "Iteration 1661, loss = 0.19867052\n",
      "Iteration 1662, loss = 0.19864935\n",
      "Iteration 1663, loss = 0.19855296\n",
      "Iteration 1664, loss = 0.19840496\n",
      "Iteration 1665, loss = 0.19826811\n",
      "Iteration 1666, loss = 0.19825293\n",
      "Iteration 1667, loss = 0.19813064\n",
      "Iteration 1668, loss = 0.19799414\n",
      "Iteration 1669, loss = 0.19791370\n",
      "Iteration 1670, loss = 0.19784384\n",
      "Iteration 1671, loss = 0.19765511\n",
      "Iteration 1672, loss = 0.19760113\n",
      "Iteration 1673, loss = 0.19746915\n",
      "Iteration 1674, loss = 0.19734019\n",
      "Iteration 1675, loss = 0.19728314\n",
      "Iteration 1676, loss = 0.19716540\n",
      "Iteration 1677, loss = 0.19709476\n",
      "Iteration 1678, loss = 0.19697731\n",
      "Iteration 1679, loss = 0.19690882\n",
      "Iteration 1680, loss = 0.19676709\n",
      "Iteration 1681, loss = 0.19668504\n",
      "Iteration 1682, loss = 0.19660166\n",
      "Iteration 1683, loss = 0.19646954\n",
      "Iteration 1684, loss = 0.19634563\n",
      "Iteration 1685, loss = 0.19629229\n",
      "Iteration 1686, loss = 0.19614216\n",
      "Iteration 1687, loss = 0.19606754\n",
      "Iteration 1688, loss = 0.19601192\n",
      "Iteration 1689, loss = 0.19585584\n",
      "Iteration 1690, loss = 0.19579323\n",
      "Iteration 1691, loss = 0.19566820\n",
      "Iteration 1692, loss = 0.19553755\n",
      "Iteration 1693, loss = 0.19543671\n",
      "Iteration 1694, loss = 0.19537445\n",
      "Iteration 1695, loss = 0.19524990\n",
      "Iteration 1696, loss = 0.19519335\n",
      "Iteration 1697, loss = 0.19508130\n",
      "Iteration 1698, loss = 0.19495454\n",
      "Iteration 1699, loss = 0.19483590\n",
      "Iteration 1700, loss = 0.19477569\n",
      "Iteration 1701, loss = 0.19473357\n",
      "Iteration 1702, loss = 0.19456976\n",
      "Iteration 1703, loss = 0.19455186\n",
      "Iteration 1704, loss = 0.19443200\n",
      "Iteration 1705, loss = 0.19428185\n",
      "Iteration 1706, loss = 0.19415662\n",
      "Iteration 1707, loss = 0.19403839\n",
      "Iteration 1708, loss = 0.19401881\n",
      "Iteration 1709, loss = 0.19385632\n",
      "Iteration 1710, loss = 0.19379101\n",
      "Iteration 1711, loss = 0.19369648\n",
      "Iteration 1712, loss = 0.19355414\n",
      "Iteration 1713, loss = 0.19350820\n",
      "Iteration 1714, loss = 0.19336929\n",
      "Iteration 1715, loss = 0.19326540\n",
      "Iteration 1716, loss = 0.19316109\n",
      "Iteration 1717, loss = 0.19305874\n",
      "Iteration 1718, loss = 0.19296259\n",
      "Iteration 1719, loss = 0.19285247\n",
      "Iteration 1720, loss = 0.19282164\n",
      "Iteration 1721, loss = 0.19265261\n",
      "Iteration 1722, loss = 0.19258005\n",
      "Iteration 1723, loss = 0.19246915\n",
      "Iteration 1724, loss = 0.19238144\n",
      "Iteration 1725, loss = 0.19228774\n",
      "Iteration 1726, loss = 0.19219592\n",
      "Iteration 1727, loss = 0.19209798\n",
      "Iteration 1728, loss = 0.19197795\n",
      "Iteration 1729, loss = 0.19189258\n",
      "Iteration 1730, loss = 0.19179121\n",
      "Iteration 1731, loss = 0.19169084\n",
      "Iteration 1732, loss = 0.19162532\n",
      "Iteration 1733, loss = 0.19156642\n",
      "Iteration 1734, loss = 0.19140603\n",
      "Iteration 1735, loss = 0.19126216\n",
      "Iteration 1736, loss = 0.19121391\n",
      "Iteration 1737, loss = 0.19109972\n",
      "Iteration 1738, loss = 0.19098215\n",
      "Iteration 1739, loss = 0.19087247\n",
      "Iteration 1740, loss = 0.19078077\n",
      "Iteration 1741, loss = 0.19065893\n",
      "Iteration 1742, loss = 0.19054990\n",
      "Iteration 1743, loss = 0.19045651\n",
      "Iteration 1744, loss = 0.19034030\n",
      "Iteration 1745, loss = 0.19028739\n",
      "Iteration 1746, loss = 0.19019032\n",
      "Iteration 1747, loss = 0.19011146\n",
      "Iteration 1748, loss = 0.18994441\n",
      "Iteration 1749, loss = 0.18987987\n",
      "Iteration 1750, loss = 0.18975810\n",
      "Iteration 1751, loss = 0.18969307\n",
      "Iteration 1752, loss = 0.18962681\n",
      "Iteration 1753, loss = 0.18954627\n",
      "Iteration 1754, loss = 0.18942734\n",
      "Iteration 1755, loss = 0.18931796\n",
      "Iteration 1756, loss = 0.18916371\n",
      "Iteration 1757, loss = 0.18909866\n",
      "Iteration 1758, loss = 0.18911048\n",
      "Iteration 1759, loss = 0.18897720\n",
      "Iteration 1760, loss = 0.18876540\n",
      "Iteration 1761, loss = 0.18872382\n",
      "Iteration 1762, loss = 0.18858844\n",
      "Iteration 1763, loss = 0.18848006\n",
      "Iteration 1764, loss = 0.18839266\n",
      "Iteration 1765, loss = 0.18827229\n",
      "Iteration 1766, loss = 0.18840330\n",
      "Iteration 1767, loss = 0.18808210\n",
      "Iteration 1768, loss = 0.18796537\n",
      "Iteration 1769, loss = 0.18789315\n",
      "Iteration 1770, loss = 0.18794930\n",
      "Iteration 1771, loss = 0.18769948\n",
      "Iteration 1772, loss = 0.18758538\n",
      "Iteration 1773, loss = 0.18758022\n",
      "Iteration 1774, loss = 0.18739461\n",
      "Iteration 1775, loss = 0.18731163\n",
      "Iteration 1776, loss = 0.18721787\n",
      "Iteration 1777, loss = 0.18719749\n",
      "Iteration 1778, loss = 0.18702809\n",
      "Iteration 1779, loss = 0.18694977\n",
      "Iteration 1780, loss = 0.18683306\n",
      "Iteration 1781, loss = 0.18680768\n",
      "Iteration 1782, loss = 0.18666836\n",
      "Iteration 1783, loss = 0.18653590\n",
      "Iteration 127, loss = 0.40225874\n",
      "Iteration 128, loss = 0.40165356\n",
      "Iteration 129, loss = 0.40095204\n",
      "Iteration 130, loss = 0.40034662\n",
      "Iteration 131, loss = 0.39968815\n",
      "Iteration 132, loss = 0.39906793\n",
      "Iteration 133, loss = 0.39847365\n",
      "Iteration 134, loss = 0.39786429\n",
      "Iteration 135, loss = 0.39731890\n",
      "Iteration 136, loss = 0.39670597\n",
      "Iteration 137, loss = 0.39613580\n",
      "Iteration 138, loss = 0.39554672\n",
      "Iteration 139, loss = 0.39496598\n",
      "Iteration 140, loss = 0.39444668\n",
      "Iteration 141, loss = 0.39386588\n",
      "Iteration 142, loss = 0.39334504\n",
      "Iteration 143, loss = 0.39281693\n",
      "Iteration 144, loss = 0.39228977\n",
      "Iteration 145, loss = 0.39177237\n",
      "Iteration 146, loss = 0.39125673\n",
      "Iteration 147, loss = 0.39079245\n",
      "Iteration 148, loss = 0.39025913\n",
      "Iteration 149, loss = 0.38976617\n",
      "Iteration 150, loss = 0.38928011\n",
      "Iteration 151, loss = 0.38878363\n",
      "Iteration 152, loss = 0.38831376\n",
      "Iteration 153, loss = 0.38783769\n",
      "Iteration 154, loss = 0.38739732\n",
      "Iteration 155, loss = 0.38694086\n",
      "Iteration 156, loss = 0.38647352\n",
      "Iteration 157, loss = 0.38601493\n",
      "Iteration 158, loss = 0.38560102\n",
      "Iteration 159, loss = 0.38515024\n",
      "Iteration 160, loss = 0.38470992\n",
      "Iteration 161, loss = 0.38427365\n",
      "Iteration 162, loss = 0.38386096\n",
      "Iteration 163, loss = 0.38342572\n",
      "Iteration 164, loss = 0.38302524\n",
      "Iteration 165, loss = 0.38257370\n",
      "Iteration 166, loss = 0.38224675\n",
      "Iteration 167, loss = 0.38179782\n",
      "Iteration 168, loss = 0.38140600\n",
      "Iteration 169, loss = 0.38099685\n",
      "Iteration 170, loss = 0.38059779\n",
      "Iteration 171, loss = 0.38023333\n",
      "Iteration 172, loss = 0.37981971\n",
      "Iteration 173, loss = 0.37945102\n",
      "Iteration 174, loss = 0.37909607\n",
      "Iteration 175, loss = 0.37871391\n",
      "Iteration 176, loss = 0.37833123\n",
      "Iteration 177, loss = 0.37797757\n",
      "Iteration 178, loss = 0.37760443\n",
      "Iteration 179, loss = 0.37726177\n",
      "Iteration 180, loss = 0.37687928\n",
      "Iteration 181, loss = 0.37654942\n",
      "Iteration 182, loss = 0.37616548\n",
      "Iteration 183, loss = 0.37583385\n",
      "Iteration 184, loss = 0.37548492\n",
      "Iteration 185, loss = 0.37514670\n",
      "Iteration 186, loss = 0.37481138\n",
      "Iteration 187, loss = 0.37446838\n",
      "Iteration 188, loss = 0.37415349\n",
      "Iteration 189, loss = 0.37380314\n",
      "Iteration 190, loss = 0.37347418\n",
      "Iteration 191, loss = 0.37314626\n",
      "Iteration 192, loss = 0.37281703\n",
      "Iteration 193, loss = 0.37248914\n",
      "Iteration 194, loss = 0.37221634\n",
      "Iteration 195, loss = 0.37184201\n",
      "Iteration 196, loss = 0.37152368\n",
      "Iteration 197, loss = 0.37121458\n",
      "Iteration 198, loss = 0.37091653\n",
      "Iteration 199, loss = 0.37059709\n",
      "Iteration 200, loss = 0.37028065\n",
      "Iteration 201, loss = 0.36999090\n",
      "Iteration 202, loss = 0.36968058\n",
      "Iteration 203, loss = 0.36939275\n",
      "Iteration 204, loss = 0.36907416\n",
      "Iteration 205, loss = 0.36878184\n",
      "Iteration 206, loss = 0.36849295\n",
      "Iteration 207, loss = 0.36820948\n",
      "Iteration 208, loss = 0.36790926\n",
      "Iteration 209, loss = 0.36760179\n",
      "Iteration 210, loss = 0.36731799\n",
      "Iteration 211, loss = 0.36704520\n",
      "Iteration 212, loss = 0.36674173\n",
      "Iteration 213, loss = 0.36646325\n",
      "Iteration 214, loss = 0.36621839\n",
      "Iteration 215, loss = 0.36590104\n",
      "Iteration 216, loss = 0.36565909\n",
      "Iteration 217, loss = 0.36536346\n",
      "Iteration 218, loss = 0.36507810\n",
      "Iteration 219, loss = 0.36482084\n",
      "Iteration 220, loss = 0.36455720\n",
      "Iteration 221, loss = 0.36428306\n",
      "Iteration 222, loss = 0.36400810\n",
      "Iteration 223, loss = 0.36375231\n",
      "Iteration 224, loss = 0.36348651\n",
      "Iteration 225, loss = 0.36321951\n",
      "Iteration 226, loss = 0.36298226\n",
      "Iteration 227, loss = 0.36270921\n",
      "Iteration 228, loss = 0.36245863\n",
      "Iteration 229, loss = 0.36220219\n",
      "Iteration 230, loss = 0.36194524\n",
      "Iteration 231, loss = 0.36170038\n",
      "Iteration 232, loss = 0.36148446\n",
      "Iteration 233, loss = 0.36122867\n",
      "Iteration 234, loss = 0.36095412\n",
      "Iteration 235, loss = 0.36071848\n",
      "Iteration 236, loss = 0.36047945\n",
      "Iteration 237, loss = 0.36023127\n",
      "Iteration 238, loss = 0.35998169\n",
      "Iteration 239, loss = 0.35974565\n",
      "Iteration 240, loss = 0.35950184\n",
      "Iteration 241, loss = 0.35925888\n",
      "Iteration 242, loss = 0.35904151\n",
      "Iteration 243, loss = 0.35882709\n",
      "Iteration 244, loss = 0.35858025\n",
      "Iteration 245, loss = 0.35835774\n",
      "Iteration 246, loss = 0.35811729\n",
      "Iteration 247, loss = 0.35789006\n",
      "Iteration 248, loss = 0.35765322\n",
      "Iteration 249, loss = 0.35740947\n",
      "Iteration 250, loss = 0.35720633\n",
      "Iteration 251, loss = 0.35700232\n",
      "Iteration 252, loss = 0.35675591\n",
      "Iteration 253, loss = 0.35654014\n",
      "Iteration 254, loss = 0.35630502\n",
      "Iteration 255, loss = 0.35608141\n",
      "Iteration 256, loss = 0.35587175\n",
      "Iteration 257, loss = 0.35565838\n",
      "Iteration 258, loss = 0.35545080\n",
      "Iteration 259, loss = 0.35521355\n",
      "Iteration 260, loss = 0.35501317\n",
      "Iteration 261, loss = 0.35479249\n",
      "Iteration 262, loss = 0.35457410\n",
      "Iteration 263, loss = 0.35437015\n",
      "Iteration 264, loss = 0.35414328\n",
      "Iteration 265, loss = 0.35393664\n",
      "Iteration 266, loss = 0.35373533\n",
      "Iteration 267, loss = 0.35353269\n",
      "Iteration 268, loss = 0.35331912\n",
      "Iteration 269, loss = 0.35311772\n",
      "Iteration 270, loss = 0.35290761\n",
      "Iteration 271, loss = 0.35270861\n",
      "Iteration 272, loss = 0.35249519\n",
      "Iteration 273, loss = 0.35229183\n",
      "Iteration 274, loss = 0.35214014\n",
      "Iteration 275, loss = 0.35188424\n",
      "Iteration 276, loss = 0.35169411\n",
      "Iteration 277, loss = 0.35148686\n",
      "Iteration 278, loss = 0.35129745\n",
      "Iteration 279, loss = 0.35109642\n",
      "Iteration 280, loss = 0.35091844\n",
      "Iteration 281, loss = 0.35071554\n",
      "Iteration 282, loss = 0.35052407\n",
      "Iteration 283, loss = 0.35031648\n",
      "Iteration 284, loss = 0.35011932\n",
      "Iteration 285, loss = 0.34993045\n",
      "Iteration 286, loss = 0.34972956\n",
      "Iteration 287, loss = 0.34953385\n",
      "Iteration 288, loss = 0.34935794\n",
      "Iteration 289, loss = 0.34916435\n",
      "Iteration 290, loss = 0.34896052\n",
      "Iteration 291, loss = 0.34880363\n",
      "Iteration 292, loss = 0.34860639\n",
      "Iteration 293, loss = 0.34840633\n",
      "Iteration 294, loss = 0.34824224\n",
      "Iteration 295, loss = 0.34806761\n",
      "Iteration 296, loss = 0.34786317\n",
      "Iteration 297, loss = 0.34766797\n",
      "Iteration 298, loss = 0.34754053\n",
      "Iteration 299, loss = 0.34731002\n",
      "Iteration 300, loss = 0.34714227\n",
      "Iteration 301, loss = 0.34696740\n",
      "Iteration 302, loss = 0.34677200\n",
      "Iteration 303, loss = 0.34660297\n",
      "Iteration 304, loss = 0.34642586\n",
      "Iteration 305, loss = 0.34625967\n",
      "Iteration 306, loss = 0.34606707\n",
      "Iteration 307, loss = 0.34590897\n",
      "Iteration 308, loss = 0.34572633\n",
      "Iteration 309, loss = 0.34556286\n",
      "Iteration 310, loss = 0.34536689\n",
      "Iteration 311, loss = 0.34520944\n",
      "Iteration 312, loss = 0.34502057\n",
      "Iteration 313, loss = 0.34484175\n",
      "Iteration 314, loss = 0.34466926\n",
      "Iteration 315, loss = 0.34452235\n",
      "Iteration 316, loss = 0.34432956\n",
      "Iteration 317, loss = 0.34417582\n",
      "Iteration 318, loss = 0.34400154\n",
      "Iteration 319, loss = 0.34383491\n",
      "Iteration 320, loss = 0.34367275\n",
      "Iteration 321, loss = 0.34349114\n",
      "Iteration 322, loss = 0.34334521\n",
      "Iteration 323, loss = 0.34316223\n",
      "Iteration 324, loss = 0.34298775\n",
      "Iteration 325, loss = 0.34283793\n",
      "Iteration 326, loss = 0.34269188\n",
      "Iteration 327, loss = 0.34251733\n",
      "Iteration 328, loss = 0.34233872\n",
      "Iteration 329, loss = 0.34217551\n",
      "Iteration 330, loss = 0.34202113\n",
      "Iteration 331, loss = 0.34185529\n",
      "Iteration 332, loss = 0.34170523\n",
      "Iteration 333, loss = 0.34152885\n",
      "Iteration 334, loss = 0.34138409\n",
      "Iteration 335, loss = 0.34123633\n",
      "Iteration 336, loss = 0.34108017\n",
      "Iteration 337, loss = 0.34094285\n",
      "Iteration 338, loss = 0.34075086\n",
      "Iteration 339, loss = 0.34061415\n",
      "Iteration 340, loss = 0.34043730\n",
      "Iteration 341, loss = 0.34030396\n",
      "Iteration 342, loss = 0.34015111\n",
      "Iteration 343, loss = 0.34001293\n",
      "Iteration 344, loss = 0.33990428\n",
      "Iteration 345, loss = 0.33970918\n",
      "Iteration 346, loss = 0.33953232\n",
      "Iteration 347, loss = 0.33939416\n",
      "Iteration 348, loss = 0.33923689\n",
      "Iteration 349, loss = 0.33908980\n",
      "Iteration 350, loss = 0.33896228\n",
      "Iteration 351, loss = 0.33880409\n",
      "Iteration 352, loss = 0.33862719\n",
      "Iteration 353, loss = 0.33848160\n",
      "Iteration 354, loss = 0.33832844\n",
      "Iteration 355, loss = 0.33820179\n",
      "Iteration 356, loss = 0.33804588\n",
      "Iteration 357, loss = 0.33791195\n",
      "Iteration 358, loss = 0.33775934\n",
      "Iteration 359, loss = 0.33762869\n",
      "Iteration 360, loss = 0.33749179\n",
      "Iteration 361, loss = 0.33734178\n",
      "Iteration 362, loss = 0.33719883\n",
      "Iteration 363, loss = 0.33706015\n",
      "Iteration 364, loss = 0.33692883\n",
      "Iteration 365, loss = 0.33678596\n",
      "Iteration 366, loss = 0.33661956\n",
      "Iteration 367, loss = 0.33649281\n",
      "Iteration 368, loss = 0.33636172\n",
      "Iteration 369, loss = 0.33619692\n",
      "Iteration 370, loss = 0.33607138\n",
      "Iteration 371, loss = 0.33592771\n",
      "Iteration 372, loss = 0.33578669\n",
      "Iteration 373, loss = 0.33567251\n",
      "Iteration 374, loss = 0.33553819\n",
      "Iteration 375, loss = 0.33542627\n",
      "Iteration 376, loss = 0.33523642\n",
      "Iteration 377, loss = 0.33511015\n",
      "Iteration 378, loss = 0.33497752\n",
      "Iteration 379, loss = 0.33482933\n",
      "Iteration 380, loss = 0.33473121\n",
      "Iteration 381, loss = 0.33456814\n",
      "Iteration 382, loss = 0.33443907\n",
      "Iteration 383, loss = 0.33430305\n",
      "Iteration 384, loss = 0.33417494\n",
      "Iteration 385, loss = 0.33403919\n",
      "Iteration 386, loss = 0.33391398\n",
      "Iteration 387, loss = 0.33377354\n",
      "Iteration 388, loss = 0.33365244\n",
      "Iteration 389, loss = 0.33353618\n",
      "Iteration 390, loss = 0.33340118\n",
      "Iteration 391, loss = 0.33326336\n",
      "Iteration 392, loss = 0.33312836\n",
      "Iteration 393, loss = 0.33298704\n",
      "Iteration 394, loss = 0.33287606\n",
      "Iteration 395, loss = 0.33275341\n",
      "Iteration 396, loss = 0.33261018\n",
      "Iteration 397, loss = 0.33248599\n",
      "Iteration 398, loss = 0.33237823\n",
      "Iteration 399, loss = 0.33223865\n",
      "Iteration 400, loss = 0.33211458\n",
      "Iteration 401, loss = 0.33197756\n",
      "Iteration 402, loss = 0.33184885\n",
      "Iteration 403, loss = 0.33173849\n",
      "Iteration 404, loss = 0.33160776\n",
      "Iteration 405, loss = 0.33148122\n",
      "Iteration 406, loss = 0.33136421\n",
      "Iteration 407, loss = 0.33124143\n",
      "Iteration 408, loss = 0.33110894\n",
      "Iteration 409, loss = 0.33099508\n",
      "Iteration 410, loss = 0.33086801\n",
      "Iteration 411, loss = 0.33075431\n",
      "Iteration 412, loss = 0.33060508\n",
      "Iteration 413, loss = 0.33050698\n",
      "Iteration 414, loss = 0.33045030\n",
      "Iteration 415, loss = 0.33026332\n",
      "Iteration 416, loss = 0.33014626\n",
      "Iteration 417, loss = 0.33002800\n",
      "Iteration 418, loss = 0.32989829\n",
      "Iteration 419, loss = 0.32977420\n",
      "Iteration 420, loss = 0.32966652\n",
      "Iteration 421, loss = 0.32954468\n",
      "Iteration 422, loss = 0.32942230\n",
      "Iteration 423, loss = 0.32930326\n",
      "Iteration 424, loss = 0.32920075\n",
      "Iteration 425, loss = 0.32907501\n",
      "Iteration 426, loss = 0.32897458\n",
      "Iteration 427, loss = 0.32884626\n",
      "Iteration 428, loss = 0.32873454\n",
      "Iteration 429, loss = 0.32860288\n",
      "Iteration 430, loss = 0.32848288\n",
      "Iteration 431, loss = 0.32836642\n",
      "Iteration 432, loss = 0.32827872\n",
      "Iteration 433, loss = 0.32814549\n",
      "Iteration 434, loss = 0.32806109\n",
      "Iteration 435, loss = 0.32792013\n",
      "Iteration 436, loss = 0.32779544\n",
      "Iteration 437, loss = 0.32769168\n",
      "Iteration 438, loss = 0.32758513\n",
      "Iteration 439, loss = 0.32747961\n",
      "Iteration 440, loss = 0.32735672\n",
      "Iteration 441, loss = 0.32725312\n",
      "Iteration 442, loss = 0.32713074\n",
      "Iteration 443, loss = 0.32702641\n",
      "Iteration 444, loss = 0.32691761\n",
      "Iteration 445, loss = 0.32679695\n",
      "Iteration 446, loss = 0.32670208\n",
      "Iteration 447, loss = 0.32659728\n",
      "Iteration 448, loss = 0.32646355\n",
      "Iteration 449, loss = 0.32638807\n",
      "Iteration 450, loss = 0.32625322\n",
      "Iteration 451, loss = 0.32617720\n",
      "Iteration 452, loss = 0.32605987\n",
      "Iteration 453, loss = 0.32594945\n",
      "Iteration 454, loss = 0.32581531\n",
      "Iteration 455, loss = 0.32573789\n",
      "Iteration 456, loss = 0.32562534\n",
      "Iteration 457, loss = 0.32549657\n",
      "Iteration 458, loss = 0.32540233\n",
      "Iteration 459, loss = 0.32527645\n",
      "Iteration 460, loss = 0.32517829\n",
      "Iteration 461, loss = 0.32505619\n",
      "Iteration 462, loss = 0.32498253\n",
      "Iteration 463, loss = 0.32487580\n",
      "Iteration 464, loss = 0.32475128\n",
      "Iteration 465, loss = 0.32464817\n",
      "Iteration 466, loss = 0.32455038\n",
      "Iteration 467, loss = 0.32443955\n",
      "Iteration 468, loss = 0.32432842\n",
      "Iteration 469, loss = 0.32424848\n",
      "Iteration 470, loss = 0.32413366\n",
      "Iteration 471, loss = 0.32403817\n",
      "Iteration 472, loss = 0.32392053\n",
      "Iteration 473, loss = 0.32381189\n",
      "Iteration 474, loss = 0.32370948\n",
      "Iteration 475, loss = 0.32363471\n",
      "Iteration 476, loss = 0.32349823\n",
      "Iteration 477, loss = 0.32341221\n",
      "Iteration 478, loss = 0.32329870\n",
      "Iteration 479, loss = 0.32321385\n",
      "Iteration 480, loss = 0.32310728\n",
      "Iteration 481, loss = 0.32300274\n",
      "Iteration 482, loss = 0.32290212\n",
      "Iteration 483, loss = 0.32279782\n",
      "Iteration 484, loss = 0.32271414\n",
      "Iteration 485, loss = 0.32259148\n",
      "Iteration 486, loss = 0.32251584\n",
      "Iteration 487, loss = 0.32239245\n",
      "Iteration 488, loss = 0.32233221\n",
      "Iteration 489, loss = 0.32220479\n",
      "Iteration 490, loss = 0.32209844\n",
      "Iteration 491, loss = 0.32201180\n",
      "Iteration 492, loss = 0.32189314\n",
      "Iteration 493, loss = 0.32179341\n",
      "Iteration 494, loss = 0.32170329\n",
      "Iteration 495, loss = 0.32159503\n",
      "Iteration 496, loss = 0.32150571\n",
      "Iteration 497, loss = 0.32140769\n",
      "Iteration 498, loss = 0.32131510\n",
      "Iteration 499, loss = 0.32122568\n",
      "Iteration 500, loss = 0.32111811\n",
      "Iteration 501, loss = 0.32102308\n",
      "Iteration 502, loss = 0.32091759\n",
      "Iteration 503, loss = 0.32083219\n",
      "Iteration 504, loss = 0.32072555\n",
      "Iteration 505, loss = 0.32064502\n",
      "Iteration 506, loss = 0.32054728\n",
      "Iteration 507, loss = 0.32043792\n",
      "Iteration 508, loss = 0.32035782\n",
      "Iteration 509, loss = 0.32024968\n",
      "Iteration 510, loss = 0.32017262\n",
      "Iteration 511, loss = 0.32006428\n",
      "Iteration 512, loss = 0.31997135\n",
      "Iteration 513, loss = 0.31988643\n",
      "Iteration 514, loss = 0.31978102\n",
      "Iteration 515, loss = 0.31969266\n",
      "Iteration 516, loss = 0.31960308\n",
      "Iteration 517, loss = 0.31950310\n",
      "Iteration 518, loss = 0.31941201\n",
      "Iteration 519, loss = 0.31932182\n",
      "Iteration 520, loss = 0.31924313\n",
      "Iteration 521, loss = 0.31914382\n",
      "Iteration 522, loss = 0.31904130\n",
      "Iteration 523, loss = 0.31895771\n",
      "Iteration 524, loss = 0.31886608\n",
      "Iteration 525, loss = 0.31877278\n",
      "Iteration 526, loss = 0.31868168\n",
      "Iteration 527, loss = 0.31859668\n",
      "Iteration 528, loss = 0.31850536\n",
      "Iteration 529, loss = 0.31842132\n",
      "Iteration 530, loss = 0.31836331\n",
      "Iteration 531, loss = 0.31821800\n",
      "Iteration 532, loss = 0.31814030\n",
      "Iteration 533, loss = 0.31803440\n",
      "Iteration 534, loss = 0.31795522\n",
      "Iteration 535, loss = 0.31786071\n",
      "Iteration 536, loss = 0.31776302\n",
      "Iteration 537, loss = 0.31767340\n",
      "Iteration 538, loss = 0.31759343\n",
      "Iteration 539, loss = 0.31750025\n",
      "Iteration 540, loss = 0.31741128\n",
      "Iteration 541, loss = 0.31731560\n",
      "Iteration 542, loss = 0.31723566\n",
      "Iteration 543, loss = 0.31714530\n",
      "Iteration 544, loss = 0.31705949\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74792982\n",
      "Iteration 2, loss = 0.74472083\n",
      "Iteration 3, loss = 0.73959315\n",
      "Iteration 4, loss = 0.73395133\n",
      "Iteration 5, loss = 0.72804927\n",
      "Iteration 6, loss = 0.72177004\n",
      "Iteration 7, loss = 0.71614038\n",
      "Iteration 8, loss = 0.71022597\n",
      "Iteration 9, loss = 0.70522457\n",
      "Iteration 10, loss = 0.70056091\n",
      "Iteration 11, loss = 0.69589334\n",
      "Iteration 12, loss = 0.69162589\n",
      "Iteration 13, loss = 0.68751233\n",
      "Iteration 14, loss = 0.68382580\n",
      "Iteration 15, loss = 0.68048311\n",
      "Iteration 16, loss = 0.67700113\n",
      "Iteration 17, loss = 0.67371922\n",
      "Iteration 18, loss = 0.67046470\n",
      "Iteration 19, loss = 0.66729875\n",
      "Iteration 20, loss = 0.66424432\n",
      "Iteration 21, loss = 0.66124731\n",
      "Iteration 22, loss = 0.65824712\n",
      "Iteration 23, loss = 0.65531112\n",
      "Iteration 24, loss = 0.65243341\n",
      "Iteration 25, loss = 0.64946812\n",
      "Iteration 26, loss = 0.64651550\n",
      "Iteration 27, loss = 0.64367641\n",
      "Iteration 28, loss = 0.64064015\n",
      "Iteration 29, loss = 0.63776510\n",
      "Iteration 30, loss = 0.63484014\n",
      "Iteration 31, loss = 0.63192493\n",
      "Iteration 32, loss = 0.62899495\n",
      "Iteration 33, loss = 0.62611639\n",
      "Iteration 34, loss = 0.62311900\n",
      "Iteration 35, loss = 0.62020021\n",
      "Iteration 36, loss = 0.61722179\n",
      "Iteration 37, loss = 0.61419494\n",
      "Iteration 38, loss = 0.61127504\n",
      "Iteration 39, loss = 0.60821405\n",
      "Iteration 40, loss = 0.60514190\n",
      "Iteration 41, loss = 0.60204810\n",
      "Iteration 42, loss = 0.59898036\n",
      "Iteration 43, loss = 0.59596049\n",
      "Iteration 44, loss = 0.59285802\n",
      "Iteration 45, loss = 0.58980601\n",
      "Iteration 46, loss = 0.58666254\n",
      "Iteration 47, loss = 0.58363177\n",
      "Iteration 48, loss = 0.58045500\n",
      "Iteration 49, loss = 0.57736464\n",
      "Iteration 50, loss = 0.57434699\n",
      "Iteration 51, loss = 0.57111385\n",
      "Iteration 52, loss = 0.56798190\n",
      "Iteration 53, loss = 0.56493183\n",
      "Iteration 54, loss = 0.56176021\n",
      "Iteration 55, loss = 0.55862870\n",
      "Iteration 56, loss = 0.55551655\n",
      "Iteration 57, loss = 0.55233262\n",
      "Iteration 58, loss = 0.54919123\n",
      "Iteration 59, loss = 0.54602726\n",
      "Iteration 60, loss = 0.54285811\n",
      "Iteration 61, loss = 0.53975101\n",
      "Iteration 62, loss = 0.53662547\n",
      "Iteration 63, loss = 0.53345697\n",
      "Iteration 64, loss = 0.53026459\n",
      "Iteration 65, loss = 0.52720458\n",
      "Iteration 66, loss = 0.52403136\n",
      "Iteration 67, loss = 0.52091116\n",
      "Iteration 68, loss = 0.51772434\n",
      "Iteration 69, loss = 0.51473818\n",
      "Iteration 70, loss = 0.51169881\n",
      "Iteration 71, loss = 0.50863310\n",
      "Iteration 72, loss = 0.50550455\n",
      "Iteration 73, loss = 0.50243690\n",
      "Iteration 74, loss = 0.49949686\n",
      "Iteration 75, loss = 0.49649342\n",
      "Iteration 76, loss = 0.49351591\n",
      "Iteration 77, loss = 0.49057787\n",
      "Iteration 78, loss = 0.48763344\n",
      "Iteration 158, loss = 0.35600315\n",
      "Iteration 159, loss = 0.35563017\n",
      "Iteration 160, loss = 0.35527611\n",
      "Iteration 161, loss = 0.35490234\n",
      "Iteration 162, loss = 0.35454254\n",
      "Iteration 163, loss = 0.35419649\n",
      "Iteration 164, loss = 0.35384318\n",
      "Iteration 165, loss = 0.35350815\n",
      "Iteration 166, loss = 0.35314300\n",
      "Iteration 167, loss = 0.35279635\n",
      "Iteration 168, loss = 0.35249606\n",
      "Iteration 169, loss = 0.35212800\n",
      "Iteration 170, loss = 0.35179999\n",
      "Iteration 171, loss = 0.35152232\n",
      "Iteration 172, loss = 0.35117755\n",
      "Iteration 173, loss = 0.35085810\n",
      "Iteration 174, loss = 0.35054138\n",
      "Iteration 175, loss = 0.35023752\n",
      "Iteration 176, loss = 0.34993097\n",
      "Iteration 177, loss = 0.34965366\n",
      "Iteration 178, loss = 0.34931490\n",
      "Iteration 179, loss = 0.34901810\n",
      "Iteration 180, loss = 0.34871940\n",
      "Iteration 181, loss = 0.34843410\n",
      "Iteration 182, loss = 0.34812914\n",
      "Iteration 183, loss = 0.34784154\n",
      "Iteration 184, loss = 0.34754057\n",
      "Iteration 185, loss = 0.34729263\n",
      "Iteration 186, loss = 0.34698588\n",
      "Iteration 187, loss = 0.34669476\n",
      "Iteration 188, loss = 0.34641925\n",
      "Iteration 189, loss = 0.34614478\n",
      "Iteration 190, loss = 0.34588005\n",
      "Iteration 191, loss = 0.34557597\n",
      "Iteration 192, loss = 0.34532094\n",
      "Iteration 193, loss = 0.34502467\n",
      "Iteration 194, loss = 0.34478262\n",
      "Iteration 195, loss = 0.34449353\n",
      "Iteration 196, loss = 0.34427370\n",
      "Iteration 197, loss = 0.34399349\n",
      "Iteration 198, loss = 0.34368809\n",
      "Iteration 199, loss = 0.34344487\n",
      "Iteration 200, loss = 0.34320161\n",
      "Iteration 201, loss = 0.34293593\n",
      "Iteration 202, loss = 0.34267470\n",
      "Iteration 203, loss = 0.34243266\n",
      "Iteration 204, loss = 0.34218276\n",
      "Iteration 205, loss = 0.34193961\n",
      "Iteration 206, loss = 0.34169181\n",
      "Iteration 207, loss = 0.34145898\n",
      "Iteration 208, loss = 0.34121434\n",
      "Iteration 209, loss = 0.34097579\n",
      "Iteration 210, loss = 0.34073902\n",
      "Iteration 211, loss = 0.34049160\n",
      "Iteration 212, loss = 0.34027792\n",
      "Iteration 213, loss = 0.34002617\n",
      "Iteration 214, loss = 0.33981371\n",
      "Iteration 215, loss = 0.33957051\n",
      "Iteration 216, loss = 0.33934980\n",
      "Iteration 217, loss = 0.33913430\n",
      "Iteration 218, loss = 0.33889894\n",
      "Iteration 219, loss = 0.33869047\n",
      "Iteration 220, loss = 0.33845901\n",
      "Iteration 221, loss = 0.33822508\n",
      "Iteration 222, loss = 0.33801578\n",
      "Iteration 223, loss = 0.33780370\n",
      "Iteration 224, loss = 0.33757206\n",
      "Iteration 225, loss = 0.33735236\n",
      "Iteration 226, loss = 0.33715060\n",
      "Iteration 227, loss = 0.33693456\n",
      "Iteration 228, loss = 0.33672685\n",
      "Iteration 229, loss = 0.33649711\n",
      "Iteration 230, loss = 0.33630084\n",
      "Iteration 231, loss = 0.33610130\n",
      "Iteration 232, loss = 0.33588571\n",
      "Iteration 233, loss = 0.33571424\n",
      "Iteration 234, loss = 0.33550173\n",
      "Iteration 235, loss = 0.33527956\n",
      "Iteration 236, loss = 0.33510607\n",
      "Iteration 237, loss = 0.33489255\n",
      "Iteration 238, loss = 0.33469091\n",
      "Iteration 239, loss = 0.33450035\n",
      "Iteration 240, loss = 0.33431729\n",
      "Iteration 241, loss = 0.33412445\n",
      "Iteration 242, loss = 0.33395154\n",
      "Iteration 243, loss = 0.33373963\n",
      "Iteration 244, loss = 0.33357266\n",
      "Iteration 245, loss = 0.33336803\n",
      "Iteration 246, loss = 0.33317240\n",
      "Iteration 247, loss = 0.33300334\n",
      "Iteration 248, loss = 0.33282658\n",
      "Iteration 249, loss = 0.33265541\n",
      "Iteration 250, loss = 0.33245663\n",
      "Iteration 251, loss = 0.33228792\n",
      "Iteration 252, loss = 0.33209381\n",
      "Iteration 253, loss = 0.33193444\n",
      "Iteration 254, loss = 0.33175174\n",
      "Iteration 255, loss = 0.33157923\n",
      "Iteration 256, loss = 0.33141204\n",
      "Iteration 257, loss = 0.33122845\n",
      "Iteration 258, loss = 0.33108012\n",
      "Iteration 259, loss = 0.33088881\n",
      "Iteration 260, loss = 0.33071185\n",
      "Iteration 261, loss = 0.33058205\n",
      "Iteration 262, loss = 0.33039691\n",
      "Iteration 263, loss = 0.33024664\n",
      "Iteration 264, loss = 0.33004596\n",
      "Iteration 265, loss = 0.32990437\n",
      "Iteration 266, loss = 0.32974400\n",
      "Iteration 267, loss = 0.32957279\n",
      "Iteration 268, loss = 0.32941573\n",
      "Iteration 269, loss = 0.32927608\n",
      "Iteration 270, loss = 0.32910399\n",
      "Iteration 271, loss = 0.32893778\n",
      "Iteration 272, loss = 0.32880775\n",
      "Iteration 273, loss = 0.32863119\n",
      "Iteration 274, loss = 0.32847584\n",
      "Iteration 275, loss = 0.32831037\n",
      "Iteration 276, loss = 0.32816969\n",
      "Iteration 277, loss = 0.32800132\n",
      "Iteration 278, loss = 0.32788304\n",
      "Iteration 279, loss = 0.32769743\n",
      "Iteration 280, loss = 0.32755238\n",
      "Iteration 281, loss = 0.32740834\n",
      "Iteration 282, loss = 0.32725159\n",
      "Iteration 283, loss = 0.32709634\n",
      "Iteration 284, loss = 0.32695926\n",
      "Iteration 285, loss = 0.32684330\n",
      "Iteration 286, loss = 0.32666566\n",
      "Iteration 287, loss = 0.32651270\n",
      "Iteration 288, loss = 0.32637098\n",
      "Iteration 289, loss = 0.32623957\n",
      "Iteration 290, loss = 0.32609534\n",
      "Iteration 291, loss = 0.32596896\n",
      "Iteration 292, loss = 0.32582145\n",
      "Iteration 293, loss = 0.32568385\n",
      "Iteration 294, loss = 0.32559027\n",
      "Iteration 295, loss = 0.32539774\n",
      "Iteration 296, loss = 0.32525241\n",
      "Iteration 297, loss = 0.32513313\n",
      "Iteration 298, loss = 0.32499186\n",
      "Iteration 299, loss = 0.32484594\n",
      "Iteration 300, loss = 0.32474232\n",
      "Iteration 301, loss = 0.32458486\n",
      "Iteration 302, loss = 0.32443449\n",
      "Iteration 303, loss = 0.32430135\n",
      "Iteration 304, loss = 0.32418739\n",
      "Iteration 305, loss = 0.32405528\n",
      "Iteration 306, loss = 0.32391875\n",
      "Iteration 307, loss = 0.32378149\n",
      "Iteration 308, loss = 0.32365514\n",
      "Iteration 309, loss = 0.32352491\n",
      "Iteration 310, loss = 0.32340385\n",
      "Iteration 311, loss = 0.32326659\n",
      "Iteration 312, loss = 0.32314922\n",
      "Iteration 313, loss = 0.32301481\n",
      "Iteration 314, loss = 0.32288026\n",
      "Iteration 315, loss = 0.32275184\n",
      "Iteration 316, loss = 0.32262694\n",
      "Iteration 317, loss = 0.32250770\n",
      "Iteration 318, loss = 0.32237355\n",
      "Iteration 319, loss = 0.32225498\n",
      "Iteration 320, loss = 0.32212491\n",
      "Iteration 321, loss = 0.32200389\n",
      "Iteration 322, loss = 0.32187496\n",
      "Iteration 323, loss = 0.32176036\n",
      "Iteration 324, loss = 0.32164696\n",
      "Iteration 325, loss = 0.32151263\n",
      "Iteration 326, loss = 0.32138556\n",
      "Iteration 327, loss = 0.32125776\n",
      "Iteration 328, loss = 0.32112826\n",
      "Iteration 329, loss = 0.32102664\n",
      "Iteration 330, loss = 0.32089666\n",
      "Iteration 331, loss = 0.32078587\n",
      "Iteration 332, loss = 0.32066565\n",
      "Iteration 333, loss = 0.32054749\n",
      "Iteration 334, loss = 0.32043185\n",
      "Iteration 335, loss = 0.32031928\n",
      "Iteration 336, loss = 0.32018910\n",
      "Iteration 337, loss = 0.32007110\n",
      "Iteration 338, loss = 0.31995659\n",
      "Iteration 339, loss = 0.31986178\n",
      "Iteration 340, loss = 0.31972115\n",
      "Iteration 341, loss = 0.31960888\n",
      "Iteration 342, loss = 0.31951493\n",
      "Iteration 343, loss = 0.31938268\n",
      "Iteration 344, loss = 0.31926772\n",
      "Iteration 345, loss = 0.31915959\n",
      "Iteration 346, loss = 0.31907677\n",
      "Iteration 347, loss = 0.31893376\n",
      "Iteration 348, loss = 0.31881506\n",
      "Iteration 349, loss = 0.31871460\n",
      "Iteration 350, loss = 0.31858902\n",
      "Iteration 351, loss = 0.31847344\n",
      "Iteration 352, loss = 0.31836085\n",
      "Iteration 353, loss = 0.31825041\n",
      "Iteration 354, loss = 0.31817993\n",
      "Iteration 355, loss = 0.31802841\n",
      "Iteration 356, loss = 0.31793955\n",
      "Iteration 357, loss = 0.31782102\n",
      "Iteration 358, loss = 0.31770170\n",
      "Iteration 359, loss = 0.31758753\n",
      "Iteration 360, loss = 0.31751081\n",
      "Iteration 361, loss = 0.31737497\n",
      "Iteration 362, loss = 0.31726806\n",
      "Iteration 363, loss = 0.31717551\n",
      "Iteration 364, loss = 0.31705755\n",
      "Iteration 365, loss = 0.31697878\n",
      "Iteration 366, loss = 0.31684475\n",
      "Iteration 367, loss = 0.31674401\n",
      "Iteration 368, loss = 0.31662769\n",
      "Iteration 369, loss = 0.31653503\n",
      "Iteration 370, loss = 0.31641704\n",
      "Iteration 371, loss = 0.31632417\n",
      "Iteration 372, loss = 0.31620564\n",
      "Iteration 373, loss = 0.31610866\n",
      "Iteration 374, loss = 0.31600484\n",
      "Iteration 375, loss = 0.31588925\n",
      "Iteration 376, loss = 0.31579758\n",
      "Iteration 377, loss = 0.31568530\n",
      "Iteration 378, loss = 0.31557834\n",
      "Iteration 379, loss = 0.31548369\n",
      "Iteration 380, loss = 0.31538457\n",
      "Iteration 381, loss = 0.31526969\n",
      "Iteration 382, loss = 0.31517212\n",
      "Iteration 383, loss = 0.31508165\n",
      "Iteration 384, loss = 0.31497413\n",
      "Iteration 385, loss = 0.31486879\n",
      "Iteration 386, loss = 0.31476881\n",
      "Iteration 387, loss = 0.31468284\n",
      "Iteration 388, loss = 0.31456730\n",
      "Iteration 389, loss = 0.31448108\n",
      "Iteration 390, loss = 0.31437920\n",
      "Iteration 391, loss = 0.31425508\n",
      "Iteration 392, loss = 0.31416275\n",
      "Iteration 393, loss = 0.31405927\n",
      "Iteration 394, loss = 0.31398453\n",
      "Iteration 395, loss = 0.31385871\n",
      "Iteration 396, loss = 0.31381658\n",
      "Iteration 397, loss = 0.31366555\n",
      "Iteration 398, loss = 0.31357859\n",
      "Iteration 399, loss = 0.31347345\n",
      "Iteration 400, loss = 0.31341895\n",
      "Iteration 401, loss = 0.31328925\n",
      "Iteration 402, loss = 0.31319506\n",
      "Iteration 403, loss = 0.31308565\n",
      "Iteration 404, loss = 0.31298689\n",
      "Iteration 405, loss = 0.31288992\n",
      "Iteration 406, loss = 0.31281083\n",
      "Iteration 407, loss = 0.31271133\n",
      "Iteration 408, loss = 0.31260114\n",
      "Iteration 409, loss = 0.31250989\n",
      "Iteration 410, loss = 0.31240538\n",
      "Iteration 411, loss = 0.31230252\n",
      "Iteration 412, loss = 0.31223316\n",
      "Iteration 413, loss = 0.31211686\n",
      "Iteration 414, loss = 0.31201981\n",
      "Iteration 415, loss = 0.31193764\n",
      "Iteration 416, loss = 0.31183811\n",
      "Iteration 417, loss = 0.31171850\n",
      "Iteration 418, loss = 0.31164394\n",
      "Iteration 419, loss = 0.31153780\n",
      "Iteration 420, loss = 0.31143290\n",
      "Iteration 421, loss = 0.31135436\n",
      "Iteration 422, loss = 0.31124553\n",
      "Iteration 423, loss = 0.31115725\n",
      "Iteration 424, loss = 0.31106070\n",
      "Iteration 425, loss = 0.31098185\n",
      "Iteration 426, loss = 0.31087909\n",
      "Iteration 427, loss = 0.31077954\n",
      "Iteration 428, loss = 0.31068217\n",
      "Iteration 429, loss = 0.31058917\n",
      "Iteration 430, loss = 0.31051424\n",
      "Iteration 431, loss = 0.31042490\n",
      "Iteration 432, loss = 0.31032028\n",
      "Iteration 433, loss = 0.31022887\n",
      "Iteration 434, loss = 0.31013391\n",
      "Iteration 435, loss = 0.31004830\n",
      "Iteration 436, loss = 0.30994908\n",
      "Iteration 437, loss = 0.30985641\n",
      "Iteration 438, loss = 0.30979969\n",
      "Iteration 439, loss = 0.30967369\n",
      "Iteration 440, loss = 0.30959664\n",
      "Iteration 441, loss = 0.30950511\n",
      "Iteration 442, loss = 0.30940105\n",
      "Iteration 443, loss = 0.30932870\n",
      "Iteration 444, loss = 0.30922752\n",
      "Iteration 445, loss = 0.30914956\n",
      "Iteration 446, loss = 0.30905644\n",
      "Iteration 447, loss = 0.30895048\n",
      "Iteration 448, loss = 0.30888053\n",
      "Iteration 449, loss = 0.30877071\n",
      "Iteration 450, loss = 0.30868378\n",
      "Iteration 451, loss = 0.30861191\n",
      "Iteration 452, loss = 0.30854757\n",
      "Iteration 453, loss = 0.30842515\n",
      "Iteration 454, loss = 0.30833179\n",
      "Iteration 455, loss = 0.30824627\n",
      "Iteration 456, loss = 0.30815969\n",
      "Iteration 457, loss = 0.30807115\n",
      "Iteration 458, loss = 0.30800458\n",
      "Iteration 459, loss = 0.30789562\n",
      "Iteration 460, loss = 0.30780450\n",
      "Iteration 461, loss = 0.30772132\n",
      "Iteration 462, loss = 0.30764126\n",
      "Iteration 463, loss = 0.30754474\n",
      "Iteration 464, loss = 0.30747640\n",
      "Iteration 465, loss = 0.30737666\n",
      "Iteration 466, loss = 0.30730094\n",
      "Iteration 467, loss = 0.30720593\n",
      "Iteration 468, loss = 0.30711382\n",
      "Iteration 469, loss = 0.30702782\n",
      "Iteration 470, loss = 0.30693799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73361591\n",
      "Iteration 2, loss = 0.73136142\n",
      "Iteration 3, loss = 0.72816600\n",
      "Iteration 4, loss = 0.72403940\n",
      "Iteration 5, loss = 0.71942384\n",
      "Iteration 6, loss = 0.71441913\n",
      "Iteration 7, loss = 0.70926878\n",
      "Iteration 8, loss = 0.70405746\n",
      "Iteration 9, loss = 0.69870749\n",
      "Iteration 10, loss = 0.69350518\n",
      "Iteration 11, loss = 0.68819859\n",
      "Iteration 12, loss = 0.68303688\n",
      "Iteration 13, loss = 0.67797317\n",
      "Iteration 14, loss = 0.67275586\n",
      "Iteration 15, loss = 0.66791133\n",
      "Iteration 16, loss = 0.66300208\n",
      "Iteration 17, loss = 0.65810774\n",
      "Iteration 18, loss = 0.65349181\n",
      "Iteration 19, loss = 0.64884149\n",
      "Iteration 20, loss = 0.64425010\n",
      "Iteration 21, loss = 0.63978054\n",
      "Iteration 22, loss = 0.63535654\n",
      "Iteration 23, loss = 0.63100492\n",
      "Iteration 24, loss = 0.62677819\n",
      "Iteration 25, loss = 0.62251915\n",
      "Iteration 26, loss = 0.61829688\n",
      "Iteration 27, loss = 0.61433065\n",
      "Iteration 28, loss = 0.61012163\n",
      "Iteration 29, loss = 0.60602892\n",
      "Iteration 30, loss = 0.60204001\n",
      "Iteration 31, loss = 0.59813129\n",
      "Iteration 32, loss = 0.59413196\n",
      "Iteration 33, loss = 0.59037343\n",
      "Iteration 34, loss = 0.58653732\n",
      "Iteration 35, loss = 0.58284727\n",
      "Iteration 36, loss = 0.57911383\n",
      "Iteration 37, loss = 0.57541109\n",
      "Iteration 38, loss = 0.57172555\n",
      "Iteration 39, loss = 0.56819598\n",
      "Iteration 40, loss = 0.56460092\n",
      "Iteration 41, loss = 0.56110328\n",
      "Iteration 42, loss = 0.55751842\n",
      "Iteration 43, loss = 0.55410500\n",
      "Iteration 44, loss = 0.55070032\n",
      "Iteration 45, loss = 0.54732397\n",
      "Iteration 46, loss = 0.54404157\n",
      "Iteration 47, loss = 0.54076387\n",
      "Iteration 48, loss = 0.53754370\n",
      "Iteration 49, loss = 0.53427780\n",
      "Iteration 50, loss = 0.53122538\n",
      "Iteration 51, loss = 0.52815376\n",
      "Iteration 52, loss = 0.52497070\n",
      "Iteration 53, loss = 0.52192963\n",
      "Iteration 54, loss = 0.51902022\n",
      "Iteration 55, loss = 0.51604568\n",
      "Iteration 56, loss = 0.51312697\n",
      "Iteration 57, loss = 0.51026405\n",
      "Iteration 58, loss = 0.50744139\n",
      "Iteration 59, loss = 0.50472038\n",
      "Iteration 60, loss = 0.50187820\n",
      "Iteration 61, loss = 0.49920144\n",
      "Iteration 62, loss = 0.49655369\n",
      "Iteration 63, loss = 0.49388847\n",
      "Iteration 64, loss = 0.49128204\n",
      "Iteration 65, loss = 0.48879921\n",
      "Iteration 66, loss = 0.48629685\n",
      "Iteration 67, loss = 0.48378992\n",
      "Iteration 68, loss = 0.48137080\n",
      "Iteration 69, loss = 0.47895582\n",
      "Iteration 70, loss = 0.47664770\n",
      "Iteration 71, loss = 0.47435491\n",
      "Iteration 72, loss = 0.47208063\n",
      "Iteration 73, loss = 0.46982679\n",
      "Iteration 74, loss = 0.46772054\n",
      "Iteration 75, loss = 0.46558346\n",
      "Iteration 76, loss = 0.46343664\n",
      "Iteration 77, loss = 0.46136230\n",
      "Iteration 78, loss = 0.45932884\n",
      "Iteration 79, loss = 0.45739686\n",
      "Iteration 80, loss = 0.45543606\n",
      "Iteration 81, loss = 0.45351184\n",
      "Iteration 82, loss = 0.45160780\n",
      "Iteration 83, loss = 0.44979160\n",
      "Iteration 84, loss = 0.44798643\n",
      "Iteration 85, loss = 0.44626888\n",
      "Iteration 86, loss = 0.44451627\n",
      "Iteration 87, loss = 0.44281615\n",
      "Iteration 88, loss = 0.44119580\n",
      "Iteration 89, loss = 0.43956221\n",
      "Iteration 90, loss = 0.43798377\n",
      "Iteration 91, loss = 0.43638534\n",
      "Iteration 92, loss = 0.43485106\n",
      "Iteration 93, loss = 0.43336748\n",
      "Iteration 94, loss = 0.43190276\n",
      "Iteration 95, loss = 0.43047529\n",
      "Iteration 96, loss = 0.42908954\n",
      "Iteration 97, loss = 0.42773716\n",
      "Iteration 98, loss = 0.42639232\n",
      "Iteration 99, loss = 0.42499218\n",
      "Iteration 100, loss = 0.42376900\n",
      "Iteration 101, loss = 0.42249407\n",
      "Iteration 102, loss = 0.42123692\n",
      "Iteration 103, loss = 0.42005862\n",
      "Iteration 104, loss = 0.41888257\n",
      "Iteration 105, loss = 0.41772112\n",
      "Iteration 106, loss = 0.41657345\n",
      "Iteration 107, loss = 0.41543255\n",
      "Iteration 108, loss = 0.41433824\n",
      "Iteration 109, loss = 0.41328741\n",
      "Iteration 110, loss = 0.41222180\n",
      "Iteration 111, loss = 0.41113736\n",
      "Iteration 112, loss = 0.41021384\n",
      "Iteration 113, loss = 0.40916933\n",
      "Iteration 114, loss = 0.40823725\n",
      "Iteration 115, loss = 0.40724186\n",
      "Iteration 116, loss = 0.40634804\n",
      "Iteration 117, loss = 0.40544535\n",
      "Iteration 118, loss = 0.40450570\n",
      "Iteration 119, loss = 0.40361403\n",
      "Iteration 120, loss = 0.40274693\n",
      "Iteration 121, loss = 0.40189059\n",
      "Iteration 122, loss = 0.40105795\n",
      "Iteration 123, loss = 0.40023584\n",
      "Iteration 124, loss = 0.39944209\n",
      "Iteration 125, loss = 0.39866204\n",
      "Iteration 126, loss = 0.39785968\n",
      "Iteration 127, loss = 0.39710798\n",
      "Iteration 128, loss = 0.39635731\n",
      "Iteration 129, loss = 0.39558565\n",
      "Iteration 130, loss = 0.39492313\n",
      "Iteration 131, loss = 0.39416923\n",
      "Iteration 132, loss = 0.39345538\n",
      "Iteration 133, loss = 0.39278956\n",
      "Iteration 134, loss = 0.39213258\n",
      "Iteration 135, loss = 0.39146338\n",
      "Iteration 136, loss = 0.39078999\n",
      "Iteration 137, loss = 0.39013430\n",
      "Iteration 138, loss = 0.38949769\n",
      "Iteration 139, loss = 0.38887241\n",
      "Iteration 140, loss = 0.38829337\n",
      "Iteration 141, loss = 0.38770229\n",
      "Iteration 142, loss = 0.38706613\n",
      "Iteration 143, loss = 0.38644599\n",
      "Iteration 144, loss = 0.38593332\n",
      "Iteration 145, loss = 0.38536081\n",
      "Iteration 146, loss = 0.38479288\n",
      "Iteration 147, loss = 0.38421338\n",
      "Iteration 148, loss = 0.38368032\n",
      "Iteration 149, loss = 0.38315861\n",
      "Iteration 150, loss = 0.38260903\n",
      "Iteration 151, loss = 0.38211207\n",
      "Iteration 152, loss = 0.38159600\n",
      "Iteration 153, loss = 0.38108321\n",
      "Iteration 154, loss = 0.38057509\n",
      "Iteration 155, loss = 0.38010583\n",
      "Iteration 156, loss = 0.37964422\n",
      "Iteration 157, loss = 0.37912003\n",
      "Iteration 158, loss = 0.37865881\n",
      "Iteration 159, loss = 0.37820281\n",
      "Iteration 160, loss = 0.37772553\n",
      "Iteration 161, loss = 0.37730151\n",
      "Iteration 162, loss = 0.37683754\n",
      "Iteration 163, loss = 0.37639639\n",
      "Iteration 164, loss = 0.37598643\n",
      "Iteration 165, loss = 0.37554592\n",
      "Iteration 166, loss = 0.37511226\n",
      "Iteration 167, loss = 0.37468929\n",
      "Iteration 168, loss = 0.37427520\n",
      "Iteration 169, loss = 0.37387399\n",
      "Iteration 170, loss = 0.37347522\n",
      "Iteration 171, loss = 0.37307674\n",
      "Iteration 172, loss = 0.37268506\n",
      "Iteration 173, loss = 0.37229194\n",
      "Iteration 174, loss = 0.37187296\n",
      "Iteration 175, loss = 0.37150373\n",
      "Iteration 176, loss = 0.37112940\n",
      "Iteration 177, loss = 0.37076099\n",
      "Iteration 178, loss = 0.37039351\n",
      "Iteration 179, loss = 0.36999952\n",
      "Iteration 180, loss = 0.36967945\n",
      "Iteration 181, loss = 0.36928645\n",
      "Iteration 182, loss = 0.36895541\n",
      "Iteration 183, loss = 0.36859780\n",
      "Iteration 1318, loss = 0.19014526\n",
      "Iteration 1319, loss = 0.18990888\n",
      "Iteration 1320, loss = 0.18973045\n",
      "Iteration 1321, loss = 0.18957849\n",
      "Iteration 1322, loss = 0.18945248\n",
      "Iteration 1323, loss = 0.18930171\n",
      "Iteration 1324, loss = 0.18907367\n",
      "Iteration 1325, loss = 0.18895494\n",
      "Iteration 1326, loss = 0.18874653\n",
      "Iteration 1327, loss = 0.18872851\n",
      "Iteration 1328, loss = 0.18849173\n",
      "Iteration 1329, loss = 0.18829666\n",
      "Iteration 1330, loss = 0.18813177\n",
      "Iteration 1331, loss = 0.18804445\n",
      "Iteration 1332, loss = 0.18782431\n",
      "Iteration 1333, loss = 0.18763631\n",
      "Iteration 1334, loss = 0.18749527\n",
      "Iteration 1335, loss = 0.18755598\n",
      "Iteration 1336, loss = 0.18743250\n",
      "Iteration 1337, loss = 0.18701776\n",
      "Iteration 1338, loss = 0.18688516\n",
      "Iteration 1339, loss = 0.18678121\n",
      "Iteration 1340, loss = 0.18651734\n",
      "Iteration 1341, loss = 0.18649609\n",
      "Iteration 1342, loss = 0.18634749\n",
      "Iteration 1343, loss = 0.18609325\n",
      "Iteration 1344, loss = 0.18594409\n",
      "Iteration 1345, loss = 0.18574448\n",
      "Iteration 1346, loss = 0.18561622\n",
      "Iteration 1347, loss = 0.18542515\n",
      "Iteration 1348, loss = 0.18526157\n",
      "Iteration 1349, loss = 0.18520628\n",
      "Iteration 1350, loss = 0.18499361\n",
      "Iteration 1351, loss = 0.18481767\n",
      "Iteration 1352, loss = 0.18482100\n",
      "Iteration 1353, loss = 0.18475162\n",
      "Iteration 1354, loss = 0.18439356\n",
      "Iteration 1355, loss = 0.18421451\n",
      "Iteration 1356, loss = 0.18404516\n",
      "Iteration 1357, loss = 0.18385766\n",
      "Iteration 1358, loss = 0.18401288\n",
      "Iteration 1359, loss = 0.18359495\n",
      "Iteration 1360, loss = 0.18345810\n",
      "Iteration 1361, loss = 0.18328726\n",
      "Iteration 1362, loss = 0.18315717\n",
      "Iteration 1363, loss = 0.18297497\n",
      "Iteration 1364, loss = 0.18292015\n",
      "Iteration 1365, loss = 0.18265163\n",
      "Iteration 1366, loss = 0.18253554\n",
      "Iteration 1367, loss = 0.18246631\n",
      "Iteration 1368, loss = 0.18224446\n",
      "Iteration 1369, loss = 0.18201075\n",
      "Iteration 1370, loss = 0.18188652\n",
      "Iteration 1371, loss = 0.18178360\n",
      "Iteration 1372, loss = 0.18153760\n",
      "Iteration 1373, loss = 0.18161398\n",
      "Iteration 1374, loss = 0.18128931\n",
      "Iteration 1375, loss = 0.18105438\n",
      "Iteration 1376, loss = 0.18091398\n",
      "Iteration 1377, loss = 0.18080384\n",
      "Iteration 1378, loss = 0.18075859\n",
      "Iteration 1379, loss = 0.18054752\n",
      "Iteration 1380, loss = 0.18041091\n",
      "Iteration 1381, loss = 0.18033771\n",
      "Iteration 1382, loss = 0.18016119\n",
      "Iteration 1383, loss = 0.18002102\n",
      "Iteration 1384, loss = 0.17974270\n",
      "Iteration 1385, loss = 0.17951551\n",
      "Iteration 1386, loss = 0.17955702\n",
      "Iteration 1387, loss = 0.17928079\n",
      "Iteration 1388, loss = 0.17918696\n",
      "Iteration 1389, loss = 0.17890096\n",
      "Iteration 1390, loss = 0.17879649\n",
      "Iteration 1391, loss = 0.17865146\n",
      "Iteration 1392, loss = 0.17848707\n",
      "Iteration 1393, loss = 0.17829345\n",
      "Iteration 1394, loss = 0.17832248\n",
      "Iteration 1395, loss = 0.17800885\n",
      "Iteration 1396, loss = 0.17786084\n",
      "Iteration 1397, loss = 0.17786696\n",
      "Iteration 1398, loss = 0.17772580\n",
      "Iteration 1399, loss = 0.17766194\n",
      "Iteration 1400, loss = 0.17724951\n",
      "Iteration 1401, loss = 0.17709724\n",
      "Iteration 1402, loss = 0.17694475\n",
      "Iteration 1403, loss = 0.17676812\n",
      "Iteration 1404, loss = 0.17666700\n",
      "Iteration 1405, loss = 0.17652294\n",
      "Iteration 1406, loss = 0.17637921\n",
      "Iteration 1407, loss = 0.17622543\n",
      "Iteration 1408, loss = 0.17603596\n",
      "Iteration 1409, loss = 0.17585424\n",
      "Iteration 1410, loss = 0.17568939\n",
      "Iteration 1411, loss = 0.17578423\n",
      "Iteration 1412, loss = 0.17541693\n",
      "Iteration 1413, loss = 0.17538048\n",
      "Iteration 1414, loss = 0.17511847\n",
      "Iteration 1415, loss = 0.17493623\n",
      "Iteration 1416, loss = 0.17511044\n",
      "Iteration 1417, loss = 0.17468284\n",
      "Iteration 1418, loss = 0.17450925\n",
      "Iteration 1419, loss = 0.17456333\n",
      "Iteration 1420, loss = 0.17432296\n",
      "Iteration 1421, loss = 0.17407413\n",
      "Iteration 1422, loss = 0.17394033\n",
      "Iteration 1423, loss = 0.17380150\n",
      "Iteration 1424, loss = 0.17359224\n",
      "Iteration 1425, loss = 0.17348172\n",
      "Iteration 1426, loss = 0.17328705\n",
      "Iteration 1427, loss = 0.17314049\n",
      "Iteration 1428, loss = 0.17297761\n",
      "Iteration 1429, loss = 0.17291348\n",
      "Iteration 1430, loss = 0.17272544\n",
      "Iteration 1431, loss = 0.17256950\n",
      "Iteration 1432, loss = 0.17247803\n",
      "Iteration 1433, loss = 0.17224121\n",
      "Iteration 1434, loss = 0.17225964\n",
      "Iteration 1435, loss = 0.17191498\n",
      "Iteration 1436, loss = 0.17178668\n",
      "Iteration 1437, loss = 0.17171542\n",
      "Iteration 1438, loss = 0.17148765\n",
      "Iteration 1439, loss = 0.17133651\n",
      "Iteration 1440, loss = 0.17133446\n",
      "Iteration 1441, loss = 0.17110660\n",
      "Iteration 1442, loss = 0.17092815\n",
      "Iteration 1443, loss = 0.17106070\n",
      "Iteration 1444, loss = 0.17056549\n",
      "Iteration 1445, loss = 0.17051884\n",
      "Iteration 1446, loss = 0.17049154\n",
      "Iteration 1447, loss = 0.17037179\n",
      "Iteration 1448, loss = 0.17010229\n",
      "Iteration 1449, loss = 0.16990498\n",
      "Iteration 1450, loss = 0.16975774\n",
      "Iteration 1451, loss = 0.16957781\n",
      "Iteration 1452, loss = 0.16939594\n",
      "Iteration 1453, loss = 0.16950070\n",
      "Iteration 1454, loss = 0.16925517\n",
      "Iteration 1455, loss = 0.16914039\n",
      "Iteration 1456, loss = 0.16887962\n",
      "Iteration 1457, loss = 0.16878070\n",
      "Iteration 1458, loss = 0.16851198\n",
      "Iteration 1459, loss = 0.16840204\n",
      "Iteration 1460, loss = 0.16828089\n",
      "Iteration 1461, loss = 0.16842508\n",
      "Iteration 1462, loss = 0.16796771\n",
      "Iteration 1463, loss = 0.16797530\n",
      "Iteration 1464, loss = 0.16786406\n",
      "Iteration 1465, loss = 0.16752127\n",
      "Iteration 1466, loss = 0.16730315\n",
      "Iteration 1467, loss = 0.16732267\n",
      "Iteration 1468, loss = 0.16713641\n",
      "Iteration 1469, loss = 0.16694381\n",
      "Iteration 1470, loss = 0.16675558\n",
      "Iteration 1471, loss = 0.16665733\n",
      "Iteration 1472, loss = 0.16645581\n",
      "Iteration 1473, loss = 0.16640218\n",
      "Iteration 1474, loss = 0.16623619\n",
      "Iteration 1475, loss = 0.16602661\n",
      "Iteration 1476, loss = 0.16594438\n",
      "Iteration 1477, loss = 0.16573119\n",
      "Iteration 1478, loss = 0.16561445\n",
      "Iteration 1479, loss = 0.16545776\n",
      "Iteration 1480, loss = 0.16534037\n",
      "Iteration 1481, loss = 0.16509382\n",
      "Iteration 1482, loss = 0.16502606\n",
      "Iteration 1483, loss = 0.16490513\n",
      "Iteration 1484, loss = 0.16471052\n",
      "Iteration 1485, loss = 0.16459097\n",
      "Iteration 1486, loss = 0.16439599\n",
      "Iteration 1487, loss = 0.16437448\n",
      "Iteration 1488, loss = 0.16408686\n",
      "Iteration 1489, loss = 0.16397509\n",
      "Iteration 1490, loss = 0.16395129\n",
      "Iteration 1491, loss = 0.16365073\n",
      "Iteration 1492, loss = 0.16355627\n",
      "Iteration 1493, loss = 0.16335039\n",
      "Iteration 1494, loss = 0.16325572\n",
      "Iteration 1495, loss = 0.16317284\n",
      "Iteration 1496, loss = 0.16302387\n",
      "Iteration 1497, loss = 0.16288288\n",
      "Iteration 1498, loss = 0.16270841\n",
      "Iteration 1499, loss = 0.16242499\n",
      "Iteration 1500, loss = 0.16229375\n",
      "Iteration 1501, loss = 0.16217350\n",
      "Iteration 1502, loss = 0.16212811\n",
      "Iteration 1503, loss = 0.16191696\n",
      "Iteration 1504, loss = 0.16202571\n",
      "Iteration 1505, loss = 0.16179208\n",
      "Iteration 1506, loss = 0.16141981\n",
      "Iteration 1507, loss = 0.16149963\n",
      "Iteration 1508, loss = 0.16114393\n",
      "Iteration 1509, loss = 0.16094974\n",
      "Iteration 1510, loss = 0.16083903\n",
      "Iteration 1511, loss = 0.16076792\n",
      "Iteration 1512, loss = 0.16079531\n",
      "Iteration 1513, loss = 0.16044646\n",
      "Iteration 1514, loss = 0.16065088\n",
      "Iteration 1515, loss = 0.16010364\n",
      "Iteration 1516, loss = 0.15994278\n",
      "Iteration 1517, loss = 0.15980994\n",
      "Iteration 1518, loss = 0.15984525\n",
      "Iteration 1519, loss = 0.15958375\n",
      "Iteration 1520, loss = 0.15944857\n",
      "Iteration 1521, loss = 0.15938002\n",
      "Iteration 1522, loss = 0.15915400\n",
      "Iteration 1523, loss = 0.15913252\n",
      "Iteration 1524, loss = 0.15889826\n",
      "Iteration 1525, loss = 0.15864422\n",
      "Iteration 1526, loss = 0.15846961\n",
      "Iteration 1527, loss = 0.15835946\n",
      "Iteration 1528, loss = 0.15831177\n",
      "Iteration 1529, loss = 0.15830297\n",
      "Iteration 1530, loss = 0.15797032\n",
      "Iteration 1531, loss = 0.15778577\n",
      "Iteration 1532, loss = 0.15794377\n",
      "Iteration 1533, loss = 0.15754180\n",
      "Iteration 1534, loss = 0.15737372\n",
      "Iteration 1535, loss = 0.15730245\n",
      "Iteration 1536, loss = 0.15707849\n",
      "Iteration 1537, loss = 0.15690937\n",
      "Iteration 1538, loss = 0.15679234\n",
      "Iteration 1539, loss = 0.15664285\n",
      "Iteration 1540, loss = 0.15648953\n",
      "Iteration 1541, loss = 0.15646830\n",
      "Iteration 1542, loss = 0.15627772\n",
      "Iteration 1543, loss = 0.15612839\n",
      "Iteration 1544, loss = 0.15597198\n",
      "Iteration 1545, loss = 0.15576335\n",
      "Iteration 1546, loss = 0.15572028\n",
      "Iteration 1547, loss = 0.15546234\n",
      "Iteration 1548, loss = 0.15540387\n",
      "Iteration 1549, loss = 0.15526932\n",
      "Iteration 1550, loss = 0.15515655\n",
      "Iteration 1551, loss = 0.15491566\n",
      "Iteration 1552, loss = 0.15505926\n",
      "Iteration 1553, loss = 0.15483989\n",
      "Iteration 1554, loss = 0.15446599\n",
      "Iteration 1555, loss = 0.15447890\n",
      "Iteration 1556, loss = 0.15423812\n",
      "Iteration 1557, loss = 0.15411395\n",
      "Iteration 1558, loss = 0.15408746\n",
      "Iteration 1559, loss = 0.15394503\n",
      "Iteration 1560, loss = 0.15384646\n",
      "Iteration 1561, loss = 0.15345418\n",
      "Iteration 1562, loss = 0.15337614\n",
      "Iteration 1563, loss = 0.15322095\n",
      "Iteration 1564, loss = 0.15315183\n",
      "Iteration 1565, loss = 0.15292709\n",
      "Iteration 1566, loss = 0.15273704\n",
      "Iteration 1567, loss = 0.15278916\n",
      "Iteration 1568, loss = 0.15260654\n",
      "Iteration 1569, loss = 0.15242193\n",
      "Iteration 1570, loss = 0.15235681\n",
      "Iteration 1571, loss = 0.15212704\n",
      "Iteration 1572, loss = 0.15215717\n",
      "Iteration 1573, loss = 0.15174036\n",
      "Iteration 1574, loss = 0.15185611\n",
      "Iteration 1575, loss = 0.15183977\n",
      "Iteration 1576, loss = 0.15138484\n",
      "Iteration 1577, loss = 0.15118153\n",
      "Iteration 1578, loss = 0.15111247\n",
      "Iteration 1579, loss = 0.15097406\n",
      "Iteration 1580, loss = 0.15075718\n",
      "Iteration 1581, loss = 0.15074678\n",
      "Iteration 1582, loss = 0.15054975\n",
      "Iteration 1583, loss = 0.15059345\n",
      "Iteration 1584, loss = 0.15017303\n",
      "Iteration 1585, loss = 0.15001967\n",
      "Iteration 1586, loss = 0.14992630\n",
      "Iteration 1587, loss = 0.14982535\n",
      "Iteration 1588, loss = 0.14978557\n",
      "Iteration 1589, loss = 0.14950518\n",
      "Iteration 1590, loss = 0.14930027\n",
      "Iteration 1591, loss = 0.14920654\n",
      "Iteration 1592, loss = 0.14907649\n",
      "Iteration 1593, loss = 0.14908945\n",
      "Iteration 1594, loss = 0.14874857\n",
      "Iteration 1595, loss = 0.14858514\n",
      "Iteration 1596, loss = 0.14870625\n",
      "Iteration 1597, loss = 0.14833477\n",
      "Iteration 1598, loss = 0.14840007\n",
      "Iteration 1599, loss = 0.14818071\n",
      "Iteration 1600, loss = 0.14787057\n",
      "Iteration 1601, loss = 0.14795546\n",
      "Iteration 1602, loss = 0.14786766\n",
      "Iteration 1603, loss = 0.14764066\n",
      "Iteration 1604, loss = 0.14734050\n",
      "Iteration 1605, loss = 0.14723227\n",
      "Iteration 1606, loss = 0.14719098\n",
      "Iteration 1607, loss = 0.14703870\n",
      "Iteration 1608, loss = 0.14685330\n",
      "Iteration 1609, loss = 0.14663760\n",
      "Iteration 1610, loss = 0.14648005\n",
      "Iteration 1611, loss = 0.14640827\n",
      "Iteration 1612, loss = 0.14624465\n",
      "Iteration 1613, loss = 0.14615949\n",
      "Iteration 1614, loss = 0.14591038\n",
      "Iteration 1615, loss = 0.14587629\n",
      "Iteration 1616, loss = 0.14567918\n",
      "Iteration 1617, loss = 0.14557016\n",
      "Iteration 1618, loss = 0.14554605\n",
      "Iteration 1619, loss = 0.14524041\n",
      "Iteration 1620, loss = 0.14512307\n",
      "Iteration 1621, loss = 0.14501716\n",
      "Iteration 1622, loss = 0.14483048\n",
      "Iteration 1623, loss = 0.14484481\n",
      "Iteration 1624, loss = 0.14459926\n",
      "Iteration 1625, loss = 0.14446969\n",
      "Iteration 1626, loss = 0.14441353\n",
      "Iteration 1627, loss = 0.14417045\n",
      "Iteration 1628, loss = 0.14398772\n",
      "Iteration 1629, loss = 0.14387453\n",
      "Iteration 1630, loss = 0.14379552\n",
      "Iteration 1631, loss = 0.14362439\n",
      "Iteration 1632, loss = 0.14343850\n",
      "Iteration 1633, loss = 0.14352080\n",
      "Iteration 1634, loss = 0.14322209\n",
      "Iteration 1635, loss = 0.14300903\n",
      "Iteration 1636, loss = 0.14298727\n",
      "Iteration 1637, loss = 0.14284375\n",
      "Iteration 1638, loss = 0.14267000\n",
      "Iteration 1639, loss = 0.14248442\n",
      "Iteration 1640, loss = 0.14233659\n",
      "Iteration 1641, loss = 0.14244931\n",
      "Iteration 1642, loss = 0.14203887\n",
      "Iteration 1643, loss = 0.14224977\n",
      "Iteration 1644, loss = 0.14174418\n",
      "Iteration 1645, loss = 0.14165299\n",
      "Iteration 1646, loss = 0.14146527\n",
      "Iteration 1647, loss = 0.14145313\n",
      "Iteration 1648, loss = 0.14138022\n",
      "Iteration 1649, loss = 0.14108822\n",
      "Iteration 1650, loss = 0.14104611\n",
      "Iteration 1651, loss = 0.14081250\n",
      "Iteration 1652, loss = 0.14077424\n",
      "Iteration 1653, loss = 0.14061983\n",
      "Iteration 1654, loss = 0.14042480\n",
      "Iteration 1655, loss = 0.14030205\n",
      "Iteration 1656, loss = 0.14009713\n",
      "Iteration 1657, loss = 0.14008279\n",
      "Iteration 1658, loss = 0.13982942\n",
      "Iteration 1659, loss = 0.13972302\n",
      "Iteration 1660, loss = 0.13960366\n",
      "Iteration 1661, loss = 0.13950179\n",
      "Iteration 1662, loss = 0.13940628\n",
      "Iteration 1663, loss = 0.13936722\n",
      "Iteration 1664, loss = 0.13919977\n",
      "Iteration 1665, loss = 0.13893855\n",
      "Iteration 1666, loss = 0.13898919\n",
      "Iteration 1667, loss = 0.13871553\n",
      "Iteration 1668, loss = 0.13883196\n",
      "Iteration 1669, loss = 0.13847005\n",
      "Iteration 1670, loss = 0.13838611\n",
      "Iteration 1671, loss = 0.13812925\n",
      "Iteration 1672, loss = 0.13807660\n",
      "Iteration 1673, loss = 0.13791451\n",
      "Iteration 1674, loss = 0.13774892\n",
      "Iteration 1675, loss = 0.13760518\n",
      "Iteration 1676, loss = 0.13757855\n",
      "Iteration 1677, loss = 0.13743736\n",
      "Iteration 1678, loss = 0.13719997\n",
      "Iteration 1679, loss = 0.13705805\n",
      "Iteration 1680, loss = 0.13686775\n",
      "Iteration 1681, loss = 0.13706166\n",
      "Iteration 1682, loss = 0.13667285\n",
      "Iteration 1683, loss = 0.13666821\n",
      "Iteration 1684, loss = 0.13648301\n",
      "Iteration 1685, loss = 0.13626954\n",
      "Iteration 1686, loss = 0.13624811\n",
      "Iteration 1687, loss = 0.13599123\n",
      "Iteration 1688, loss = 0.13584295\n",
      "Iteration 1689, loss = 0.13575170\n",
      "Iteration 1690, loss = 0.13584481\n",
      "Iteration 1691, loss = 0.13554994\n",
      "Iteration 1692, loss = 0.13547840\n",
      "Iteration 1693, loss = 0.13513495\n",
      "Iteration 1694, loss = 0.13505517\n",
      "Iteration 1695, loss = 0.13509437\n",
      "Iteration 1696, loss = 0.13485622\n",
      "Iteration 1697, loss = 0.13467198\n",
      "Iteration 1698, loss = 0.13444167\n",
      "Iteration 1699, loss = 0.13431008\n",
      "Iteration 1700, loss = 0.13421053\n",
      "Iteration 1701, loss = 0.13401775\n",
      "Iteration 1702, loss = 0.13402272\n",
      "Iteration 1703, loss = 0.13376338\n",
      "Iteration 1704, loss = 0.13369865\n",
      "Iteration 1705, loss = 0.13369073\n",
      "Iteration 1706, loss = 0.13342464\n",
      "Iteration 1707, loss = 0.13328723\n",
      "Iteration 1708, loss = 0.13328466\n",
      "Iteration 1709, loss = 0.13310447\n",
      "Iteration 1710, loss = 0.13300676\n",
      "Iteration 1711, loss = 0.13268173\n",
      "Iteration 1712, loss = 0.13263918\n",
      "Iteration 1713, loss = 0.13242597\n",
      "Iteration 1714, loss = 0.13230718\n",
      "Iteration 1715, loss = 0.13217131\n",
      "Iteration 1716, loss = 0.13197303\n",
      "Iteration 1717, loss = 0.13189046\n",
      "Iteration 1718, loss = 0.13167997\n",
      "Iteration 1719, loss = 0.13162185\n",
      "Iteration 1720, loss = 0.13172891\n",
      "Iteration 1721, loss = 0.13152880\n",
      "Iteration 1722, loss = 0.13123136\n",
      "Iteration 1723, loss = 0.13101841\n",
      "Iteration 1724, loss = 0.13093056\n",
      "Iteration 1725, loss = 0.13083761\n",
      "Iteration 1726, loss = 0.13064886\n",
      "Iteration 1727, loss = 0.13059435\n",
      "Iteration 1728, loss = 0.13041493\n",
      "Iteration 1729, loss = 0.13039895\n",
      "Iteration 1730, loss = 0.13021872\n",
      "Iteration 1731, loss = 0.12992500\n",
      "Iteration 1732, loss = 0.12981803\n",
      "Iteration 1733, loss = 0.12966250\n",
      "Iteration 1734, loss = 0.12960872\n",
      "Iteration 1735, loss = 0.12937648\n",
      "Iteration 1736, loss = 0.12936183\n",
      "Iteration 1737, loss = 0.12912672\n",
      "Iteration 1738, loss = 0.12898557\n",
      "Iteration 1739, loss = 0.12910028\n",
      "Iteration 1740, loss = 0.12869481\n",
      "Iteration 1741, loss = 0.12870909\n",
      "Iteration 1742, loss = 0.12840979\n",
      "Iteration 1743, loss = 0.12831418\n",
      "Iteration 1744, loss = 0.12830152\n",
      "Iteration 1745, loss = 0.12806740\n",
      "Iteration 1746, loss = 0.12802137\n",
      "Iteration 1747, loss = 0.12778237\n",
      "Iteration 1748, loss = 0.12780893\n",
      "Iteration 1749, loss = 0.12772826\n",
      "Iteration 1750, loss = 0.12750904\n",
      "Iteration 1751, loss = 0.12725172\n",
      "Iteration 1752, loss = 0.12719446\n",
      "Iteration 1753, loss = 0.12705331\n",
      "Iteration 1754, loss = 0.12685040\n",
      "Iteration 1755, loss = 0.12676320\n",
      "Iteration 1756, loss = 0.12661994\n",
      "Iteration 1757, loss = 0.12665568\n",
      "Iteration 1758, loss = 0.12636679\n",
      "Iteration 1759, loss = 0.12624750\n",
      "Iteration 1760, loss = 0.12626335\n",
      "Iteration 1761, loss = 0.12601309\n",
      "Iteration 1762, loss = 0.12582686\n",
      "Iteration 1763, loss = 0.12575531\n",
      "Iteration 1764, loss = 0.12568153\n",
      "Iteration 1765, loss = 0.12547246\n",
      "Iteration 1766, loss = 0.12536651\n",
      "Iteration 1767, loss = 0.12525268\n",
      "Iteration 1768, loss = 0.12517997\n",
      "Iteration 1769, loss = 0.12526646\n",
      "Iteration 1770, loss = 0.12486278\n",
      "Iteration 1771, loss = 0.12485672\n",
      "Iteration 1772, loss = 0.12482870\n",
      "Iteration 1773, loss = 0.12441032\n",
      "Iteration 1774, loss = 0.12439071\n",
      "Iteration 1775, loss = 0.12428796\n",
      "Iteration 1776, loss = 0.12418701\n",
      "Iteration 1777, loss = 0.12411733\n",
      "Iteration 1778, loss = 0.12417783\n",
      "Iteration 1779, loss = 0.12387638\n",
      "Iteration 1780, loss = 0.12361990\n",
      "Iteration 1781, loss = 0.12341118\n",
      "Iteration 1782, loss = 0.12338557\n",
      "Iteration 1783, loss = 0.12325262\n",
      "Iteration 1784, loss = 0.12304136\n",
      "Iteration 1785, loss = 0.12292776\n",
      "Iteration 1786, loss = 0.12282897\n",
      "Iteration 1787, loss = 0.12278798\n",
      "Iteration 1788, loss = 0.12258613\n",
      "Iteration 1789, loss = 0.12242981\n",
      "Iteration 1790, loss = 0.12229937\n",
      "Iteration 1791, loss = 0.12215955\n",
      "Iteration 1792, loss = 0.12199381\n",
      "Iteration 1793, loss = 0.12192601\n",
      "Iteration 1794, loss = 0.12189339\n",
      "Iteration 1795, loss = 0.12162102\n",
      "Iteration 1796, loss = 0.12153302\n",
      "Iteration 1797, loss = 0.12165261\n",
      "Iteration 79, loss = 0.48470015\n",
      "Iteration 80, loss = 0.48199151\n",
      "Iteration 81, loss = 0.47909531\n",
      "Iteration 82, loss = 0.47626804\n",
      "Iteration 83, loss = 0.47341856\n",
      "Iteration 84, loss = 0.47078050\n",
      "Iteration 85, loss = 0.46812993\n",
      "Iteration 86, loss = 0.46544681\n",
      "Iteration 87, loss = 0.46272102\n",
      "Iteration 88, loss = 0.46014518\n",
      "Iteration 89, loss = 0.45763681\n",
      "Iteration 90, loss = 0.45510667\n",
      "Iteration 91, loss = 0.45261863\n",
      "Iteration 92, loss = 0.45029946\n",
      "Iteration 93, loss = 0.44785120\n",
      "Iteration 94, loss = 0.44553370\n",
      "Iteration 95, loss = 0.44324198\n",
      "Iteration 96, loss = 0.44098274\n",
      "Iteration 97, loss = 0.43887260\n",
      "Iteration 98, loss = 0.43675201\n",
      "Iteration 99, loss = 0.43460439\n",
      "Iteration 100, loss = 0.43252873\n",
      "Iteration 101, loss = 0.43054453\n",
      "Iteration 102, loss = 0.42853925\n",
      "Iteration 103, loss = 0.42661582\n",
      "Iteration 104, loss = 0.42476704\n",
      "Iteration 105, loss = 0.42299219\n",
      "Iteration 106, loss = 0.42117130\n",
      "Iteration 107, loss = 0.41935293\n",
      "Iteration 108, loss = 0.41765777\n",
      "Iteration 109, loss = 0.41598763\n",
      "Iteration 110, loss = 0.41432307\n",
      "Iteration 111, loss = 0.41279391\n",
      "Iteration 112, loss = 0.41120068\n",
      "Iteration 113, loss = 0.40975280\n",
      "Iteration 114, loss = 0.40826311\n",
      "Iteration 115, loss = 0.40680795\n",
      "Iteration 116, loss = 0.40543416\n",
      "Iteration 117, loss = 0.40407962\n",
      "Iteration 118, loss = 0.40271432\n",
      "Iteration 119, loss = 0.40147613\n",
      "Iteration 120, loss = 0.40021297\n",
      "Iteration 121, loss = 0.39897011\n",
      "Iteration 122, loss = 0.39779367\n",
      "Iteration 123, loss = 0.39661980\n",
      "Iteration 124, loss = 0.39548151\n",
      "Iteration 125, loss = 0.39439148\n",
      "Iteration 126, loss = 0.39332434\n",
      "Iteration 127, loss = 0.39224769\n",
      "Iteration 128, loss = 0.39123224\n",
      "Iteration 129, loss = 0.39025057\n",
      "Iteration 130, loss = 0.38930193\n",
      "Iteration 131, loss = 0.38832899\n",
      "Iteration 132, loss = 0.38739886\n",
      "Iteration 133, loss = 0.38649475\n",
      "Iteration 134, loss = 0.38568561\n",
      "Iteration 135, loss = 0.38474618\n",
      "Iteration 136, loss = 0.38392700\n",
      "Iteration 137, loss = 0.38309786\n",
      "Iteration 138, loss = 0.38229468\n",
      "Iteration 139, loss = 0.38151369\n",
      "Iteration 140, loss = 0.38075036\n",
      "Iteration 141, loss = 0.38002796\n",
      "Iteration 142, loss = 0.37929694\n",
      "Iteration 143, loss = 0.37857492\n",
      "Iteration 144, loss = 0.37787916\n",
      "Iteration 145, loss = 0.37722836\n",
      "Iteration 146, loss = 0.37656705\n",
      "Iteration 147, loss = 0.37588905\n",
      "Iteration 148, loss = 0.37529178\n",
      "Iteration 149, loss = 0.37467148\n",
      "Iteration 150, loss = 0.37406017\n",
      "Iteration 151, loss = 0.37345431\n",
      "Iteration 152, loss = 0.37282613\n",
      "Iteration 153, loss = 0.37228165\n",
      "Iteration 154, loss = 0.37177197\n",
      "Iteration 155, loss = 0.37118288\n",
      "Iteration 156, loss = 0.37062927\n",
      "Iteration 157, loss = 0.37014813\n",
      "Iteration 158, loss = 0.36957955\n",
      "Iteration 159, loss = 0.36907969\n",
      "Iteration 160, loss = 0.36857222\n",
      "Iteration 161, loss = 0.36809443\n",
      "Iteration 162, loss = 0.36758642\n",
      "Iteration 163, loss = 0.36714148\n",
      "Iteration 164, loss = 0.36666538\n",
      "Iteration 165, loss = 0.36623583\n",
      "Iteration 166, loss = 0.36573945\n",
      "Iteration 167, loss = 0.36531450\n",
      "Iteration 168, loss = 0.36484734\n",
      "Iteration 169, loss = 0.36442581\n",
      "Iteration 170, loss = 0.36399902\n",
      "Iteration 171, loss = 0.36358986\n",
      "Iteration 172, loss = 0.36316418\n",
      "Iteration 173, loss = 0.36279690\n",
      "Iteration 174, loss = 0.36237426\n",
      "Iteration 175, loss = 0.36195623\n",
      "Iteration 176, loss = 0.36155726\n",
      "Iteration 177, loss = 0.36117840\n",
      "Iteration 178, loss = 0.36076264\n",
      "Iteration 179, loss = 0.36041000\n",
      "Iteration 180, loss = 0.36005992\n",
      "Iteration 181, loss = 0.35968120\n",
      "Iteration 182, loss = 0.35931694\n",
      "Iteration 183, loss = 0.35895554\n",
      "Iteration 184, loss = 0.35860776\n",
      "Iteration 185, loss = 0.35825242\n",
      "Iteration 186, loss = 0.35789650\n",
      "Iteration 187, loss = 0.35759613\n",
      "Iteration 188, loss = 0.35720704\n",
      "Iteration 189, loss = 0.35694122\n",
      "Iteration 190, loss = 0.35662894\n",
      "Iteration 191, loss = 0.35627462\n",
      "Iteration 192, loss = 0.35596323\n",
      "Iteration 193, loss = 0.35562723\n",
      "Iteration 194, loss = 0.35530762\n",
      "Iteration 195, loss = 0.35500678\n",
      "Iteration 196, loss = 0.35467310\n",
      "Iteration 197, loss = 0.35438916\n",
      "Iteration 198, loss = 0.35409007\n",
      "Iteration 199, loss = 0.35377911\n",
      "Iteration 200, loss = 0.35349455\n",
      "Iteration 201, loss = 0.35318740\n",
      "Iteration 202, loss = 0.35289710\n",
      "Iteration 203, loss = 0.35261959\n",
      "Iteration 204, loss = 0.35234249\n",
      "Iteration 205, loss = 0.35207066\n",
      "Iteration 206, loss = 0.35180541\n",
      "Iteration 207, loss = 0.35152011\n",
      "Iteration 208, loss = 0.35124111\n",
      "Iteration 209, loss = 0.35099297\n",
      "Iteration 210, loss = 0.35068222\n",
      "Iteration 211, loss = 0.35048619\n",
      "Iteration 212, loss = 0.35019161\n",
      "Iteration 213, loss = 0.34991514\n",
      "Iteration 214, loss = 0.34968106\n",
      "Iteration 215, loss = 0.34940672\n",
      "Iteration 216, loss = 0.34915714\n",
      "Iteration 217, loss = 0.34892052\n",
      "Iteration 218, loss = 0.34866274\n",
      "Iteration 219, loss = 0.34840808\n",
      "Iteration 220, loss = 0.34818632\n",
      "Iteration 221, loss = 0.34794163\n",
      "Iteration 222, loss = 0.34768458\n",
      "Iteration 223, loss = 0.34745324\n",
      "Iteration 224, loss = 0.34726394\n",
      "Iteration 225, loss = 0.34697053\n",
      "Iteration 226, loss = 0.34674273\n",
      "Iteration 227, loss = 0.34654784\n",
      "Iteration 228, loss = 0.34631370\n",
      "Iteration 229, loss = 0.34607921\n",
      "Iteration 230, loss = 0.34587842\n",
      "Iteration 231, loss = 0.34564530\n",
      "Iteration 232, loss = 0.34541606\n",
      "Iteration 233, loss = 0.34518617\n",
      "Iteration 234, loss = 0.34496398\n",
      "Iteration 235, loss = 0.34474676\n",
      "Iteration 236, loss = 0.34454486\n",
      "Iteration 237, loss = 0.34430674\n",
      "Iteration 238, loss = 0.34412352\n",
      "Iteration 239, loss = 0.34388548\n",
      "Iteration 240, loss = 0.34370366\n",
      "Iteration 241, loss = 0.34345394\n",
      "Iteration 242, loss = 0.34324564\n",
      "Iteration 243, loss = 0.34309200\n",
      "Iteration 244, loss = 0.34286704\n",
      "Iteration 245, loss = 0.34263213\n",
      "Iteration 246, loss = 0.34243440\n",
      "Iteration 247, loss = 0.34224744\n",
      "Iteration 248, loss = 0.34204016\n",
      "Iteration 249, loss = 0.34185853\n",
      "Iteration 250, loss = 0.34166087\n",
      "Iteration 251, loss = 0.34146930\n",
      "Iteration 252, loss = 0.34128869\n",
      "Iteration 253, loss = 0.34113059\n",
      "Iteration 254, loss = 0.34089675\n",
      "Iteration 255, loss = 0.34069281\n",
      "Iteration 256, loss = 0.34052256\n",
      "Iteration 257, loss = 0.34034896\n",
      "Iteration 258, loss = 0.34013689\n",
      "Iteration 259, loss = 0.33996183\n",
      "Iteration 260, loss = 0.33977460\n",
      "Iteration 261, loss = 0.33959600\n",
      "Iteration 262, loss = 0.33941094\n",
      "Iteration 263, loss = 0.33922749\n",
      "Iteration 264, loss = 0.33904711\n",
      "Iteration 265, loss = 0.33885843\n",
      "Iteration 266, loss = 0.33869893\n",
      "Iteration 267, loss = 0.33849466\n",
      "Iteration 268, loss = 0.33834198\n",
      "Iteration 269, loss = 0.33818847\n",
      "Iteration 270, loss = 0.33798604\n",
      "Iteration 271, loss = 0.33780853\n",
      "Iteration 272, loss = 0.33766149\n",
      "Iteration 273, loss = 0.33746141\n",
      "Iteration 274, loss = 0.33729721\n",
      "Iteration 275, loss = 0.33715250\n",
      "Iteration 276, loss = 0.33699225\n",
      "Iteration 277, loss = 0.33676441\n",
      "Iteration 278, loss = 0.33660920\n",
      "Iteration 279, loss = 0.33646584\n",
      "Iteration 280, loss = 0.33628009\n",
      "Iteration 281, loss = 0.33611074\n",
      "Iteration 282, loss = 0.33593998\n",
      "Iteration 283, loss = 0.33578684\n",
      "Iteration 284, loss = 0.33562034\n",
      "Iteration 285, loss = 0.33547237\n",
      "Iteration 286, loss = 0.33529476\n",
      "Iteration 287, loss = 0.33513807\n",
      "Iteration 288, loss = 0.33497744\n",
      "Iteration 289, loss = 0.33483083\n",
      "Iteration 290, loss = 0.33475363\n",
      "Iteration 291, loss = 0.33449651\n",
      "Iteration 292, loss = 0.33437700\n",
      "Iteration 293, loss = 0.33421544\n",
      "Iteration 294, loss = 0.33401240\n",
      "Iteration 295, loss = 0.33387214\n",
      "Iteration 296, loss = 0.33376600\n",
      "Iteration 297, loss = 0.33358492\n",
      "Iteration 298, loss = 0.33339605\n",
      "Iteration 299, loss = 0.33325677\n",
      "Iteration 300, loss = 0.33317991\n",
      "Iteration 301, loss = 0.33293666\n",
      "Iteration 302, loss = 0.33279170\n",
      "Iteration 303, loss = 0.33265363\n",
      "Iteration 304, loss = 0.33251803\n",
      "Iteration 305, loss = 0.33233739\n",
      "Iteration 306, loss = 0.33221786\n",
      "Iteration 307, loss = 0.33205049\n",
      "Iteration 308, loss = 0.33193806\n",
      "Iteration 309, loss = 0.33175457\n",
      "Iteration 310, loss = 0.33161752\n",
      "Iteration 311, loss = 0.33146601\n",
      "Iteration 312, loss = 0.33133868\n",
      "Iteration 313, loss = 0.33118117\n",
      "Iteration 314, loss = 0.33104280\n",
      "Iteration 315, loss = 0.33090190\n",
      "Iteration 316, loss = 0.33079968\n",
      "Iteration 317, loss = 0.33060196\n",
      "Iteration 318, loss = 0.33049897\n",
      "Iteration 319, loss = 0.33031461\n",
      "Iteration 320, loss = 0.33020091\n",
      "Iteration 321, loss = 0.33007067\n",
      "Iteration 322, loss = 0.32989191\n",
      "Iteration 323, loss = 0.32977962\n",
      "Iteration 324, loss = 0.32963317\n",
      "Iteration 325, loss = 0.32949776\n",
      "Iteration 326, loss = 0.32938902\n",
      "Iteration 327, loss = 0.32920251\n",
      "Iteration 328, loss = 0.32906343\n",
      "Iteration 329, loss = 0.32893728\n",
      "Iteration 330, loss = 0.32879899\n",
      "Iteration 331, loss = 0.32865809\n",
      "Iteration 332, loss = 0.32853675\n",
      "Iteration 333, loss = 0.32839494\n",
      "Iteration 334, loss = 0.32825105\n",
      "Iteration 335, loss = 0.32816039\n",
      "Iteration 336, loss = 0.32801756\n",
      "Iteration 337, loss = 0.32789061\n",
      "Iteration 338, loss = 0.32777618\n",
      "Iteration 339, loss = 0.32760189\n",
      "Iteration 340, loss = 0.32746530\n",
      "Iteration 341, loss = 0.32735295\n",
      "Iteration 342, loss = 0.32719902\n",
      "Iteration 343, loss = 0.32707504\n",
      "Iteration 344, loss = 0.32693165\n",
      "Iteration 345, loss = 0.32681860\n",
      "Iteration 346, loss = 0.32671319\n",
      "Iteration 347, loss = 0.32656350\n",
      "Iteration 348, loss = 0.32643175\n",
      "Iteration 349, loss = 0.32638903\n",
      "Iteration 350, loss = 0.32624476\n",
      "Iteration 351, loss = 0.32606216\n",
      "Iteration 352, loss = 0.32591527\n",
      "Iteration 353, loss = 0.32580845\n",
      "Iteration 354, loss = 0.32566141\n",
      "Iteration 355, loss = 0.32553825\n",
      "Iteration 356, loss = 0.32541358\n",
      "Iteration 357, loss = 0.32528534\n",
      "Iteration 358, loss = 0.32516021\n",
      "Iteration 359, loss = 0.32501020\n",
      "Iteration 360, loss = 0.32489616\n",
      "Iteration 361, loss = 0.32477379\n",
      "Iteration 362, loss = 0.32465736\n",
      "Iteration 363, loss = 0.32451943\n",
      "Iteration 364, loss = 0.32437906\n",
      "Iteration 365, loss = 0.32425912\n",
      "Iteration 366, loss = 0.32415670\n",
      "Iteration 367, loss = 0.32400026\n",
      "Iteration 368, loss = 0.32391267\n",
      "Iteration 369, loss = 0.32378571\n",
      "Iteration 370, loss = 0.32363035\n",
      "Iteration 371, loss = 0.32349425\n",
      "Iteration 372, loss = 0.32337776\n",
      "Iteration 373, loss = 0.32326742\n",
      "Iteration 374, loss = 0.32314047\n",
      "Iteration 375, loss = 0.32302036\n",
      "Iteration 376, loss = 0.32289428\n",
      "Iteration 377, loss = 0.32283097\n",
      "Iteration 378, loss = 0.32268223\n",
      "Iteration 379, loss = 0.32252646\n",
      "Iteration 380, loss = 0.32242856\n",
      "Iteration 381, loss = 0.32237522\n",
      "Iteration 382, loss = 0.32219150\n",
      "Iteration 383, loss = 0.32206711\n",
      "Iteration 384, loss = 0.32195142\n",
      "Iteration 385, loss = 0.32181847\n",
      "Iteration 386, loss = 0.32167906\n",
      "Iteration 387, loss = 0.32157012\n",
      "Iteration 388, loss = 0.32143683\n",
      "Iteration 389, loss = 0.32130405\n",
      "Iteration 390, loss = 0.32122086\n",
      "Iteration 391, loss = 0.32106636\n",
      "Iteration 392, loss = 0.32095490\n",
      "Iteration 393, loss = 0.32082041\n",
      "Iteration 394, loss = 0.32069129\n",
      "Iteration 395, loss = 0.32061625\n",
      "Iteration 396, loss = 0.32048447\n",
      "Iteration 397, loss = 0.32035213\n",
      "Iteration 398, loss = 0.32026735\n",
      "Iteration 399, loss = 0.32011182\n",
      "Iteration 400, loss = 0.32001209\n",
      "Iteration 401, loss = 0.31988372\n",
      "Iteration 402, loss = 0.31977643\n",
      "Iteration 403, loss = 0.31965197\n",
      "Iteration 404, loss = 0.31956513\n",
      "Iteration 405, loss = 0.31945182\n",
      "Iteration 406, loss = 0.31932796\n",
      "Iteration 407, loss = 0.31918106\n",
      "Iteration 408, loss = 0.31905772\n",
      "Iteration 409, loss = 0.31895657\n",
      "Iteration 410, loss = 0.31883937\n",
      "Iteration 411, loss = 0.31874824\n",
      "Iteration 412, loss = 0.31860721\n",
      "Iteration 413, loss = 0.31855083\n",
      "Iteration 414, loss = 0.31838712\n",
      "Iteration 415, loss = 0.31826768\n",
      "Iteration 416, loss = 0.31819904\n",
      "Iteration 417, loss = 0.31808173\n",
      "Iteration 418, loss = 0.31794872\n",
      "Iteration 419, loss = 0.31784513\n",
      "Iteration 420, loss = 0.31770329\n",
      "Iteration 421, loss = 0.31760994\n",
      "Iteration 422, loss = 0.31751129\n",
      "Iteration 423, loss = 0.31739067\n",
      "Iteration 424, loss = 0.31728809\n",
      "Iteration 425, loss = 0.31720192\n",
      "Iteration 426, loss = 0.31709040\n",
      "Iteration 427, loss = 0.31694851\n",
      "Iteration 428, loss = 0.31686984\n",
      "Iteration 429, loss = 0.31673761\n",
      "Iteration 430, loss = 0.31665046\n",
      "Iteration 431, loss = 0.31651069\n",
      "Iteration 432, loss = 0.31639268\n",
      "Iteration 433, loss = 0.31632770\n",
      "Iteration 434, loss = 0.31621420\n",
      "Iteration 435, loss = 0.31607548\n",
      "Iteration 436, loss = 0.31597717\n",
      "Iteration 437, loss = 0.31587911\n",
      "Iteration 438, loss = 0.31581958\n",
      "Iteration 439, loss = 0.31561998\n",
      "Iteration 440, loss = 0.31557466\n",
      "Iteration 441, loss = 0.31541917\n",
      "Iteration 442, loss = 0.31530834\n",
      "Iteration 443, loss = 0.31517188\n",
      "Iteration 444, loss = 0.31506204\n",
      "Iteration 445, loss = 0.31503837\n",
      "Iteration 446, loss = 0.31489810\n",
      "Iteration 447, loss = 0.31482204\n",
      "Iteration 448, loss = 0.31466649\n",
      "Iteration 449, loss = 0.31451232\n",
      "Iteration 450, loss = 0.31442585\n",
      "Iteration 451, loss = 0.31431690\n",
      "Iteration 452, loss = 0.31420682\n",
      "Iteration 453, loss = 0.31409348\n",
      "Iteration 454, loss = 0.31400235\n",
      "Iteration 455, loss = 0.31391240\n",
      "Iteration 456, loss = 0.31379993\n",
      "Iteration 457, loss = 0.31368555\n",
      "Iteration 458, loss = 0.31355056\n",
      "Iteration 459, loss = 0.31348109\n",
      "Iteration 460, loss = 0.31334936\n",
      "Iteration 461, loss = 0.31325445\n",
      "Iteration 462, loss = 0.31314555\n",
      "Iteration 463, loss = 0.31304701\n",
      "Iteration 464, loss = 0.31295247\n",
      "Iteration 465, loss = 0.31281747\n",
      "Iteration 466, loss = 0.31272548\n",
      "Iteration 467, loss = 0.31261212\n",
      "Iteration 468, loss = 0.31250010\n",
      "Iteration 469, loss = 0.31243280\n",
      "Iteration 470, loss = 0.31231343\n",
      "Iteration 471, loss = 0.31218285\n",
      "Iteration 472, loss = 0.31208262\n",
      "Iteration 473, loss = 0.31198260\n",
      "Iteration 474, loss = 0.31189300\n",
      "Iteration 475, loss = 0.31178128\n",
      "Iteration 476, loss = 0.31167242\n",
      "Iteration 477, loss = 0.31156244\n",
      "Iteration 478, loss = 0.31148805\n",
      "Iteration 479, loss = 0.31136141\n",
      "Iteration 480, loss = 0.31127230\n",
      "Iteration 481, loss = 0.31115807\n",
      "Iteration 482, loss = 0.31107384\n",
      "Iteration 483, loss = 0.31094623\n",
      "Iteration 484, loss = 0.31087713\n",
      "Iteration 485, loss = 0.31078347\n",
      "Iteration 486, loss = 0.31074108\n",
      "Iteration 487, loss = 0.31054566\n",
      "Iteration 488, loss = 0.31046659\n",
      "Iteration 489, loss = 0.31034260\n",
      "Iteration 490, loss = 0.31024076\n",
      "Iteration 491, loss = 0.31018616\n",
      "Iteration 492, loss = 0.31003629\n",
      "Iteration 493, loss = 0.30994886\n",
      "Iteration 494, loss = 0.30982399\n",
      "Iteration 495, loss = 0.30974274\n",
      "Iteration 496, loss = 0.30964142\n",
      "Iteration 497, loss = 0.30954023\n",
      "Iteration 498, loss = 0.30945200\n",
      "Iteration 499, loss = 0.30933533\n",
      "Iteration 500, loss = 0.30924885\n",
      "Iteration 501, loss = 0.30912945\n",
      "Iteration 502, loss = 0.30902538\n",
      "Iteration 503, loss = 0.30896424\n",
      "Iteration 504, loss = 0.30882030\n",
      "Iteration 505, loss = 0.30871841\n",
      "Iteration 506, loss = 0.30861576\n",
      "Iteration 507, loss = 0.30851585\n",
      "Iteration 508, loss = 0.30845542\n",
      "Iteration 509, loss = 0.30831035\n",
      "Iteration 510, loss = 0.30822676\n",
      "Iteration 511, loss = 0.30815141\n",
      "Iteration 512, loss = 0.30801633\n",
      "Iteration 513, loss = 0.30791935\n",
      "Iteration 514, loss = 0.30782016\n",
      "Iteration 515, loss = 0.30775133\n",
      "Iteration 516, loss = 0.30763235\n",
      "Iteration 517, loss = 0.30754775\n",
      "Iteration 518, loss = 0.30747632\n",
      "Iteration 519, loss = 0.30737939\n",
      "Iteration 520, loss = 0.30725992\n",
      "Iteration 521, loss = 0.30714987\n",
      "Iteration 522, loss = 0.30702789\n",
      "Iteration 523, loss = 0.30693316\n",
      "Iteration 524, loss = 0.30682753\n",
      "Iteration 525, loss = 0.30675732\n",
      "Iteration 526, loss = 0.30665471\n",
      "Iteration 527, loss = 0.30655435\n",
      "Iteration 528, loss = 0.30645733\n",
      "Iteration 529, loss = 0.30632945\n",
      "Iteration 530, loss = 0.30624182\n",
      "Iteration 531, loss = 0.30615887\n",
      "Iteration 532, loss = 0.30607371\n",
      "Iteration 533, loss = 0.30596981\n",
      "Iteration 534, loss = 0.30583411\n",
      "Iteration 535, loss = 0.30576700\n",
      "Iteration 536, loss = 0.30566437\n",
      "Iteration 537, loss = 0.30556499\n",
      "Iteration 538, loss = 0.30548997\n",
      "Iteration 539, loss = 0.30536432\n",
      "Iteration 540, loss = 0.30527065\n",
      "Iteration 541, loss = 0.30516373\n",
      "Iteration 542, loss = 0.30506330\n",
      "Iteration 543, loss = 0.30496959\n",
      "Iteration 544, loss = 0.30488650\n",
      "Iteration 545, loss = 0.30479315\n",
      "Iteration 546, loss = 0.30469235\n",
      "Iteration 547, loss = 0.30459343\n",
      "Iteration 548, loss = 0.30447290\n",
      "Iteration 549, loss = 0.30438349\n",
      "Iteration 550, loss = 0.30433661\n",
      "Iteration 551, loss = 0.30418255\n",
      "Iteration 552, loss = 0.30408091\n",
      "Iteration 553, loss = 0.30398729\n",
      "Iteration 554, loss = 0.30388019\n",
      "Iteration 555, loss = 0.30379124\n",
      "Iteration 556, loss = 0.30371055\n",
      "Iteration 557, loss = 0.30360684\n",
      "Iteration 558, loss = 0.30351290\n",
      "Iteration 559, loss = 0.30340767\n",
      "Iteration 560, loss = 0.30331199\n",
      "Iteration 561, loss = 0.30322822\n",
      "Iteration 562, loss = 0.30313868\n",
      "Iteration 563, loss = 0.30303911\n",
      "Iteration 564, loss = 0.30292730\n",
      "Iteration 565, loss = 0.30284170\n",
      "Iteration 566, loss = 0.30274658\n",
      "Iteration 567, loss = 0.30265878\n",
      "Iteration 568, loss = 0.30254672\n",
      "Iteration 569, loss = 0.30246945\n",
      "Iteration 570, loss = 0.30237142\n",
      "Iteration 571, loss = 0.30226967\n",
      "Iteration 572, loss = 0.30217148\n",
      "Iteration 573, loss = 0.30206479\n",
      "Iteration 574, loss = 0.30202831\n",
      "Iteration 751, loss = 0.28223438\n",
      "Iteration 752, loss = 0.28211985\n",
      "Iteration 753, loss = 0.28199042\n",
      "Iteration 754, loss = 0.28186600\n",
      "Iteration 755, loss = 0.28175000\n",
      "Iteration 756, loss = 0.28164621\n",
      "Iteration 757, loss = 0.28148681\n",
      "Iteration 758, loss = 0.28136620\n",
      "Iteration 759, loss = 0.28123379\n",
      "Iteration 760, loss = 0.28111817\n",
      "Iteration 761, loss = 0.28097076\n",
      "Iteration 762, loss = 0.28085008\n",
      "Iteration 763, loss = 0.28074164\n",
      "Iteration 764, loss = 0.28063357\n",
      "Iteration 765, loss = 0.28046159\n",
      "Iteration 766, loss = 0.28042403\n",
      "Iteration 767, loss = 0.28022030\n",
      "Iteration 768, loss = 0.28009362\n",
      "Iteration 769, loss = 0.27996063\n",
      "Iteration 770, loss = 0.27986768\n",
      "Iteration 771, loss = 0.27973259\n",
      "Iteration 772, loss = 0.27959130\n",
      "Iteration 773, loss = 0.27947674\n",
      "Iteration 774, loss = 0.27933069\n",
      "Iteration 775, loss = 0.27921079\n",
      "Iteration 776, loss = 0.27909547\n",
      "Iteration 777, loss = 0.27896055\n",
      "Iteration 778, loss = 0.27883305\n",
      "Iteration 779, loss = 0.27869980\n",
      "Iteration 780, loss = 0.27857096\n",
      "Iteration 781, loss = 0.27846548\n",
      "Iteration 782, loss = 0.27833947\n",
      "Iteration 783, loss = 0.27825605\n",
      "Iteration 784, loss = 0.27808189\n",
      "Iteration 785, loss = 0.27795477\n",
      "Iteration 786, loss = 0.27782776\n",
      "Iteration 787, loss = 0.27772393\n",
      "Iteration 788, loss = 0.27758285\n",
      "Iteration 789, loss = 0.27746476\n",
      "Iteration 790, loss = 0.27731178\n",
      "Iteration 791, loss = 0.27725545\n",
      "Iteration 792, loss = 0.27709158\n",
      "Iteration 793, loss = 0.27696358\n",
      "Iteration 794, loss = 0.27681142\n",
      "Iteration 795, loss = 0.27670799\n",
      "Iteration 796, loss = 0.27656900\n",
      "Iteration 797, loss = 0.27643615\n",
      "Iteration 798, loss = 0.27639699\n",
      "Iteration 799, loss = 0.27624353\n",
      "Iteration 800, loss = 0.27612665\n",
      "Iteration 801, loss = 0.27598080\n",
      "Iteration 802, loss = 0.27584507\n",
      "Iteration 803, loss = 0.27570264\n",
      "Iteration 804, loss = 0.27558053\n",
      "Iteration 805, loss = 0.27545927\n",
      "Iteration 806, loss = 0.27538608\n",
      "Iteration 807, loss = 0.27521668\n",
      "Iteration 808, loss = 0.27510856\n",
      "Iteration 809, loss = 0.27497030\n",
      "Iteration 810, loss = 0.27484833\n",
      "Iteration 811, loss = 0.27472123\n",
      "Iteration 812, loss = 0.27459643\n",
      "Iteration 813, loss = 0.27446036\n",
      "Iteration 814, loss = 0.27436254\n",
      "Iteration 815, loss = 0.27423039\n",
      "Iteration 816, loss = 0.27411020\n",
      "Iteration 817, loss = 0.27396709\n",
      "Iteration 818, loss = 0.27384786\n",
      "Iteration 819, loss = 0.27374319\n",
      "Iteration 820, loss = 0.27358223\n",
      "Iteration 821, loss = 0.27350356\n",
      "Iteration 822, loss = 0.27335300\n",
      "Iteration 823, loss = 0.27323124\n",
      "Iteration 824, loss = 0.27313332\n",
      "Iteration 825, loss = 0.27296986\n",
      "Iteration 826, loss = 0.27292198\n",
      "Iteration 827, loss = 0.27276797\n",
      "Iteration 828, loss = 0.27260875\n",
      "Iteration 829, loss = 0.27249507\n",
      "Iteration 830, loss = 0.27237998\n",
      "Iteration 831, loss = 0.27226036\n",
      "Iteration 832, loss = 0.27214244\n",
      "Iteration 833, loss = 0.27198200\n",
      "Iteration 834, loss = 0.27188438\n",
      "Iteration 835, loss = 0.27175193\n",
      "Iteration 836, loss = 0.27160895\n",
      "Iteration 837, loss = 0.27149989\n",
      "Iteration 838, loss = 0.27136549\n",
      "Iteration 839, loss = 0.27123206\n",
      "Iteration 840, loss = 0.27115359\n",
      "Iteration 841, loss = 0.27101136\n",
      "Iteration 842, loss = 0.27085953\n",
      "Iteration 843, loss = 0.27073840\n",
      "Iteration 844, loss = 0.27062419\n",
      "Iteration 845, loss = 0.27050185\n",
      "Iteration 846, loss = 0.27037528\n",
      "Iteration 847, loss = 0.27024791\n",
      "Iteration 848, loss = 0.27012738\n",
      "Iteration 849, loss = 0.27000799\n",
      "Iteration 850, loss = 0.26987646\n",
      "Iteration 851, loss = 0.26975488\n",
      "Iteration 852, loss = 0.26965058\n",
      "Iteration 853, loss = 0.26954318\n",
      "Iteration 854, loss = 0.26940953\n",
      "Iteration 855, loss = 0.26927749\n",
      "Iteration 856, loss = 0.26917641\n",
      "Iteration 857, loss = 0.26902581\n",
      "Iteration 858, loss = 0.26892868\n",
      "Iteration 859, loss = 0.26880963\n",
      "Iteration 860, loss = 0.26867269\n",
      "Iteration 861, loss = 0.26854840\n",
      "Iteration 862, loss = 0.26841476\n",
      "Iteration 863, loss = 0.26833484\n",
      "Iteration 864, loss = 0.26816638\n",
      "Iteration 865, loss = 0.26805207\n",
      "Iteration 866, loss = 0.26793293\n",
      "Iteration 867, loss = 0.26780899\n",
      "Iteration 868, loss = 0.26768810\n",
      "Iteration 869, loss = 0.26766490\n",
      "Iteration 870, loss = 0.26743602\n",
      "Iteration 871, loss = 0.26733173\n",
      "Iteration 872, loss = 0.26719580\n",
      "Iteration 873, loss = 0.26705600\n",
      "Iteration 874, loss = 0.26694284\n",
      "Iteration 875, loss = 0.26682278\n",
      "Iteration 876, loss = 0.26669357\n",
      "Iteration 877, loss = 0.26659114\n",
      "Iteration 878, loss = 0.26644263\n",
      "Iteration 879, loss = 0.26636395\n",
      "Iteration 880, loss = 0.26620386\n",
      "Iteration 881, loss = 0.26608486\n",
      "Iteration 882, loss = 0.26596215\n",
      "Iteration 883, loss = 0.26585050\n",
      "Iteration 884, loss = 0.26572985\n",
      "Iteration 885, loss = 0.26562594\n",
      "Iteration 886, loss = 0.26548921\n",
      "Iteration 887, loss = 0.26538367\n",
      "Iteration 888, loss = 0.26522963\n",
      "Iteration 889, loss = 0.26512589\n",
      "Iteration 890, loss = 0.26497016\n",
      "Iteration 891, loss = 0.26486835\n",
      "Iteration 892, loss = 0.26475190\n",
      "Iteration 893, loss = 0.26463371\n",
      "Iteration 894, loss = 0.26452780\n",
      "Iteration 895, loss = 0.26437723\n",
      "Iteration 896, loss = 0.26424162\n",
      "Iteration 897, loss = 0.26411813\n",
      "Iteration 898, loss = 0.26399049\n",
      "Iteration 899, loss = 0.26386942\n",
      "Iteration 900, loss = 0.26377261\n",
      "Iteration 901, loss = 0.26364131\n",
      "Iteration 902, loss = 0.26355638\n",
      "Iteration 903, loss = 0.26341310\n",
      "Iteration 904, loss = 0.26325682\n",
      "Iteration 905, loss = 0.26313263\n",
      "Iteration 906, loss = 0.26303364\n",
      "Iteration 907, loss = 0.26290734\n",
      "Iteration 908, loss = 0.26278472\n",
      "Iteration 909, loss = 0.26265255\n",
      "Iteration 910, loss = 0.26258286\n",
      "Iteration 911, loss = 0.26241562\n",
      "Iteration 912, loss = 0.26229962\n",
      "Iteration 913, loss = 0.26221099\n",
      "Iteration 914, loss = 0.26205618\n",
      "Iteration 915, loss = 0.26195785\n",
      "Iteration 916, loss = 0.26183372\n",
      "Iteration 917, loss = 0.26168386\n",
      "Iteration 918, loss = 0.26158916\n",
      "Iteration 919, loss = 0.26143835\n",
      "Iteration 920, loss = 0.26135416\n",
      "Iteration 921, loss = 0.26120498\n",
      "Iteration 922, loss = 0.26108494\n",
      "Iteration 923, loss = 0.26095959\n",
      "Iteration 924, loss = 0.26088450\n",
      "Iteration 925, loss = 0.26071808\n",
      "Iteration 926, loss = 0.26064022\n",
      "Iteration 927, loss = 0.26052392\n",
      "Iteration 928, loss = 0.26036321\n",
      "Iteration 929, loss = 0.26024219\n",
      "Iteration 930, loss = 0.26011442\n",
      "Iteration 931, loss = 0.26001281\n",
      "Iteration 932, loss = 0.25991935\n",
      "Iteration 933, loss = 0.25982113\n",
      "Iteration 934, loss = 0.25964343\n",
      "Iteration 935, loss = 0.25955367\n",
      "Iteration 936, loss = 0.25938635\n",
      "Iteration 937, loss = 0.25927694\n",
      "Iteration 938, loss = 0.25914091\n",
      "Iteration 939, loss = 0.25903318\n",
      "Iteration 940, loss = 0.25890091\n",
      "Iteration 941, loss = 0.25880850\n",
      "Iteration 942, loss = 0.25871840\n",
      "Iteration 943, loss = 0.25855072\n",
      "Iteration 944, loss = 0.25842631\n",
      "Iteration 945, loss = 0.25832308\n",
      "Iteration 946, loss = 0.25819763\n",
      "Iteration 947, loss = 0.25809357\n",
      "Iteration 948, loss = 0.25794745\n",
      "Iteration 949, loss = 0.25783848\n",
      "Iteration 950, loss = 0.25773647\n",
      "Iteration 951, loss = 0.25769673\n",
      "Iteration 952, loss = 0.25751036\n",
      "Iteration 953, loss = 0.25735422\n",
      "Iteration 954, loss = 0.25723035\n",
      "Iteration 955, loss = 0.25710762\n",
      "Iteration 956, loss = 0.25698459\n",
      "Iteration 957, loss = 0.25687072\n",
      "Iteration 958, loss = 0.25674127\n",
      "Iteration 959, loss = 0.25669085\n",
      "Iteration 960, loss = 0.25649867\n",
      "Iteration 961, loss = 0.25642135\n",
      "Iteration 962, loss = 0.25628867\n",
      "Iteration 963, loss = 0.25616825\n",
      "Iteration 964, loss = 0.25604517\n",
      "Iteration 965, loss = 0.25589817\n",
      "Iteration 966, loss = 0.25585175\n",
      "Iteration 967, loss = 0.25566031\n",
      "Iteration 968, loss = 0.25553812\n",
      "Iteration 969, loss = 0.25541711\n",
      "Iteration 970, loss = 0.25540530\n",
      "Iteration 971, loss = 0.25519202\n",
      "Iteration 972, loss = 0.25505353\n",
      "Iteration 973, loss = 0.25493704\n",
      "Iteration 974, loss = 0.25482946\n",
      "Iteration 975, loss = 0.25474491\n",
      "Iteration 976, loss = 0.25460839\n",
      "Iteration 977, loss = 0.25445076\n",
      "Iteration 978, loss = 0.25432214\n",
      "Iteration 979, loss = 0.25419857\n",
      "Iteration 980, loss = 0.25410896\n",
      "Iteration 981, loss = 0.25395076\n",
      "Iteration 982, loss = 0.25391898\n",
      "Iteration 983, loss = 0.25375885\n",
      "Iteration 984, loss = 0.25361386\n",
      "Iteration 985, loss = 0.25347506\n",
      "Iteration 986, loss = 0.25335381\n",
      "Iteration 987, loss = 0.25322415\n",
      "Iteration 988, loss = 0.25311470\n",
      "Iteration 989, loss = 0.25298181\n",
      "Iteration 990, loss = 0.25290416\n",
      "Iteration 991, loss = 0.25279497\n",
      "Iteration 992, loss = 0.25265535\n",
      "Iteration 993, loss = 0.25252575\n",
      "Iteration 994, loss = 0.25242447\n",
      "Iteration 995, loss = 0.25225127\n",
      "Iteration 996, loss = 0.25216525\n",
      "Iteration 997, loss = 0.25204896\n",
      "Iteration 998, loss = 0.25191548\n",
      "Iteration 999, loss = 0.25176770\n",
      "Iteration 1000, loss = 0.25164119\n",
      "Iteration 1001, loss = 0.25151773\n",
      "Iteration 1002, loss = 0.25141316\n",
      "Iteration 1003, loss = 0.25129223\n",
      "Iteration 1004, loss = 0.25118578\n",
      "Iteration 1005, loss = 0.25109578\n",
      "Iteration 1006, loss = 0.25090553\n",
      "Iteration 1007, loss = 0.25077647\n",
      "Iteration 1008, loss = 0.25068844\n",
      "Iteration 1009, loss = 0.25056192\n",
      "Iteration 1010, loss = 0.25042394\n",
      "Iteration 1011, loss = 0.25030748\n",
      "Iteration 1012, loss = 0.25021721\n",
      "Iteration 1013, loss = 0.25008413\n",
      "Iteration 1014, loss = 0.24995205\n",
      "Iteration 1015, loss = 0.24982512\n",
      "Iteration 1016, loss = 0.24971720\n",
      "Iteration 1017, loss = 0.24956681\n",
      "Iteration 1018, loss = 0.24945094\n",
      "Iteration 1019, loss = 0.24932044\n",
      "Iteration 1020, loss = 0.24924522\n",
      "Iteration 1021, loss = 0.24906386\n",
      "Iteration 1022, loss = 0.24899073\n",
      "Iteration 1023, loss = 0.24885031\n",
      "Iteration 1024, loss = 0.24873430\n",
      "Iteration 1025, loss = 0.24858190\n",
      "Iteration 1026, loss = 0.24848612\n",
      "Iteration 1027, loss = 0.24838309\n",
      "Iteration 1028, loss = 0.24823382\n",
      "Iteration 1029, loss = 0.24815903\n",
      "Iteration 1030, loss = 0.24806762\n",
      "Iteration 1031, loss = 0.24792161\n",
      "Iteration 1032, loss = 0.24776036\n",
      "Iteration 1033, loss = 0.24763375\n",
      "Iteration 1034, loss = 0.24755200\n",
      "Iteration 1035, loss = 0.24738635\n",
      "Iteration 1036, loss = 0.24728051\n",
      "Iteration 1037, loss = 0.24712467\n",
      "Iteration 1038, loss = 0.24699713\n",
      "Iteration 1039, loss = 0.24688076\n",
      "Iteration 1040, loss = 0.24676315\n",
      "Iteration 1041, loss = 0.24665863\n",
      "Iteration 1042, loss = 0.24662127\n",
      "Iteration 1043, loss = 0.24640642\n",
      "Iteration 1044, loss = 0.24626347\n",
      "Iteration 1045, loss = 0.24613317\n",
      "Iteration 1046, loss = 0.24607034\n",
      "Iteration 1047, loss = 0.24590621\n",
      "Iteration 1048, loss = 0.24581737\n",
      "Iteration 1049, loss = 0.24568053\n",
      "Iteration 1050, loss = 0.24558346\n",
      "Iteration 1051, loss = 0.24543446\n",
      "Iteration 1052, loss = 0.24530977\n",
      "Iteration 1053, loss = 0.24522706\n",
      "Iteration 1054, loss = 0.24511268\n",
      "Iteration 1055, loss = 0.24493314\n",
      "Iteration 1056, loss = 0.24483317\n",
      "Iteration 1057, loss = 0.24472498\n",
      "Iteration 1058, loss = 0.24457242\n",
      "Iteration 1059, loss = 0.24446299\n",
      "Iteration 1060, loss = 0.24435823\n",
      "Iteration 1061, loss = 0.24421916\n",
      "Iteration 1062, loss = 0.24408592\n",
      "Iteration 1063, loss = 0.24397363\n",
      "Iteration 1064, loss = 0.24397310\n",
      "Iteration 1065, loss = 0.24373789\n",
      "Iteration 1066, loss = 0.24360977\n",
      "Iteration 1067, loss = 0.24349623\n",
      "Iteration 1068, loss = 0.24337582\n",
      "Iteration 1069, loss = 0.24331016\n",
      "Iteration 1070, loss = 0.24320863\n",
      "Iteration 1071, loss = 0.24300195\n",
      "Iteration 1072, loss = 0.24288486\n",
      "Iteration 1073, loss = 0.24278699\n",
      "Iteration 1074, loss = 0.24264184\n",
      "Iteration 1075, loss = 0.24257422\n",
      "Iteration 1076, loss = 0.24242582\n",
      "Iteration 1077, loss = 0.24229010\n",
      "Iteration 1078, loss = 0.24217476\n",
      "Iteration 1079, loss = 0.24208298\n",
      "Iteration 1080, loss = 0.24195505\n",
      "Iteration 1081, loss = 0.24180849\n",
      "Iteration 1082, loss = 0.24170468\n",
      "Iteration 1083, loss = 0.24157035\n",
      "Iteration 1084, loss = 0.24143846\n",
      "Iteration 1085, loss = 0.24133217\n",
      "Iteration 1086, loss = 0.24123220\n",
      "Iteration 1087, loss = 0.24109197\n",
      "Iteration 1088, loss = 0.24099185\n",
      "Iteration 1089, loss = 0.24087571\n",
      "Iteration 1090, loss = 0.24070827\n",
      "Iteration 1091, loss = 0.24068016\n",
      "Iteration 1092, loss = 0.24050161\n",
      "Iteration 1093, loss = 0.24035827\n",
      "Iteration 1094, loss = 0.24028225\n",
      "Iteration 1095, loss = 0.24013540\n",
      "Iteration 1096, loss = 0.23998725\n",
      "Iteration 1097, loss = 0.23990976\n",
      "Iteration 1098, loss = 0.23976093\n",
      "Iteration 1099, loss = 0.23965895\n",
      "Iteration 1100, loss = 0.23951595\n",
      "Iteration 1101, loss = 0.23944204\n",
      "Iteration 1102, loss = 0.23930426\n",
      "Iteration 1103, loss = 0.23917501\n",
      "Iteration 1104, loss = 0.23903860\n",
      "Iteration 1105, loss = 0.23894194\n",
      "Iteration 1106, loss = 0.23881530\n",
      "Iteration 1107, loss = 0.23867600\n",
      "Iteration 1108, loss = 0.23854810\n",
      "Iteration 1109, loss = 0.23842935\n",
      "Iteration 1110, loss = 0.23832385\n",
      "Iteration 1111, loss = 0.23823407\n",
      "Iteration 1112, loss = 0.23816046\n",
      "Iteration 1113, loss = 0.23796640\n",
      "Iteration 1114, loss = 0.23782137\n",
      "Iteration 1115, loss = 0.23770405\n",
      "Iteration 1116, loss = 0.23760205\n",
      "Iteration 1117, loss = 0.23748057\n",
      "Iteration 1118, loss = 0.23735123\n",
      "Iteration 1119, loss = 0.23722008\n",
      "Iteration 1120, loss = 0.23709309\n",
      "Iteration 1121, loss = 0.23707505\n",
      "Iteration 1122, loss = 0.23687697\n",
      "Iteration 1123, loss = 0.23676743\n",
      "Iteration 1124, loss = 0.23663122\n",
      "Iteration 1125, loss = 0.23651125\n",
      "Iteration 1126, loss = 0.23641778\n",
      "Iteration 1127, loss = 0.23625664\n",
      "Iteration 1128, loss = 0.23615845\n",
      "Iteration 1129, loss = 0.23600879\n",
      "Iteration 1130, loss = 0.23595782\n",
      "Iteration 1131, loss = 0.23576492\n",
      "Iteration 1132, loss = 0.23562053\n",
      "Iteration 1133, loss = 0.23550690\n",
      "Iteration 1134, loss = 0.23538878\n",
      "Iteration 1135, loss = 0.23529794\n",
      "Iteration 1136, loss = 0.23516796\n",
      "Iteration 1137, loss = 0.23508425\n",
      "Iteration 1138, loss = 0.23495141\n",
      "Iteration 1139, loss = 0.23478357\n",
      "Iteration 1140, loss = 0.23466820\n",
      "Iteration 1141, loss = 0.23452946\n",
      "Iteration 1142, loss = 0.23442711\n",
      "Iteration 1143, loss = 0.23429706\n",
      "Iteration 1144, loss = 0.23419461\n",
      "Iteration 1145, loss = 0.23406627\n",
      "Iteration 1146, loss = 0.23394178\n",
      "Iteration 1147, loss = 0.23380637\n",
      "Iteration 1148, loss = 0.23370986\n",
      "Iteration 1149, loss = 0.23360022\n",
      "Iteration 1150, loss = 0.23350281\n",
      "Iteration 1151, loss = 0.23332834\n",
      "Iteration 1152, loss = 0.23322242\n",
      "Iteration 1153, loss = 0.23309691\n",
      "Iteration 1154, loss = 0.23296856\n",
      "Iteration 1155, loss = 0.23284674\n",
      "Iteration 1156, loss = 0.23279151\n",
      "Iteration 1157, loss = 0.23272421\n",
      "Iteration 1158, loss = 0.23248135\n",
      "Iteration 1159, loss = 0.23234361\n",
      "Iteration 1160, loss = 0.23222034\n",
      "Iteration 1161, loss = 0.23211132\n",
      "Iteration 1162, loss = 0.23200623\n",
      "Iteration 1163, loss = 0.23187564\n",
      "Iteration 1164, loss = 0.23173771\n",
      "Iteration 1165, loss = 0.23164733\n",
      "Iteration 1166, loss = 0.23149608\n",
      "Iteration 1167, loss = 0.23138651\n",
      "Iteration 1168, loss = 0.23125352\n",
      "Iteration 1169, loss = 0.23114066\n",
      "Iteration 1170, loss = 0.23098753\n",
      "Iteration 1171, loss = 0.23088364\n",
      "Iteration 1172, loss = 0.23074844\n",
      "Iteration 1173, loss = 0.23062298\n",
      "Iteration 1174, loss = 0.23050313\n",
      "Iteration 1175, loss = 0.23037566\n",
      "Iteration 1176, loss = 0.23024854\n",
      "Iteration 1177, loss = 0.23018480\n",
      "Iteration 1178, loss = 0.23004518\n",
      "Iteration 1179, loss = 0.22989248\n",
      "Iteration 1180, loss = 0.22988834\n",
      "Iteration 1181, loss = 0.22965868\n",
      "Iteration 1182, loss = 0.22956335\n",
      "Iteration 1183, loss = 0.22942398\n",
      "Iteration 1184, loss = 0.22929198\n",
      "Iteration 1185, loss = 0.22915085\n",
      "Iteration 1186, loss = 0.22903869\n",
      "Iteration 1187, loss = 0.22891080\n",
      "Iteration 1188, loss = 0.22883271\n",
      "Iteration 1189, loss = 0.22868926\n",
      "Iteration 1190, loss = 0.22855626\n",
      "Iteration 1191, loss = 0.22845249\n",
      "Iteration 1192, loss = 0.22834216\n",
      "Iteration 1193, loss = 0.22818548\n",
      "Iteration 1194, loss = 0.22805635\n",
      "Iteration 1195, loss = 0.22795733\n",
      "Iteration 1196, loss = 0.22784628\n",
      "Iteration 1197, loss = 0.22770518\n",
      "Iteration 1198, loss = 0.22756958\n",
      "Iteration 1199, loss = 0.22743684\n",
      "Iteration 1200, loss = 0.22732882\n",
      "Iteration 1201, loss = 0.22728477\n",
      "Iteration 1202, loss = 0.22707056\n",
      "Iteration 1203, loss = 0.22697895\n",
      "Iteration 1204, loss = 0.22687451\n",
      "Iteration 1205, loss = 0.22678518\n",
      "Iteration 1206, loss = 0.22665163\n",
      "Iteration 1207, loss = 0.22648211\n",
      "Iteration 1208, loss = 0.22635276\n",
      "Iteration 1209, loss = 0.22628191\n",
      "Iteration 1210, loss = 0.22612199\n",
      "Iteration 1211, loss = 0.22598913\n",
      "Iteration 1212, loss = 0.22589466\n",
      "Iteration 1213, loss = 0.22577400\n",
      "Iteration 1214, loss = 0.22562533\n",
      "Iteration 1215, loss = 0.22555735\n",
      "Iteration 1216, loss = 0.22538382\n",
      "Iteration 1217, loss = 0.22528616\n",
      "Iteration 1218, loss = 0.22517303\n",
      "Iteration 1219, loss = 0.22502845\n",
      "Iteration 1220, loss = 0.22493450\n",
      "Iteration 1221, loss = 0.22479103\n",
      "Iteration 1222, loss = 0.22466877\n",
      "Iteration 1223, loss = 0.22459863\n",
      "Iteration 1224, loss = 0.22442181\n",
      "Iteration 1225, loss = 0.22428754\n",
      "Iteration 1226, loss = 0.22417412\n",
      "Iteration 1227, loss = 0.22405144\n",
      "Iteration 1228, loss = 0.22395322\n",
      "Iteration 1229, loss = 0.22381558\n",
      "Iteration 1230, loss = 0.22374373\n",
      "Iteration 1231, loss = 0.22359105\n",
      "Iteration 1232, loss = 0.22344814\n",
      "Iteration 1233, loss = 0.22340917\n",
      "Iteration 1234, loss = 0.22323662\n",
      "Iteration 1235, loss = 0.22309787\n",
      "Iteration 1236, loss = 0.22295571\n",
      "Iteration 1237, loss = 0.22283092\n",
      "Iteration 1238, loss = 0.22274817\n",
      "Iteration 1239, loss = 0.22261947Iteration 570, loss = 0.33483171\n",
      "Iteration 571, loss = 0.33469744\n",
      "Iteration 572, loss = 0.33456750\n",
      "Iteration 573, loss = 0.33443259\n",
      "Iteration 574, loss = 0.33429106\n",
      "Iteration 575, loss = 0.33414342\n",
      "Iteration 576, loss = 0.33403972\n",
      "Iteration 577, loss = 0.33387570\n",
      "Iteration 578, loss = 0.33376139\n",
      "Iteration 579, loss = 0.33359823\n",
      "Iteration 580, loss = 0.33348168\n",
      "Iteration 581, loss = 0.33333245\n",
      "Iteration 582, loss = 0.33319214\n",
      "Iteration 583, loss = 0.33311645\n",
      "Iteration 584, loss = 0.33292657\n",
      "Iteration 585, loss = 0.33278534\n",
      "Iteration 586, loss = 0.33265087\n",
      "Iteration 587, loss = 0.33253789\n",
      "Iteration 588, loss = 0.33238123\n",
      "Iteration 589, loss = 0.33224225\n",
      "Iteration 590, loss = 0.33216478\n",
      "Iteration 591, loss = 0.33197005\n",
      "Iteration 592, loss = 0.33185951\n",
      "Iteration 593, loss = 0.33169612\n",
      "Iteration 594, loss = 0.33159793\n",
      "Iteration 595, loss = 0.33143569\n",
      "Iteration 596, loss = 0.33130368\n",
      "Iteration 597, loss = 0.33116661\n",
      "Iteration 598, loss = 0.33101531\n",
      "Iteration 599, loss = 0.33089593\n",
      "Iteration 600, loss = 0.33075589\n",
      "Iteration 601, loss = 0.33060996\n",
      "Iteration 602, loss = 0.33054305\n",
      "Iteration 603, loss = 0.33037201\n",
      "Iteration 604, loss = 0.33020489\n",
      "Iteration 605, loss = 0.33014509\n",
      "Iteration 606, loss = 0.32993709\n",
      "Iteration 607, loss = 0.32981364\n",
      "Iteration 608, loss = 0.32966896\n",
      "Iteration 609, loss = 0.32953242\n",
      "Iteration 610, loss = 0.32939693\n",
      "Iteration 611, loss = 0.32925083\n",
      "Iteration 612, loss = 0.32912610\n",
      "Iteration 613, loss = 0.32898610\n",
      "Iteration 614, loss = 0.32886255\n",
      "Iteration 615, loss = 0.32873017\n",
      "Iteration 616, loss = 0.32858306\n",
      "Iteration 617, loss = 0.32846724\n",
      "Iteration 618, loss = 0.32836357\n",
      "Iteration 619, loss = 0.32818914\n",
      "Iteration 620, loss = 0.32805929\n",
      "Iteration 621, loss = 0.32792332\n",
      "Iteration 622, loss = 0.32778459\n",
      "Iteration 623, loss = 0.32763640\n",
      "Iteration 624, loss = 0.32752588\n",
      "Iteration 625, loss = 0.32739699\n",
      "Iteration 626, loss = 0.32727550\n",
      "Iteration 627, loss = 0.32711804\n",
      "Iteration 628, loss = 0.32706361\n",
      "Iteration 629, loss = 0.32686271\n",
      "Iteration 630, loss = 0.32674348\n",
      "Iteration 631, loss = 0.32660083\n",
      "Iteration 632, loss = 0.32643681\n",
      "Iteration 633, loss = 0.32630752\n",
      "Iteration 634, loss = 0.32617250\n",
      "Iteration 635, loss = 0.32606356\n",
      "Iteration 636, loss = 0.32592524\n",
      "Iteration 637, loss = 0.32577702\n",
      "Iteration 638, loss = 0.32564326\n",
      "Iteration 639, loss = 0.32553349\n",
      "Iteration 640, loss = 0.32541107\n",
      "Iteration 641, loss = 0.32530284\n",
      "Iteration 642, loss = 0.32510773\n",
      "Iteration 643, loss = 0.32498686\n",
      "Iteration 644, loss = 0.32483516\n",
      "Iteration 645, loss = 0.32472759\n",
      "Iteration 646, loss = 0.32461493\n",
      "Iteration 647, loss = 0.32445475\n",
      "Iteration 648, loss = 0.32432719\n",
      "Iteration 649, loss = 0.32417761\n",
      "Iteration 650, loss = 0.32404807\n",
      "Iteration 651, loss = 0.32391006\n",
      "Iteration 652, loss = 0.32378889\n",
      "Iteration 653, loss = 0.32364524\n",
      "Iteration 654, loss = 0.32352252\n",
      "Iteration 655, loss = 0.32337695\n",
      "Iteration 656, loss = 0.32326641\n",
      "Iteration 657, loss = 0.32314411\n",
      "Iteration 658, loss = 0.32298094\n",
      "Iteration 659, loss = 0.32287446\n",
      "Iteration 660, loss = 0.32273907\n",
      "Iteration 661, loss = 0.32265579\n",
      "Iteration 662, loss = 0.32246706\n",
      "Iteration 663, loss = 0.32232963\n",
      "Iteration 664, loss = 0.32221947\n",
      "Iteration 665, loss = 0.32206375\n",
      "Iteration 666, loss = 0.32194196\n",
      "Iteration 667, loss = 0.32179784\n",
      "Iteration 668, loss = 0.32168853\n",
      "Iteration 669, loss = 0.32153280\n",
      "Iteration 670, loss = 0.32149378\n",
      "Iteration 671, loss = 0.32127402\n",
      "Iteration 672, loss = 0.32114522\n",
      "Iteration 673, loss = 0.32101396\n",
      "Iteration 674, loss = 0.32088430\n",
      "Iteration 675, loss = 0.32077934\n",
      "Iteration 676, loss = 0.32061842\n",
      "Iteration 677, loss = 0.32051019\n",
      "Iteration 678, loss = 0.32035307\n",
      "Iteration 679, loss = 0.32022897\n",
      "Iteration 680, loss = 0.32011892\n",
      "Iteration 681, loss = 0.31997121\n",
      "Iteration 682, loss = 0.31983452\n",
      "Iteration 683, loss = 0.31972668\n",
      "Iteration 684, loss = 0.31956326\n",
      "Iteration 685, loss = 0.31948842\n",
      "Iteration 686, loss = 0.31928832\n",
      "Iteration 687, loss = 0.31919772\n",
      "Iteration 688, loss = 0.31904864\n",
      "Iteration 689, loss = 0.31891849\n",
      "Iteration 690, loss = 0.31879620\n",
      "Iteration 691, loss = 0.31866216\n",
      "Iteration 692, loss = 0.31852063\n",
      "Iteration 693, loss = 0.31841205\n",
      "Iteration 694, loss = 0.31827606\n",
      "Iteration 695, loss = 0.31814654\n",
      "Iteration 696, loss = 0.31804060\n",
      "Iteration 697, loss = 0.31789261\n",
      "Iteration 698, loss = 0.31774238\n",
      "Iteration 699, loss = 0.31762093\n",
      "Iteration 700, loss = 0.31748580\n",
      "Iteration 701, loss = 0.31740123\n",
      "Iteration 702, loss = 0.31722784\n",
      "Iteration 703, loss = 0.31708432\n",
      "Iteration 704, loss = 0.31694616\n",
      "Iteration 705, loss = 0.31683649\n",
      "Iteration 706, loss = 0.31671516\n",
      "Iteration 707, loss = 0.31657223\n",
      "Iteration 708, loss = 0.31644567\n",
      "Iteration 709, loss = 0.31632087\n",
      "Iteration 710, loss = 0.31621472\n",
      "Iteration 711, loss = 0.31606903\n",
      "Iteration 712, loss = 0.31592522\n",
      "Iteration 713, loss = 0.31585661\n",
      "Iteration 714, loss = 0.31565869\n",
      "Iteration 715, loss = 0.31553534\n",
      "Iteration 716, loss = 0.31542862\n",
      "Iteration 717, loss = 0.31532291\n",
      "Iteration 718, loss = 0.31517623\n",
      "Iteration 719, loss = 0.31504920\n",
      "Iteration 720, loss = 0.31492883\n",
      "Iteration 721, loss = 0.31476070\n",
      "Iteration 722, loss = 0.31464609\n",
      "Iteration 723, loss = 0.31453039\n",
      "Iteration 724, loss = 0.31436259\n",
      "Iteration 725, loss = 0.31423997\n",
      "Iteration 726, loss = 0.31413238\n",
      "Iteration 727, loss = 0.31399025\n",
      "Iteration 728, loss = 0.31385533\n",
      "Iteration 729, loss = 0.31375740\n",
      "Iteration 730, loss = 0.31359534\n",
      "Iteration 731, loss = 0.31348496\n",
      "Iteration 732, loss = 0.31333041\n",
      "Iteration 733, loss = 0.31319043\n",
      "Iteration 734, loss = 0.31304607\n",
      "Iteration 735, loss = 0.31294065\n",
      "Iteration 736, loss = 0.31277396\n",
      "Iteration 737, loss = 0.31271199\n",
      "Iteration 738, loss = 0.31256134\n",
      "Iteration 739, loss = 0.31243293\n",
      "Iteration 740, loss = 0.31229187\n",
      "Iteration 741, loss = 0.31215543\n",
      "Iteration 742, loss = 0.31200505\n",
      "Iteration 743, loss = 0.31188208\n",
      "Iteration 744, loss = 0.31174066\n",
      "Iteration 745, loss = 0.31162612\n",
      "Iteration 746, loss = 0.31153758\n",
      "Iteration 747, loss = 0.31136264\n",
      "Iteration 748, loss = 0.31126702\n",
      "Iteration 749, loss = 0.31111808\n",
      "Iteration 750, loss = 0.31099890\n",
      "Iteration 751, loss = 0.31085251\n",
      "Iteration 752, loss = 0.31071925\n",
      "Iteration 753, loss = 0.31066288\n",
      "Iteration 754, loss = 0.31047564\n",
      "Iteration 755, loss = 0.31032633\n",
      "Iteration 756, loss = 0.31021015\n",
      "Iteration 757, loss = 0.31011081\n",
      "Iteration 758, loss = 0.30996395\n",
      "Iteration 759, loss = 0.30981561\n",
      "Iteration 760, loss = 0.30970027\n",
      "Iteration 761, loss = 0.30961391\n",
      "Iteration 762, loss = 0.30942835\n",
      "Iteration 763, loss = 0.30930335\n",
      "Iteration 764, loss = 0.30919407\n",
      "Iteration 765, loss = 0.30912068\n",
      "Iteration 766, loss = 0.30890881\n",
      "Iteration 767, loss = 0.30882751\n",
      "Iteration 768, loss = 0.30868725\n",
      "Iteration 769, loss = 0.30856405\n",
      "Iteration 770, loss = 0.30845234\n",
      "Iteration 771, loss = 0.30828081\n",
      "Iteration 772, loss = 0.30815738\n",
      "Iteration 773, loss = 0.30802321\n",
      "Iteration 774, loss = 0.30790032\n",
      "Iteration 775, loss = 0.30777835\n",
      "Iteration 776, loss = 0.30764488\n",
      "Iteration 777, loss = 0.30751675\n",
      "Iteration 778, loss = 0.30744529\n",
      "Iteration 779, loss = 0.30725664\n",
      "Iteration 780, loss = 0.30714629\n",
      "Iteration 781, loss = 0.30700031\n",
      "Iteration 782, loss = 0.30691848\n",
      "Iteration 783, loss = 0.30674038\n",
      "Iteration 784, loss = 0.30665062\n",
      "Iteration 785, loss = 0.30650251\n",
      "Iteration 786, loss = 0.30636228\n",
      "Iteration 787, loss = 0.30626771\n",
      "Iteration 788, loss = 0.30613847\n",
      "Iteration 789, loss = 0.30601363\n",
      "Iteration 790, loss = 0.30585962\n",
      "Iteration 791, loss = 0.30574318\n",
      "Iteration 792, loss = 0.30560856\n",
      "Iteration 793, loss = 0.30546679\n",
      "Iteration 794, loss = 0.30534905\n",
      "Iteration 795, loss = 0.30521330\n",
      "Iteration 796, loss = 0.30509417\n",
      "Iteration 797, loss = 0.30496768\n",
      "Iteration 798, loss = 0.30484624\n",
      "Iteration 799, loss = 0.30469796\n",
      "Iteration 800, loss = 0.30459087\n",
      "Iteration 801, loss = 0.30444678\n",
      "Iteration 802, loss = 0.30434885\n",
      "Iteration 803, loss = 0.30421249\n",
      "Iteration 804, loss = 0.30405846\n",
      "Iteration 805, loss = 0.30395214\n",
      "Iteration 806, loss = 0.30379420\n",
      "Iteration 807, loss = 0.30369769\n",
      "Iteration 808, loss = 0.30355086\n",
      "Iteration 809, loss = 0.30341324\n",
      "Iteration 810, loss = 0.30327307\n",
      "Iteration 811, loss = 0.30315338\n",
      "Iteration 812, loss = 0.30305269\n",
      "Iteration 813, loss = 0.30290688\n",
      "Iteration 814, loss = 0.30278238\n",
      "Iteration 815, loss = 0.30271185\n",
      "Iteration 816, loss = 0.30251541\n",
      "Iteration 817, loss = 0.30238547\n",
      "Iteration 818, loss = 0.30226736\n",
      "Iteration 819, loss = 0.30213365\n",
      "Iteration 820, loss = 0.30199986\n",
      "Iteration 821, loss = 0.30189022\n",
      "Iteration 822, loss = 0.30177781\n",
      "Iteration 823, loss = 0.30165159\n",
      "Iteration 824, loss = 0.30151589\n",
      "Iteration 825, loss = 0.30135320\n",
      "Iteration 826, loss = 0.30124345\n",
      "Iteration 827, loss = 0.30111804\n",
      "Iteration 828, loss = 0.30102464\n",
      "Iteration 829, loss = 0.30085569\n",
      "Iteration 830, loss = 0.30072550\n",
      "Iteration 831, loss = 0.30058001\n",
      "Iteration 832, loss = 0.30045774\n",
      "Iteration 833, loss = 0.30033458\n",
      "Iteration 834, loss = 0.30020782\n",
      "Iteration 835, loss = 0.30006788\n",
      "Iteration 836, loss = 0.29995962\n",
      "Iteration 837, loss = 0.29984137\n",
      "Iteration 838, loss = 0.29970941\n",
      "Iteration 839, loss = 0.29956008\n",
      "Iteration 840, loss = 0.29942911\n",
      "Iteration 841, loss = 0.29931661\n",
      "Iteration 842, loss = 0.29917835\n",
      "Iteration 843, loss = 0.29903789\n",
      "Iteration 844, loss = 0.29893773\n",
      "Iteration 845, loss = 0.29879595\n",
      "Iteration 846, loss = 0.29866169\n",
      "Iteration 847, loss = 0.29852737\n",
      "Iteration 848, loss = 0.29841010\n",
      "Iteration 849, loss = 0.29828195\n",
      "Iteration 850, loss = 0.29817119\n",
      "Iteration 851, loss = 0.29801839\n",
      "Iteration 852, loss = 0.29787751\n",
      "Iteration 853, loss = 0.29776926\n",
      "Iteration 854, loss = 0.29765011\n",
      "Iteration 855, loss = 0.29749872\n",
      "Iteration 856, loss = 0.29736918\n",
      "Iteration 857, loss = 0.29723796\n",
      "Iteration 858, loss = 0.29712055\n",
      "Iteration 859, loss = 0.29698018\n",
      "Iteration 860, loss = 0.29686977\n",
      "Iteration 861, loss = 0.29672607\n",
      "Iteration 862, loss = 0.29665526\n",
      "Iteration 863, loss = 0.29647410\n",
      "Iteration 864, loss = 0.29634518\n",
      "Iteration 865, loss = 0.29625013\n",
      "Iteration 866, loss = 0.29607543\n",
      "Iteration 867, loss = 0.29595014\n",
      "Iteration 868, loss = 0.29587249\n",
      "Iteration 869, loss = 0.29570062\n",
      "Iteration 870, loss = 0.29557844\n",
      "Iteration 871, loss = 0.29544546\n",
      "Iteration 872, loss = 0.29531039\n",
      "Iteration 873, loss = 0.29521238\n",
      "Iteration 874, loss = 0.29506187\n",
      "Iteration 875, loss = 0.29499468\n",
      "Iteration 876, loss = 0.29479937\n",
      "Iteration 877, loss = 0.29465206\n",
      "Iteration 878, loss = 0.29457099\n",
      "Iteration 879, loss = 0.29440780\n",
      "Iteration 880, loss = 0.29427263\n",
      "Iteration 881, loss = 0.29415611\n",
      "Iteration 882, loss = 0.29404568\n",
      "Iteration 883, loss = 0.29389733\n",
      "Iteration 884, loss = 0.29376636\n",
      "Iteration 885, loss = 0.29365029\n",
      "Iteration 886, loss = 0.29350016\n",
      "Iteration 887, loss = 0.29336518\n",
      "Iteration 888, loss = 0.29325574\n",
      "Iteration 889, loss = 0.29314483\n",
      "Iteration 890, loss = 0.29298073\n",
      "Iteration 891, loss = 0.29288712\n",
      "Iteration 892, loss = 0.29273856\n",
      "Iteration 893, loss = 0.29265760\n",
      "Iteration 894, loss = 0.29250485\n",
      "Iteration 895, loss = 0.29237643\n",
      "Iteration 896, loss = 0.29226133\n",
      "Iteration 897, loss = 0.29210685\n",
      "Iteration 898, loss = 0.29202329\n",
      "Iteration 899, loss = 0.29184022\n",
      "Iteration 900, loss = 0.29176437\n",
      "Iteration 901, loss = 0.29160322\n",
      "Iteration 902, loss = 0.29147683\n",
      "Iteration 903, loss = 0.29133166\n",
      "Iteration 904, loss = 0.29123830\n",
      "Iteration 905, loss = 0.29111130\n",
      "Iteration 906, loss = 0.29096610\n",
      "Iteration 907, loss = 0.29095723\n",
      "Iteration 908, loss = 0.29071360\n",
      "Iteration 909, loss = 0.29064092\n",
      "Iteration 910, loss = 0.29043428\n",
      "Iteration 911, loss = 0.29031939\n",
      "Iteration 912, loss = 0.29018538\n",
      "Iteration 913, loss = 0.29005137\n",
      "Iteration 914, loss = 0.28996686\n",
      "Iteration 915, loss = 0.28989126\n",
      "Iteration 916, loss = 0.28968663\n",
      "Iteration 917, loss = 0.28957051\n",
      "Iteration 918, loss = 0.28941776\n",
      "Iteration 919, loss = 0.28931416\n",
      "Iteration 920, loss = 0.28917080\n",
      "Iteration 921, loss = 0.28903650\n",
      "Iteration 922, loss = 0.28891133\n",
      "Iteration 923, loss = 0.28880380\n",
      "Iteration 924, loss = 0.28867012\n",
      "Iteration 925, loss = 0.28854420\n",
      "Iteration 926, loss = 0.28840647\n",
      "Iteration 927, loss = 0.28830349\n",
      "Iteration 928, loss = 0.28816309\n",
      "Iteration 929, loss = 0.28802459\n",
      "Iteration 930, loss = 0.28790885\n",
      "Iteration 931, loss = 0.28778885\n",
      "Iteration 932, loss = 0.28761830\n",
      "Iteration 933, loss = 0.28749777\n",
      "Iteration 934, loss = 0.28735900\n",
      "Iteration 935, loss = 0.28725604\n",
      "Iteration 936, loss = 0.28713093\n",
      "Iteration 937, loss = 0.28701897\n",
      "Iteration 938, loss = 0.28686506\n",
      "Iteration 939, loss = 0.28674501\n",
      "Iteration 940, loss = 0.28663465\n",
      "Iteration 941, loss = 0.28649467\n",
      "Iteration 942, loss = 0.28642271\n",
      "Iteration 943, loss = 0.28622653\n",
      "Iteration 944, loss = 0.28609920\n",
      "Iteration 945, loss = 0.28597539\n",
      "Iteration 946, loss = 0.28587057\n",
      "Iteration 947, loss = 0.28572023\n",
      "Iteration 948, loss = 0.28566499\n",
      "Iteration 949, loss = 0.28546972\n",
      "Iteration 950, loss = 0.28534984\n",
      "Iteration 951, loss = 0.28526571\n",
      "Iteration 952, loss = 0.28512591\n",
      "Iteration 953, loss = 0.28500235\n",
      "Iteration 954, loss = 0.28485087\n",
      "Iteration 955, loss = 0.28472104\n",
      "Iteration 956, loss = 0.28459250\n",
      "Iteration 957, loss = 0.28448597\n",
      "Iteration 958, loss = 0.28435540\n",
      "Iteration 959, loss = 0.28422347\n",
      "Iteration 960, loss = 0.28411743\n",
      "Iteration 961, loss = 0.28399434\n",
      "Iteration 962, loss = 0.28390210\n",
      "Iteration 963, loss = 0.28370129\n",
      "Iteration 964, loss = 0.28360174\n",
      "Iteration 965, loss = 0.28345188\n",
      "Iteration 966, loss = 0.28332507\n",
      "Iteration 967, loss = 0.28320958\n",
      "Iteration 968, loss = 0.28313283\n",
      "Iteration 969, loss = 0.28297745\n",
      "Iteration 970, loss = 0.28287077\n",
      "Iteration 971, loss = 0.28271661\n",
      "Iteration 972, loss = 0.28263725\n",
      "Iteration 973, loss = 0.28248774\n",
      "Iteration 974, loss = 0.28235886\n",
      "Iteration 975, loss = 0.28219524\n",
      "Iteration 976, loss = 0.28209414\n",
      "Iteration 977, loss = 0.28195754\n",
      "Iteration 978, loss = 0.28182977\n",
      "Iteration 979, loss = 0.28168521\n",
      "Iteration 980, loss = 0.28156940\n",
      "Iteration 981, loss = 0.28146257\n",
      "Iteration 982, loss = 0.28129889\n",
      "Iteration 983, loss = 0.28119918\n",
      "Iteration 984, loss = 0.28108110\n",
      "Iteration 985, loss = 0.28093590\n",
      "Iteration 986, loss = 0.28085279\n",
      "Iteration 987, loss = 0.28068278\n",
      "Iteration 988, loss = 0.28059640\n",
      "Iteration 989, loss = 0.28045948\n",
      "Iteration 990, loss = 0.28032575\n",
      "Iteration 991, loss = 0.28016649\n",
      "Iteration 992, loss = 0.28010541\n",
      "Iteration 993, loss = 0.27992966\n",
      "Iteration 994, loss = 0.27981708\n",
      "Iteration 995, loss = 0.27967356\n",
      "Iteration 996, loss = 0.27956610\n",
      "Iteration 997, loss = 0.27942992\n",
      "Iteration 998, loss = 0.27932893\n",
      "Iteration 999, loss = 0.27919192\n",
      "Iteration 1000, loss = 0.27904339\n",
      "Iteration 1001, loss = 0.27890706\n",
      "Iteration 1002, loss = 0.27877622\n",
      "Iteration 1003, loss = 0.27865998\n",
      "Iteration 1004, loss = 0.27852724\n",
      "Iteration 1005, loss = 0.27840413\n",
      "Iteration 1006, loss = 0.27830568\n",
      "Iteration 1007, loss = 0.27816577\n",
      "Iteration 1008, loss = 0.27804085\n",
      "Iteration 1009, loss = 0.27788970\n",
      "Iteration 1010, loss = 0.27779357\n",
      "Iteration 1011, loss = 0.27774001\n",
      "Iteration 1012, loss = 0.27751282\n",
      "Iteration 1013, loss = 0.27740922\n",
      "Iteration 1014, loss = 0.27727047\n",
      "Iteration 1015, loss = 0.27716596\n",
      "Iteration 1016, loss = 0.27703122\n",
      "Iteration 1017, loss = 0.27692057\n",
      "Iteration 1018, loss = 0.27679211\n",
      "Iteration 1019, loss = 0.27664720\n",
      "Iteration 1020, loss = 0.27650489\n",
      "Iteration 1021, loss = 0.27637854\n",
      "Iteration 1022, loss = 0.27630851\n",
      "Iteration 1023, loss = 0.27620324\n",
      "Iteration 1024, loss = 0.27599301\n",
      "Iteration 1025, loss = 0.27588672\n",
      "Iteration 1026, loss = 0.27574344\n",
      "Iteration 1027, loss = 0.27562411\n",
      "Iteration 1028, loss = 0.27559453\n",
      "Iteration 1029, loss = 0.27538648\n",
      "Iteration 1030, loss = 0.27524070\n",
      "Iteration 1031, loss = 0.27516292\n",
      "Iteration 1032, loss = 0.27498443\n",
      "Iteration 1033, loss = 0.27495081\n",
      "Iteration 1034, loss = 0.27475369\n",
      "Iteration 1035, loss = 0.27461682\n",
      "Iteration 1036, loss = 0.27448788\n",
      "Iteration 1037, loss = 0.27444242\n",
      "Iteration 1038, loss = 0.27423174\n",
      "Iteration 1039, loss = 0.27411187\n",
      "Iteration 1040, loss = 0.27400256\n",
      "Iteration 1041, loss = 0.27388024\n",
      "Iteration 1042, loss = 0.27381196\n",
      "Iteration 1043, loss = 0.27365253\n",
      "Iteration 1044, loss = 0.27350829\n",
      "Iteration 1045, loss = 0.27334550\n",
      "Iteration 1046, loss = 0.27323697\n",
      "Iteration 1047, loss = 0.27310009\n",
      "Iteration 1048, loss = 0.27301862\n",
      "Iteration 1049, loss = 0.27284888\n",
      "Iteration 1050, loss = 0.27276549\n",
      "Iteration 1051, loss = 0.27260542\n",
      "Iteration 1052, loss = 0.27249639\n",
      "Iteration 1053, loss = 0.27236106\n",
      "Iteration 1054, loss = 0.27223067\n",
      "Iteration 1055, loss = 0.27212370\n",
      "Iteration 1056, loss = 0.27199481\n",
      "Iteration 1057, loss = 0.27189778\n",
      "Iteration 1058, loss = 0.27175373\n",
      "Iteration 1059, loss = 0.27160487\n",
      "Iteration 1060, loss = 0.27147944\n",
      "Iteration 1061, loss = 0.27134069\n",
      "Iteration 1062, loss = 0.27121820\n",
      "Iteration 1063, loss = 0.27111355\n",
      "Iteration 1757, loss = 0.17195319\n",
      "Iteration 1758, loss = 0.17183249\n",
      "Iteration 1759, loss = 0.17177754\n",
      "Iteration 1760, loss = 0.17167787\n",
      "Iteration 1761, loss = 0.17154425\n",
      "Iteration 1762, loss = 0.17147821\n",
      "Iteration 1763, loss = 0.17143182\n",
      "Iteration 1764, loss = 0.17131571\n",
      "Iteration 1765, loss = 0.17123670\n",
      "Iteration 1766, loss = 0.17110976\n",
      "Iteration 1767, loss = 0.17101191\n",
      "Iteration 1768, loss = 0.17092074\n",
      "Iteration 1769, loss = 0.17086140\n",
      "Iteration 1770, loss = 0.17075309\n",
      "Iteration 1771, loss = 0.17067033\n",
      "Iteration 1772, loss = 0.17057950\n",
      "Iteration 1773, loss = 0.17047085\n",
      "Iteration 1774, loss = 0.17035059\n",
      "Iteration 1775, loss = 0.17028970\n",
      "Iteration 1776, loss = 0.17019351\n",
      "Iteration 1777, loss = 0.17013830\n",
      "Iteration 1778, loss = 0.17002298\n",
      "Iteration 1779, loss = 0.17000336\n",
      "Iteration 1780, loss = 0.16985314\n",
      "Iteration 1781, loss = 0.16975091\n",
      "Iteration 1782, loss = 0.16963680\n",
      "Iteration 1783, loss = 0.16956350\n",
      "Iteration 1784, loss = 0.16952341\n",
      "Iteration 1785, loss = 0.16941308\n",
      "Iteration 1786, loss = 0.16929189\n",
      "Iteration 1787, loss = 0.16928959\n",
      "Iteration 1788, loss = 0.16914145\n",
      "Iteration 1789, loss = 0.16900929\n",
      "Iteration 1790, loss = 0.16894901\n",
      "Iteration 1791, loss = 0.16890034\n",
      "Iteration 1792, loss = 0.16877220\n",
      "Iteration 1793, loss = 0.16868181\n",
      "Iteration 1794, loss = 0.16859553\n",
      "Iteration 1795, loss = 0.16854838\n",
      "Iteration 1796, loss = 0.16843637\n",
      "Iteration 1797, loss = 0.16835896\n",
      "Iteration 1798, loss = 0.16822144\n",
      "Iteration 1799, loss = 0.16817372\n",
      "Iteration 1800, loss = 0.16804825\n",
      "Iteration 1801, loss = 0.16798256\n",
      "Iteration 1802, loss = 0.16788235\n",
      "Iteration 1803, loss = 0.16777824\n",
      "Iteration 1804, loss = 0.16776292\n",
      "Iteration 1805, loss = 0.16767191\n",
      "Iteration 1806, loss = 0.16751805\n",
      "Iteration 1807, loss = 0.16746110\n",
      "Iteration 1808, loss = 0.16733804\n",
      "Iteration 1809, loss = 0.16731121\n",
      "Iteration 1810, loss = 0.16728311\n",
      "Iteration 1811, loss = 0.16709729\n",
      "Iteration 1812, loss = 0.16706529\n",
      "Iteration 1813, loss = 0.16690034\n",
      "Iteration 1814, loss = 0.16684008\n",
      "Iteration 1815, loss = 0.16671586\n",
      "Iteration 1816, loss = 0.16665500\n",
      "Iteration 1817, loss = 0.16661220\n",
      "Iteration 1818, loss = 0.16650091\n",
      "Iteration 1819, loss = 0.16643880\n",
      "Iteration 1820, loss = 0.16634002\n",
      "Iteration 1821, loss = 0.16621535\n",
      "Iteration 1822, loss = 0.16615819\n",
      "Iteration 1823, loss = 0.16604205\n",
      "Iteration 1824, loss = 0.16603584\n",
      "Iteration 1825, loss = 0.16586718\n",
      "Iteration 1826, loss = 0.16578698\n",
      "Iteration 1827, loss = 0.16577701\n",
      "Iteration 1828, loss = 0.16560045\n",
      "Iteration 1829, loss = 0.16550839\n",
      "Iteration 1830, loss = 0.16554469\n",
      "Iteration 1831, loss = 0.16536556\n",
      "Iteration 1832, loss = 0.16529434\n",
      "Iteration 1833, loss = 0.16519534\n",
      "Iteration 1834, loss = 0.16507362\n",
      "Iteration 1835, loss = 0.16500611\n",
      "Iteration 1836, loss = 0.16490965\n",
      "Iteration 1837, loss = 0.16484463\n",
      "Iteration 1838, loss = 0.16474893\n",
      "Iteration 1839, loss = 0.16466495\n",
      "Iteration 1840, loss = 0.16456627\n",
      "Iteration 1841, loss = 0.16446341\n",
      "Iteration 1842, loss = 0.16439366\n",
      "Iteration 1843, loss = 0.16430811\n",
      "Iteration 1844, loss = 0.16422011\n",
      "Iteration 1845, loss = 0.16414402\n",
      "Iteration 1846, loss = 0.16409272\n",
      "Iteration 1847, loss = 0.16394622\n",
      "Iteration 1848, loss = 0.16385921\n",
      "Iteration 1849, loss = 0.16382522\n",
      "Iteration 1850, loss = 0.16381019\n",
      "Iteration 1851, loss = 0.16362417\n",
      "Iteration 1852, loss = 0.16360052\n",
      "Iteration 1853, loss = 0.16344589\n",
      "Iteration 1854, loss = 0.16339964\n",
      "Iteration 1855, loss = 0.16328212\n",
      "Iteration 1856, loss = 0.16318731\n",
      "Iteration 1857, loss = 0.16311638\n",
      "Iteration 1858, loss = 0.16300838\n",
      "Iteration 1859, loss = 0.16291902\n",
      "Iteration 1860, loss = 0.16293318\n",
      "Iteration 1861, loss = 0.16290028\n",
      "Iteration 1862, loss = 0.16271463\n",
      "Iteration 1863, loss = 0.16261947\n",
      "Iteration 1864, loss = 0.16261032\n",
      "Iteration 1865, loss = 0.16242255\n",
      "Iteration 1866, loss = 0.16232541\n",
      "Iteration 1867, loss = 0.16226155\n",
      "Iteration 1868, loss = 0.16220004\n",
      "Iteration 1869, loss = 0.16214013\n",
      "Iteration 1870, loss = 0.16207525\n",
      "Iteration 1871, loss = 0.16189462\n",
      "Iteration 1872, loss = 0.16186930\n",
      "Iteration 1873, loss = 0.16186574\n",
      "Iteration 1874, loss = 0.16170442\n",
      "Iteration 1875, loss = 0.16166065\n",
      "Iteration 1876, loss = 0.16147427\n",
      "Iteration 1877, loss = 0.16143670\n",
      "Iteration 1878, loss = 0.16136985\n",
      "Iteration 1879, loss = 0.16123693\n",
      "Iteration 1880, loss = 0.16115213\n",
      "Iteration 1881, loss = 0.16104976\n",
      "Iteration 1882, loss = 0.16096964\n",
      "Iteration 1883, loss = 0.16096837\n",
      "Iteration 1884, loss = 0.16083920\n",
      "Iteration 1885, loss = 0.16085409\n",
      "Iteration 1886, loss = 0.16062310\n",
      "Iteration 1887, loss = 0.16054588\n",
      "Iteration 1888, loss = 0.16051348\n",
      "Iteration 1889, loss = 0.16039371\n",
      "Iteration 1890, loss = 0.16033644\n",
      "Iteration 1891, loss = 0.16022195\n",
      "Iteration 1892, loss = 0.16014604\n",
      "Iteration 1893, loss = 0.16006621\n",
      "Iteration 1894, loss = 0.16004989\n",
      "Iteration 1895, loss = 0.15995740\n",
      "Iteration 1896, loss = 0.15979713\n",
      "Iteration 1897, loss = 0.15972722\n",
      "Iteration 1898, loss = 0.15967744\n",
      "Iteration 1899, loss = 0.15957404\n",
      "Iteration 1900, loss = 0.15945340\n",
      "Iteration 1901, loss = 0.15940390\n",
      "Iteration 1902, loss = 0.15930462\n",
      "Iteration 1903, loss = 0.15921737\n",
      "Iteration 1904, loss = 0.15913650\n",
      "Iteration 1905, loss = 0.15908541\n",
      "Iteration 1906, loss = 0.15902002\n",
      "Iteration 1907, loss = 0.15895058\n",
      "Iteration 1908, loss = 0.15883758\n",
      "Iteration 1909, loss = 0.15872348\n",
      "Iteration 1910, loss = 0.15870301\n",
      "Iteration 1911, loss = 0.15862201\n",
      "Iteration 1912, loss = 0.15848024\n",
      "Iteration 1913, loss = 0.15841303\n",
      "Iteration 1914, loss = 0.15836430\n",
      "Iteration 1915, loss = 0.15824062\n",
      "Iteration 1916, loss = 0.15816221\n",
      "Iteration 1917, loss = 0.15816000\n",
      "Iteration 1918, loss = 0.15800780\n",
      "Iteration 1919, loss = 0.15798167\n",
      "Iteration 1920, loss = 0.15781761\n",
      "Iteration 1921, loss = 0.15784678\n",
      "Iteration 1922, loss = 0.15765405\n",
      "Iteration 1923, loss = 0.15759432\n",
      "Iteration 1924, loss = 0.15757976\n",
      "Iteration 1925, loss = 0.15744235\n",
      "Iteration 1926, loss = 0.15744795\n",
      "Iteration 1927, loss = 0.15731857\n",
      "Iteration 1928, loss = 0.15726747\n",
      "Iteration 1929, loss = 0.15711327\n",
      "Iteration 1930, loss = 0.15704496\n",
      "Iteration 1931, loss = 0.15695694\n",
      "Iteration 1932, loss = 0.15692445\n",
      "Iteration 1933, loss = 0.15685320\n",
      "Iteration 1934, loss = 0.15669185\n",
      "Iteration 1935, loss = 0.15663336\n",
      "Iteration 1936, loss = 0.15657303\n",
      "Iteration 1937, loss = 0.15645267\n",
      "Iteration 1938, loss = 0.15638062\n",
      "Iteration 1939, loss = 0.15631669\n",
      "Iteration 1940, loss = 0.15621779\n",
      "Iteration 1941, loss = 0.15612183\n",
      "Iteration 1942, loss = 0.15604747\n",
      "Iteration 1943, loss = 0.15597439\n",
      "Iteration 1944, loss = 0.15596835\n",
      "Iteration 1945, loss = 0.15585893\n",
      "Iteration 1946, loss = 0.15571492\n",
      "Iteration 1947, loss = 0.15563385\n",
      "Iteration 1948, loss = 0.15560781\n",
      "Iteration 1949, loss = 0.15548878\n",
      "Iteration 1950, loss = 0.15540209\n",
      "Iteration 1951, loss = 0.15536543\n",
      "Iteration 1952, loss = 0.15527688\n",
      "Iteration 1953, loss = 0.15521484\n",
      "Iteration 1954, loss = 0.15516055\n",
      "Iteration 1955, loss = 0.15505582\n",
      "Iteration 1956, loss = 0.15496343\n",
      "Iteration 1957, loss = 0.15488556\n",
      "Iteration 1958, loss = 0.15476391\n",
      "Iteration 1959, loss = 0.15477335\n",
      "Iteration 1960, loss = 0.15469710\n",
      "Iteration 1961, loss = 0.15456356\n",
      "Iteration 1962, loss = 0.15448472\n",
      "Iteration 1963, loss = 0.15441388\n",
      "Iteration 1964, loss = 0.15432876\n",
      "Iteration 1965, loss = 0.15427871\n",
      "Iteration 1966, loss = 0.15417542\n",
      "Iteration 1967, loss = 0.15409512\n",
      "Iteration 1968, loss = 0.15399181\n",
      "Iteration 1969, loss = 0.15393087\n",
      "Iteration 1970, loss = 0.15390956\n",
      "Iteration 1971, loss = 0.15376971\n",
      "Iteration 1972, loss = 0.15369395\n",
      "Iteration 1973, loss = 0.15360899\n",
      "Iteration 1974, loss = 0.15362521\n",
      "Iteration 1975, loss = 0.15346276\n",
      "Iteration 1976, loss = 0.15341329\n",
      "Iteration 1977, loss = 0.15337349\n",
      "Iteration 1978, loss = 0.15324791\n",
      "Iteration 1979, loss = 0.15324154\n",
      "Iteration 1980, loss = 0.15323550\n",
      "Iteration 1981, loss = 0.15300594\n",
      "Iteration 1982, loss = 0.15290931\n",
      "Iteration 1983, loss = 0.15290562\n",
      "Iteration 1984, loss = 0.15279161\n",
      "Iteration 1985, loss = 0.15273744\n",
      "Iteration 1986, loss = 0.15264369\n",
      "Iteration 1987, loss = 0.15254208\n",
      "Iteration 1988, loss = 0.15247558\n",
      "Iteration 1989, loss = 0.15241145\n",
      "Iteration 1990, loss = 0.15234590\n",
      "Iteration 1991, loss = 0.15228116\n",
      "Iteration 1992, loss = 0.15214219\n",
      "Iteration 1993, loss = 0.15207193\n",
      "Iteration 1994, loss = 0.15199210\n",
      "Iteration 1995, loss = 0.15192700\n",
      "Iteration 1996, loss = 0.15184831\n",
      "Iteration 1997, loss = 0.15178030\n",
      "Iteration 1998, loss = 0.15169833\n",
      "Iteration 1999, loss = 0.15165849\n",
      "Iteration 2000, loss = 0.15159142\n",
      "Iteration 2001, loss = 0.15146707\n",
      "Iteration 2002, loss = 0.15147388\n",
      "Iteration 2003, loss = 0.15134547\n",
      "Iteration 2004, loss = 0.15131487\n",
      "Iteration 2005, loss = 0.15115970\n",
      "Iteration 2006, loss = 0.15115161\n",
      "Iteration 2007, loss = 0.15101469\n",
      "Iteration 2008, loss = 0.15097898\n",
      "Iteration 2009, loss = 0.15087977\n",
      "Iteration 2010, loss = 0.15090247\n",
      "Iteration 2011, loss = 0.15072405\n",
      "Iteration 2012, loss = 0.15066692\n",
      "Iteration 2013, loss = 0.15065863\n",
      "Iteration 2014, loss = 0.15053005\n",
      "Iteration 2015, loss = 0.15040570\n",
      "Iteration 2016, loss = 0.15036838\n",
      "Iteration 2017, loss = 0.15031351\n",
      "Iteration 2018, loss = 0.15023860\n",
      "Iteration 2019, loss = 0.15016379\n",
      "Iteration 2020, loss = 0.15010575\n",
      "Iteration 2021, loss = 0.15000966\n",
      "Iteration 2022, loss = 0.14999324\n",
      "Iteration 2023, loss = 0.14981537\n",
      "Iteration 2024, loss = 0.14977769\n",
      "Iteration 2025, loss = 0.14967444\n",
      "Iteration 2026, loss = 0.14962355\n",
      "Iteration 2027, loss = 0.14954914\n",
      "Iteration 2028, loss = 0.14945290\n",
      "Iteration 2029, loss = 0.14938331\n",
      "Iteration 2030, loss = 0.14931791\n",
      "Iteration 2031, loss = 0.14926455\n",
      "Iteration 2032, loss = 0.14916309\n",
      "Iteration 2033, loss = 0.14907382\n",
      "Iteration 2034, loss = 0.14908381\n",
      "Iteration 2035, loss = 0.14895948\n",
      "Iteration 2036, loss = 0.14889615\n",
      "Iteration 2037, loss = 0.14879685\n",
      "Iteration 2038, loss = 0.14878268\n",
      "Iteration 2039, loss = 0.14864651\n",
      "Iteration 2040, loss = 0.14869601\n",
      "Iteration 2041, loss = 0.14850143\n",
      "Iteration 2042, loss = 0.14845242\n",
      "Iteration 2043, loss = 0.14835801\n",
      "Iteration 2044, loss = 0.14831813\n",
      "Iteration 2045, loss = 0.14830458\n",
      "Iteration 2046, loss = 0.14815943\n",
      "Iteration 2047, loss = 0.14806139\n",
      "Iteration 2048, loss = 0.14803175\n",
      "Iteration 2049, loss = 0.14794648\n",
      "Iteration 2050, loss = 0.14788867\n",
      "Iteration 2051, loss = 0.14781547\n",
      "Iteration 2052, loss = 0.14774022\n",
      "Iteration 2053, loss = 0.14764174\n",
      "Iteration 2054, loss = 0.14755619\n",
      "Iteration 2055, loss = 0.14748623\n",
      "Iteration 2056, loss = 0.14738926\n",
      "Iteration 2057, loss = 0.14731158\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75419584\n",
      "Iteration 2, loss = 0.75234109\n",
      "Iteration 3, loss = 0.74957821\n",
      "Iteration 4, loss = 0.74627924\n",
      "Iteration 5, loss = 0.74247065\n",
      "Iteration 6, loss = 0.73827483\n",
      "Iteration 7, loss = 0.73408175\n",
      "Iteration 8, loss = 0.72984880\n",
      "Iteration 9, loss = 0.72559281\n",
      "Iteration 10, loss = 0.72136624\n",
      "Iteration 11, loss = 0.71716616\n",
      "Iteration 12, loss = 0.71302778\n",
      "Iteration 13, loss = 0.70895388\n",
      "Iteration 14, loss = 0.70497842\n",
      "Iteration 15, loss = 0.70107335\n",
      "Iteration 16, loss = 0.69716818\n",
      "Iteration 17, loss = 0.69333141\n",
      "Iteration 18, loss = 0.68960085\n",
      "Iteration 19, loss = 0.68562670\n",
      "Iteration 20, loss = 0.68195601\n",
      "Iteration 21, loss = 0.67814905\n",
      "Iteration 22, loss = 0.67444285\n",
      "Iteration 23, loss = 0.67076977\n",
      "Iteration 24, loss = 0.66703245\n",
      "Iteration 25, loss = 0.66337485\n",
      "Iteration 26, loss = 0.65966128\n",
      "Iteration 27, loss = 0.65597342\n",
      "Iteration 28, loss = 0.65232327\n",
      "Iteration 29, loss = 0.64860155\n",
      "Iteration 30, loss = 0.64495526\n",
      "Iteration 31, loss = 0.64133528\n",
      "Iteration 32, loss = 0.63749029\n",
      "Iteration 33, loss = 0.63392871\n",
      "Iteration 34, loss = 0.63021566\n",
      "Iteration 35, loss = 0.62639714\n",
      "Iteration 36, loss = 0.62279537\n",
      "Iteration 37, loss = 0.61908098\n",
      "Iteration 38, loss = 0.61538211\n",
      "Iteration 39, loss = 0.61175124\n",
      "Iteration 40, loss = 0.60802793\n",
      "Iteration 41, loss = 0.60434714\n",
      "Iteration 42, loss = 0.60065125\n",
      "Iteration 43, loss = 0.59702418\n",
      "Iteration 44, loss = 0.59338721\n",
      "Iteration 45, loss = 0.58966798\n",
      "Iteration 46, loss = 0.58611293\n",
      "Iteration 47, loss = 0.58242850\n",
      "Iteration 48, loss = 0.57892635\n",
      "Iteration 49, loss = 0.57534054\n",
      "Iteration 50, loss = 0.57182321\n",
      "Iteration 51, loss = 0.56824855\n",
      "Iteration 52, loss = 0.56482712\n",
      "Iteration 53, loss = 0.56123984\n",
      "Iteration 54, loss = 0.55783899\n",
      "Iteration 55, loss = 0.55433341\n",
      "Iteration 56, loss = 0.55099431\n",
      "Iteration 57, loss = 0.54768666\n",
      "Iteration 58, loss = 0.54426453\n",
      "Iteration 59, loss = 0.54098443\n",
      "Iteration 60, loss = 0.53778502\n",
      "Iteration 61, loss = 0.53458854\n",
      "Iteration 62, loss = 0.53142706\n",
      "Iteration 63, loss = 0.52826682\n",
      "Iteration 64, loss = 0.52533119\n",
      "Iteration 65, loss = 0.52217422\n",
      "Iteration 66, loss = 0.51921622\n",
      "Iteration 67, loss = 0.51623233\n",
      "Iteration 68, loss = 0.51344768\n",
      "Iteration 69, loss = 0.51059279\n",
      "Iteration 70, loss = 0.50783027\n",
      "Iteration 71, loss = 0.50500642\n",
      "Iteration 72, loss = 0.50232592\n",
      "Iteration 73, loss = 0.49969152\n",
      "Iteration 74, loss = 0.49707324\n",
      "Iteration 75, loss = 0.49453693\n",
      "Iteration 76, loss = 0.49203102\n",
      "Iteration 77, loss = 0.48948799\n",
      "Iteration 78, loss = 0.48713372\n",
      "Iteration 79, loss = 0.48482844\n",
      "Iteration 80, loss = 0.48243633\n",
      "Iteration 81, loss = 0.48024199\n",
      "Iteration 82, loss = 0.47806453\n",
      "Iteration 83, loss = 0.47589808\n",
      "Iteration 84, loss = 0.47366278\n",
      "Iteration 85, loss = 0.47164448\n",
      "Iteration 86, loss = 0.46969599\n",
      "Iteration 87, loss = 0.46771053\n",
      "Iteration 88, loss = 0.46573116\n",
      "Iteration 89, loss = 0.46386014\n",
      "Iteration 90, loss = 0.46207291\n",
      "Iteration 91, loss = 0.46025994\n",
      "Iteration 92, loss = 0.45847362\n",
      "Iteration 93, loss = 0.45680735\n",
      "Iteration 94, loss = 0.45524271\n",
      "Iteration 95, loss = 0.45345766\n",
      "Iteration 96, loss = 0.45193826\n",
      "Iteration 97, loss = 0.45036793\n",
      "Iteration 98, loss = 0.44891244\n",
      "Iteration 99, loss = 0.44733533\n",
      "Iteration 100, loss = 0.44589543\n",
      "Iteration 101, loss = 0.44451565\n",
      "Iteration 102, loss = 0.44318005\n",
      "Iteration 103, loss = 0.44183958\n",
      "Iteration 104, loss = 0.44050245\n",
      "Iteration 105, loss = 0.43921266\n",
      "Iteration 106, loss = 0.43801097\n",
      "Iteration 107, loss = 0.43679085\n",
      "Iteration 108, loss = 0.43556351\n",
      "Iteration 109, loss = 0.43438348\n",
      "Iteration 110, loss = 0.43328160\n",
      "Iteration 111, loss = 0.43216975\n",
      "Iteration 112, loss = 0.43109055\n",
      "Iteration 113, loss = 0.42998801\n",
      "Iteration 114, loss = 0.42894794\n",
      "Iteration 115, loss = 0.42798108\n",
      "Iteration 116, loss = 0.42693762\n",
      "Iteration 117, loss = 0.42598614\n",
      "Iteration 118, loss = 0.42504392\n",
      "Iteration 119, loss = 0.42411601\n",
      "Iteration 120, loss = 0.42321743\n",
      "Iteration 121, loss = 0.42232418\n",
      "Iteration 122, loss = 0.42144570\n",
      "Iteration 123, loss = 0.42057882\n",
      "Iteration 124, loss = 0.41974927\n",
      "Iteration 125, loss = 0.41895653\n",
      "Iteration 126, loss = 0.41809104\n",
      "Iteration 127, loss = 0.41731862\n",
      "Iteration 128, loss = 0.41661102\n",
      "Iteration 129, loss = 0.41574380\n",
      "Iteration 130, loss = 0.41505022\n",
      "Iteration 131, loss = 0.41426327\n",
      "Iteration 132, loss = 0.41352725\n",
      "Iteration 133, loss = 0.41283020\n",
      "Iteration 134, loss = 0.41216034\n",
      "Iteration 135, loss = 0.41147125\n",
      "Iteration 136, loss = 0.41078901\n",
      "Iteration 137, loss = 0.41009215\n",
      "Iteration 138, loss = 0.40942437\n",
      "Iteration 139, loss = 0.40881116\n",
      "Iteration 140, loss = 0.40812873\n",
      "Iteration 141, loss = 0.40749248\n",
      "Iteration 142, loss = 0.40692231\n",
      "Iteration 143, loss = 0.40630456\n",
      "Iteration 144, loss = 0.40568326\n",
      "Iteration 145, loss = 0.40511503\n",
      "Iteration 146, loss = 0.40450242\n",
      "Iteration 147, loss = 0.40395437\n",
      "Iteration 148, loss = 0.40335684\n",
      "Iteration 149, loss = 0.40281890\n",
      "Iteration 150, loss = 0.40223409\n",
      "Iteration 151, loss = 0.40174242\n",
      "Iteration 152, loss = 0.40114490\n",
      "Iteration 153, loss = 0.40062470\n",
      "Iteration 154, loss = 0.40010750\n",
      "Iteration 155, loss = 0.39958313\n",
      "Iteration 156, loss = 0.39907853\n",
      "Iteration 157, loss = 0.39856441\n",
      "Iteration 158, loss = 0.39805108\n",
      "Iteration 159, loss = 0.39760879\n",
      "Iteration 160, loss = 0.39706326\n",
      "Iteration 161, loss = 0.39659389\n",
      "Iteration 162, loss = 0.39611289\n",
      "Iteration 163, loss = 0.39565126\n",
      "Iteration 164, loss = 0.39523316\n",
      "Iteration 165, loss = 0.39475218\n",
      "Iteration 166, loss = 0.39430264\n",
      "Iteration 167, loss = 0.39380631\n",
      "Iteration 168, loss = 0.39335020\n",
      "Iteration 169, loss = 0.39294523\n",
      "Iteration 170, loss = 0.39252720\n",
      "Iteration 171, loss = 0.39207752\n",
      "Iteration 172, loss = 0.39163510\n",
      "Iteration 173, loss = 0.39120638\n",
      "Iteration 174, loss = 0.39080902\n",
      "Iteration 175, loss = 0.39039518\n",
      "Iteration 176, loss = 0.38999543\n",
      "Iteration 177, loss = 0.38955703\n",
      "Iteration 178, loss = 0.38917233\n",
      "Iteration 179, loss = 0.38878622\n",
      "Iteration 180, loss = 0.38838692\n",
      "Iteration 181, loss = 0.38802744\n",
      "Iteration 182, loss = 0.38758436\n",
      "Iteration 183, loss = 0.38718902\n",
      "Iteration 184, loss = 0.38682516\n",
      "Iteration 185, loss = 0.38645992\n",
      "Iteration 1798, loss = 0.12141676\n",
      "Iteration 1799, loss = 0.12124175\n",
      "Iteration 1800, loss = 0.12105233\n",
      "Iteration 1801, loss = 0.12101949\n",
      "Iteration 1802, loss = 0.12078441\n",
      "Iteration 1803, loss = 0.12064476\n",
      "Iteration 1804, loss = 0.12050372\n",
      "Iteration 1805, loss = 0.12038797\n",
      "Iteration 1806, loss = 0.12055019\n",
      "Iteration 1807, loss = 0.12012193\n",
      "Iteration 1808, loss = 0.12006106\n",
      "Iteration 1809, loss = 0.11989335\n",
      "Iteration 1810, loss = 0.11986287\n",
      "Iteration 1811, loss = 0.11978866\n",
      "Iteration 1812, loss = 0.11980805\n",
      "Iteration 1813, loss = 0.11941012\n",
      "Iteration 1814, loss = 0.11931158\n",
      "Iteration 1815, loss = 0.11916049\n",
      "Iteration 1816, loss = 0.11912620\n",
      "Iteration 1817, loss = 0.11908281\n",
      "Iteration 1818, loss = 0.11882835\n",
      "Iteration 1819, loss = 0.11871053\n",
      "Iteration 1820, loss = 0.11868640\n",
      "Iteration 1821, loss = 0.11858859\n",
      "Iteration 1822, loss = 0.11841326\n",
      "Iteration 1823, loss = 0.11816188\n",
      "Iteration 1824, loss = 0.11806713\n",
      "Iteration 1825, loss = 0.11803450\n",
      "Iteration 1826, loss = 0.11784891\n",
      "Iteration 1827, loss = 0.11771416\n",
      "Iteration 1828, loss = 0.11772666\n",
      "Iteration 1829, loss = 0.11761343\n",
      "Iteration 1830, loss = 0.11749912\n",
      "Iteration 1831, loss = 0.11721141\n",
      "Iteration 1832, loss = 0.11725163\n",
      "Iteration 1833, loss = 0.11714317\n",
      "Iteration 1834, loss = 0.11690898\n",
      "Iteration 1835, loss = 0.11703842\n",
      "Iteration 1836, loss = 0.11663738\n",
      "Iteration 1837, loss = 0.11648032\n",
      "Iteration 1838, loss = 0.11636510\n",
      "Iteration 1839, loss = 0.11631529\n",
      "Iteration 1840, loss = 0.11622636\n",
      "Iteration 1841, loss = 0.11606476\n",
      "Iteration 1842, loss = 0.11594414\n",
      "Iteration 1843, loss = 0.11579268\n",
      "Iteration 1844, loss = 0.11582553\n",
      "Iteration 1845, loss = 0.11554310\n",
      "Iteration 1846, loss = 0.11572522\n",
      "Iteration 1847, loss = 0.11538988\n",
      "Iteration 1848, loss = 0.11525499\n",
      "Iteration 1849, loss = 0.11504343\n",
      "Iteration 1850, loss = 0.11498245\n",
      "Iteration 1851, loss = 0.11493610\n",
      "Iteration 1852, loss = 0.11471267\n",
      "Iteration 1853, loss = 0.11480231\n",
      "Iteration 1854, loss = 0.11453020\n",
      "Iteration 1855, loss = 0.11455881\n",
      "Iteration 1856, loss = 0.11430376\n",
      "Iteration 1857, loss = 0.11418649\n",
      "Iteration 1858, loss = 0.11398075\n",
      "Iteration 1859, loss = 0.11395467\n",
      "Iteration 1860, loss = 0.11387552\n",
      "Iteration 1861, loss = 0.11368413\n",
      "Iteration 1862, loss = 0.11357571\n",
      "Iteration 1863, loss = 0.11346140\n",
      "Iteration 1864, loss = 0.11345762\n",
      "Iteration 1865, loss = 0.11317531\n",
      "Iteration 1866, loss = 0.11324840\n",
      "Iteration 1867, loss = 0.11315757\n",
      "Iteration 1868, loss = 0.11299599\n",
      "Iteration 1869, loss = 0.11275889\n",
      "Iteration 1870, loss = 0.11280743\n",
      "Iteration 1871, loss = 0.11245282\n",
      "Iteration 1872, loss = 0.11251587\n",
      "Iteration 1873, loss = 0.11232319\n",
      "Iteration 1874, loss = 0.11223262\n",
      "Iteration 1875, loss = 0.11205132\n",
      "Iteration 1876, loss = 0.11189954\n",
      "Iteration 1877, loss = 0.11196595\n",
      "Iteration 1878, loss = 0.11168192\n",
      "Iteration 1879, loss = 0.11171410\n",
      "Iteration 1880, loss = 0.11147365\n",
      "Iteration 1881, loss = 0.11138596\n",
      "Iteration 1882, loss = 0.11128434\n",
      "Iteration 1883, loss = 0.11110060\n",
      "Iteration 1884, loss = 0.11098872\n",
      "Iteration 1885, loss = 0.11105358\n",
      "Iteration 1886, loss = 0.11091910\n",
      "Iteration 1887, loss = 0.11080302\n",
      "Iteration 1888, loss = 0.11056573\n",
      "Iteration 1889, loss = 0.11041279\n",
      "Iteration 1890, loss = 0.11032615\n",
      "Iteration 1891, loss = 0.11019709\n",
      "Iteration 1892, loss = 0.11009615\n",
      "Iteration 1893, loss = 0.11005610\n",
      "Iteration 1894, loss = 0.10989902\n",
      "Iteration 1895, loss = 0.10981440\n",
      "Iteration 1896, loss = 0.10983354\n",
      "Iteration 1897, loss = 0.10975523\n",
      "Iteration 1898, loss = 0.10945813\n",
      "Iteration 1899, loss = 0.10932023\n",
      "Iteration 1900, loss = 0.10924095\n",
      "Iteration 1901, loss = 0.10918178\n",
      "Iteration 1902, loss = 0.10902014\n",
      "Iteration 1903, loss = 0.10888299\n",
      "Iteration 1904, loss = 0.10888644\n",
      "Iteration 1905, loss = 0.10873548\n",
      "Iteration 1906, loss = 0.10880364\n",
      "Iteration 1907, loss = 0.10849539\n",
      "Iteration 1908, loss = 0.10837152\n",
      "Iteration 1909, loss = 0.10827537\n",
      "Iteration 1910, loss = 0.10818337\n",
      "Iteration 1911, loss = 0.10817652\n",
      "Iteration 1912, loss = 0.10789169\n",
      "Iteration 1913, loss = 0.10783992\n",
      "Iteration 1914, loss = 0.10774453\n",
      "Iteration 1915, loss = 0.10764683\n",
      "Iteration 1916, loss = 0.10752254\n",
      "Iteration 1917, loss = 0.10762071\n",
      "Iteration 1918, loss = 0.10726317\n",
      "Iteration 1919, loss = 0.10723044\n",
      "Iteration 1920, loss = 0.10714842\n",
      "Iteration 1921, loss = 0.10701165\n",
      "Iteration 1922, loss = 0.10689812\n",
      "Iteration 1923, loss = 0.10684148\n",
      "Iteration 1924, loss = 0.10664303\n",
      "Iteration 1925, loss = 0.10669435\n",
      "Iteration 1926, loss = 0.10654831\n",
      "Iteration 1927, loss = 0.10643005\n",
      "Iteration 1928, loss = 0.10631138\n",
      "Iteration 1929, loss = 0.10615489\n",
      "Iteration 1930, loss = 0.10607817\n",
      "Iteration 1931, loss = 0.10609485\n",
      "Iteration 1932, loss = 0.10583605\n",
      "Iteration 1933, loss = 0.10573379\n",
      "Iteration 1934, loss = 0.10570320\n",
      "Iteration 1935, loss = 0.10564092\n",
      "Iteration 1936, loss = 0.10548788\n",
      "Iteration 1937, loss = 0.10528264\n",
      "Iteration 1938, loss = 0.10527361\n",
      "Iteration 1939, loss = 0.10509967\n",
      "Iteration 1940, loss = 0.10491288\n",
      "Iteration 1941, loss = 0.10488605\n",
      "Iteration 1942, loss = 0.10476321\n",
      "Iteration 1943, loss = 0.10470090\n",
      "Iteration 1944, loss = 0.10456363\n",
      "Iteration 1945, loss = 0.10482473\n",
      "Iteration 1946, loss = 0.10446501\n",
      "Iteration 1947, loss = 0.10429744\n",
      "Iteration 1948, loss = 0.10420194\n",
      "Iteration 1949, loss = 0.10404861\n",
      "Iteration 1950, loss = 0.10431407\n",
      "Iteration 1951, loss = 0.10389693\n",
      "Iteration 1952, loss = 0.10377568\n",
      "Iteration 1953, loss = 0.10360104\n",
      "Iteration 1954, loss = 0.10354247\n",
      "Iteration 1955, loss = 0.10346665\n",
      "Iteration 1956, loss = 0.10339371\n",
      "Iteration 1957, loss = 0.10326982\n",
      "Iteration 1958, loss = 0.10314539\n",
      "Iteration 1959, loss = 0.10305829\n",
      "Iteration 1960, loss = 0.10300492\n",
      "Iteration 1961, loss = 0.10284594\n",
      "Iteration 1962, loss = 0.10276896\n",
      "Iteration 1963, loss = 0.10261044\n",
      "Iteration 1964, loss = 0.10249396\n",
      "Iteration 1965, loss = 0.10273350\n",
      "Iteration 1966, loss = 0.10228950\n",
      "Iteration 1967, loss = 0.10218867\n",
      "Iteration 1968, loss = 0.10217190\n",
      "Iteration 1969, loss = 0.10206416\n",
      "Iteration 1970, loss = 0.10188853\n",
      "Iteration 1971, loss = 0.10190657\n",
      "Iteration 1972, loss = 0.10167736\n",
      "Iteration 1973, loss = 0.10166460\n",
      "Iteration 1974, loss = 0.10150715\n",
      "Iteration 1975, loss = 0.10148329\n",
      "Iteration 1976, loss = 0.10148869\n",
      "Iteration 1977, loss = 0.10158643\n",
      "Iteration 1978, loss = 0.10107660\n",
      "Iteration 1979, loss = 0.10102044\n",
      "Iteration 1980, loss = 0.10118896\n",
      "Iteration 1981, loss = 0.10085733\n",
      "Iteration 1982, loss = 0.10089876\n",
      "Iteration 1983, loss = 0.10066856\n",
      "Iteration 1984, loss = 0.10050217\n",
      "Iteration 1985, loss = 0.10041404\n",
      "Iteration 1986, loss = 0.10045034\n",
      "Iteration 1987, loss = 0.10027040\n",
      "Iteration 1988, loss = 0.10026959\n",
      "Iteration 1989, loss = 0.10008379\n",
      "Iteration 1990, loss = 0.09996110\n",
      "Iteration 1991, loss = 0.10014583\n",
      "Iteration 1992, loss = 0.09976694\n",
      "Iteration 1993, loss = 0.09977644\n",
      "Iteration 1994, loss = 0.09960128\n",
      "Iteration 1995, loss = 0.09949091\n",
      "Iteration 1996, loss = 0.09942432\n",
      "Iteration 1997, loss = 0.09935907\n",
      "Iteration 1998, loss = 0.09929425\n",
      "Iteration 1999, loss = 0.09915283\n",
      "Iteration 2000, loss = 0.09906596\n",
      "Iteration 2001, loss = 0.09886879\n",
      "Iteration 2002, loss = 0.09883517\n",
      "Iteration 2003, loss = 0.09877640\n",
      "Iteration 2004, loss = 0.09860598\n",
      "Iteration 2005, loss = 0.09851923\n",
      "Iteration 2006, loss = 0.09845792\n",
      "Iteration 2007, loss = 0.09832889\n",
      "Iteration 2008, loss = 0.09824197\n",
      "Iteration 2009, loss = 0.09827718\n",
      "Iteration 2010, loss = 0.09810320\n",
      "Iteration 2011, loss = 0.09829527\n",
      "Iteration 2012, loss = 0.09787076\n",
      "Iteration 2013, loss = 0.09783473\n",
      "Iteration 2014, loss = 0.09774011\n",
      "Iteration 2015, loss = 0.09759550\n",
      "Iteration 2016, loss = 0.09756932\n",
      "Iteration 2017, loss = 0.09754951\n",
      "Iteration 2018, loss = 0.09733709\n",
      "Iteration 2019, loss = 0.09733317\n",
      "Iteration 2020, loss = 0.09722543\n",
      "Iteration 2021, loss = 0.09707472\n",
      "Iteration 2022, loss = 0.09698960\n",
      "Iteration 2023, loss = 0.09694213\n",
      "Iteration 2024, loss = 0.09686100\n",
      "Iteration 2025, loss = 0.09678318\n",
      "Iteration 2026, loss = 0.09665716\n",
      "Iteration 2027, loss = 0.09653616\n",
      "Iteration 2028, loss = 0.09650289\n",
      "Iteration 2029, loss = 0.09635901\n",
      "Iteration 2030, loss = 0.09628652\n",
      "Iteration 2031, loss = 0.09648090\n",
      "Iteration 2032, loss = 0.09630598\n",
      "Iteration 2033, loss = 0.09601500\n",
      "Iteration 2034, loss = 0.09592910\n",
      "Iteration 2035, loss = 0.09589778\n",
      "Iteration 2036, loss = 0.09580988\n",
      "Iteration 2037, loss = 0.09579455\n",
      "Iteration 2038, loss = 0.09561305\n",
      "Iteration 2039, loss = 0.09548716\n",
      "Iteration 2040, loss = 0.09539675\n",
      "Iteration 2041, loss = 0.09537016\n",
      "Iteration 2042, loss = 0.09547629\n",
      "Iteration 2043, loss = 0.09517813\n",
      "Iteration 2044, loss = 0.09505862\n",
      "Iteration 2045, loss = 0.09507435\n",
      "Iteration 2046, loss = 0.09496334\n",
      "Iteration 2047, loss = 0.09494652\n",
      "Iteration 2048, loss = 0.09478390\n",
      "Iteration 2049, loss = 0.09488198\n",
      "Iteration 2050, loss = 0.09466049\n",
      "Iteration 2051, loss = 0.09461479\n",
      "Iteration 2052, loss = 0.09447364\n",
      "Iteration 2053, loss = 0.09445816\n",
      "Iteration 2054, loss = 0.09429423\n",
      "Iteration 2055, loss = 0.09416456\n",
      "Iteration 2056, loss = 0.09411562\n",
      "Iteration 2057, loss = 0.09403466\n",
      "Iteration 2058, loss = 0.09389259\n",
      "Iteration 2059, loss = 0.09381845\n",
      "Iteration 2060, loss = 0.09379913\n",
      "Iteration 2061, loss = 0.09365226\n",
      "Iteration 2062, loss = 0.09355493\n",
      "Iteration 2063, loss = 0.09358196\n",
      "Iteration 2064, loss = 0.09344385\n",
      "Iteration 2065, loss = 0.09331648\n",
      "Iteration 2066, loss = 0.09332541\n",
      "Iteration 2067, loss = 0.09316879\n",
      "Iteration 2068, loss = 0.09322423\n",
      "Iteration 2069, loss = 0.09303711\n",
      "Iteration 2070, loss = 0.09310710\n",
      "Iteration 2071, loss = 0.09300513\n",
      "Iteration 2072, loss = 0.09280097\n",
      "Iteration 2073, loss = 0.09276939\n",
      "Iteration 2074, loss = 0.09258340\n",
      "Iteration 2075, loss = 0.09247917\n",
      "Iteration 2076, loss = 0.09243059\n",
      "Iteration 2077, loss = 0.09248654\n",
      "Iteration 2078, loss = 0.09246359\n",
      "Iteration 2079, loss = 0.09228525\n",
      "Iteration 2080, loss = 0.09218325\n",
      "Iteration 2081, loss = 0.09208473\n",
      "Iteration 2082, loss = 0.09210365\n",
      "Iteration 2083, loss = 0.09206120\n",
      "Iteration 2084, loss = 0.09180730\n",
      "Iteration 2085, loss = 0.09184624\n",
      "Iteration 2086, loss = 0.09166468\n",
      "Iteration 2087, loss = 0.09157319\n",
      "Iteration 2088, loss = 0.09154962\n",
      "Iteration 2089, loss = 0.09142097\n",
      "Iteration 2090, loss = 0.09140184\n",
      "Iteration 2091, loss = 0.09126734\n",
      "Iteration 2092, loss = 0.09119160\n",
      "Iteration 2093, loss = 0.09115248\n",
      "Iteration 2094, loss = 0.09110361\n",
      "Iteration 2095, loss = 0.09098883\n",
      "Iteration 2096, loss = 0.09087826\n",
      "Iteration 2097, loss = 0.09084004\n",
      "Iteration 2098, loss = 0.09079378\n",
      "Iteration 2099, loss = 0.09059995\n",
      "Iteration 2100, loss = 0.09051703\n",
      "Iteration 2101, loss = 0.09063607\n",
      "Iteration 2102, loss = 0.09045690\n",
      "Iteration 2103, loss = 0.09032759\n",
      "Iteration 2104, loss = 0.09029533\n",
      "Iteration 2105, loss = 0.09015920\n",
      "Iteration 2106, loss = 0.09007822\n",
      "Iteration 2107, loss = 0.09000461\n",
      "Iteration 2108, loss = 0.09005397\n",
      "Iteration 2109, loss = 0.08984151\n",
      "Iteration 2110, loss = 0.08980272\n",
      "Iteration 2111, loss = 0.08973348\n",
      "Iteration 2112, loss = 0.08959576\n",
      "Iteration 2113, loss = 0.08959723\n",
      "Iteration 2114, loss = 0.08946522\n",
      "Iteration 2115, loss = 0.08946940\n",
      "Iteration 2116, loss = 0.08935859\n",
      "Iteration 2117, loss = 0.08931446\n",
      "Iteration 2118, loss = 0.08921492\n",
      "Iteration 2119, loss = 0.08921918\n",
      "Iteration 2120, loss = 0.08912477\n",
      "Iteration 2121, loss = 0.08899624\n",
      "Iteration 2122, loss = 0.08892451\n",
      "Iteration 2123, loss = 0.08889922\n",
      "Iteration 2124, loss = 0.08876938\n",
      "Iteration 2125, loss = 0.08882192\n",
      "Iteration 2126, loss = 0.08861278\n",
      "Iteration 2127, loss = 0.08853201\n",
      "Iteration 2128, loss = 0.08849684\n",
      "Iteration 2129, loss = 0.08840922\n",
      "Iteration 2130, loss = 0.08829211\n",
      "Iteration 2131, loss = 0.08827222\n",
      "Iteration 2132, loss = 0.08815872\n",
      "Iteration 2133, loss = 0.08811080\n",
      "Iteration 2134, loss = 0.08801230\n",
      "Iteration 2135, loss = 0.08802215\n",
      "Iteration 2136, loss = 0.08804458\n",
      "Iteration 2137, loss = 0.08780884\n",
      "Iteration 2138, loss = 0.08773368\n",
      "Iteration 2139, loss = 0.08775276\n",
      "Iteration 2140, loss = 0.08787486\n",
      "Iteration 2141, loss = 0.08755683\n",
      "Iteration 2142, loss = 0.08744208\n",
      "Iteration 2143, loss = 0.08738133\n",
      "Iteration 2144, loss = 0.08733250\n",
      "Iteration 2145, loss = 0.08725606\n",
      "Iteration 2146, loss = 0.08724279\n",
      "Iteration 2147, loss = 0.08708030\n",
      "Iteration 2148, loss = 0.08711767\n",
      "Iteration 2149, loss = 0.08700772\n",
      "Iteration 2150, loss = 0.08693014\n",
      "Iteration 2151, loss = 0.08681186\n",
      "Iteration 2152, loss = 0.08675549\n",
      "Iteration 2153, loss = 0.08664852\n",
      "Iteration 2154, loss = 0.08671307\n",
      "Iteration 2155, loss = 0.08654791\n",
      "Iteration 2156, loss = 0.08646606\n",
      "Iteration 2157, loss = 0.08643136\n",
      "Iteration 2158, loss = 0.08635101\n",
      "Iteration 2159, loss = 0.08628730\n",
      "Iteration 2160, loss = 0.08622015\n",
      "Iteration 2161, loss = 0.08622607\n",
      "Iteration 2162, loss = 0.08603709\n",
      "Iteration 2163, loss = 0.08606682\n",
      "Iteration 2164, loss = 0.08596491\n",
      "Iteration 2165, loss = 0.08595415\n",
      "Iteration 2166, loss = 0.08583248\n",
      "Iteration 2167, loss = 0.08590513\n",
      "Iteration 2168, loss = 0.08567312\n",
      "Iteration 2169, loss = 0.08568254\n",
      "Iteration 2170, loss = 0.08553842\n",
      "Iteration 2171, loss = 0.08548011\n",
      "Iteration 2172, loss = 0.08545930\n",
      "Iteration 2173, loss = 0.08537283\n",
      "Iteration 2174, loss = 0.08525549\n",
      "Iteration 2175, loss = 0.08527445\n",
      "Iteration 2176, loss = 0.08525913\n",
      "Iteration 2177, loss = 0.08515506\n",
      "Iteration 2178, loss = 0.08503617\n",
      "Iteration 2179, loss = 0.08498141\n",
      "Iteration 2180, loss = 0.08492468\n",
      "Iteration 2181, loss = 0.08483846\n",
      "Iteration 2182, loss = 0.08485749\n",
      "Iteration 2183, loss = 0.08474159\n",
      "Iteration 2184, loss = 0.08461995\n",
      "Iteration 2185, loss = 0.08456379\n",
      "Iteration 2186, loss = 0.08453632\n",
      "Iteration 2187, loss = 0.08445593\n",
      "Iteration 2188, loss = 0.08440737\n",
      "Iteration 2189, loss = 0.08447617\n",
      "Iteration 2190, loss = 0.08429563\n",
      "Iteration 2191, loss = 0.08416924\n",
      "Iteration 2192, loss = 0.08416267\n",
      "Iteration 2193, loss = 0.08417726\n",
      "Iteration 2194, loss = 0.08401976\n",
      "Iteration 2195, loss = 0.08402504\n",
      "Iteration 2196, loss = 0.08386019\n",
      "Iteration 2197, loss = 0.08398805\n",
      "Iteration 2198, loss = 0.08372758\n",
      "Iteration 2199, loss = 0.08370468\n",
      "Iteration 2200, loss = 0.08367549\n",
      "Iteration 2201, loss = 0.08357553\n",
      "Iteration 2202, loss = 0.08359404\n",
      "Iteration 2203, loss = 0.08344213\n",
      "Iteration 2204, loss = 0.08343940\n",
      "Iteration 2205, loss = 0.08338903\n",
      "Iteration 2206, loss = 0.08341040\n",
      "Iteration 2207, loss = 0.08325974\n",
      "Iteration 2208, loss = 0.08327976\n",
      "Iteration 2209, loss = 0.08306954\n",
      "Iteration 2210, loss = 0.08302239\n",
      "Iteration 2211, loss = 0.08297734\n",
      "Iteration 2212, loss = 0.08298389\n",
      "Iteration 2213, loss = 0.08293297\n",
      "Iteration 2214, loss = 0.08279005\n",
      "Iteration 2215, loss = 0.08273311\n",
      "Iteration 2216, loss = 0.08272785\n",
      "Iteration 2217, loss = 0.08263693\n",
      "Iteration 2218, loss = 0.08257715\n",
      "Iteration 2219, loss = 0.08249990\n",
      "Iteration 2220, loss = 0.08252675\n",
      "Iteration 2221, loss = 0.08236522\n",
      "Iteration 2222, loss = 0.08232148\n",
      "Iteration 2223, loss = 0.08227683\n",
      "Iteration 2224, loss = 0.08220847\n",
      "Iteration 2225, loss = 0.08211947\n",
      "Iteration 2226, loss = 0.08219675\n",
      "Iteration 2227, loss = 0.08216769\n",
      "Iteration 2228, loss = 0.08201495\n",
      "Iteration 2229, loss = 0.08190172\n",
      "Iteration 2230, loss = 0.08197752\n",
      "Iteration 2231, loss = 0.08182726\n",
      "Iteration 2232, loss = 0.08174939\n",
      "Iteration 2233, loss = 0.08175358\n",
      "Iteration 2234, loss = 0.08168520\n",
      "Iteration 2235, loss = 0.08159859\n",
      "Iteration 2236, loss = 0.08156732\n",
      "Iteration 2237, loss = 0.08159949\n",
      "Iteration 2238, loss = 0.08141896\n",
      "Iteration 2239, loss = 0.08134434\n",
      "Iteration 2240, loss = 0.08132762\n",
      "Iteration 2241, loss = 0.08124984\n",
      "Iteration 2242, loss = 0.08123551\n",
      "Iteration 2243, loss = 0.08111370\n",
      "Iteration 2244, loss = 0.08110179\n",
      "Iteration 2245, loss = 0.08112593\n",
      "Iteration 2246, loss = 0.08098774\n",
      "Iteration 2247, loss = 0.08092749\n",
      "Iteration 2248, loss = 0.08087563\n",
      "Iteration 2249, loss = 0.08077684\n",
      "Iteration 2250, loss = 0.08081026\n",
      "Iteration 2251, loss = 0.08068830\n",
      "Iteration 2252, loss = 0.08061229\n",
      "Iteration 2253, loss = 0.08066361\n",
      "Iteration 2254, loss = 0.08055756\n",
      "Iteration 2255, loss = 0.08057623\n",
      "Iteration 2256, loss = 0.08042678\n",
      "Iteration 2257, loss = 0.08035086\n",
      "Iteration 2258, loss = 0.08044029\n",
      "Iteration 2259, loss = 0.08029912\n",
      "Iteration 2260, loss = 0.08027346\n",
      "Iteration 2261, loss = 0.08015975\n",
      "Iteration 2262, loss = 0.08008483\n",
      "Iteration 2263, loss = 0.08003464\n",
      "Iteration 2264, loss = 0.08001678\n",
      "Iteration 2265, loss = 0.07999332\n",
      "Iteration 2266, loss = 0.07991031\n",
      "Iteration 2267, loss = 0.07982056\n",
      "Iteration 2268, loss = 0.07977984\n",
      "Iteration 2269, loss = 0.07977829\n",
      "Iteration 2270, loss = 0.07969667\n",
      "Iteration 2271, loss = 0.07970702\n",
      "Iteration 2272, loss = 0.07954779\n",
      "Iteration 2273, loss = 0.07949795\n",
      "Iteration 2274, loss = 0.07945387\n",
      "Iteration 2275, loss = 0.07940712\n",
      "Iteration 2276, loss = 0.07942370\n",
      "Iteration 2277, loss = 0.07935594\n",
      "Iteration 1784, loss = 0.18639594\n",
      "Iteration 1785, loss = 0.18632541\n",
      "Iteration 1786, loss = 0.18620854\n",
      "Iteration 1787, loss = 0.18613744\n",
      "Iteration 1788, loss = 0.18602025\n",
      "Iteration 1789, loss = 0.18596821\n",
      "Iteration 1790, loss = 0.18583585\n",
      "Iteration 1791, loss = 0.18595667\n",
      "Iteration 1792, loss = 0.18576750\n",
      "Iteration 1793, loss = 0.18562666\n",
      "Iteration 1794, loss = 0.18547373\n",
      "Iteration 1795, loss = 0.18537338\n",
      "Iteration 1796, loss = 0.18526637\n",
      "Iteration 1797, loss = 0.18533738\n",
      "Iteration 1798, loss = 0.18509834\n",
      "Iteration 1799, loss = 0.18500018\n",
      "Iteration 1800, loss = 0.18493474\n",
      "Iteration 1801, loss = 0.18480146\n",
      "Iteration 1802, loss = 0.18472022\n",
      "Iteration 1803, loss = 0.18456083\n",
      "Iteration 1804, loss = 0.18446501\n",
      "Iteration 1805, loss = 0.18437022\n",
      "Iteration 1806, loss = 0.18428424\n",
      "Iteration 1807, loss = 0.18420186\n",
      "Iteration 1808, loss = 0.18405939\n",
      "Iteration 1809, loss = 0.18401168\n",
      "Iteration 1810, loss = 0.18393462\n",
      "Iteration 1811, loss = 0.18378364\n",
      "Iteration 1812, loss = 0.18382300\n",
      "Iteration 1813, loss = 0.18359993\n",
      "Iteration 1814, loss = 0.18349657\n",
      "Iteration 1815, loss = 0.18339411\n",
      "Iteration 1816, loss = 0.18333481\n",
      "Iteration 1817, loss = 0.18320821\n",
      "Iteration 1818, loss = 0.18312388\n",
      "Iteration 1819, loss = 0.18309242\n",
      "Iteration 1820, loss = 0.18293013\n",
      "Iteration 1821, loss = 0.18285986\n",
      "Iteration 1822, loss = 0.18272836\n",
      "Iteration 1823, loss = 0.18260536\n",
      "Iteration 1824, loss = 0.18253202\n",
      "Iteration 1825, loss = 0.18246084\n",
      "Iteration 1826, loss = 0.18234790\n",
      "Iteration 1827, loss = 0.18225915\n",
      "Iteration 1828, loss = 0.18214150\n",
      "Iteration 1829, loss = 0.18207983\n",
      "Iteration 1830, loss = 0.18197122\n",
      "Iteration 1831, loss = 0.18196000\n",
      "Iteration 1832, loss = 0.18175830\n",
      "Iteration 1833, loss = 0.18171154\n",
      "Iteration 1834, loss = 0.18169477\n",
      "Iteration 1835, loss = 0.18148310\n",
      "Iteration 1836, loss = 0.18148774\n",
      "Iteration 1837, loss = 0.18130697\n",
      "Iteration 1838, loss = 0.18123936\n",
      "Iteration 1839, loss = 0.18110747\n",
      "Iteration 1840, loss = 0.18101299\n",
      "Iteration 1841, loss = 0.18098184\n",
      "Iteration 1842, loss = 0.18083100\n",
      "Iteration 1843, loss = 0.18073924\n",
      "Iteration 1844, loss = 0.18071224\n",
      "Iteration 1845, loss = 0.18050241\n",
      "Iteration 1846, loss = 0.18043747\n",
      "Iteration 1847, loss = 0.18031504\n",
      "Iteration 1848, loss = 0.18024848\n",
      "Iteration 1849, loss = 0.18017271\n",
      "Iteration 1850, loss = 0.18008085\n",
      "Iteration 1851, loss = 0.18002130\n",
      "Iteration 1852, loss = 0.17989542\n",
      "Iteration 1853, loss = 0.17976455\n",
      "Iteration 1854, loss = 0.17974020\n",
      "Iteration 1855, loss = 0.17958215\n",
      "Iteration 1856, loss = 0.17954802\n",
      "Iteration 1857, loss = 0.17943266\n",
      "Iteration 1858, loss = 0.17931534\n",
      "Iteration 1859, loss = 0.17924364\n",
      "Iteration 1860, loss = 0.17908096\n",
      "Iteration 1861, loss = 0.17904412\n",
      "Iteration 1862, loss = 0.17901467\n",
      "Iteration 1863, loss = 0.17886097\n",
      "Iteration 1864, loss = 0.17879536\n",
      "Iteration 1865, loss = 0.17873264\n",
      "Iteration 1866, loss = 0.17859290\n",
      "Iteration 1867, loss = 0.17846076\n",
      "Iteration 1868, loss = 0.17838142\n",
      "Iteration 1869, loss = 0.17829251\n",
      "Iteration 1870, loss = 0.17825407\n",
      "Iteration 1871, loss = 0.17809139\n",
      "Iteration 1872, loss = 0.17807567\n",
      "Iteration 1873, loss = 0.17793307\n",
      "Iteration 1874, loss = 0.17778479\n",
      "Iteration 1875, loss = 0.17770965\n",
      "Iteration 1876, loss = 0.17762061\n",
      "Iteration 1877, loss = 0.17749537\n",
      "Iteration 1878, loss = 0.17750831\n",
      "Iteration 1879, loss = 0.17734005\n",
      "Iteration 1880, loss = 0.17732107\n",
      "Iteration 1881, loss = 0.17719310\n",
      "Iteration 1882, loss = 0.17703795\n",
      "Iteration 1883, loss = 0.17696633\n",
      "Iteration 1884, loss = 0.17685628\n",
      "Iteration 1885, loss = 0.17681780\n",
      "Iteration 1886, loss = 0.17670495\n",
      "Iteration 1887, loss = 0.17658938\n",
      "Iteration 1888, loss = 0.17660253\n",
      "Iteration 1889, loss = 0.17644139\n",
      "Iteration 1890, loss = 0.17634396\n",
      "Iteration 1891, loss = 0.17622824\n",
      "Iteration 1892, loss = 0.17624586\n",
      "Iteration 1893, loss = 0.17606926\n",
      "Iteration 1894, loss = 0.17594477\n",
      "Iteration 1895, loss = 0.17584307\n",
      "Iteration 1896, loss = 0.17579050\n",
      "Iteration 1897, loss = 0.17565840\n",
      "Iteration 1898, loss = 0.17560202\n",
      "Iteration 1899, loss = 0.17548002\n",
      "Iteration 1900, loss = 0.17546128\n",
      "Iteration 1901, loss = 0.17537649\n",
      "Iteration 1902, loss = 0.17540495\n",
      "Iteration 1903, loss = 0.17521839\n",
      "Iteration 1904, loss = 0.17513883\n",
      "Iteration 1905, loss = 0.17493564\n",
      "Iteration 1906, loss = 0.17488159\n",
      "Iteration 1907, loss = 0.17480289\n",
      "Iteration 1908, loss = 0.17474813\n",
      "Iteration 1909, loss = 0.17455688\n",
      "Iteration 1910, loss = 0.17452482\n",
      "Iteration 1911, loss = 0.17440338\n",
      "Iteration 1912, loss = 0.17429876\n",
      "Iteration 1913, loss = 0.17422131\n",
      "Iteration 1914, loss = 0.17413927\n",
      "Iteration 1915, loss = 0.17409419\n",
      "Iteration 1916, loss = 0.17394848\n",
      "Iteration 1917, loss = 0.17382985\n",
      "Iteration 1918, loss = 0.17378222\n",
      "Iteration 1919, loss = 0.17367138\n",
      "Iteration 1920, loss = 0.17357765\n",
      "Iteration 1921, loss = 0.17355131\n",
      "Iteration 1922, loss = 0.17348849\n",
      "Iteration 1923, loss = 0.17330813\n",
      "Iteration 1924, loss = 0.17320588\n",
      "Iteration 1925, loss = 0.17316891\n",
      "Iteration 1926, loss = 0.17305050\n",
      "Iteration 1927, loss = 0.17294493\n",
      "Iteration 1928, loss = 0.17290366\n",
      "Iteration 1929, loss = 0.17280539\n",
      "Iteration 1930, loss = 0.17269993\n",
      "Iteration 1931, loss = 0.17260672\n",
      "Iteration 1932, loss = 0.17250125\n",
      "Iteration 1933, loss = 0.17241187\n",
      "Iteration 1934, loss = 0.17236502\n",
      "Iteration 1935, loss = 0.17227080\n",
      "Iteration 1936, loss = 0.17214721\n",
      "Iteration 1937, loss = 0.17215861\n",
      "Iteration 1938, loss = 0.17196811\n",
      "Iteration 1939, loss = 0.17191205\n",
      "Iteration 1940, loss = 0.17181010\n",
      "Iteration 1941, loss = 0.17179402\n",
      "Iteration 1942, loss = 0.17164029\n",
      "Iteration 1943, loss = 0.17154153\n",
      "Iteration 1944, loss = 0.17153626\n",
      "Iteration 1945, loss = 0.17133859\n",
      "Iteration 1946, loss = 0.17133203\n",
      "Iteration 1947, loss = 0.17114433\n",
      "Iteration 1948, loss = 0.17107992\n",
      "Iteration 1949, loss = 0.17101130\n",
      "Iteration 1950, loss = 0.17091131\n",
      "Iteration 1951, loss = 0.17081563\n",
      "Iteration 1952, loss = 0.17075047\n",
      "Iteration 1953, loss = 0.17066667\n",
      "Iteration 1954, loss = 0.17059252\n",
      "Iteration 1955, loss = 0.17061948\n",
      "Iteration 1956, loss = 0.17039850\n",
      "Iteration 1957, loss = 0.17034570\n",
      "Iteration 1958, loss = 0.17031145\n",
      "Iteration 1959, loss = 0.17013003\n",
      "Iteration 1960, loss = 0.17009820\n",
      "Iteration 1961, loss = 0.16996494\n",
      "Iteration 1962, loss = 0.16999394\n",
      "Iteration 1963, loss = 0.16984910\n",
      "Iteration 1964, loss = 0.16972494\n",
      "Iteration 1965, loss = 0.16959710\n",
      "Iteration 1966, loss = 0.16953364\n",
      "Iteration 1967, loss = 0.16942886\n",
      "Iteration 1968, loss = 0.16935147\n",
      "Iteration 1969, loss = 0.16930075\n",
      "Iteration 1970, loss = 0.16916416\n",
      "Iteration 1971, loss = 0.16909278\n",
      "Iteration 1972, loss = 0.16903952\n",
      "Iteration 1973, loss = 0.16891697\n",
      "Iteration 1974, loss = 0.16884872\n",
      "Iteration 1975, loss = 0.16878001\n",
      "Iteration 1976, loss = 0.16864818\n",
      "Iteration 1977, loss = 0.16859210\n",
      "Iteration 1978, loss = 0.16852264\n",
      "Iteration 1979, loss = 0.16847350\n",
      "Iteration 1980, loss = 0.16835594\n",
      "Iteration 1981, loss = 0.16822626\n",
      "Iteration 1982, loss = 0.16815959\n",
      "Iteration 1983, loss = 0.16807799\n",
      "Iteration 1984, loss = 0.16814711\n",
      "Iteration 1985, loss = 0.16794468\n",
      "Iteration 1986, loss = 0.16781113\n",
      "Iteration 1987, loss = 0.16773709\n",
      "Iteration 1988, loss = 0.16763317\n",
      "Iteration 1989, loss = 0.16754094\n",
      "Iteration 1990, loss = 0.16749356\n",
      "Iteration 1991, loss = 0.16740043\n",
      "Iteration 1992, loss = 0.16728791\n",
      "Iteration 1993, loss = 0.16742603\n",
      "Iteration 1994, loss = 0.16711286\n",
      "Iteration 1995, loss = 0.16713532\n",
      "Iteration 1996, loss = 0.16701359\n",
      "Iteration 1997, loss = 0.16688500\n",
      "Iteration 1998, loss = 0.16680201\n",
      "Iteration 1999, loss = 0.16672498\n",
      "Iteration 2000, loss = 0.16660081\n",
      "Iteration 2001, loss = 0.16658941\n",
      "Iteration 2002, loss = 0.16645155\n",
      "Iteration 2003, loss = 0.16644897\n",
      "Iteration 2004, loss = 0.16630226\n",
      "Iteration 2005, loss = 0.16632021\n",
      "Iteration 2006, loss = 0.16615486\n",
      "Iteration 2007, loss = 0.16604895\n",
      "Iteration 2008, loss = 0.16595762\n",
      "Iteration 2009, loss = 0.16586199\n",
      "Iteration 2010, loss = 0.16591139\n",
      "Iteration 2011, loss = 0.16575640\n",
      "Iteration 2012, loss = 0.16564423\n",
      "Iteration 2013, loss = 0.16556359\n",
      "Iteration 2014, loss = 0.16545333\n",
      "Iteration 2015, loss = 0.16544013\n",
      "Iteration 2016, loss = 0.16543554\n",
      "Iteration 2017, loss = 0.16515548\n",
      "Iteration 2018, loss = 0.16520255\n",
      "Iteration 2019, loss = 0.16500686\n",
      "Iteration 2020, loss = 0.16496139\n",
      "Iteration 2021, loss = 0.16487441\n",
      "Iteration 2022, loss = 0.16475988\n",
      "Iteration 2023, loss = 0.16469206\n",
      "Iteration 2024, loss = 0.16469178\n",
      "Iteration 2025, loss = 0.16450473\n",
      "Iteration 2026, loss = 0.16443551\n",
      "Iteration 2027, loss = 0.16438513\n",
      "Iteration 2028, loss = 0.16432888\n",
      "Iteration 2029, loss = 0.16423049\n",
      "Iteration 2030, loss = 0.16408299\n",
      "Iteration 2031, loss = 0.16401739\n",
      "Iteration 2032, loss = 0.16395970\n",
      "Iteration 2033, loss = 0.16387291\n",
      "Iteration 2034, loss = 0.16375473\n",
      "Iteration 2035, loss = 0.16371346\n",
      "Iteration 2036, loss = 0.16358898\n",
      "Iteration 2037, loss = 0.16351981\n",
      "Iteration 2038, loss = 0.16348966\n",
      "Iteration 2039, loss = 0.16335922\n",
      "Iteration 2040, loss = 0.16329516\n",
      "Iteration 2041, loss = 0.16322409\n",
      "Iteration 2042, loss = 0.16314757\n",
      "Iteration 2043, loss = 0.16303000\n",
      "Iteration 2044, loss = 0.16295127\n",
      "Iteration 2045, loss = 0.16285103\n",
      "Iteration 2046, loss = 0.16281606\n",
      "Iteration 2047, loss = 0.16272454\n",
      "Iteration 2048, loss = 0.16262370\n",
      "Iteration 2049, loss = 0.16257146\n",
      "Iteration 2050, loss = 0.16253189\n",
      "Iteration 2051, loss = 0.16237766\n",
      "Iteration 2052, loss = 0.16228537\n",
      "Iteration 2053, loss = 0.16230492\n",
      "Iteration 2054, loss = 0.16210030\n",
      "Iteration 2055, loss = 0.16203201\n",
      "Iteration 2056, loss = 0.16195115\n",
      "Iteration 2057, loss = 0.16193500\n",
      "Iteration 2058, loss = 0.16188193\n",
      "Iteration 2059, loss = 0.16205435\n",
      "Iteration 2060, loss = 0.16163034\n",
      "Iteration 2061, loss = 0.16159859\n",
      "Iteration 2062, loss = 0.16145482\n",
      "Iteration 2063, loss = 0.16136254\n",
      "Iteration 2064, loss = 0.16129355\n",
      "Iteration 2065, loss = 0.16118821\n",
      "Iteration 2066, loss = 0.16115127\n",
      "Iteration 2067, loss = 0.16114470\n",
      "Iteration 2068, loss = 0.16095278\n",
      "Iteration 2069, loss = 0.16090709\n",
      "Iteration 2070, loss = 0.16091829\n",
      "Iteration 2071, loss = 0.16072735\n",
      "Iteration 2072, loss = 0.16062139\n",
      "Iteration 2073, loss = 0.16059024\n",
      "Iteration 2074, loss = 0.16045056\n",
      "Iteration 2075, loss = 0.16043380\n",
      "Iteration 2076, loss = 0.16029351\n",
      "Iteration 2077, loss = 0.16028587\n",
      "Iteration 2078, loss = 0.16013123\n",
      "Iteration 2079, loss = 0.16011782\n",
      "Iteration 2080, loss = 0.15997676\n",
      "Iteration 2081, loss = 0.15991146\n",
      "Iteration 2082, loss = 0.15984135\n",
      "Iteration 2083, loss = 0.15984113\n",
      "Iteration 2084, loss = 0.15967606\n",
      "Iteration 2085, loss = 0.15956063\n",
      "Iteration 2086, loss = 0.15950140\n",
      "Iteration 2087, loss = 0.15940629\n",
      "Iteration 2088, loss = 0.15934682\n",
      "Iteration 2089, loss = 0.15929947\n",
      "Iteration 2090, loss = 0.15919469\n",
      "Iteration 2091, loss = 0.15910749\n",
      "Iteration 2092, loss = 0.15908138\n",
      "Iteration 2093, loss = 0.15895617\n",
      "Iteration 2094, loss = 0.15888145\n",
      "Iteration 2095, loss = 0.15889734\n",
      "Iteration 2096, loss = 0.15887962\n",
      "Iteration 2097, loss = 0.15859909\n",
      "Iteration 2098, loss = 0.15853211\n",
      "Iteration 2099, loss = 0.15846299\n",
      "Iteration 2100, loss = 0.15842370\n",
      "Iteration 2101, loss = 0.15831293\n",
      "Iteration 2102, loss = 0.15824757\n",
      "Iteration 2103, loss = 0.15838128\n",
      "Iteration 2104, loss = 0.15812950\n",
      "Iteration 2105, loss = 0.15799441\n",
      "Iteration 2106, loss = 0.15788844\n",
      "Iteration 2107, loss = 0.15781655\n",
      "Iteration 2108, loss = 0.15772222\n",
      "Iteration 2109, loss = 0.15785159\n",
      "Iteration 2110, loss = 0.15771565\n",
      "Iteration 2111, loss = 0.15749883\n",
      "Iteration 2112, loss = 0.15746266\n",
      "Iteration 2113, loss = 0.15736966\n",
      "Iteration 2114, loss = 0.15725788\n",
      "Iteration 2115, loss = 0.15722731\n",
      "Iteration 2116, loss = 0.15714085\n",
      "Iteration 2117, loss = 0.15704586\n",
      "Iteration 2118, loss = 0.15695036\n",
      "Iteration 2119, loss = 0.15689547\n",
      "Iteration 2120, loss = 0.15680107\n",
      "Iteration 2121, loss = 0.15669496\n",
      "Iteration 2122, loss = 0.15663232\n",
      "Iteration 2123, loss = 0.15661456\n",
      "Iteration 2124, loss = 0.15651789\n",
      "Iteration 2125, loss = 0.15646503\n",
      "Iteration 2126, loss = 0.15628933\n",
      "Iteration 2127, loss = 0.15622860\n",
      "Iteration 2128, loss = 0.15625078\n",
      "Iteration 2129, loss = 0.15613119\n",
      "Iteration 2130, loss = 0.15601528\n",
      "Iteration 2131, loss = 0.15596221\n",
      "Iteration 2132, loss = 0.15589744\n",
      "Iteration 2133, loss = 0.15587202\n",
      "Iteration 2134, loss = 0.15574189\n",
      "Iteration 2135, loss = 0.15563403\n",
      "Iteration 2136, loss = 0.15556952\n",
      "Iteration 2137, loss = 0.15555136\n",
      "Iteration 2138, loss = 0.15542455\n",
      "Iteration 2139, loss = 0.15535261\n",
      "Iteration 2140, loss = 0.15529901\n",
      "Iteration 2141, loss = 0.15525087\n",
      "Iteration 2142, loss = 0.15507758\n",
      "Iteration 2143, loss = 0.15511788\n",
      "Iteration 2144, loss = 0.15507066\n",
      "Iteration 2145, loss = 0.15486009\n",
      "Iteration 2146, loss = 0.15483554\n",
      "Iteration 2147, loss = 0.15475458\n",
      "Iteration 2148, loss = 0.15462926\n",
      "Iteration 2149, loss = 0.15459025\n",
      "Iteration 2150, loss = 0.15449081\n",
      "Iteration 2151, loss = 0.15450964\n",
      "Iteration 2152, loss = 0.15433491\n",
      "Iteration 2153, loss = 0.15423551\n",
      "Iteration 2154, loss = 0.15418581\n",
      "Iteration 2155, loss = 0.15409205\n",
      "Iteration 2156, loss = 0.15397928\n",
      "Iteration 2157, loss = 0.15389927\n",
      "Iteration 2158, loss = 0.15386747\n",
      "Iteration 2159, loss = 0.15380774\n",
      "Iteration 2160, loss = 0.15372955\n",
      "Iteration 2161, loss = 0.15361131\n",
      "Iteration 2162, loss = 0.15358009\n",
      "Iteration 2163, loss = 0.15345698\n",
      "Iteration 2164, loss = 0.15341057\n",
      "Iteration 2165, loss = 0.15337841\n",
      "Iteration 2166, loss = 0.15322069\n",
      "Iteration 2167, loss = 0.15316120\n",
      "Iteration 2168, loss = 0.15311871\n",
      "Iteration 2169, loss = 0.15309164\n",
      "Iteration 2170, loss = 0.15297327\n",
      "Iteration 2171, loss = 0.15286434\n",
      "Iteration 2172, loss = 0.15279703\n",
      "Iteration 2173, loss = 0.15272873\n",
      "Iteration 2174, loss = 0.15261645\n",
      "Iteration 2175, loss = 0.15261902\n",
      "Iteration 2176, loss = 0.15254372\n",
      "Iteration 2177, loss = 0.15243412\n",
      "Iteration 2178, loss = 0.15242218\n",
      "Iteration 2179, loss = 0.15227272\n",
      "Iteration 2180, loss = 0.15223262\n",
      "Iteration 2181, loss = 0.15213349\n",
      "Iteration 2182, loss = 0.15204420\n",
      "Iteration 2183, loss = 0.15199343\n",
      "Iteration 2184, loss = 0.15196579\n",
      "Iteration 2185, loss = 0.15182820\n",
      "Iteration 2186, loss = 0.15178443\n",
      "Iteration 2187, loss = 0.15174135\n",
      "Iteration 2188, loss = 0.15161242\n",
      "Iteration 2189, loss = 0.15153379\n",
      "Iteration 2190, loss = 0.15143440\n",
      "Iteration 2191, loss = 0.15139811\n",
      "Iteration 2192, loss = 0.15136135\n",
      "Iteration 2193, loss = 0.15119824\n",
      "Iteration 2194, loss = 0.15117350\n",
      "Iteration 2195, loss = 0.15146491\n",
      "Iteration 2196, loss = 0.15098098\n",
      "Iteration 2197, loss = 0.15090665\n",
      "Iteration 2198, loss = 0.15092564\n",
      "Iteration 2199, loss = 0.15074707\n",
      "Iteration 2200, loss = 0.15068595\n",
      "Iteration 2201, loss = 0.15066087\n",
      "Iteration 2202, loss = 0.15055810\n",
      "Iteration 2203, loss = 0.15045037\n",
      "Iteration 2204, loss = 0.15050977\n",
      "Iteration 2205, loss = 0.15034380\n",
      "Iteration 2206, loss = 0.15024278\n",
      "Iteration 2207, loss = 0.15014946\n",
      "Iteration 2208, loss = 0.15008882\n",
      "Iteration 2209, loss = 0.15002908\n",
      "Iteration 2210, loss = 0.14996998\n",
      "Iteration 2211, loss = 0.14990075\n",
      "Iteration 2212, loss = 0.14977984\n",
      "Iteration 2213, loss = 0.14970338\n",
      "Iteration 2214, loss = 0.14964448\n",
      "Iteration 2215, loss = 0.14956544\n",
      "Iteration 2216, loss = 0.14963389\n",
      "Iteration 2217, loss = 0.14942177\n",
      "Iteration 2218, loss = 0.14938378\n",
      "Iteration 2219, loss = 0.14930320\n",
      "Iteration 2220, loss = 0.14925656\n",
      "Iteration 2221, loss = 0.14912861\n",
      "Iteration 2222, loss = 0.14908339\n",
      "Iteration 2223, loss = 0.14903121\n",
      "Iteration 2224, loss = 0.14890634\n",
      "Iteration 2225, loss = 0.14881463\n",
      "Iteration 2226, loss = 0.14877853\n",
      "Iteration 2227, loss = 0.14872335\n",
      "Iteration 2228, loss = 0.14864112\n",
      "Iteration 2229, loss = 0.14848740\n",
      "Iteration 2230, loss = 0.14850812\n",
      "Iteration 2231, loss = 0.14839102\n",
      "Iteration 2232, loss = 0.14830014\n",
      "Iteration 2233, loss = 0.14825564\n",
      "Iteration 2234, loss = 0.14823490\n",
      "Iteration 2235, loss = 0.14814979\n",
      "Iteration 2236, loss = 0.14801726\n",
      "Iteration 2237, loss = 0.14797850\n",
      "Iteration 2238, loss = 0.14791017\n",
      "Iteration 2239, loss = 0.14777771\n",
      "Iteration 2240, loss = 0.14794688\n",
      "Iteration 2241, loss = 0.14767389\n",
      "Iteration 2242, loss = 0.14785507\n",
      "Iteration 2243, loss = 0.14751976\n",
      "Iteration 2244, loss = 0.14743256\n",
      "Iteration 2245, loss = 0.14736395\n",
      "Iteration 2246, loss = 0.14729039\n",
      "Iteration 2247, loss = 0.14720388\n",
      "Iteration 2248, loss = 0.14730686\n",
      "Iteration 2249, loss = 0.14707930\n",
      "Iteration 2250, loss = 0.14698039\n",
      "Iteration 2251, loss = 0.14690772\n",
      "Iteration 2252, loss = 0.14685658\n",
      "Iteration 2253, loss = 0.14682715\n",
      "Iteration 2254, loss = 0.14669482\n",
      "Iteration 2255, loss = 0.14662975\n",
      "Iteration 2256, loss = 0.14658001\n",
      "Iteration 2257, loss = 0.14654474\n",
      "Iteration 2258, loss = 0.14647032\n",
      "Iteration 2259, loss = 0.14638643\n",
      "Iteration 2260, loss = 0.14629070\n",
      "Iteration 2261, loss = 0.14619185\n",
      "Iteration 2262, loss = 0.14614055\n",
      "Iteration 2263, loss = 0.14612119\n",
      "Iteration 1238, loss = 0.27065220\n",
      "Iteration 1239, loss = 0.27057476\n",
      "Iteration 1240, loss = 0.27046585\n",
      "Iteration 1241, loss = 0.27026402\n",
      "Iteration 1242, loss = 0.27013590\n",
      "Iteration 1243, loss = 0.27000020\n",
      "Iteration 1244, loss = 0.27009949\n",
      "Iteration 1245, loss = 0.26971171\n",
      "Iteration 1246, loss = 0.26957053\n",
      "Iteration 1247, loss = 0.26947673\n",
      "Iteration 1248, loss = 0.26930384\n",
      "Iteration 1249, loss = 0.26934969\n",
      "Iteration 1250, loss = 0.26909436\n",
      "Iteration 1251, loss = 0.26888957\n",
      "Iteration 1252, loss = 0.26879771\n",
      "Iteration 1253, loss = 0.26864808\n",
      "Iteration 1254, loss = 0.26850959\n",
      "Iteration 1255, loss = 0.26835994\n",
      "Iteration 1256, loss = 0.26821533\n",
      "Iteration 1257, loss = 0.26812528\n",
      "Iteration 1258, loss = 0.26793662\n",
      "Iteration 1259, loss = 0.26782841\n",
      "Iteration 1260, loss = 0.26768190\n",
      "Iteration 1261, loss = 0.26764208\n",
      "Iteration 1262, loss = 0.26740810\n",
      "Iteration 1263, loss = 0.26728280\n",
      "Iteration 1264, loss = 0.26714861\n",
      "Iteration 1265, loss = 0.26700374\n",
      "Iteration 1266, loss = 0.26685835\n",
      "Iteration 1267, loss = 0.26676082\n",
      "Iteration 1268, loss = 0.26663862\n",
      "Iteration 1269, loss = 0.26645777\n",
      "Iteration 1270, loss = 0.26652320\n",
      "Iteration 1271, loss = 0.26622959\n",
      "Iteration 1272, loss = 0.26607604\n",
      "Iteration 1273, loss = 0.26591277\n",
      "Iteration 1274, loss = 0.26579119\n",
      "Iteration 1275, loss = 0.26571784\n",
      "Iteration 1276, loss = 0.26551009\n",
      "Iteration 1277, loss = 0.26535521\n",
      "Iteration 1278, loss = 0.26535023\n",
      "Iteration 1279, loss = 0.26510541\n",
      "Iteration 1280, loss = 0.26510133\n",
      "Iteration 1281, loss = 0.26482496\n",
      "Iteration 1282, loss = 0.26472206\n",
      "Iteration 1283, loss = 0.26458614\n",
      "Iteration 1284, loss = 0.26449276\n",
      "Iteration 1285, loss = 0.26432609\n",
      "Iteration 1286, loss = 0.26415282\n",
      "Iteration 1287, loss = 0.26412411\n",
      "Iteration 1288, loss = 0.26386600\n",
      "Iteration 1289, loss = 0.26373545\n",
      "Iteration 1290, loss = 0.26363159\n",
      "Iteration 1291, loss = 0.26352076\n",
      "Iteration 1292, loss = 0.26333047\n",
      "Iteration 1293, loss = 0.26321186\n",
      "Iteration 1294, loss = 0.26305818\n",
      "Iteration 1295, loss = 0.26304202\n",
      "Iteration 1296, loss = 0.26282862\n",
      "Iteration 1297, loss = 0.26268157\n",
      "Iteration 1298, loss = 0.26254319\n",
      "Iteration 1299, loss = 0.26251516\n",
      "Iteration 1300, loss = 0.26224759\n",
      "Iteration 1301, loss = 0.26215370\n",
      "Iteration 1302, loss = 0.26199880\n",
      "Iteration 1303, loss = 0.26187487\n",
      "Iteration 1304, loss = 0.26170793\n",
      "Iteration 1305, loss = 0.26161267\n",
      "Iteration 1306, loss = 0.26142851\n",
      "Iteration 1307, loss = 0.26132719\n",
      "Iteration 1308, loss = 0.26118878\n",
      "Iteration 1309, loss = 0.26111778\n",
      "Iteration 1310, loss = 0.26091757\n",
      "Iteration 1311, loss = 0.26075962\n",
      "Iteration 1312, loss = 0.26066441\n",
      "Iteration 1313, loss = 0.26049614\n",
      "Iteration 1314, loss = 0.26036026\n",
      "Iteration 1315, loss = 0.26027595\n",
      "Iteration 1316, loss = 0.26010991\n",
      "Iteration 1317, loss = 0.25995950\n",
      "Iteration 1318, loss = 0.25984423\n",
      "Iteration 1319, loss = 0.25983666\n",
      "Iteration 1320, loss = 0.25955339\n",
      "Iteration 1321, loss = 0.25943965\n",
      "Iteration 1322, loss = 0.25929938\n",
      "Iteration 1323, loss = 0.25915535\n",
      "Iteration 1324, loss = 0.25908984\n",
      "Iteration 1325, loss = 0.25888348\n",
      "Iteration 1326, loss = 0.25875224\n",
      "Iteration 1327, loss = 0.25861105\n",
      "Iteration 1328, loss = 0.25849122\n",
      "Iteration 1329, loss = 0.25836228\n",
      "Iteration 1330, loss = 0.25821645\n",
      "Iteration 1331, loss = 0.25812587\n",
      "Iteration 1332, loss = 0.25791915\n",
      "Iteration 1333, loss = 0.25781923\n",
      "Iteration 1334, loss = 0.25773144\n",
      "Iteration 1335, loss = 0.25750265\n",
      "Iteration 1336, loss = 0.25741642\n",
      "Iteration 1337, loss = 0.25726366\n",
      "Iteration 1338, loss = 0.25717750\n",
      "Iteration 1339, loss = 0.25703284\n",
      "Iteration 1340, loss = 0.25688709\n",
      "Iteration 1341, loss = 0.25671664\n",
      "Iteration 1342, loss = 0.25659520\n",
      "Iteration 1343, loss = 0.25644541\n",
      "Iteration 1344, loss = 0.25634434\n",
      "Iteration 1345, loss = 0.25617613\n",
      "Iteration 1346, loss = 0.25607460\n",
      "Iteration 1347, loss = 0.25591259\n",
      "Iteration 1348, loss = 0.25578160\n",
      "Iteration 1349, loss = 0.25569928\n",
      "Iteration 1350, loss = 0.25558011\n",
      "Iteration 1351, loss = 0.25535356\n",
      "Iteration 1352, loss = 0.25522656\n",
      "Iteration 1353, loss = 0.25513208\n",
      "Iteration 1354, loss = 0.25495189\n",
      "Iteration 1355, loss = 0.25483945\n",
      "Iteration 1356, loss = 0.25468859\n",
      "Iteration 1357, loss = 0.25469412\n",
      "Iteration 1358, loss = 0.25440950\n",
      "Iteration 1359, loss = 0.25431336\n",
      "Iteration 1360, loss = 0.25418551\n",
      "Iteration 1361, loss = 0.25401361\n",
      "Iteration 1362, loss = 0.25387238\n",
      "Iteration 1363, loss = 0.25373955\n",
      "Iteration 1364, loss = 0.25358218\n",
      "Iteration 1365, loss = 0.25352059\n",
      "Iteration 1366, loss = 0.25336373\n",
      "Iteration 1367, loss = 0.25320766\n",
      "Iteration 1368, loss = 0.25315665\n",
      "Iteration 1369, loss = 0.25300475\n",
      "Iteration 1370, loss = 0.25281960\n",
      "Iteration 1371, loss = 0.25268654\n",
      "Iteration 1372, loss = 0.25258530\n",
      "Iteration 1373, loss = 0.25247616\n",
      "Iteration 1374, loss = 0.25230662\n",
      "Iteration 1375, loss = 0.25214160\n",
      "Iteration 1376, loss = 0.25203174\n",
      "Iteration 1377, loss = 0.25188704\n",
      "Iteration 1378, loss = 0.25178657\n",
      "Iteration 1379, loss = 0.25161837\n",
      "Iteration 1380, loss = 0.25151253\n",
      "Iteration 1381, loss = 0.25141525\n",
      "Iteration 1382, loss = 0.25125473\n",
      "Iteration 1383, loss = 0.25117869\n",
      "Iteration 1384, loss = 0.25098320\n",
      "Iteration 1385, loss = 0.25089356\n",
      "Iteration 1386, loss = 0.25069043\n",
      "Iteration 1387, loss = 0.25059876\n",
      "Iteration 1388, loss = 0.25040097\n",
      "Iteration 1389, loss = 0.25032451\n",
      "Iteration 1390, loss = 0.25018978\n",
      "Iteration 1391, loss = 0.25002058\n",
      "Iteration 1392, loss = 0.24983550\n",
      "Iteration 1393, loss = 0.24974764\n",
      "Iteration 1394, loss = 0.24966909\n",
      "Iteration 1395, loss = 0.24953673\n",
      "Iteration 1396, loss = 0.24935182\n",
      "Iteration 1397, loss = 0.24922302\n",
      "Iteration 1398, loss = 0.24910118\n",
      "Iteration 1399, loss = 0.24893661\n",
      "Iteration 1400, loss = 0.24889435\n",
      "Iteration 1401, loss = 0.24864753\n",
      "Iteration 1402, loss = 0.24856462\n",
      "Iteration 1403, loss = 0.24845231\n",
      "Iteration 1404, loss = 0.24830109\n",
      "Iteration 1405, loss = 0.24815070\n",
      "Iteration 1406, loss = 0.24801648\n",
      "Iteration 1407, loss = 0.24794467\n",
      "Iteration 1408, loss = 0.24786048\n",
      "Iteration 1409, loss = 0.24759072\n",
      "Iteration 1410, loss = 0.24751342\n",
      "Iteration 1411, loss = 0.24731854\n",
      "Iteration 1412, loss = 0.24727788\n",
      "Iteration 1413, loss = 0.24710248\n",
      "Iteration 1414, loss = 0.24694733\n",
      "Iteration 1415, loss = 0.24678875\n",
      "Iteration 1416, loss = 0.24669154\n",
      "Iteration 1417, loss = 0.24653312\n",
      "Iteration 1418, loss = 0.24649090\n",
      "Iteration 1419, loss = 0.24629633\n",
      "Iteration 1420, loss = 0.24617472\n",
      "Iteration 1421, loss = 0.24599585\n",
      "Iteration 1422, loss = 0.24588402\n",
      "Iteration 1423, loss = 0.24574966\n",
      "Iteration 1424, loss = 0.24560627\n",
      "Iteration 1425, loss = 0.24549781\n",
      "Iteration 1426, loss = 0.24530753\n",
      "Iteration 1427, loss = 0.24526336\n",
      "Iteration 1428, loss = 0.24514095\n",
      "Iteration 1429, loss = 0.24500583\n",
      "Iteration 1430, loss = 0.24500767\n",
      "Iteration 1431, loss = 0.24469387\n",
      "Iteration 1432, loss = 0.24451815\n",
      "Iteration 1433, loss = 0.24437278\n",
      "Iteration 1434, loss = 0.24430298\n",
      "Iteration 1435, loss = 0.24411089\n",
      "Iteration 1436, loss = 0.24403560\n",
      "Iteration 1437, loss = 0.24389124\n",
      "Iteration 1438, loss = 0.24381996\n",
      "Iteration 1439, loss = 0.24360025\n",
      "Iteration 1440, loss = 0.24346319\n",
      "Iteration 1441, loss = 0.24349695\n",
      "Iteration 1442, loss = 0.24319046\n",
      "Iteration 1443, loss = 0.24307213\n",
      "Iteration 1444, loss = 0.24293390\n",
      "Iteration 1445, loss = 0.24287641\n",
      "Iteration 1446, loss = 0.24264755\n",
      "Iteration 1447, loss = 0.24252782\n",
      "Iteration 1448, loss = 0.24241604\n",
      "Iteration 1449, loss = 0.24233125\n",
      "Iteration 1450, loss = 0.24211487\n",
      "Iteration 1451, loss = 0.24212690\n",
      "Iteration 1452, loss = 0.24183794\n",
      "Iteration 1453, loss = 0.24171709\n",
      "Iteration 1454, loss = 0.24156509\n",
      "Iteration 1455, loss = 0.24146234\n",
      "Iteration 1456, loss = 0.24134251\n",
      "Iteration 1457, loss = 0.24116654\n",
      "Iteration 1458, loss = 0.24113766\n",
      "Iteration 1459, loss = 0.24091633\n",
      "Iteration 1460, loss = 0.24079323\n",
      "Iteration 1461, loss = 0.24067875\n",
      "Iteration 1462, loss = 0.24052049\n",
      "Iteration 1463, loss = 0.24036880\n",
      "Iteration 1464, loss = 0.24028274\n",
      "Iteration 1465, loss = 0.24012354\n",
      "Iteration 1466, loss = 0.23994981\n",
      "Iteration 1467, loss = 0.23982380\n",
      "Iteration 1468, loss = 0.23974124\n",
      "Iteration 1469, loss = 0.23958007\n",
      "Iteration 1470, loss = 0.23949354\n",
      "Iteration 1471, loss = 0.23935407\n",
      "Iteration 1472, loss = 0.23919904\n",
      "Iteration 1473, loss = 0.23904133\n",
      "Iteration 1474, loss = 0.23891387\n",
      "Iteration 1475, loss = 0.23881866\n",
      "Iteration 1476, loss = 0.23869993\n",
      "Iteration 1477, loss = 0.23848397\n",
      "Iteration 1478, loss = 0.23836699\n",
      "Iteration 1479, loss = 0.23823641\n",
      "Iteration 1480, loss = 0.23810300\n",
      "Iteration 1481, loss = 0.23798456\n",
      "Iteration 1482, loss = 0.23788872\n",
      "Iteration 1483, loss = 0.23778875\n",
      "Iteration 1484, loss = 0.23763696\n",
      "Iteration 1485, loss = 0.23744243\n",
      "Iteration 1486, loss = 0.23729105\n",
      "Iteration 1487, loss = 0.23716117\n",
      "Iteration 1488, loss = 0.23703097\n",
      "Iteration 1489, loss = 0.23691240\n",
      "Iteration 1490, loss = 0.23679245\n",
      "Iteration 1491, loss = 0.23663932\n",
      "Iteration 1492, loss = 0.23655039\n",
      "Iteration 1493, loss = 0.23636531\n",
      "Iteration 1494, loss = 0.23624039\n",
      "Iteration 1495, loss = 0.23613505\n",
      "Iteration 1496, loss = 0.23596716\n",
      "Iteration 1497, loss = 0.23587358\n",
      "Iteration 1498, loss = 0.23576274\n",
      "Iteration 1499, loss = 0.23561926\n",
      "Iteration 1500, loss = 0.23544910\n",
      "Iteration 1501, loss = 0.23529328\n",
      "Iteration 1502, loss = 0.23517221\n",
      "Iteration 1503, loss = 0.23500142\n",
      "Iteration 1504, loss = 0.23489402\n",
      "Iteration 1505, loss = 0.23474252\n",
      "Iteration 1506, loss = 0.23461241\n",
      "Iteration 1507, loss = 0.23447903\n",
      "Iteration 1508, loss = 0.23434404\n",
      "Iteration 1509, loss = 0.23425497\n",
      "Iteration 1510, loss = 0.23417234\n",
      "Iteration 1511, loss = 0.23395261\n",
      "Iteration 1512, loss = 0.23379758\n",
      "Iteration 1513, loss = 0.23368572\n",
      "Iteration 1514, loss = 0.23370761\n",
      "Iteration 1515, loss = 0.23342287\n",
      "Iteration 1516, loss = 0.23333749\n",
      "Iteration 1517, loss = 0.23314096\n",
      "Iteration 1518, loss = 0.23299333\n",
      "Iteration 1519, loss = 0.23292090\n",
      "Iteration 1520, loss = 0.23277238\n",
      "Iteration 1521, loss = 0.23280443\n",
      "Iteration 1522, loss = 0.23249153\n",
      "Iteration 1523, loss = 0.23231217\n",
      "Iteration 1524, loss = 0.23219311\n",
      "Iteration 1525, loss = 0.23204261\n",
      "Iteration 1526, loss = 0.23189206\n",
      "Iteration 1527, loss = 0.23177448\n",
      "Iteration 1528, loss = 0.23164301\n",
      "Iteration 1529, loss = 0.23143945\n",
      "Iteration 1530, loss = 0.23140152\n",
      "Iteration 1531, loss = 0.23122321\n",
      "Iteration 1532, loss = 0.23112781\n",
      "Iteration 1533, loss = 0.23093164\n",
      "Iteration 1534, loss = 0.23084959\n",
      "Iteration 1535, loss = 0.23065462\n",
      "Iteration 1536, loss = 0.23051826\n",
      "Iteration 1537, loss = 0.23037949\n",
      "Iteration 1538, loss = 0.23023367\n",
      "Iteration 1539, loss = 0.23014155\n",
      "Iteration 1540, loss = 0.22998680\n",
      "Iteration 1541, loss = 0.22991138\n",
      "Iteration 1542, loss = 0.22975736\n",
      "Iteration 1543, loss = 0.22955824\n",
      "Iteration 1544, loss = 0.22943531\n",
      "Iteration 1545, loss = 0.22969351\n",
      "Iteration 1546, loss = 0.22928198\n",
      "Iteration 1547, loss = 0.22911417\n",
      "Iteration 1548, loss = 0.22898117\n",
      "Iteration 1549, loss = 0.22878112\n",
      "Iteration 1550, loss = 0.22876546\n",
      "Iteration 1551, loss = 0.22852970\n",
      "Iteration 1552, loss = 0.22840149\n",
      "Iteration 1553, loss = 0.22822288\n",
      "Iteration 1554, loss = 0.22806962\n",
      "Iteration 1555, loss = 0.22797171\n",
      "Iteration 1556, loss = 0.22783020\n",
      "Iteration 1557, loss = 0.22767819\n",
      "Iteration 1558, loss = 0.22760797\n",
      "Iteration 1559, loss = 0.22747751\n",
      "Iteration 1560, loss = 0.22728393\n",
      "Iteration 1561, loss = 0.22715578\n",
      "Iteration 1562, loss = 0.22714107\n",
      "Iteration 1563, loss = 0.22687221\n",
      "Iteration 1564, loss = 0.22676311\n",
      "Iteration 1565, loss = 0.22673509\n",
      "Iteration 1566, loss = 0.22651484\n",
      "Iteration 1567, loss = 0.22633891\n",
      "Iteration 1568, loss = 0.22627084\n",
      "Iteration 1569, loss = 0.22610255\n",
      "Iteration 1570, loss = 0.22595399\n",
      "Iteration 1571, loss = 0.22587129\n",
      "Iteration 1572, loss = 0.22567900\n",
      "Iteration 1573, loss = 0.22554352\n",
      "Iteration 1574, loss = 0.22555648\n",
      "Iteration 1575, loss = 0.22532342\n",
      "Iteration 1576, loss = 0.22515390\n",
      "Iteration 1577, loss = 0.22511365\n",
      "Iteration 1578, loss = 0.22486868\n",
      "Iteration 1579, loss = 0.22477177\n",
      "Iteration 1580, loss = 0.22464530\n",
      "Iteration 1581, loss = 0.22449371\n",
      "Iteration 1582, loss = 0.22437800\n",
      "Iteration 1583, loss = 0.22425326\n",
      "Iteration 1584, loss = 0.22416824\n",
      "Iteration 1585, loss = 0.22404219\n",
      "Iteration 1586, loss = 0.22381969\n",
      "Iteration 1587, loss = 0.22378713\n",
      "Iteration 1588, loss = 0.22355095\n",
      "Iteration 1589, loss = 0.22343793\n",
      "Iteration 1590, loss = 0.22336039\n",
      "Iteration 1591, loss = 0.22320124\n",
      "Iteration 1592, loss = 0.22304966\n",
      "Iteration 1593, loss = 0.22291491\n",
      "Iteration 1594, loss = 0.22284582\n",
      "Iteration 1595, loss = 0.22272926\n",
      "Iteration 1596, loss = 0.22260433\n",
      "Iteration 1597, loss = 0.22240648\n",
      "Iteration 1598, loss = 0.22222862\n",
      "Iteration 1599, loss = 0.22210186\n",
      "Iteration 1600, loss = 0.22199198\n",
      "Iteration 1601, loss = 0.22188615\n",
      "Iteration 1602, loss = 0.22183585\n",
      "Iteration 1603, loss = 0.22158196\n",
      "Iteration 1604, loss = 0.22141977\n",
      "Iteration 1605, loss = 0.22134811\n",
      "Iteration 1606, loss = 0.22139030\n",
      "Iteration 1607, loss = 0.22106344\n",
      "Iteration 1608, loss = 0.22120017\n",
      "Iteration 1609, loss = 0.22075232\n",
      "Iteration 1610, loss = 0.22061420\n",
      "Iteration 1611, loss = 0.22055997\n",
      "Iteration 1612, loss = 0.22039091\n",
      "Iteration 1613, loss = 0.22025052\n",
      "Iteration 1614, loss = 0.22012946\n",
      "Iteration 1615, loss = 0.22009199\n",
      "Iteration 1616, loss = 0.21986236\n",
      "Iteration 1617, loss = 0.21975821\n",
      "Iteration 1618, loss = 0.21958156\n",
      "Iteration 1619, loss = 0.21943252\n",
      "Iteration 1620, loss = 0.21934457\n",
      "Iteration 1621, loss = 0.21917113\n",
      "Iteration 1622, loss = 0.21918899\n",
      "Iteration 1623, loss = 0.21897002\n",
      "Iteration 1624, loss = 0.21880266\n",
      "Iteration 1625, loss = 0.21870163\n",
      "Iteration 1626, loss = 0.21858415\n",
      "Iteration 1627, loss = 0.21856033\n",
      "Iteration 1628, loss = 0.21826342\n",
      "Iteration 1629, loss = 0.21822576\n",
      "Iteration 1630, loss = 0.21804252\n",
      "Iteration 1631, loss = 0.21786953\n",
      "Iteration 1632, loss = 0.21775874\n",
      "Iteration 1633, loss = 0.21766727\n",
      "Iteration 1634, loss = 0.21757431\n",
      "Iteration 1635, loss = 0.21735773\n",
      "Iteration 1636, loss = 0.21724368\n",
      "Iteration 1637, loss = 0.21711903\n",
      "Iteration 1638, loss = 0.21695906\n",
      "Iteration 1639, loss = 0.21685357\n",
      "Iteration 1640, loss = 0.21667611\n",
      "Iteration 1641, loss = 0.21663776\n",
      "Iteration 1642, loss = 0.21658726\n",
      "Iteration 1643, loss = 0.21633179\n",
      "Iteration 1644, loss = 0.21619700\n",
      "Iteration 1645, loss = 0.21605076\n",
      "Iteration 1646, loss = 0.21588862\n",
      "Iteration 1647, loss = 0.21577316\n",
      "Iteration 1648, loss = 0.21574033\n",
      "Iteration 1649, loss = 0.21552454\n",
      "Iteration 1650, loss = 0.21537025\n",
      "Iteration 1651, loss = 0.21524288\n",
      "Iteration 1652, loss = 0.21517054\n",
      "Iteration 1653, loss = 0.21507987\n",
      "Iteration 1654, loss = 0.21488368\n",
      "Iteration 1655, loss = 0.21474276\n",
      "Iteration 1656, loss = 0.21457359\n",
      "Iteration 1657, loss = 0.21448668\n",
      "Iteration 1658, loss = 0.21436523\n",
      "Iteration 1659, loss = 0.21423491\n",
      "Iteration 1660, loss = 0.21408836\n",
      "Iteration 1661, loss = 0.21395613\n",
      "Iteration 1662, loss = 0.21382672\n",
      "Iteration 1663, loss = 0.21385708\n",
      "Iteration 1664, loss = 0.21352326\n",
      "Iteration 1665, loss = 0.21340069\n",
      "Iteration 1666, loss = 0.21330771\n",
      "Iteration 1667, loss = 0.21326231\n",
      "Iteration 1668, loss = 0.21304584\n",
      "Iteration 1669, loss = 0.21288459\n",
      "Iteration 1670, loss = 0.21275316\n",
      "Iteration 1671, loss = 0.21273337\n",
      "Iteration 1672, loss = 0.21256122\n",
      "Iteration 1673, loss = 0.21243260\n",
      "Iteration 1674, loss = 0.21224583\n",
      "Iteration 1675, loss = 0.21215901\n",
      "Iteration 1676, loss = 0.21209415\n",
      "Iteration 1677, loss = 0.21186500\n",
      "Iteration 1678, loss = 0.21171679\n",
      "Iteration 1679, loss = 0.21161809\n",
      "Iteration 1680, loss = 0.21148773\n",
      "Iteration 1681, loss = 0.21138298\n",
      "Iteration 1682, loss = 0.21128003\n",
      "Iteration 1683, loss = 0.21124180\n",
      "Iteration 1684, loss = 0.21092275\n",
      "Iteration 1685, loss = 0.21088251\n",
      "Iteration 1686, loss = 0.21074182\n",
      "Iteration 1687, loss = 0.21057933\n",
      "Iteration 1688, loss = 0.21051094\n",
      "Iteration 1689, loss = 0.21028185\n",
      "Iteration 1690, loss = 0.21019320\n",
      "Iteration 1691, loss = 0.21005472\n",
      "Iteration 1692, loss = 0.20995715\n",
      "Iteration 1693, loss = 0.20981543\n",
      "Iteration 1694, loss = 0.20970930\n",
      "Iteration 1695, loss = 0.20958484\n",
      "Iteration 1696, loss = 0.20945102\n",
      "Iteration 1697, loss = 0.20931471\n",
      "Iteration 1698, loss = 0.20920630\n",
      "Iteration 1699, loss = 0.20904944\n",
      "Iteration 1700, loss = 0.20892084\n",
      "Iteration 1701, loss = 0.20881065\n",
      "Iteration 1702, loss = 0.20863338\n",
      "Iteration 1703, loss = 0.20850116\n",
      "Iteration 1704, loss = 0.20845748\n",
      "Iteration 1705, loss = 0.20833598\n",
      "Iteration 1706, loss = 0.20813052\n",
      "Iteration 1707, loss = 0.20799852\n",
      "Iteration 1708, loss = 0.20797560\n",
      "Iteration 1709, loss = 0.20776455\n",
      "Iteration 1710, loss = 0.20764444\n",
      "Iteration 1711, loss = 0.20753897\n",
      "Iteration 1712, loss = 0.20740031\n",
      "Iteration 1713, loss = 0.20730779\n",
      "Iteration 1714, loss = 0.20715889\n",
      "Iteration 1715, loss = 0.20711415\n",
      "Iteration 1716, loss = 0.20691960\n",
      "Iteration 1717, loss = 0.20676824\n",
      "Iteration 184, loss = 0.36826269\n",
      "Iteration 185, loss = 0.36790096\n",
      "Iteration 186, loss = 0.36756128\n",
      "Iteration 187, loss = 0.36724898\n",
      "Iteration 188, loss = 0.36690403\n",
      "Iteration 189, loss = 0.36658425\n",
      "Iteration 190, loss = 0.36626407\n",
      "Iteration 191, loss = 0.36592178\n",
      "Iteration 192, loss = 0.36560663\n",
      "Iteration 193, loss = 0.36528546\n",
      "Iteration 194, loss = 0.36498802\n",
      "Iteration 195, loss = 0.36467220\n",
      "Iteration 196, loss = 0.36436911\n",
      "Iteration 197, loss = 0.36406561\n",
      "Iteration 198, loss = 0.36376735\n",
      "Iteration 199, loss = 0.36346392\n",
      "Iteration 200, loss = 0.36316799\n",
      "Iteration 201, loss = 0.36290066\n",
      "Iteration 202, loss = 0.36258322\n",
      "Iteration 203, loss = 0.36228760\n",
      "Iteration 204, loss = 0.36201163\n",
      "Iteration 205, loss = 0.36175326\n",
      "Iteration 206, loss = 0.36145446\n",
      "Iteration 207, loss = 0.36114962\n",
      "Iteration 208, loss = 0.36086265\n",
      "Iteration 209, loss = 0.36059157\n",
      "Iteration 210, loss = 0.36032983\n",
      "Iteration 211, loss = 0.36005689\n",
      "Iteration 212, loss = 0.35978925\n",
      "Iteration 213, loss = 0.35953321\n",
      "Iteration 214, loss = 0.35926361\n",
      "Iteration 215, loss = 0.35899120\n",
      "Iteration 216, loss = 0.35872098\n",
      "Iteration 217, loss = 0.35848446\n",
      "Iteration 218, loss = 0.35821795\n",
      "Iteration 219, loss = 0.35797064\n",
      "Iteration 220, loss = 0.35771459\n",
      "Iteration 221, loss = 0.35745334\n",
      "Iteration 222, loss = 0.35721040\n",
      "Iteration 223, loss = 0.35697435\n",
      "Iteration 224, loss = 0.35671846\n",
      "Iteration 225, loss = 0.35647045\n",
      "Iteration 226, loss = 0.35624133\n",
      "Iteration 227, loss = 0.35601250\n",
      "Iteration 228, loss = 0.35576386\n",
      "Iteration 229, loss = 0.35552242\n",
      "Iteration 230, loss = 0.35529042\n",
      "Iteration 231, loss = 0.35507537\n",
      "Iteration 232, loss = 0.35483501\n",
      "Iteration 233, loss = 0.35458789\n",
      "Iteration 234, loss = 0.35438871\n",
      "Iteration 235, loss = 0.35415286\n",
      "Iteration 236, loss = 0.35392183\n",
      "Iteration 237, loss = 0.35372696\n",
      "Iteration 238, loss = 0.35347859\n",
      "Iteration 239, loss = 0.35327523\n",
      "Iteration 240, loss = 0.35304245\n",
      "Iteration 241, loss = 0.35285429\n",
      "Iteration 242, loss = 0.35262978\n",
      "Iteration 243, loss = 0.35242994\n",
      "Iteration 244, loss = 0.35220195\n",
      "Iteration 245, loss = 0.35197126\n",
      "Iteration 246, loss = 0.35179115\n",
      "Iteration 247, loss = 0.35157011\n",
      "Iteration 248, loss = 0.35134821\n",
      "Iteration 249, loss = 0.35116921\n",
      "Iteration 250, loss = 0.35095559\n",
      "Iteration 251, loss = 0.35073445\n",
      "Iteration 252, loss = 0.35053857\n",
      "Iteration 253, loss = 0.35034271\n",
      "Iteration 254, loss = 0.35015217\n",
      "Iteration 255, loss = 0.34993155\n",
      "Iteration 256, loss = 0.34972649\n",
      "Iteration 257, loss = 0.34954838\n",
      "Iteration 258, loss = 0.34933243\n",
      "Iteration 259, loss = 0.34916328\n",
      "Iteration 260, loss = 0.34895328\n",
      "Iteration 261, loss = 0.34875749\n",
      "Iteration 262, loss = 0.34856131\n",
      "Iteration 263, loss = 0.34837443\n",
      "Iteration 264, loss = 0.34817478\n",
      "Iteration 265, loss = 0.34797981\n",
      "Iteration 266, loss = 0.34779201\n",
      "Iteration 267, loss = 0.34761572\n",
      "Iteration 268, loss = 0.34743563\n",
      "Iteration 269, loss = 0.34724532\n",
      "Iteration 270, loss = 0.34705149\n",
      "Iteration 271, loss = 0.34686149\n",
      "Iteration 272, loss = 0.34669402\n",
      "Iteration 273, loss = 0.34653054\n",
      "Iteration 274, loss = 0.34635441\n",
      "Iteration 275, loss = 0.34615768\n",
      "Iteration 276, loss = 0.34596569\n",
      "Iteration 277, loss = 0.34577581\n",
      "Iteration 278, loss = 0.34561626\n",
      "Iteration 279, loss = 0.34543681\n",
      "Iteration 280, loss = 0.34525434\n",
      "Iteration 281, loss = 0.34509044\n",
      "Iteration 282, loss = 0.34489208\n",
      "Iteration 283, loss = 0.34472557\n",
      "Iteration 284, loss = 0.34455938\n",
      "Iteration 285, loss = 0.34438327\n",
      "Iteration 286, loss = 0.34421185\n",
      "Iteration 287, loss = 0.34402869\n",
      "Iteration 288, loss = 0.34392334\n",
      "Iteration 289, loss = 0.34370751\n",
      "Iteration 290, loss = 0.34353081\n",
      "Iteration 291, loss = 0.34338311\n",
      "Iteration 292, loss = 0.34319940\n",
      "Iteration 293, loss = 0.34302738\n",
      "Iteration 294, loss = 0.34287267\n",
      "Iteration 295, loss = 0.34271450\n",
      "Iteration 296, loss = 0.34253815\n",
      "Iteration 297, loss = 0.34236721\n",
      "Iteration 298, loss = 0.34222716\n",
      "Iteration 299, loss = 0.34204510\n",
      "Iteration 300, loss = 0.34189406\n",
      "Iteration 301, loss = 0.34172738\n",
      "Iteration 302, loss = 0.34157741\n",
      "Iteration 303, loss = 0.34142150\n",
      "Iteration 304, loss = 0.34125126\n",
      "Iteration 305, loss = 0.34109220\n",
      "Iteration 306, loss = 0.34094374\n",
      "Iteration 307, loss = 0.34079084\n",
      "Iteration 308, loss = 0.34064009\n",
      "Iteration 309, loss = 0.34047208\n",
      "Iteration 310, loss = 0.34031877\n",
      "Iteration 311, loss = 0.34016356\n",
      "Iteration 312, loss = 0.34001864\n",
      "Iteration 313, loss = 0.33987663\n",
      "Iteration 314, loss = 0.33971717\n",
      "Iteration 315, loss = 0.33955982\n",
      "Iteration 316, loss = 0.33939971\n",
      "Iteration 317, loss = 0.33926398\n",
      "Iteration 318, loss = 0.33910716\n",
      "Iteration 319, loss = 0.33895902\n",
      "Iteration 320, loss = 0.33881690\n",
      "Iteration 321, loss = 0.33866279\n",
      "Iteration 322, loss = 0.33854103\n",
      "Iteration 323, loss = 0.33838208\n",
      "Iteration 324, loss = 0.33822779\n",
      "Iteration 325, loss = 0.33808727\n",
      "Iteration 326, loss = 0.33794003\n",
      "Iteration 327, loss = 0.33781592\n",
      "Iteration 328, loss = 0.33767128\n",
      "Iteration 329, loss = 0.33756302\n",
      "Iteration 330, loss = 0.33738803\n",
      "Iteration 331, loss = 0.33722480\n",
      "Iteration 332, loss = 0.33709836\n",
      "Iteration 333, loss = 0.33697789\n",
      "Iteration 334, loss = 0.33683938\n",
      "Iteration 335, loss = 0.33668394\n",
      "Iteration 336, loss = 0.33655545\n",
      "Iteration 337, loss = 0.33640254\n",
      "Iteration 338, loss = 0.33626698\n",
      "Iteration 339, loss = 0.33613526\n",
      "Iteration 340, loss = 0.33598611\n",
      "Iteration 341, loss = 0.33586990\n",
      "Iteration 342, loss = 0.33572092\n",
      "Iteration 343, loss = 0.33557746\n",
      "Iteration 344, loss = 0.33544640\n",
      "Iteration 345, loss = 0.33531140\n",
      "Iteration 346, loss = 0.33517873\n",
      "Iteration 347, loss = 0.33505135\n",
      "Iteration 348, loss = 0.33491066\n",
      "Iteration 349, loss = 0.33478234\n",
      "Iteration 350, loss = 0.33465392\n",
      "Iteration 351, loss = 0.33449972\n",
      "Iteration 352, loss = 0.33439058\n",
      "Iteration 353, loss = 0.33424204\n",
      "Iteration 354, loss = 0.33411124\n",
      "Iteration 355, loss = 0.33398962\n",
      "Iteration 356, loss = 0.33385653\n",
      "Iteration 357, loss = 0.33372090\n",
      "Iteration 358, loss = 0.33360437\n",
      "Iteration 359, loss = 0.33346970\n",
      "Iteration 360, loss = 0.33334126\n",
      "Iteration 361, loss = 0.33322086\n",
      "Iteration 362, loss = 0.33308674\n",
      "Iteration 363, loss = 0.33295451\n",
      "Iteration 364, loss = 0.33282953\n",
      "Iteration 365, loss = 0.33270978\n",
      "Iteration 366, loss = 0.33258591\n",
      "Iteration 367, loss = 0.33245792\n",
      "Iteration 368, loss = 0.33233483\n",
      "Iteration 369, loss = 0.33220676\n",
      "Iteration 370, loss = 0.33214169\n",
      "Iteration 371, loss = 0.33196808\n",
      "Iteration 372, loss = 0.33184644\n",
      "Iteration 373, loss = 0.33172862\n",
      "Iteration 374, loss = 0.33160138\n",
      "Iteration 375, loss = 0.33149114\n",
      "Iteration 376, loss = 0.33137259\n",
      "Iteration 377, loss = 0.33123758\n",
      "Iteration 378, loss = 0.33111875\n",
      "Iteration 379, loss = 0.33101519\n",
      "Iteration 380, loss = 0.33087468\n",
      "Iteration 381, loss = 0.33075617\n",
      "Iteration 382, loss = 0.33065282\n",
      "Iteration 383, loss = 0.33053489\n",
      "Iteration 384, loss = 0.33041018\n",
      "Iteration 385, loss = 0.33028235\n",
      "Iteration 386, loss = 0.33020714\n",
      "Iteration 387, loss = 0.33006873\n",
      "Iteration 388, loss = 0.32994759\n",
      "Iteration 389, loss = 0.32982067\n",
      "Iteration 390, loss = 0.32969717\n",
      "Iteration 391, loss = 0.32959888\n",
      "Iteration 392, loss = 0.32947656\n",
      "Iteration 393, loss = 0.32935655\n",
      "Iteration 394, loss = 0.32926322\n",
      "Iteration 395, loss = 0.32912953\n",
      "Iteration 396, loss = 0.32901558\n",
      "Iteration 397, loss = 0.32889465\n",
      "Iteration 398, loss = 0.32877998\n",
      "Iteration 399, loss = 0.32866603\n",
      "Iteration 400, loss = 0.32855133\n",
      "Iteration 401, loss = 0.32846497\n",
      "Iteration 402, loss = 0.32832467\n",
      "Iteration 403, loss = 0.32822375\n",
      "Iteration 404, loss = 0.32811593\n",
      "Iteration 405, loss = 0.32800507\n",
      "Iteration 406, loss = 0.32789555\n",
      "Iteration 407, loss = 0.32776416\n",
      "Iteration 408, loss = 0.32765792\n",
      "Iteration 409, loss = 0.32757153\n",
      "Iteration 410, loss = 0.32743117\n",
      "Iteration 411, loss = 0.32735571\n",
      "Iteration 412, loss = 0.32722332\n",
      "Iteration 413, loss = 0.32710443\n",
      "Iteration 414, loss = 0.32700968\n",
      "Iteration 415, loss = 0.32688341\n",
      "Iteration 416, loss = 0.32677963\n",
      "Iteration 417, loss = 0.32667141\n",
      "Iteration 418, loss = 0.32656046\n",
      "Iteration 419, loss = 0.32645091\n",
      "Iteration 420, loss = 0.32633430\n",
      "Iteration 421, loss = 0.32623732\n",
      "Iteration 422, loss = 0.32615422\n",
      "Iteration 423, loss = 0.32600747\n",
      "Iteration 424, loss = 0.32591702\n",
      "Iteration 425, loss = 0.32580383\n",
      "Iteration 426, loss = 0.32568762\n",
      "Iteration 427, loss = 0.32558714\n",
      "Iteration 428, loss = 0.32548744\n",
      "Iteration 429, loss = 0.32538500\n",
      "Iteration 430, loss = 0.32525192\n",
      "Iteration 431, loss = 0.32516183\n",
      "Iteration 432, loss = 0.32504935\n",
      "Iteration 433, loss = 0.32494373\n",
      "Iteration 434, loss = 0.32484520\n",
      "Iteration 435, loss = 0.32473396\n",
      "Iteration 436, loss = 0.32462147\n",
      "Iteration 437, loss = 0.32453276\n",
      "Iteration 438, loss = 0.32441121\n",
      "Iteration 439, loss = 0.32431186\n",
      "Iteration 440, loss = 0.32421351\n",
      "Iteration 441, loss = 0.32410031\n",
      "Iteration 442, loss = 0.32398424\n",
      "Iteration 443, loss = 0.32389008\n",
      "Iteration 444, loss = 0.32378861\n",
      "Iteration 445, loss = 0.32368550\n",
      "Iteration 446, loss = 0.32357853\n",
      "Iteration 447, loss = 0.32348907\n",
      "Iteration 448, loss = 0.32337737\n",
      "Iteration 449, loss = 0.32328070\n",
      "Iteration 450, loss = 0.32316039\n",
      "Iteration 451, loss = 0.32306782\n",
      "Iteration 452, loss = 0.32297114\n",
      "Iteration 453, loss = 0.32287565\n",
      "Iteration 454, loss = 0.32275842\n",
      "Iteration 455, loss = 0.32267347\n",
      "Iteration 456, loss = 0.32258347\n",
      "Iteration 457, loss = 0.32245036\n",
      "Iteration 458, loss = 0.32234050\n",
      "Iteration 459, loss = 0.32224588\n",
      "Iteration 460, loss = 0.32213705\n",
      "Iteration 461, loss = 0.32202881\n",
      "Iteration 462, loss = 0.32192915\n",
      "Iteration 463, loss = 0.32183930\n",
      "Iteration 464, loss = 0.32173299\n",
      "Iteration 465, loss = 0.32162297\n",
      "Iteration 466, loss = 0.32152882\n",
      "Iteration 467, loss = 0.32141229\n",
      "Iteration 468, loss = 0.32131659\n",
      "Iteration 469, loss = 0.32122967\n",
      "Iteration 470, loss = 0.32111759\n",
      "Iteration 471, loss = 0.32104360\n",
      "Iteration 472, loss = 0.32094626\n",
      "Iteration 473, loss = 0.32082096\n",
      "Iteration 474, loss = 0.32072057\n",
      "Iteration 475, loss = 0.32062503\n",
      "Iteration 476, loss = 0.32051125\n",
      "Iteration 477, loss = 0.32040938\n",
      "Iteration 478, loss = 0.32031595\n",
      "Iteration 479, loss = 0.32020530\n",
      "Iteration 480, loss = 0.32010030\n",
      "Iteration 481, loss = 0.32001918\n",
      "Iteration 482, loss = 0.31993309\n",
      "Iteration 483, loss = 0.31981550\n",
      "Iteration 484, loss = 0.31971586\n",
      "Iteration 485, loss = 0.31965261\n",
      "Iteration 486, loss = 0.31951973\n",
      "Iteration 487, loss = 0.31945526\n",
      "Iteration 488, loss = 0.31932298\n",
      "Iteration 489, loss = 0.31923619\n",
      "Iteration 490, loss = 0.31911273\n",
      "Iteration 491, loss = 0.31902649\n",
      "Iteration 492, loss = 0.31891524\n",
      "Iteration 493, loss = 0.31882832\n",
      "Iteration 494, loss = 0.31872123\n",
      "Iteration 495, loss = 0.31862916\n",
      "Iteration 496, loss = 0.31851950\n",
      "Iteration 497, loss = 0.31843153\n",
      "Iteration 498, loss = 0.31833817\n",
      "Iteration 499, loss = 0.31824933\n",
      "Iteration 500, loss = 0.31814200\n",
      "Iteration 501, loss = 0.31803185\n",
      "Iteration 502, loss = 0.31794643\n",
      "Iteration 503, loss = 0.31782971\n",
      "Iteration 504, loss = 0.31773531\n",
      "Iteration 505, loss = 0.31765915\n",
      "Iteration 506, loss = 0.31754399\n",
      "Iteration 507, loss = 0.31744161\n",
      "Iteration 508, loss = 0.31734211\n",
      "Iteration 509, loss = 0.31724152\n",
      "Iteration 510, loss = 0.31714801\n",
      "Iteration 511, loss = 0.31706047\n",
      "Iteration 512, loss = 0.31694804\n",
      "Iteration 513, loss = 0.31684757\n",
      "Iteration 514, loss = 0.31676313\n",
      "Iteration 515, loss = 0.31667423\n",
      "Iteration 516, loss = 0.31657371\n",
      "Iteration 517, loss = 0.31649917\n",
      "Iteration 518, loss = 0.31638085\n",
      "Iteration 519, loss = 0.31627950\n",
      "Iteration 520, loss = 0.31617336\n",
      "Iteration 521, loss = 0.31610414\n",
      "Iteration 522, loss = 0.31598264\n",
      "Iteration 523, loss = 0.31590584\n",
      "Iteration 524, loss = 0.31578527\n",
      "Iteration 525, loss = 0.31569925\n",
      "Iteration 526, loss = 0.31559433\n",
      "Iteration 527, loss = 0.31553185\n",
      "Iteration 528, loss = 0.31543057\n",
      "Iteration 529, loss = 0.31530243\n",
      "Iteration 530, loss = 0.31522176\n",
      "Iteration 531, loss = 0.31513028\n",
      "Iteration 532, loss = 0.31502025\n",
      "Iteration 533, loss = 0.31492952\n",
      "Iteration 534, loss = 0.31485041\n",
      "Iteration 535, loss = 0.31474720\n",
      "Iteration 536, loss = 0.31463969\n",
      "Iteration 537, loss = 0.31454314\n",
      "Iteration 538, loss = 0.31446855\n",
      "Iteration 539, loss = 0.31435375\n",
      "Iteration 540, loss = 0.31428152\n",
      "Iteration 541, loss = 0.31416487\n",
      "Iteration 542, loss = 0.31407570\n",
      "Iteration 543, loss = 0.31398125\n",
      "Iteration 544, loss = 0.31389735\n",
      "Iteration 545, loss = 0.31378963\n",
      "Iteration 546, loss = 0.31370731\n",
      "Iteration 547, loss = 0.31360886\n",
      "Iteration 548, loss = 0.31350825\n",
      "Iteration 549, loss = 0.31342887\n",
      "Iteration 550, loss = 0.31332585\n",
      "Iteration 551, loss = 0.31322961\n",
      "Iteration 552, loss = 0.31313540\n",
      "Iteration 553, loss = 0.31305670\n",
      "Iteration 554, loss = 0.31295530\n",
      "Iteration 555, loss = 0.31286177\n",
      "Iteration 556, loss = 0.31277166\n",
      "Iteration 557, loss = 0.31268441\n",
      "Iteration 558, loss = 0.31258600\n",
      "Iteration 559, loss = 0.31250152\n",
      "Iteration 560, loss = 0.31239953\n",
      "Iteration 561, loss = 0.31231964\n",
      "Iteration 562, loss = 0.31223424\n",
      "Iteration 563, loss = 0.31213604\n",
      "Iteration 564, loss = 0.31204540\n",
      "Iteration 565, loss = 0.31195932\n",
      "Iteration 566, loss = 0.31186809\n",
      "Iteration 567, loss = 0.31178601\n",
      "Iteration 568, loss = 0.31168596\n",
      "Iteration 569, loss = 0.31159080\n",
      "Iteration 570, loss = 0.31149727\n",
      "Iteration 571, loss = 0.31142329\n",
      "Iteration 572, loss = 0.31132871\n",
      "Iteration 573, loss = 0.31122604\n",
      "Iteration 574, loss = 0.31113846\n",
      "Iteration 575, loss = 0.31107563\n",
      "Iteration 576, loss = 0.31096999\n",
      "Iteration 577, loss = 0.31089196\n",
      "Iteration 578, loss = 0.31080575\n",
      "Iteration 579, loss = 0.31069870\n",
      "Iteration 580, loss = 0.31061367\n",
      "Iteration 581, loss = 0.31052995\n",
      "Iteration 582, loss = 0.31043285\n",
      "Iteration 583, loss = 0.31036967\n",
      "Iteration 584, loss = 0.31025471\n",
      "Iteration 585, loss = 0.31017133\n",
      "Iteration 586, loss = 0.31007893\n",
      "Iteration 587, loss = 0.30998573\n",
      "Iteration 588, loss = 0.30989698\n",
      "Iteration 589, loss = 0.30980705\n",
      "Iteration 590, loss = 0.30977916\n",
      "Iteration 591, loss = 0.30963945\n",
      "Iteration 592, loss = 0.30955732\n",
      "Iteration 593, loss = 0.30951100\n",
      "Iteration 594, loss = 0.30936781\n",
      "Iteration 595, loss = 0.30928932\n",
      "Iteration 596, loss = 0.30920621\n",
      "Iteration 597, loss = 0.30911409\n",
      "Iteration 598, loss = 0.30902234\n",
      "Iteration 599, loss = 0.30894183\n",
      "Iteration 600, loss = 0.30884531\n",
      "Iteration 601, loss = 0.30875689\n",
      "Iteration 602, loss = 0.30870171\n",
      "Iteration 603, loss = 0.30858058\n",
      "Iteration 604, loss = 0.30849181\n",
      "Iteration 605, loss = 0.30840901\n",
      "Iteration 606, loss = 0.30831722\n",
      "Iteration 607, loss = 0.30823380\n",
      "Iteration 608, loss = 0.30814074\n",
      "Iteration 609, loss = 0.30805409\n",
      "Iteration 610, loss = 0.30796605\n",
      "Iteration 611, loss = 0.30790578\n",
      "Iteration 612, loss = 0.30780047\n",
      "Iteration 613, loss = 0.30771746\n",
      "Iteration 614, loss = 0.30762275\n",
      "Iteration 615, loss = 0.30754358\n",
      "Iteration 616, loss = 0.30745883\n",
      "Iteration 617, loss = 0.30737054\n",
      "Iteration 618, loss = 0.30729728\n",
      "Iteration 619, loss = 0.30719162\n",
      "Iteration 620, loss = 0.30710748\n",
      "Iteration 621, loss = 0.30704541\n",
      "Iteration 622, loss = 0.30694238\n",
      "Iteration 623, loss = 0.30683845\n",
      "Iteration 624, loss = 0.30677875\n",
      "Iteration 625, loss = 0.30668298\n",
      "Iteration 626, loss = 0.30658691\n",
      "Iteration 627, loss = 0.30652392\n",
      "Iteration 628, loss = 0.30643884\n",
      "Iteration 629, loss = 0.30632983\n",
      "Iteration 630, loss = 0.30624750\n",
      "Iteration 631, loss = 0.30615873\n",
      "Iteration 632, loss = 0.30607134\n",
      "Iteration 633, loss = 0.30600440\n",
      "Iteration 634, loss = 0.30590611\n",
      "Iteration 635, loss = 0.30583151\n",
      "Iteration 636, loss = 0.30573914\n",
      "Iteration 637, loss = 0.30563639\n",
      "Iteration 638, loss = 0.30559191\n",
      "Iteration 639, loss = 0.30548997\n",
      "Iteration 640, loss = 0.30540006\n",
      "Iteration 641, loss = 0.30531151\n",
      "Iteration 642, loss = 0.30524554\n",
      "Iteration 643, loss = 0.30515714\n",
      "Iteration 644, loss = 0.30509004\n",
      "Iteration 645, loss = 0.30497583\n",
      "Iteration 646, loss = 0.30487774\n",
      "Iteration 647, loss = 0.30482697\n",
      "Iteration 648, loss = 0.30471527\n",
      "Iteration 649, loss = 0.30464537\n",
      "Iteration 650, loss = 0.30454341\n",
      "Iteration 651, loss = 0.30447148\n",
      "Iteration 652, loss = 0.30437677\n",
      "Iteration 653, loss = 0.30430465\n",
      "Iteration 654, loss = 0.30419975\n",
      "Iteration 655, loss = 0.30413686\n",
      "Iteration 656, loss = 0.30404324\n",
      "Iteration 657, loss = 0.30396623\n",
      "Iteration 658, loss = 0.30387051\n",
      "Iteration 659, loss = 0.30379511\n",
      "Iteration 660, loss = 0.30370917\n",
      "Iteration 661, loss = 0.30363035\n",
      "Iteration 662, loss = 0.30354309\n",
      "Iteration 663, loss = 0.30345252\n",
      "Iteration 664, loss = 0.30337989\n",
      "Iteration 665, loss = 0.30327982\n",
      "Iteration 666, loss = 0.30320877\n",
      "Iteration 667, loss = 0.30311273\n",
      "Iteration 668, loss = 0.30304191\n",
      "Iteration 669, loss = 0.30296749\n",
      "Iteration 670, loss = 0.30286650\n",
      "Iteration 671, loss = 0.30279435\n",
      "Iteration 672, loss = 0.30269558\n",
      "Iteration 673, loss = 0.30263603\n",
      "Iteration 674, loss = 0.30253842\n",
      "Iteration 675, loss = 0.30246103\n",
      "Iteration 676, loss = 0.30239363\n",
      "Iteration 677, loss = 0.30229582\n",
      "Iteration 678, loss = 0.30219913\n",
      "Iteration 679, loss = 0.30212834\n",
      "Iteration 2264, loss = 0.14601094\n",
      "Iteration 2265, loss = 0.14590753\n",
      "Iteration 2266, loss = 0.14586133\n",
      "Iteration 2267, loss = 0.14580362\n",
      "Iteration 2268, loss = 0.14567892\n",
      "Iteration 2269, loss = 0.14573946\n",
      "Iteration 2270, loss = 0.14553306\n",
      "Iteration 2271, loss = 0.14549177\n",
      "Iteration 2272, loss = 0.14545024\n",
      "Iteration 2273, loss = 0.14530889\n",
      "Iteration 2274, loss = 0.14537733\n",
      "Iteration 2275, loss = 0.14520317\n",
      "Iteration 2276, loss = 0.14514777\n",
      "Iteration 2277, loss = 0.14503483\n",
      "Iteration 2278, loss = 0.14496974\n",
      "Iteration 2279, loss = 0.14492697\n",
      "Iteration 2280, loss = 0.14482979\n",
      "Iteration 2281, loss = 0.14475645\n",
      "Iteration 2282, loss = 0.14470626\n",
      "Iteration 2283, loss = 0.14461937\n",
      "Iteration 2284, loss = 0.14452635\n",
      "Iteration 2285, loss = 0.14450655\n",
      "Iteration 2286, loss = 0.14451639\n",
      "Iteration 2287, loss = 0.14433145\n",
      "Iteration 2288, loss = 0.14433229\n",
      "Iteration 2289, loss = 0.14421963\n",
      "Iteration 2290, loss = 0.14422259\n",
      "Iteration 2291, loss = 0.14404346\n",
      "Iteration 2292, loss = 0.14399627\n",
      "Iteration 2293, loss = 0.14390230\n",
      "Iteration 2294, loss = 0.14383670\n",
      "Iteration 2295, loss = 0.14376507\n",
      "Iteration 2296, loss = 0.14372364\n",
      "Iteration 2297, loss = 0.14371477\n",
      "Iteration 2298, loss = 0.14357987\n",
      "Iteration 2299, loss = 0.14359158\n",
      "Iteration 2300, loss = 0.14340269\n",
      "Iteration 2301, loss = 0.14337418\n",
      "Iteration 2302, loss = 0.14337419\n",
      "Iteration 2303, loss = 0.14319897\n",
      "Iteration 2304, loss = 0.14309016\n",
      "Iteration 2305, loss = 0.14304974\n",
      "Iteration 2306, loss = 0.14295216\n",
      "Iteration 2307, loss = 0.14291039\n",
      "Iteration 2308, loss = 0.14293811\n",
      "Iteration 2309, loss = 0.14280885\n",
      "Iteration 2310, loss = 0.14272911\n",
      "Iteration 2311, loss = 0.14271927\n",
      "Iteration 2312, loss = 0.14258856\n",
      "Iteration 2313, loss = 0.14253010\n",
      "Iteration 2314, loss = 0.14248631\n",
      "Iteration 2315, loss = 0.14238279\n",
      "Iteration 2316, loss = 0.14228676\n",
      "Iteration 2317, loss = 0.14232170\n",
      "Iteration 2318, loss = 0.14214754\n",
      "Iteration 2319, loss = 0.14207714\n",
      "Iteration 2320, loss = 0.14203687\n",
      "Iteration 2321, loss = 0.14198686\n",
      "Iteration 2322, loss = 0.14188251\n",
      "Iteration 2323, loss = 0.14180844\n",
      "Iteration 2324, loss = 0.14175151\n",
      "Iteration 2325, loss = 0.14170937\n",
      "Iteration 2326, loss = 0.14158007\n",
      "Iteration 2327, loss = 0.14162695\n",
      "Iteration 2328, loss = 0.14143386\n",
      "Iteration 2329, loss = 0.14148727\n",
      "Iteration 2330, loss = 0.14132789\n",
      "Iteration 2331, loss = 0.14123843\n",
      "Iteration 2332, loss = 0.14117156\n",
      "Iteration 2333, loss = 0.14123111\n",
      "Iteration 2334, loss = 0.14103125\n",
      "Iteration 2335, loss = 0.14095665\n",
      "Iteration 2336, loss = 0.14087686\n",
      "Iteration 2337, loss = 0.14086611\n",
      "Iteration 2338, loss = 0.14082909\n",
      "Iteration 2339, loss = 0.14070596\n",
      "Iteration 2340, loss = 0.14065823\n",
      "Iteration 2341, loss = 0.14063438\n",
      "Iteration 2342, loss = 0.14050120\n",
      "Iteration 2343, loss = 0.14042915\n",
      "Iteration 2344, loss = 0.14041021\n",
      "Iteration 2345, loss = 0.14027105\n",
      "Iteration 2346, loss = 0.14020988\n",
      "Iteration 2347, loss = 0.14021745\n",
      "Iteration 2348, loss = 0.14014587\n",
      "Iteration 2349, loss = 0.14001496\n",
      "Iteration 2350, loss = 0.14006556\n",
      "Iteration 2351, loss = 0.13987952\n",
      "Iteration 2352, loss = 0.13985597\n",
      "Iteration 2353, loss = 0.13974755\n",
      "Iteration 2354, loss = 0.13971606\n",
      "Iteration 2355, loss = 0.13968379\n",
      "Iteration 2356, loss = 0.13960729\n",
      "Iteration 2357, loss = 0.13952016\n",
      "Iteration 2358, loss = 0.13940015\n",
      "Iteration 2359, loss = 0.13942805\n",
      "Iteration 2360, loss = 0.13924565\n",
      "Iteration 2361, loss = 0.13924620\n",
      "Iteration 2362, loss = 0.13916458\n",
      "Iteration 2363, loss = 0.13904142\n",
      "Iteration 2364, loss = 0.13903735\n",
      "Iteration 2365, loss = 0.13896523\n",
      "Iteration 2366, loss = 0.13886123\n",
      "Iteration 2367, loss = 0.13882675\n",
      "Iteration 2368, loss = 0.13871828\n",
      "Iteration 2369, loss = 0.13869588\n",
      "Iteration 2370, loss = 0.13859351\n",
      "Iteration 2371, loss = 0.13871995\n",
      "Iteration 2372, loss = 0.13857506\n",
      "Iteration 2373, loss = 0.13842510\n",
      "Iteration 2374, loss = 0.13834320\n",
      "Iteration 2375, loss = 0.13825877\n",
      "Iteration 2376, loss = 0.13820445\n",
      "Iteration 2377, loss = 0.13820932\n",
      "Iteration 2378, loss = 0.13806101\n",
      "Iteration 2379, loss = 0.13802639\n",
      "Iteration 2380, loss = 0.13794462\n",
      "Iteration 2381, loss = 0.13814614\n",
      "Iteration 2382, loss = 0.13784812\n",
      "Iteration 2383, loss = 0.13773046\n",
      "Iteration 2384, loss = 0.13769627\n",
      "Iteration 2385, loss = 0.13774318\n",
      "Iteration 2386, loss = 0.13755134\n",
      "Iteration 2387, loss = 0.13756097\n",
      "Iteration 2388, loss = 0.13742324\n",
      "Iteration 2389, loss = 0.13742672\n",
      "Iteration 2390, loss = 0.13730145\n",
      "Iteration 2391, loss = 0.13729404\n",
      "Iteration 2392, loss = 0.13714102\n",
      "Iteration 2393, loss = 0.13711162\n",
      "Iteration 2394, loss = 0.13712952\n",
      "Iteration 2395, loss = 0.13698191\n",
      "Iteration 2396, loss = 0.13691208\n",
      "Iteration 2397, loss = 0.13696750\n",
      "Iteration 2398, loss = 0.13675532\n",
      "Iteration 2399, loss = 0.13668128\n",
      "Iteration 2400, loss = 0.13665016\n",
      "Iteration 2401, loss = 0.13665922\n",
      "Iteration 2402, loss = 0.13655810\n",
      "Iteration 2403, loss = 0.13642605\n",
      "Iteration 2404, loss = 0.13646275\n",
      "Iteration 2405, loss = 0.13640239\n",
      "Iteration 2406, loss = 0.13627192\n",
      "Iteration 2407, loss = 0.13620475\n",
      "Iteration 2408, loss = 0.13622022\n",
      "Iteration 2409, loss = 0.13603708\n",
      "Iteration 2410, loss = 0.13598773\n",
      "Iteration 2411, loss = 0.13603643\n",
      "Iteration 2412, loss = 0.13586373\n",
      "Iteration 2413, loss = 0.13582283\n",
      "Iteration 2414, loss = 0.13579348\n",
      "Iteration 2415, loss = 0.13567695\n",
      "Iteration 2416, loss = 0.13587806\n",
      "Iteration 2417, loss = 0.13560208\n",
      "Iteration 2418, loss = 0.13552648\n",
      "Iteration 2419, loss = 0.13540678\n",
      "Iteration 2420, loss = 0.13532729\n",
      "Iteration 2421, loss = 0.13527415\n",
      "Iteration 2422, loss = 0.13530019\n",
      "Iteration 2423, loss = 0.13516917\n",
      "Iteration 2424, loss = 0.13528481\n",
      "Iteration 2425, loss = 0.13504445\n",
      "Iteration 2426, loss = 0.13496446\n",
      "Iteration 2427, loss = 0.13498883\n",
      "Iteration 2428, loss = 0.13486280\n",
      "Iteration 2429, loss = 0.13485082\n",
      "Iteration 2430, loss = 0.13475448\n",
      "Iteration 2431, loss = 0.13471659\n",
      "Iteration 2432, loss = 0.13474173\n",
      "Iteration 2433, loss = 0.13458011\n",
      "Iteration 2434, loss = 0.13453912\n",
      "Iteration 2435, loss = 0.13443545\n",
      "Iteration 2436, loss = 0.13443194\n",
      "Iteration 2437, loss = 0.13428032\n",
      "Iteration 2438, loss = 0.13432724\n",
      "Iteration 2439, loss = 0.13416284\n",
      "Iteration 2440, loss = 0.13411362\n",
      "Iteration 2441, loss = 0.13401571\n",
      "Iteration 2442, loss = 0.13397555\n",
      "Iteration 2443, loss = 0.13395397\n",
      "Iteration 2444, loss = 0.13385775\n",
      "Iteration 2445, loss = 0.13379196\n",
      "Iteration 2446, loss = 0.13370460\n",
      "Iteration 2447, loss = 0.13376616\n",
      "Iteration 2448, loss = 0.13358958\n",
      "Iteration 2449, loss = 0.13356057\n",
      "Iteration 2450, loss = 0.13359253\n",
      "Iteration 2451, loss = 0.13353329\n",
      "Iteration 2452, loss = 0.13337061\n",
      "Iteration 2453, loss = 0.13326256\n",
      "Iteration 2454, loss = 0.13321131\n",
      "Iteration 2455, loss = 0.13313324\n",
      "Iteration 2456, loss = 0.13315297\n",
      "Iteration 2457, loss = 0.13302631\n",
      "Iteration 2458, loss = 0.13301522\n",
      "Iteration 2459, loss = 0.13289521\n",
      "Iteration 2460, loss = 0.13285456\n",
      "Iteration 2461, loss = 0.13278094\n",
      "Iteration 2462, loss = 0.13271664\n",
      "Iteration 2463, loss = 0.13277130\n",
      "Iteration 2464, loss = 0.13263335\n",
      "Iteration 2465, loss = 0.13260992\n",
      "Iteration 2466, loss = 0.13249686\n",
      "Iteration 2467, loss = 0.13240725\n",
      "Iteration 2468, loss = 0.13232862\n",
      "Iteration 2469, loss = 0.13231829\n",
      "Iteration 2470, loss = 0.13229392\n",
      "Iteration 2471, loss = 0.13215028\n",
      "Iteration 2472, loss = 0.13206726\n",
      "Iteration 2473, loss = 0.13200738\n",
      "Iteration 2474, loss = 0.13197023\n",
      "Iteration 2475, loss = 0.13212993\n",
      "Iteration 2476, loss = 0.13188664\n",
      "Iteration 2477, loss = 0.13182237\n",
      "Iteration 2478, loss = 0.13171480\n",
      "Iteration 2479, loss = 0.13163747\n",
      "Iteration 2480, loss = 0.13170868\n",
      "Iteration 2481, loss = 0.13157551\n",
      "Iteration 2482, loss = 0.13148679\n",
      "Iteration 2483, loss = 0.13154749\n",
      "Iteration 2484, loss = 0.13134645\n",
      "Iteration 2485, loss = 0.13126959\n",
      "Iteration 2486, loss = 0.13131866\n",
      "Iteration 2487, loss = 0.13115890\n",
      "Iteration 2488, loss = 0.13110135\n",
      "Iteration 2489, loss = 0.13108137\n",
      "Iteration 2490, loss = 0.13094859\n",
      "Iteration 2491, loss = 0.13089125\n",
      "Iteration 2492, loss = 0.13085135\n",
      "Iteration 2493, loss = 0.13084024\n",
      "Iteration 2494, loss = 0.13074010\n",
      "Iteration 2495, loss = 0.13084262\n",
      "Iteration 2496, loss = 0.13064761\n",
      "Iteration 2497, loss = 0.13057137\n",
      "Iteration 2498, loss = 0.13054122\n",
      "Iteration 2499, loss = 0.13054536\n",
      "Iteration 2500, loss = 0.13039101\n",
      "Iteration 2501, loss = 0.13033938\n",
      "Iteration 2502, loss = 0.13029595\n",
      "Iteration 2503, loss = 0.13023306\n",
      "Iteration 2504, loss = 0.13016567\n",
      "Iteration 2505, loss = 0.13006432\n",
      "Iteration 2506, loss = 0.13021760\n",
      "Iteration 2507, loss = 0.12993723\n",
      "Iteration 2508, loss = 0.12992034\n",
      "Iteration 2509, loss = 0.12989062\n",
      "Iteration 2510, loss = 0.12977599\n",
      "Iteration 2511, loss = 0.12974645\n",
      "Iteration 2512, loss = 0.12970207\n",
      "Iteration 2513, loss = 0.12971380\n",
      "Iteration 2514, loss = 0.12957043\n",
      "Iteration 2515, loss = 0.12953305\n",
      "Iteration 2516, loss = 0.12947563\n",
      "Iteration 2517, loss = 0.12942101\n",
      "Iteration 2518, loss = 0.12933163\n",
      "Iteration 2519, loss = 0.12934514\n",
      "Iteration 2520, loss = 0.12918659\n",
      "Iteration 2521, loss = 0.12924329\n",
      "Iteration 2522, loss = 0.12913697\n",
      "Iteration 2523, loss = 0.12900659\n",
      "Iteration 2524, loss = 0.12899382\n",
      "Iteration 2525, loss = 0.12889022\n",
      "Iteration 2526, loss = 0.12886213\n",
      "Iteration 2527, loss = 0.12883274\n",
      "Iteration 2528, loss = 0.12870649\n",
      "Iteration 2529, loss = 0.12866946\n",
      "Iteration 2530, loss = 0.12861069\n",
      "Iteration 2531, loss = 0.12857822\n",
      "Iteration 2532, loss = 0.12848849\n",
      "Iteration 2533, loss = 0.12847562\n",
      "Iteration 2534, loss = 0.12842580\n",
      "Iteration 2535, loss = 0.12834371\n",
      "Iteration 2536, loss = 0.12836982\n",
      "Iteration 2537, loss = 0.12819116\n",
      "Iteration 2538, loss = 0.12817491\n",
      "Iteration 2539, loss = 0.12824459\n",
      "Iteration 2540, loss = 0.12803740\n",
      "Iteration 2541, loss = 0.12802369\n",
      "Iteration 2542, loss = 0.12801419\n",
      "Iteration 2543, loss = 0.12788939\n",
      "Iteration 2544, loss = 0.12781393\n",
      "Iteration 2545, loss = 0.12773208\n",
      "Iteration 2546, loss = 0.12772920\n",
      "Iteration 2547, loss = 0.12772025\n",
      "Iteration 2548, loss = 0.12757514\n",
      "Iteration 2549, loss = 0.12754022\n",
      "Iteration 2550, loss = 0.12745482\n",
      "Iteration 2551, loss = 0.12740188\n",
      "Iteration 2552, loss = 0.12739736\n",
      "Iteration 2553, loss = 0.12737998\n",
      "Iteration 2554, loss = 0.12731122\n",
      "Iteration 2555, loss = 0.12719029\n",
      "Iteration 2556, loss = 0.12713268\n",
      "Iteration 2557, loss = 0.12708628\n",
      "Iteration 2558, loss = 0.12706469\n",
      "Iteration 2559, loss = 0.12699103\n",
      "Iteration 2560, loss = 0.12694123\n",
      "Iteration 2561, loss = 0.12688209\n",
      "Iteration 2562, loss = 0.12685118\n",
      "Iteration 2563, loss = 0.12686155\n",
      "Iteration 2564, loss = 0.12671621\n",
      "Iteration 2565, loss = 0.12662902\n",
      "Iteration 2566, loss = 0.12657819\n",
      "Iteration 2567, loss = 0.12650963\n",
      "Iteration 2568, loss = 0.12649123\n",
      "Iteration 2569, loss = 0.12639820\n",
      "Iteration 2570, loss = 0.12637304\n",
      "Iteration 2571, loss = 0.12635044\n",
      "Iteration 2572, loss = 0.12623771\n",
      "Iteration 2573, loss = 0.12617111\n",
      "Iteration 2574, loss = 0.12612508\n",
      "Iteration 2575, loss = 0.12624470\n",
      "Iteration 2576, loss = 0.12601738\n",
      "Iteration 2577, loss = 0.12600555\n",
      "Iteration 2578, loss = 0.12593965\n",
      "Iteration 2579, loss = 0.12586953\n",
      "Iteration 2580, loss = 0.12584650\n",
      "Iteration 2581, loss = 0.12573017\n",
      "Iteration 2582, loss = 0.12565708\n",
      "Iteration 2583, loss = 0.12560927\n",
      "Iteration 2584, loss = 0.12565038\n",
      "Iteration 2585, loss = 0.12552535\n",
      "Iteration 2586, loss = 0.12549302\n",
      "Iteration 2587, loss = 0.12548540\n",
      "Iteration 2588, loss = 0.12534093\n",
      "Iteration 2589, loss = 0.12528672\n",
      "Iteration 2590, loss = 0.12524945\n",
      "Iteration 2591, loss = 0.12518234\n",
      "Iteration 2592, loss = 0.12513057\n",
      "Iteration 2593, loss = 0.12506488\n",
      "Iteration 2594, loss = 0.12498062\n",
      "Iteration 2595, loss = 0.12497540\n",
      "Iteration 2596, loss = 0.12496700\n",
      "Iteration 2597, loss = 0.12490482\n",
      "Iteration 2598, loss = 0.12476970\n",
      "Iteration 2599, loss = 0.12476005\n",
      "Iteration 2600, loss = 0.12473080\n",
      "Iteration 2601, loss = 0.12461847\n",
      "Iteration 2602, loss = 0.12460204\n",
      "Iteration 2603, loss = 0.12458039\n",
      "Iteration 2604, loss = 0.12447697\n",
      "Iteration 2605, loss = 0.12446547\n",
      "Iteration 2606, loss = 0.12437779\n",
      "Iteration 2607, loss = 0.12430978\n",
      "Iteration 2608, loss = 0.12424047\n",
      "Iteration 2609, loss = 0.12421252\n",
      "Iteration 2610, loss = 0.12414292\n",
      "Iteration 2611, loss = 0.12406217\n",
      "Iteration 2612, loss = 0.12402034\n",
      "Iteration 2613, loss = 0.12401665\n",
      "Iteration 2614, loss = 0.12387950\n",
      "Iteration 2615, loss = 0.12392287\n",
      "Iteration 2616, loss = 0.12377764\n",
      "Iteration 2617, loss = 0.12374547\n",
      "Iteration 2618, loss = 0.12376309\n",
      "Iteration 2619, loss = 0.12366721\n",
      "Iteration 2620, loss = 0.12361545\n",
      "Iteration 2621, loss = 0.12357736\n",
      "Iteration 2622, loss = 0.12350845\n",
      "Iteration 2623, loss = 0.12342335\n",
      "Iteration 2624, loss = 0.12338951\n",
      "Iteration 2625, loss = 0.12332560\n",
      "Iteration 2626, loss = 0.12327458\n",
      "Iteration 2627, loss = 0.12322613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74690785\n",
      "Iteration 2, loss = 0.74601516\n",
      "Iteration 3, loss = 0.74466705\n",
      "Iteration 4, loss = 0.74315052\n",
      "Iteration 5, loss = 0.74134547\n",
      "Iteration 6, loss = 0.73931922\n",
      "Iteration 7, loss = 0.73761499\n",
      "Iteration 8, loss = 0.73559114\n",
      "Iteration 9, loss = 0.73375769\n",
      "Iteration 10, loss = 0.73190015\n",
      "Iteration 11, loss = 0.73012715\n",
      "Iteration 12, loss = 0.72825793\n",
      "Iteration 13, loss = 0.72641733\n",
      "Iteration 14, loss = 0.72485527\n",
      "Iteration 15, loss = 0.72311051\n",
      "Iteration 16, loss = 0.72147739\n",
      "Iteration 17, loss = 0.71976054\n",
      "Iteration 18, loss = 0.71811892\n",
      "Iteration 19, loss = 0.71654155\n",
      "Iteration 20, loss = 0.71482718\n",
      "Iteration 21, loss = 0.71321240\n",
      "Iteration 22, loss = 0.71154057\n",
      "Iteration 23, loss = 0.70996934\n",
      "Iteration 24, loss = 0.70829865\n",
      "Iteration 25, loss = 0.70674827\n",
      "Iteration 26, loss = 0.70498654\n",
      "Iteration 27, loss = 0.70335634\n",
      "Iteration 28, loss = 0.70161670\n",
      "Iteration 29, loss = 0.69993512\n",
      "Iteration 30, loss = 0.69821122\n",
      "Iteration 31, loss = 0.69644638\n",
      "Iteration 32, loss = 0.69462543\n",
      "Iteration 33, loss = 0.69286031\n",
      "Iteration 34, loss = 0.69097850\n",
      "Iteration 35, loss = 0.68911867\n",
      "Iteration 36, loss = 0.68716528\n",
      "Iteration 37, loss = 0.68520822\n",
      "Iteration 38, loss = 0.68319816\n",
      "Iteration 39, loss = 0.68116282\n",
      "Iteration 40, loss = 0.67917661\n",
      "Iteration 41, loss = 0.67702178\n",
      "Iteration 42, loss = 0.67486398\n",
      "Iteration 43, loss = 0.67272534\n",
      "Iteration 44, loss = 0.67046909\n",
      "Iteration 45, loss = 0.66821895\n",
      "Iteration 46, loss = 0.66590223\n",
      "Iteration 47, loss = 0.66356522\n",
      "Iteration 48, loss = 0.66129756\n",
      "Iteration 49, loss = 0.65881282\n",
      "Iteration 50, loss = 0.65632513\n",
      "Iteration 51, loss = 0.65376697\n",
      "Iteration 52, loss = 0.65119925\n",
      "Iteration 53, loss = 0.64864948\n",
      "Iteration 54, loss = 0.64603839\n",
      "Iteration 55, loss = 0.64339471\n",
      "Iteration 56, loss = 0.64066807\n",
      "Iteration 57, loss = 0.63791730\n",
      "Iteration 58, loss = 0.63509748\n",
      "Iteration 59, loss = 0.63225761\n",
      "Iteration 60, loss = 0.62944312\n",
      "Iteration 61, loss = 0.62658488\n",
      "Iteration 62, loss = 0.62365143\n",
      "Iteration 63, loss = 0.62072962\n",
      "Iteration 64, loss = 0.61776608\n",
      "Iteration 65, loss = 0.61471962\n",
      "Iteration 66, loss = 0.61174918\n",
      "Iteration 67, loss = 0.60862674\n",
      "Iteration 68, loss = 0.60557533\n",
      "Iteration 69, loss = 0.60250876\n",
      "Iteration 70, loss = 0.59942379\n",
      "Iteration 71, loss = 0.59618790\n",
      "Iteration 72, loss = 0.59307974\n",
      "Iteration 73, loss = 0.58986254\n",
      "Iteration 74, loss = 0.58660166\n",
      "Iteration 75, loss = 0.58343177\n",
      "Iteration 76, loss = 0.58018094\n",
      "Iteration 77, loss = 0.57693204\n",
      "Iteration 78, loss = 0.57352446\n",
      "Iteration 79, loss = 0.57033407\n",
      "Iteration 80, loss = 0.56699380\n",
      "Iteration 81, loss = 0.56357084\n",
      "Iteration 82, loss = 0.56030362\n",
      "Iteration 83, loss = 0.55688802\n",
      "Iteration 84, loss = 0.55359112\n",
      "Iteration 85, loss = 0.55026932\n",
      "Iteration 86, loss = 0.54686273\n",
      "Iteration 87, loss = 0.54351676\n",
      "Iteration 88, loss = 0.54012412\n",
      "Iteration 89, loss = 0.53684302\n",
      "Iteration 90, loss = 0.53342177\n",
      "Iteration 91, loss = 0.53025701\n",
      "Iteration 92, loss = 0.52694733\n",
      "Iteration 93, loss = 0.52371858\n",
      "Iteration 94, loss = 0.52048317\n",
      "Iteration 95, loss = 0.51724871\n",
      "Iteration 96, loss = 0.51408473\n",
      "Iteration 97, loss = 0.51080071\n",
      "Iteration 98, loss = 0.50780718\n",
      "Iteration 99, loss = 0.50475386\n",
      "Iteration 100, loss = 0.50162983\n",
      "Iteration 101, loss = 0.49857904\n",
      "Iteration 102, loss = 0.49572422\n",
      "Iteration 103, loss = 0.49253715\n",
      "Iteration 104, loss = 0.48969027\n",
      "Iteration 105, loss = 0.48679514\n",
      "Iteration 106, loss = 0.48402244\n",
      "Iteration 107, loss = 0.48124451\n",
      "Iteration 108, loss = 0.47857098\n",
      "Iteration 109, loss = 0.47576138\n",
      "Iteration 110, loss = 0.47315920\n",
      "Iteration 111, loss = 0.47066596\n",
      "Iteration 112, loss = 0.46790393\n",
      "Iteration 113, loss = 0.46557750\n",
      "Iteration 114, loss = 0.46305624\n",
      "Iteration 115, loss = 0.46069140\n",
      "Iteration 116, loss = 0.45833471\n",
      "Iteration 117, loss = 0.45605460\n",
      "Iteration 118, loss = 0.45381858\n",
      "Iteration 119, loss = 0.45167811\n",
      "Iteration 120, loss = 0.44958090\n",
      "Iteration 121, loss = 0.44748282\n",
      "Iteration 2278, loss = 0.07928458\n",
      "Iteration 2279, loss = 0.07920919\n",
      "Iteration 2280, loss = 0.07913318\n",
      "Iteration 2281, loss = 0.07928358\n",
      "Iteration 2282, loss = 0.07910366\n",
      "Iteration 2283, loss = 0.07902181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79061683\n",
      "Iteration 2, loss = 0.78762703\n",
      "Iteration 3, loss = 0.78307600\n",
      "Iteration 4, loss = 0.77725783\n",
      "Iteration 5, loss = 0.77130258\n",
      "Iteration 6, loss = 0.76473936\n",
      "Iteration 7, loss = 0.75807612\n",
      "Iteration 8, loss = 0.75125132\n",
      "Iteration 9, loss = 0.74430924\n",
      "Iteration 10, loss = 0.73779256\n",
      "Iteration 11, loss = 0.73134878\n",
      "Iteration 12, loss = 0.72473039\n",
      "Iteration 13, loss = 0.71881154\n",
      "Iteration 14, loss = 0.71277740\n",
      "Iteration 15, loss = 0.70708960\n",
      "Iteration 16, loss = 0.70145220\n",
      "Iteration 17, loss = 0.69593107\n",
      "Iteration 18, loss = 0.69073170\n",
      "Iteration 19, loss = 0.68593970\n",
      "Iteration 20, loss = 0.68069153\n",
      "Iteration 21, loss = 0.67568640\n",
      "Iteration 22, loss = 0.67083825\n",
      "Iteration 23, loss = 0.66644984\n",
      "Iteration 24, loss = 0.66187685\n",
      "Iteration 25, loss = 0.65753056\n",
      "Iteration 26, loss = 0.65293166\n",
      "Iteration 27, loss = 0.64882172\n",
      "Iteration 28, loss = 0.64453010\n",
      "Iteration 29, loss = 0.64038707\n",
      "Iteration 30, loss = 0.63609769\n",
      "Iteration 31, loss = 0.63210875\n",
      "Iteration 32, loss = 0.62798548\n",
      "Iteration 33, loss = 0.62405464\n",
      "Iteration 34, loss = 0.62008112\n",
      "Iteration 35, loss = 0.61603139\n",
      "Iteration 36, loss = 0.61221079\n",
      "Iteration 37, loss = 0.60842020\n",
      "Iteration 38, loss = 0.60444784\n",
      "Iteration 39, loss = 0.60069414\n",
      "Iteration 40, loss = 0.59689952\n",
      "Iteration 41, loss = 0.59319613\n",
      "Iteration 42, loss = 0.58952434\n",
      "Iteration 43, loss = 0.58599813\n",
      "Iteration 44, loss = 0.58243298\n",
      "Iteration 45, loss = 0.57888371\n",
      "Iteration 46, loss = 0.57543225\n",
      "Iteration 47, loss = 0.57201773\n",
      "Iteration 48, loss = 0.56860202\n",
      "Iteration 49, loss = 0.56525462\n",
      "Iteration 50, loss = 0.56201824\n",
      "Iteration 51, loss = 0.55868427\n",
      "Iteration 52, loss = 0.55545071\n",
      "Iteration 53, loss = 0.55233486\n",
      "Iteration 54, loss = 0.54918000\n",
      "Iteration 55, loss = 0.54607003\n",
      "Iteration 56, loss = 0.54296106\n",
      "Iteration 57, loss = 0.54006804\n",
      "Iteration 58, loss = 0.53708020\n",
      "Iteration 59, loss = 0.53411956\n",
      "Iteration 60, loss = 0.53123184\n",
      "Iteration 61, loss = 0.52843737\n",
      "Iteration 62, loss = 0.52565871\n",
      "Iteration 63, loss = 0.52291372\n",
      "Iteration 64, loss = 0.52010354\n",
      "Iteration 65, loss = 0.51753417\n",
      "Iteration 66, loss = 0.51489168\n",
      "Iteration 67, loss = 0.51233576\n",
      "Iteration 68, loss = 0.50988618\n",
      "Iteration 69, loss = 0.50735629\n",
      "Iteration 70, loss = 0.50487552\n",
      "Iteration 71, loss = 0.50247117\n",
      "Iteration 72, loss = 0.50017393\n",
      "Iteration 73, loss = 0.49781613\n",
      "Iteration 74, loss = 0.49564905\n",
      "Iteration 75, loss = 0.49329122\n",
      "Iteration 76, loss = 0.49117526\n",
      "Iteration 77, loss = 0.48893460\n",
      "Iteration 78, loss = 0.48690517\n",
      "Iteration 79, loss = 0.48486707\n",
      "Iteration 80, loss = 0.48286057\n",
      "Iteration 81, loss = 0.48080894\n",
      "Iteration 82, loss = 0.47888937\n",
      "Iteration 83, loss = 0.47697258\n",
      "Iteration 84, loss = 0.47515854\n",
      "Iteration 85, loss = 0.47323548\n",
      "Iteration 86, loss = 0.47143387\n",
      "Iteration 87, loss = 0.46973674\n",
      "Iteration 88, loss = 0.46798627\n",
      "Iteration 89, loss = 0.46636981\n",
      "Iteration 90, loss = 0.46462167\n",
      "Iteration 91, loss = 0.46302461\n",
      "Iteration 92, loss = 0.46147147\n",
      "Iteration 93, loss = 0.45990809\n",
      "Iteration 94, loss = 0.45839316\n",
      "Iteration 95, loss = 0.45691331\n",
      "Iteration 96, loss = 0.45539280\n",
      "Iteration 97, loss = 0.45401072\n",
      "Iteration 98, loss = 0.45261789\n",
      "Iteration 99, loss = 0.45126682\n",
      "Iteration 100, loss = 0.44987498\n",
      "Iteration 101, loss = 0.44859054\n",
      "Iteration 102, loss = 0.44728058\n",
      "Iteration 103, loss = 0.44608954\n",
      "Iteration 104, loss = 0.44474271\n",
      "Iteration 105, loss = 0.44351655\n",
      "Iteration 106, loss = 0.44232195\n",
      "Iteration 107, loss = 0.44119065\n",
      "Iteration 108, loss = 0.44004835\n",
      "Iteration 109, loss = 0.43887966\n",
      "Iteration 110, loss = 0.43786812\n",
      "Iteration 111, loss = 0.43667432\n",
      "Iteration 112, loss = 0.43559357\n",
      "Iteration 113, loss = 0.43459600\n",
      "Iteration 114, loss = 0.43363563\n",
      "Iteration 115, loss = 0.43261205\n",
      "Iteration 116, loss = 0.43163128\n",
      "Iteration 117, loss = 0.43062278\n",
      "Iteration 118, loss = 0.42967428\n",
      "Iteration 119, loss = 0.42881796\n",
      "Iteration 120, loss = 0.42786401\n",
      "Iteration 121, loss = 0.42698625\n",
      "Iteration 122, loss = 0.42608887\n",
      "Iteration 123, loss = 0.42534905\n",
      "Iteration 124, loss = 0.42440484\n",
      "Iteration 125, loss = 0.42359409\n",
      "Iteration 126, loss = 0.42274545\n",
      "Iteration 127, loss = 0.42198717\n",
      "Iteration 128, loss = 0.42117494\n",
      "Iteration 129, loss = 0.42040177\n",
      "Iteration 130, loss = 0.41969619\n",
      "Iteration 131, loss = 0.41893320\n",
      "Iteration 132, loss = 0.41813921\n",
      "Iteration 133, loss = 0.41750036\n",
      "Iteration 134, loss = 0.41672164\n",
      "Iteration 135, loss = 0.41601179\n",
      "Iteration 136, loss = 0.41529992\n",
      "Iteration 137, loss = 0.41463142\n",
      "Iteration 138, loss = 0.41394475\n",
      "Iteration 139, loss = 0.41332109\n",
      "Iteration 140, loss = 0.41262020\n",
      "Iteration 141, loss = 0.41199999\n",
      "Iteration 142, loss = 0.41134913\n",
      "Iteration 143, loss = 0.41072422\n",
      "Iteration 144, loss = 0.41006630\n",
      "Iteration 145, loss = 0.40945595\n",
      "Iteration 146, loss = 0.40891673\n",
      "Iteration 147, loss = 0.40827356\n",
      "Iteration 148, loss = 0.40771496\n",
      "Iteration 149, loss = 0.40711582\n",
      "Iteration 150, loss = 0.40653194\n",
      "Iteration 151, loss = 0.40592538\n",
      "Iteration 152, loss = 0.40540042\n",
      "Iteration 153, loss = 0.40482512\n",
      "Iteration 154, loss = 0.40429924\n",
      "Iteration 155, loss = 0.40376709\n",
      "Iteration 156, loss = 0.40321580\n",
      "Iteration 157, loss = 0.40269074\n",
      "Iteration 158, loss = 0.40214690\n",
      "Iteration 159, loss = 0.40162372\n",
      "Iteration 160, loss = 0.40108088\n",
      "Iteration 161, loss = 0.40058150\n",
      "Iteration 162, loss = 0.40009181\n",
      "Iteration 163, loss = 0.39959641\n",
      "Iteration 164, loss = 0.39907230\n",
      "Iteration 165, loss = 0.39861652\n",
      "Iteration 166, loss = 0.39808518\n",
      "Iteration 167, loss = 0.39763377\n",
      "Iteration 168, loss = 0.39719752\n",
      "Iteration 169, loss = 0.39668109\n",
      "Iteration 170, loss = 0.39621144\n",
      "Iteration 171, loss = 0.39577059\n",
      "Iteration 172, loss = 0.39529123\n",
      "Iteration 173, loss = 0.39484609\n",
      "Iteration 174, loss = 0.39438394\n",
      "Iteration 175, loss = 0.39398563\n",
      "Iteration 176, loss = 0.39349805\n",
      "Iteration 177, loss = 0.39305434\n",
      "Iteration 178, loss = 0.39261866\n",
      "Iteration 179, loss = 0.39221721\n",
      "Iteration 180, loss = 0.39177233\n",
      "Iteration 181, loss = 0.39133481\n",
      "Iteration 182, loss = 0.39093419\n",
      "Iteration 183, loss = 0.39054777\n",
      "Iteration 184, loss = 0.39007695\n",
      "Iteration 185, loss = 0.38966443\n",
      "Iteration 186, loss = 0.38928429\n",
      "Iteration 187, loss = 0.38883218\n",
      "Iteration 188, loss = 0.38846396\n",
      "Iteration 189, loss = 0.38806674\n",
      "Iteration 190, loss = 0.38769843\n",
      "Iteration 191, loss = 0.38729775\n",
      "Iteration 192, loss = 0.38688229\n",
      "Iteration 193, loss = 0.38647931\n",
      "Iteration 194, loss = 0.38606570\n",
      "Iteration 195, loss = 0.38571237\n",
      "Iteration 196, loss = 0.38531722\n",
      "Iteration 197, loss = 0.38494829\n",
      "Iteration 198, loss = 0.38459923\n",
      "Iteration 199, loss = 0.38416287\n",
      "Iteration 200, loss = 0.38383931\n",
      "Iteration 201, loss = 0.38343500\n",
      "Iteration 202, loss = 0.38308285\n",
      "Iteration 203, loss = 0.38270945\n",
      "Iteration 204, loss = 0.38231972\n",
      "Iteration 205, loss = 0.38197621\n",
      "Iteration 206, loss = 0.38160201\n",
      "Iteration 207, loss = 0.38127821\n",
      "Iteration 208, loss = 0.38093061\n",
      "Iteration 209, loss = 0.38058051\n",
      "Iteration 210, loss = 0.38017643\n",
      "Iteration 211, loss = 0.37985958\n",
      "Iteration 212, loss = 0.37948038\n",
      "Iteration 213, loss = 0.37910475\n",
      "Iteration 214, loss = 0.37876228\n",
      "Iteration 215, loss = 0.37842704\n",
      "Iteration 216, loss = 0.37809258\n",
      "Iteration 217, loss = 0.37777780\n",
      "Iteration 218, loss = 0.37744074\n",
      "Iteration 219, loss = 0.37708760\n",
      "Iteration 220, loss = 0.37675738\n",
      "Iteration 221, loss = 0.37638205\n",
      "Iteration 222, loss = 0.37607119\n",
      "Iteration 223, loss = 0.37575683\n",
      "Iteration 224, loss = 0.37545048\n",
      "Iteration 225, loss = 0.37508553\n",
      "Iteration 226, loss = 0.37479352\n",
      "Iteration 227, loss = 0.37444575\n",
      "Iteration 228, loss = 0.37414942\n",
      "Iteration 229, loss = 0.37382207\n",
      "Iteration 230, loss = 0.37351314\n",
      "Iteration 231, loss = 0.37318880\n",
      "Iteration 232, loss = 0.37286845\n",
      "Iteration 233, loss = 0.37258386\n",
      "Iteration 234, loss = 0.37233398\n",
      "Iteration 235, loss = 0.37193360\n",
      "Iteration 236, loss = 0.37162649\n",
      "Iteration 237, loss = 0.37133094\n",
      "Iteration 238, loss = 0.37102342\n",
      "Iteration 239, loss = 0.37072099\n",
      "Iteration 240, loss = 0.37042023\n",
      "Iteration 241, loss = 0.37016063\n",
      "Iteration 242, loss = 0.36985062\n",
      "Iteration 243, loss = 0.36956293\n",
      "Iteration 244, loss = 0.36925170\n",
      "Iteration 245, loss = 0.36898380\n",
      "Iteration 246, loss = 0.36865279\n",
      "Iteration 247, loss = 0.36837516\n",
      "Iteration 248, loss = 0.36814180\n",
      "Iteration 249, loss = 0.36783047\n",
      "Iteration 250, loss = 0.36752896\n",
      "Iteration 251, loss = 0.36726983\n",
      "Iteration 252, loss = 0.36698340\n",
      "Iteration 253, loss = 0.36672778\n",
      "Iteration 254, loss = 0.36642078\n",
      "Iteration 255, loss = 0.36620757\n",
      "Iteration 256, loss = 0.36592100\n",
      "Iteration 257, loss = 0.36564499\n",
      "Iteration 258, loss = 0.36542877\n",
      "Iteration 259, loss = 0.36513312\n",
      "Iteration 260, loss = 0.36484437\n",
      "Iteration 261, loss = 0.36458221\n",
      "Iteration 262, loss = 0.36432952\n",
      "Iteration 263, loss = 0.36408878\n",
      "Iteration 264, loss = 0.36385593\n",
      "Iteration 265, loss = 0.36357880\n",
      "Iteration 266, loss = 0.36333008\n",
      "Iteration 267, loss = 0.36307876\n",
      "Iteration 268, loss = 0.36284737\n",
      "Iteration 269, loss = 0.36258280\n",
      "Iteration 270, loss = 0.36233702\n",
      "Iteration 271, loss = 0.36212427\n",
      "Iteration 272, loss = 0.36185371\n",
      "Iteration 273, loss = 0.36168795\n",
      "Iteration 274, loss = 0.36137568\n",
      "Iteration 275, loss = 0.36113766\n",
      "Iteration 276, loss = 0.36091721\n",
      "Iteration 277, loss = 0.36064721\n",
      "Iteration 278, loss = 0.36040856\n",
      "Iteration 279, loss = 0.36022369\n",
      "Iteration 280, loss = 0.35994678\n",
      "Iteration 281, loss = 0.35974671\n",
      "Iteration 282, loss = 0.35951718\n",
      "Iteration 283, loss = 0.35930675\n",
      "Iteration 284, loss = 0.35905650\n",
      "Iteration 285, loss = 0.35885883\n",
      "Iteration 286, loss = 0.35869525\n",
      "Iteration 287, loss = 0.35843044\n",
      "Iteration 288, loss = 0.35815280\n",
      "Iteration 289, loss = 0.35795525\n",
      "Iteration 290, loss = 0.35775754\n",
      "Iteration 291, loss = 0.35752978\n",
      "Iteration 292, loss = 0.35728072\n",
      "Iteration 293, loss = 0.35705387\n",
      "Iteration 294, loss = 0.35687314\n",
      "Iteration 295, loss = 0.35664861\n",
      "Iteration 296, loss = 0.35643011\n",
      "Iteration 297, loss = 0.35624490\n",
      "Iteration 298, loss = 0.35602508\n",
      "Iteration 299, loss = 0.35579412\n",
      "Iteration 300, loss = 0.35558903\n",
      "Iteration 301, loss = 0.35537389\n",
      "Iteration 302, loss = 0.35518603\n",
      "Iteration 303, loss = 0.35496991\n",
      "Iteration 304, loss = 0.35477341\n",
      "Iteration 305, loss = 0.35455092\n",
      "Iteration 306, loss = 0.35435695\n",
      "Iteration 307, loss = 0.35415958\n",
      "Iteration 308, loss = 0.35394946\n",
      "Iteration 309, loss = 0.35375172\n",
      "Iteration 310, loss = 0.35357049\n",
      "Iteration 311, loss = 0.35339855\n",
      "Iteration 312, loss = 0.35313598\n",
      "Iteration 313, loss = 0.35296350\n",
      "Iteration 314, loss = 0.35274200\n",
      "Iteration 315, loss = 0.35256417\n",
      "Iteration 316, loss = 0.35237660\n",
      "Iteration 317, loss = 0.35216673\n",
      "Iteration 318, loss = 0.35196987\n",
      "Iteration 319, loss = 0.35178637\n",
      "Iteration 320, loss = 0.35157331\n",
      "Iteration 321, loss = 0.35138907\n",
      "Iteration 322, loss = 0.35120358\n",
      "Iteration 323, loss = 0.35101592\n",
      "Iteration 324, loss = 0.35085882\n",
      "Iteration 325, loss = 0.35064646\n",
      "Iteration 326, loss = 0.35043387\n",
      "Iteration 327, loss = 0.35024126\n",
      "Iteration 328, loss = 0.35009888\n",
      "Iteration 329, loss = 0.34989530\n",
      "Iteration 330, loss = 0.34971268\n",
      "Iteration 331, loss = 0.34951742\n",
      "Iteration 332, loss = 0.34934385\n",
      "Iteration 333, loss = 0.34916010\n",
      "Iteration 334, loss = 0.34898978\n",
      "Iteration 335, loss = 0.34881038\n",
      "Iteration 336, loss = 0.34862059\n",
      "Iteration 337, loss = 0.34844184\n",
      "Iteration 338, loss = 0.34824450\n",
      "Iteration 339, loss = 0.34808245\n",
      "Iteration 340, loss = 0.34789052\n",
      "Iteration 341, loss = 0.34783946\n",
      "Iteration 342, loss = 0.34759265\n",
      "Iteration 343, loss = 0.34738134\n",
      "Iteration 344, loss = 0.34720093\n",
      "Iteration 345, loss = 0.34705625\n",
      "Iteration 346, loss = 0.34683750\n",
      "Iteration 347, loss = 0.34671329\n",
      "Iteration 348, loss = 0.34650818\n",
      "Iteration 349, loss = 0.34636739\n",
      "Iteration 350, loss = 0.34619578\n",
      "Iteration 351, loss = 0.34602990\n",
      "Iteration 352, loss = 0.34582554\n",
      "Iteration 353, loss = 0.34567151\n",
      "Iteration 354, loss = 0.34551124\n",
      "Iteration 355, loss = 0.34532869\n",
      "Iteration 356, loss = 0.34518686\n",
      "Iteration 357, loss = 0.34499912\n",
      "Iteration 358, loss = 0.34485588\n",
      "Iteration 359, loss = 0.34467165\n",
      "Iteration 360, loss = 0.34454683\n",
      "Iteration 361, loss = 0.34437169\n",
      "Iteration 362, loss = 0.34417304\n",
      "Iteration 363, loss = 0.34402705\n",
      "Iteration 364, loss = 0.34385306\n",
      "Iteration 365, loss = 0.34370555\n",
      "Iteration 366, loss = 0.34356582\n",
      "Iteration 367, loss = 0.34339005\n",
      "Iteration 368, loss = 0.34323811\n",
      "Iteration 369, loss = 0.34307699\n",
      "Iteration 370, loss = 0.34291428\n",
      "Iteration 371, loss = 0.34278383\n",
      "Iteration 372, loss = 0.34260418\n",
      "Iteration 373, loss = 0.34243163\n",
      "Iteration 374, loss = 0.34228697\n",
      "Iteration 375, loss = 0.34212958\n",
      "Iteration 376, loss = 0.34197076\n",
      "Iteration 377, loss = 0.34181725\n",
      "Iteration 378, loss = 0.34169815\n",
      "Iteration 379, loss = 0.34149762\n",
      "Iteration 380, loss = 0.34135534\n",
      "Iteration 381, loss = 0.34121629\n",
      "Iteration 382, loss = 0.34106391\n",
      "Iteration 383, loss = 0.34091621\n",
      "Iteration 384, loss = 0.34073374\n",
      "Iteration 385, loss = 0.34059226\n",
      "Iteration 386, loss = 0.34043503\n",
      "Iteration 387, loss = 0.34026966\n",
      "Iteration 388, loss = 0.34010206\n",
      "Iteration 389, loss = 0.33995795\n",
      "Iteration 390, loss = 0.33985815\n",
      "Iteration 391, loss = 0.33965440\n",
      "Iteration 392, loss = 0.33949403\n",
      "Iteration 393, loss = 0.33936279\n",
      "Iteration 394, loss = 0.33919541\n",
      "Iteration 395, loss = 0.33909348\n",
      "Iteration 396, loss = 0.33897572\n",
      "Iteration 397, loss = 0.33879532\n",
      "Iteration 398, loss = 0.33859993\n",
      "Iteration 399, loss = 0.33844394\n",
      "Iteration 400, loss = 0.33834471\n",
      "Iteration 401, loss = 0.33816150\n",
      "Iteration 402, loss = 0.33799815\n",
      "Iteration 403, loss = 0.33786669\n",
      "Iteration 404, loss = 0.33770196\n",
      "Iteration 405, loss = 0.33755288\n",
      "Iteration 406, loss = 0.33739114\n",
      "Iteration 407, loss = 0.33726618\n",
      "Iteration 408, loss = 0.33710539\n",
      "Iteration 409, loss = 0.33699714\n",
      "Iteration 410, loss = 0.33681015\n",
      "Iteration 411, loss = 0.33668468\n",
      "Iteration 412, loss = 0.33655026\n",
      "Iteration 413, loss = 0.33636983\n",
      "Iteration 414, loss = 0.33626608\n",
      "Iteration 415, loss = 0.33611102\n",
      "Iteration 416, loss = 0.33593597\n",
      "Iteration 417, loss = 0.33578903\n",
      "Iteration 418, loss = 0.33567280\n",
      "Iteration 419, loss = 0.33548260\n",
      "Iteration 420, loss = 0.33538018\n",
      "Iteration 421, loss = 0.33520101\n",
      "Iteration 422, loss = 0.33505908\n",
      "Iteration 423, loss = 0.33492369\n",
      "Iteration 424, loss = 0.33478421\n",
      "Iteration 425, loss = 0.33463991\n",
      "Iteration 426, loss = 0.33454407\n",
      "Iteration 427, loss = 0.33434340\n",
      "Iteration 428, loss = 0.33417291\n",
      "Iteration 429, loss = 0.33404749\n",
      "Iteration 430, loss = 0.33390029\n",
      "Iteration 431, loss = 0.33375981\n",
      "Iteration 432, loss = 0.33363122\n",
      "Iteration 433, loss = 0.33348235\n",
      "Iteration 434, loss = 0.33332141\n",
      "Iteration 435, loss = 0.33318796\n",
      "Iteration 436, loss = 0.33305343\n",
      "Iteration 437, loss = 0.33289121\n",
      "Iteration 438, loss = 0.33276848\n",
      "Iteration 439, loss = 0.33261229\n",
      "Iteration 440, loss = 0.33247063\n",
      "Iteration 441, loss = 0.33232112\n",
      "Iteration 442, loss = 0.33220133\n",
      "Iteration 443, loss = 0.33206453\n",
      "Iteration 444, loss = 0.33192643\n",
      "Iteration 445, loss = 0.33180777\n",
      "Iteration 446, loss = 0.33163079\n",
      "Iteration 447, loss = 0.33147906\n",
      "Iteration 448, loss = 0.33135921\n",
      "Iteration 449, loss = 0.33122446\n",
      "Iteration 450, loss = 0.33107926\n",
      "Iteration 451, loss = 0.33094746\n",
      "Iteration 452, loss = 0.33080675\n",
      "Iteration 453, loss = 0.33065432\n",
      "Iteration 454, loss = 0.33052020\n",
      "Iteration 455, loss = 0.33038349\n",
      "Iteration 456, loss = 0.33026282\n",
      "Iteration 457, loss = 0.33017146\n",
      "Iteration 458, loss = 0.32998934\n",
      "Iteration 459, loss = 0.32984230\n",
      "Iteration 460, loss = 0.32972179\n",
      "Iteration 461, loss = 0.32956852\n",
      "Iteration 462, loss = 0.32943148\n",
      "Iteration 463, loss = 0.32931081\n",
      "Iteration 464, loss = 0.32917632\n",
      "Iteration 465, loss = 0.32905331\n",
      "Iteration 466, loss = 0.32888435\n",
      "Iteration 467, loss = 0.32874662\n",
      "Iteration 468, loss = 0.32865390\n",
      "Iteration 469, loss = 0.32848471\n",
      "Iteration 470, loss = 0.32834302\n",
      "Iteration 471, loss = 0.32824884\n",
      "Iteration 472, loss = 0.32813998\n",
      "Iteration 473, loss = 0.32796072\n",
      "Iteration 474, loss = 0.32787719\n",
      "Iteration 475, loss = 0.32770095\n",
      "Iteration 476, loss = 0.32753072\n",
      "Iteration 477, loss = 0.32743161\n",
      "Iteration 478, loss = 0.32727271\n",
      "Iteration 479, loss = 0.32712876\n",
      "Iteration 480, loss = 0.32703931\n",
      "Iteration 481, loss = 0.32687491\n",
      "Iteration 482, loss = 0.32673087\n",
      "Iteration 483, loss = 0.32659646\n",
      "Iteration 484, loss = 0.32645907\n",
      "Iteration 485, loss = 0.32633566\n",
      "Iteration 486, loss = 0.32621513\n",
      "Iteration 487, loss = 0.32611799\n",
      "Iteration 488, loss = 0.32596010\n",
      "Iteration 489, loss = 0.32584857\n",
      "Iteration 490, loss = 0.32570912\n",
      "\n",
      "Iteration 1240, loss = 0.22253051\n",
      "Iteration 1241, loss = 0.22236157\n",
      "Iteration 1242, loss = 0.22225866\n",
      "Iteration 1243, loss = 0.22217055\n",
      "Iteration 1244, loss = 0.22206106\n",
      "Iteration 1245, loss = 0.22191712\n",
      "Iteration 1246, loss = 0.22175491\n",
      "Iteration 1247, loss = 0.22163481\n",
      "Iteration 1248, loss = 0.22156293\n",
      "Iteration 1249, loss = 0.22144288\n",
      "Iteration 1250, loss = 0.22130847\n",
      "Iteration 1251, loss = 0.22117387\n",
      "Iteration 1252, loss = 0.22107781\n",
      "Iteration 1253, loss = 0.22094229\n",
      "Iteration 1254, loss = 0.22086052\n",
      "Iteration 1255, loss = 0.22071006\n",
      "Iteration 1256, loss = 0.22059816\n",
      "Iteration 1257, loss = 0.22045329\n",
      "Iteration 1258, loss = 0.22033979\n",
      "Iteration 1259, loss = 0.22024573\n",
      "Iteration 1260, loss = 0.22012270\n",
      "Iteration 1261, loss = 0.21997647\n",
      "Iteration 1262, loss = 0.21987788\n",
      "Iteration 1263, loss = 0.21975304\n",
      "Iteration 1264, loss = 0.21961115\n",
      "Iteration 1265, loss = 0.21950243\n",
      "Iteration 1266, loss = 0.21937254\n",
      "Iteration 1267, loss = 0.21925795\n",
      "Iteration 1268, loss = 0.21915021\n",
      "Iteration 1269, loss = 0.21904573\n",
      "Iteration 1270, loss = 0.21901505\n",
      "Iteration 1271, loss = 0.21879960\n",
      "Iteration 1272, loss = 0.21867815\n",
      "Iteration 1273, loss = 0.21854197\n",
      "Iteration 1274, loss = 0.21847596\n",
      "Iteration 1275, loss = 0.21832744\n",
      "Iteration 1276, loss = 0.21817559\n",
      "Iteration 1277, loss = 0.21809598\n",
      "Iteration 1278, loss = 0.21795019\n",
      "Iteration 1279, loss = 0.21782454\n",
      "Iteration 1280, loss = 0.21770111\n",
      "Iteration 1281, loss = 0.21760872\n",
      "Iteration 1282, loss = 0.21747438\n",
      "Iteration 1283, loss = 0.21735809\n",
      "Iteration 1284, loss = 0.21729036\n",
      "Iteration 1285, loss = 0.21714593\n",
      "Iteration 1286, loss = 0.21701847\n",
      "Iteration 1287, loss = 0.21691922\n",
      "Iteration 1288, loss = 0.21678193\n",
      "Iteration 1289, loss = 0.21662781\n",
      "Iteration 1290, loss = 0.21658662\n",
      "Iteration 1291, loss = 0.21640415\n",
      "Iteration 1292, loss = 0.21631590\n",
      "Iteration 1293, loss = 0.21623224\n",
      "Iteration 1294, loss = 0.21606029\n",
      "Iteration 1295, loss = 0.21602150\n",
      "Iteration 1296, loss = 0.21584316\n",
      "Iteration 1297, loss = 0.21572644\n",
      "Iteration 1298, loss = 0.21564286\n",
      "Iteration 1299, loss = 0.21550430\n",
      "Iteration 1300, loss = 0.21536332\n",
      "Iteration 1301, loss = 0.21525629\n",
      "Iteration 1302, loss = 0.21512573\n",
      "Iteration 1303, loss = 0.21503967\n",
      "Iteration 1304, loss = 0.21493971\n",
      "Iteration 1305, loss = 0.21477129\n",
      "Iteration 1306, loss = 0.21478267\n",
      "Iteration 1307, loss = 0.21456898\n",
      "Iteration 1308, loss = 0.21441345\n",
      "Iteration 1309, loss = 0.21431110\n",
      "Iteration 1310, loss = 0.21419228\n",
      "Iteration 1311, loss = 0.21407549\n",
      "Iteration 1312, loss = 0.21395015\n",
      "Iteration 1313, loss = 0.21389771\n",
      "Iteration 1314, loss = 0.21371390\n",
      "Iteration 1315, loss = 0.21363939\n",
      "Iteration 1316, loss = 0.21350838\n",
      "Iteration 1317, loss = 0.21336884\n",
      "Iteration 1318, loss = 0.21336995\n",
      "Iteration 1319, loss = 0.21315125\n",
      "Iteration 1320, loss = 0.21302875\n",
      "Iteration 1321, loss = 0.21293432\n",
      "Iteration 1322, loss = 0.21281594\n",
      "Iteration 1323, loss = 0.21270421\n",
      "Iteration 1324, loss = 0.21258989\n",
      "Iteration 1325, loss = 0.21246426\n",
      "Iteration 1326, loss = 0.21235126\n",
      "Iteration 1327, loss = 0.21221933\n",
      "Iteration 1328, loss = 0.21212334\n",
      "Iteration 1329, loss = 0.21200232\n",
      "Iteration 1330, loss = 0.21188815\n",
      "Iteration 1331, loss = 0.21176568\n",
      "Iteration 1332, loss = 0.21167680\n",
      "Iteration 1333, loss = 0.21156267\n",
      "Iteration 1334, loss = 0.21153495\n",
      "Iteration 1335, loss = 0.21130479\n",
      "Iteration 1336, loss = 0.21124960\n",
      "Iteration 1337, loss = 0.21107540\n",
      "Iteration 1338, loss = 0.21097152\n",
      "Iteration 1339, loss = 0.21086752\n",
      "Iteration 1340, loss = 0.21072245\n",
      "Iteration 1341, loss = 0.21070559\n",
      "Iteration 1342, loss = 0.21051787\n",
      "Iteration 1343, loss = 0.21039198\n",
      "Iteration 1344, loss = 0.21029715\n",
      "Iteration 1345, loss = 0.21017185\n",
      "Iteration 1346, loss = 0.21004560\n",
      "Iteration 1347, loss = 0.20997820\n",
      "Iteration 1348, loss = 0.20986615\n",
      "Iteration 1349, loss = 0.20969227\n",
      "Iteration 1350, loss = 0.20962066\n",
      "Iteration 1351, loss = 0.20948991\n",
      "Iteration 1352, loss = 0.20935916\n",
      "Iteration 1353, loss = 0.20924655\n",
      "Iteration 1354, loss = 0.20916097\n",
      "Iteration 1355, loss = 0.20908186\n",
      "Iteration 1356, loss = 0.20890973\n",
      "Iteration 1357, loss = 0.20884825\n",
      "Iteration 1358, loss = 0.20869611\n",
      "Iteration 1359, loss = 0.20858235\n",
      "Iteration 1360, loss = 0.20849446\n",
      "Iteration 1361, loss = 0.20837595\n",
      "Iteration 1362, loss = 0.20825218\n",
      "Iteration 1363, loss = 0.20815054\n",
      "Iteration 1364, loss = 0.20805449\n",
      "Iteration 1365, loss = 0.20787720\n",
      "Iteration 1366, loss = 0.20777956\n",
      "Iteration 1367, loss = 0.20765432\n",
      "Iteration 1368, loss = 0.20762859\n",
      "Iteration 1369, loss = 0.20744433\n",
      "Iteration 1370, loss = 0.20734198\n",
      "Iteration 1371, loss = 0.20733449\n",
      "Iteration 1372, loss = 0.20712044\n",
      "Iteration 1373, loss = 0.20701492\n",
      "Iteration 1374, loss = 0.20690992\n",
      "Iteration 1375, loss = 0.20678827\n",
      "Iteration 1376, loss = 0.20666935\n",
      "Iteration 1377, loss = 0.20654924\n",
      "Iteration 1378, loss = 0.20645989\n",
      "Iteration 1379, loss = 0.20633704\n",
      "Iteration 1380, loss = 0.20624476\n",
      "Iteration 1381, loss = 0.20622031\n",
      "Iteration 1382, loss = 0.20596526\n",
      "Iteration 1383, loss = 0.20586850\n",
      "Iteration 1384, loss = 0.20579037\n",
      "Iteration 1385, loss = 0.20566176\n",
      "Iteration 1386, loss = 0.20563860\n",
      "Iteration 1387, loss = 0.20548426\n",
      "Iteration 1388, loss = 0.20533002\n",
      "Iteration 1389, loss = 0.20519109\n",
      "Iteration 1390, loss = 0.20510224\n",
      "Iteration 1391, loss = 0.20506474\n",
      "Iteration 1392, loss = 0.20488588\n",
      "Iteration 1393, loss = 0.20479185\n",
      "Iteration 1394, loss = 0.20470852\n",
      "Iteration 1395, loss = 0.20455748\n",
      "Iteration 1396, loss = 0.20443792\n",
      "Iteration 1397, loss = 0.20432945\n",
      "Iteration 1398, loss = 0.20422406\n",
      "Iteration 1399, loss = 0.20408044\n",
      "Iteration 1400, loss = 0.20399863\n",
      "Iteration 1401, loss = 0.20386199\n",
      "Iteration 1402, loss = 0.20375655\n",
      "Iteration 1403, loss = 0.20365607\n",
      "Iteration 1404, loss = 0.20352121\n",
      "Iteration 1405, loss = 0.20345548\n",
      "Iteration 1406, loss = 0.20332757\n",
      "Iteration 1407, loss = 0.20319902\n",
      "Iteration 1408, loss = 0.20311860\n",
      "Iteration 1409, loss = 0.20297853\n",
      "Iteration 1410, loss = 0.20287453\n",
      "Iteration 1411, loss = 0.20278309\n",
      "Iteration 1412, loss = 0.20270451\n",
      "Iteration 1413, loss = 0.20257597\n",
      "Iteration 1414, loss = 0.20246912\n",
      "Iteration 1415, loss = 0.20231544\n",
      "Iteration 1416, loss = 0.20219196\n",
      "Iteration 1417, loss = 0.20220312\n",
      "Iteration 1418, loss = 0.20203173\n",
      "Iteration 1419, loss = 0.20191944\n",
      "Iteration 1420, loss = 0.20176839\n",
      "Iteration 1421, loss = 0.20167091\n",
      "Iteration 1422, loss = 0.20155093\n",
      "Iteration 1423, loss = 0.20149415\n",
      "Iteration 1424, loss = 0.20135410\n",
      "Iteration 1425, loss = 0.20124075\n",
      "Iteration 1426, loss = 0.20118005\n",
      "Iteration 1427, loss = 0.20099743\n",
      "Iteration 1428, loss = 0.20087663\n",
      "Iteration 1429, loss = 0.20080428\n",
      "Iteration 1430, loss = 0.20069104\n",
      "Iteration 1431, loss = 0.20055222\n",
      "Iteration 1432, loss = 0.20047908\n",
      "Iteration 1433, loss = 0.20045773\n",
      "Iteration 1434, loss = 0.20019744\n",
      "Iteration 1435, loss = 0.20015952\n",
      "Iteration 1436, loss = 0.20002177\n",
      "Iteration 1437, loss = 0.19990629\n",
      "Iteration 1438, loss = 0.19980833\n",
      "Iteration 1439, loss = 0.19970974\n",
      "Iteration 1440, loss = 0.19955762\n",
      "Iteration 1441, loss = 0.19946545\n",
      "Iteration 1442, loss = 0.19936071\n",
      "Iteration 1443, loss = 0.19922217\n",
      "Iteration 1444, loss = 0.19910779\n",
      "Iteration 1445, loss = 0.19900596\n",
      "Iteration 1446, loss = 0.19890774\n",
      "Iteration 1447, loss = 0.19877914\n",
      "Iteration 1448, loss = 0.19866724\n",
      "Iteration 1449, loss = 0.19861192\n",
      "Iteration 1450, loss = 0.19843360\n",
      "Iteration 1451, loss = 0.19831758\n",
      "Iteration 1452, loss = 0.19821028\n",
      "Iteration 1453, loss = 0.19814584\n",
      "Iteration 1454, loss = 0.19801438\n",
      "Iteration 1455, loss = 0.19800501\n",
      "Iteration 1456, loss = 0.19777366\n",
      "Iteration 1457, loss = 0.19777829\n",
      "Iteration 1458, loss = 0.19755226\n",
      "Iteration 1459, loss = 0.19746437\n",
      "Iteration 1460, loss = 0.19741105\n",
      "Iteration 1461, loss = 0.19733416\n",
      "Iteration 1462, loss = 0.19710992\n",
      "Iteration 1463, loss = 0.19699859\n",
      "Iteration 1464, loss = 0.19688034\n",
      "Iteration 1465, loss = 0.19676800\n",
      "Iteration 1466, loss = 0.19671073\n",
      "Iteration 1467, loss = 0.19655084\n",
      "Iteration 1468, loss = 0.19646585\n",
      "Iteration 1469, loss = 0.19632100\n",
      "Iteration 1470, loss = 0.19619937\n",
      "Iteration 1471, loss = 0.19610377\n",
      "Iteration 1472, loss = 0.19598515\n",
      "Iteration 1473, loss = 0.19595949\n",
      "Iteration 1474, loss = 0.19574379\n",
      "Iteration 1475, loss = 0.19562806\n",
      "Iteration 1476, loss = 0.19556918\n",
      "Iteration 1477, loss = 0.19542043\n",
      "Iteration 1478, loss = 0.19529501\n",
      "Iteration 1479, loss = 0.19522465\n",
      "Iteration 1480, loss = 0.19510678\n",
      "Iteration 1481, loss = 0.19497420\n",
      "Iteration 1482, loss = 0.19484076\n",
      "Iteration 1483, loss = 0.19471571\n",
      "Iteration 1484, loss = 0.19463568\n",
      "Iteration 1485, loss = 0.19455972\n",
      "Iteration 1486, loss = 0.19442360\n",
      "Iteration 1487, loss = 0.19430867\n",
      "Iteration 1488, loss = 0.19420409\n",
      "Iteration 1489, loss = 0.19411104\n",
      "Iteration 1490, loss = 0.19397307\n",
      "Iteration 1491, loss = 0.19388793\n",
      "Iteration 1492, loss = 0.19374139\n",
      "Iteration 1493, loss = 0.19361715\n",
      "Iteration 1494, loss = 0.19353394\n",
      "Iteration 1495, loss = 0.19338568\n",
      "Iteration 1496, loss = 0.19328520\n",
      "Iteration 1497, loss = 0.19320664\n",
      "Iteration 1498, loss = 0.19307096\n",
      "Iteration 1499, loss = 0.19297382\n",
      "Iteration 1500, loss = 0.19285688\n",
      "Iteration 1501, loss = 0.19272523\n",
      "Iteration 1502, loss = 0.19270760\n",
      "Iteration 1503, loss = 0.19252298\n",
      "Iteration 1504, loss = 0.19240255\n",
      "Iteration 1505, loss = 0.19229921\n",
      "Iteration 1506, loss = 0.19220292\n",
      "Iteration 1507, loss = 0.19206515\n",
      "Iteration 1508, loss = 0.19193079\n",
      "Iteration 1509, loss = 0.19183224\n",
      "Iteration 1510, loss = 0.19176446\n",
      "Iteration 1511, loss = 0.19161004\n",
      "Iteration 1512, loss = 0.19152816\n",
      "Iteration 1513, loss = 0.19142949\n",
      "Iteration 1514, loss = 0.19133756\n",
      "Iteration 1515, loss = 0.19117480\n",
      "Iteration 1516, loss = 0.19111832\n",
      "Iteration 1517, loss = 0.19096180\n",
      "Iteration 1518, loss = 0.19091590\n",
      "Iteration 1519, loss = 0.19076688\n",
      "Iteration 1520, loss = 0.19068125\n",
      "Iteration 1521, loss = 0.19051845\n",
      "Iteration 1522, loss = 0.19043252\n",
      "Iteration 1523, loss = 0.19031229\n",
      "Iteration 1524, loss = 0.19016503\n",
      "Iteration 1525, loss = 0.19011881\n",
      "Iteration 1526, loss = 0.19000855\n",
      "Iteration 1527, loss = 0.18993894\n",
      "Iteration 1528, loss = 0.18975306\n",
      "Iteration 1529, loss = 0.18963395\n",
      "Iteration 1530, loss = 0.18960057\n",
      "Iteration 1531, loss = 0.18942398\n",
      "Iteration 1532, loss = 0.18933353\n",
      "Iteration 1533, loss = 0.18921326\n",
      "Iteration 1534, loss = 0.18911488\n",
      "Iteration 1535, loss = 0.18898836\n",
      "Iteration 1536, loss = 0.18894657\n",
      "Iteration 1537, loss = 0.18878615\n",
      "Iteration 1538, loss = 0.18868255\n",
      "Iteration 1539, loss = 0.18858909\n",
      "Iteration 1540, loss = 0.18846875\n",
      "Iteration 1541, loss = 0.18832114\n",
      "Iteration 1542, loss = 0.18823763\n",
      "Iteration 1543, loss = 0.18812913\n",
      "Iteration 1544, loss = 0.18802315\n",
      "Iteration 1545, loss = 0.18788500\n",
      "Iteration 1546, loss = 0.18786294\n",
      "Iteration 1547, loss = 0.18768152\n",
      "Iteration 1548, loss = 0.18759756\n",
      "Iteration 1549, loss = 0.18744248\n",
      "Iteration 1550, loss = 0.18736986\n",
      "Iteration 1551, loss = 0.18731016\n",
      "Iteration 1552, loss = 0.18715100\n",
      "Iteration 1553, loss = 0.18702194\n",
      "Iteration 1554, loss = 0.18691534\n",
      "Iteration 1555, loss = 0.18684075\n",
      "Iteration 1556, loss = 0.18672548\n",
      "Iteration 1557, loss = 0.18668991\n",
      "Iteration 1558, loss = 0.18651139\n",
      "Iteration 1559, loss = 0.18637089\n",
      "Iteration 1560, loss = 0.18628671\n",
      "Iteration 1561, loss = 0.18615588\n",
      "Iteration 1562, loss = 0.18606273\n",
      "Iteration 1563, loss = 0.18593738\n",
      "Iteration 1564, loss = 0.18581180\n",
      "Iteration 1565, loss = 0.18570742\n",
      "Iteration 1566, loss = 0.18567937\n",
      "Iteration 1567, loss = 0.18549125\n",
      "Iteration 1568, loss = 0.18538169\n",
      "Iteration 1569, loss = 0.18535822\n",
      "Iteration 1570, loss = 0.18519860\n",
      "Iteration 1571, loss = 0.18503030\n",
      "Iteration 1572, loss = 0.18492299\n",
      "Iteration 1573, loss = 0.18481406\n",
      "Iteration 1574, loss = 0.18474556\n",
      "Iteration 1575, loss = 0.18460701\n",
      "Iteration 1576, loss = 0.18451648\n",
      "Iteration 1577, loss = 0.18444033\n",
      "Iteration 1578, loss = 0.18427953\n",
      "Iteration 1579, loss = 0.18418156\n",
      "Iteration 1580, loss = 0.18409392\n",
      "Iteration 1581, loss = 0.18393756\n",
      "Iteration 1582, loss = 0.18385052\n",
      "Iteration 1583, loss = 0.18374495\n",
      "Iteration 1584, loss = 0.18372076\n",
      "Iteration 1585, loss = 0.18357054\n",
      "Iteration 1586, loss = 0.18339238\n",
      "Iteration 1587, loss = 0.18326451\n",
      "Iteration 1588, loss = 0.18316325\n",
      "Iteration 1589, loss = 0.18305070\n",
      "Iteration 1590, loss = 0.18293839\n",
      "Iteration 1591, loss = 0.18285428\n",
      "Iteration 1592, loss = 0.18274748\n",
      "Iteration 1593, loss = 0.18260910\n",
      "Iteration 1594, loss = 0.18249961\n",
      "Iteration 1595, loss = 0.18242429\n",
      "Iteration 1596, loss = 0.18230180\n",
      "Iteration 1597, loss = 0.18220096\n",
      "Iteration 1598, loss = 0.18206745\n",
      "Iteration 1599, loss = 0.18199437\n",
      "Iteration 1600, loss = 0.18183983\n",
      "Iteration 1601, loss = 0.18173590\n",
      "Iteration 1602, loss = 0.18165053\n",
      "Iteration 1603, loss = 0.18151984\n",
      "Iteration 1604, loss = 0.18142880\n",
      "Iteration 1605, loss = 0.18142192\n",
      "Iteration 1606, loss = 0.18123197\n",
      "Iteration 1607, loss = 0.18120542\n",
      "Iteration 1608, loss = 0.18101941\n",
      "Iteration 1609, loss = 0.18099007\n",
      "Iteration 1610, loss = 0.18078776\n",
      "Iteration 1611, loss = 0.18069702\n",
      "Iteration 1612, loss = 0.18051886\n",
      "Iteration 1613, loss = 0.18042147\n",
      "Iteration 1614, loss = 0.18033951\n",
      "Iteration 1615, loss = 0.18019731\n",
      "Iteration 1616, loss = 0.18019654\n",
      "Iteration 1617, loss = 0.18005455\n",
      "Iteration 1618, loss = 0.17987444\n",
      "Iteration 1619, loss = 0.17976597\n",
      "Iteration 1620, loss = 0.17968487\n",
      "Iteration 1621, loss = 0.17954152\n",
      "Iteration 1622, loss = 0.17946207\n",
      "Iteration 1623, loss = 0.17933291\n",
      "Iteration 1624, loss = 0.17921676\n",
      "Iteration 1625, loss = 0.17912279\n",
      "Iteration 1626, loss = 0.17900217\n",
      "Iteration 1627, loss = 0.17889460\n",
      "Iteration 1628, loss = 0.17882705\n",
      "Iteration 1629, loss = 0.17873820\n",
      "Iteration 1630, loss = 0.17858151\n",
      "Iteration 1631, loss = 0.17849595\n",
      "Iteration 1632, loss = 0.17838698\n",
      "Iteration 1633, loss = 0.17822995\n",
      "Iteration 1634, loss = 0.17820648\n",
      "Iteration 1635, loss = 0.17801271\n",
      "Iteration 1636, loss = 0.17789862\n",
      "Iteration 1637, loss = 0.17784413\n",
      "Iteration 1638, loss = 0.17769871\n",
      "Iteration 1639, loss = 0.17757833\n",
      "Iteration 1640, loss = 0.17749882\n",
      "Iteration 1641, loss = 0.17737711\n",
      "Iteration 1642, loss = 0.17725687\n",
      "Iteration 1643, loss = 0.17717313\n",
      "Iteration 1644, loss = 0.17703782\n",
      "Iteration 1645, loss = 0.17694421\n",
      "Iteration 1646, loss = 0.17692536\n",
      "Iteration 1647, loss = 0.17672601\n",
      "Iteration 1648, loss = 0.17658565\n",
      "Iteration 1649, loss = 0.17650839\n",
      "Iteration 1650, loss = 0.17641899\n",
      "Iteration 1651, loss = 0.17630037\n",
      "Iteration 1652, loss = 0.17618901\n",
      "Iteration 1653, loss = 0.17605620\n",
      "Iteration 1654, loss = 0.17595654\n",
      "Iteration 1655, loss = 0.17584532\n",
      "Iteration 1656, loss = 0.17578099\n",
      "Iteration 1657, loss = 0.17572767\n",
      "Iteration 1658, loss = 0.17552441\n",
      "Iteration 1659, loss = 0.17544930\n",
      "Iteration 1660, loss = 0.17532573\n",
      "Iteration 1661, loss = 0.17525261\n",
      "Iteration 1662, loss = 0.17516997\n",
      "Iteration 1663, loss = 0.17501561\n",
      "Iteration 1664, loss = 0.17486844\n",
      "Iteration 1665, loss = 0.17480033\n",
      "Iteration 1666, loss = 0.17468204\n",
      "Iteration 1667, loss = 0.17461928\n",
      "Iteration 1668, loss = 0.17447926\n",
      "Iteration 1669, loss = 0.17434970\n",
      "Iteration 1670, loss = 0.17427763\n",
      "Iteration 1671, loss = 0.17413852\n",
      "Iteration 1672, loss = 0.17401480\n",
      "Iteration 1673, loss = 0.17397389\n",
      "Iteration 1674, loss = 0.17385583\n",
      "Iteration 1675, loss = 0.17377012\n",
      "Iteration 1676, loss = 0.17359959\n",
      "Iteration 1677, loss = 0.17346704\n",
      "Iteration 1678, loss = 0.17337962\n",
      "Iteration 1679, loss = 0.17329691\n",
      "Iteration 1680, loss = 0.17319689\n",
      "Iteration 1681, loss = 0.17305471\n",
      "Iteration 1682, loss = 0.17292666\n",
      "Iteration 1683, loss = 0.17283086\n",
      "Iteration 1684, loss = 0.17276167\n",
      "Iteration 1685, loss = 0.17262072\n",
      "Iteration 1686, loss = 0.17252762\n",
      "Iteration 1687, loss = 0.17243816\n",
      "Iteration 1688, loss = 0.17233836\n",
      "Iteration 1689, loss = 0.17226223\n",
      "Iteration 1690, loss = 0.17210160\n",
      "Iteration 1691, loss = 0.17200857\n",
      "Iteration 1692, loss = 0.17187951\n",
      "Iteration 1693, loss = 0.17177913\n",
      "Iteration 1694, loss = 0.17167103\n",
      "Iteration 1695, loss = 0.17155084\n",
      "Iteration 1696, loss = 0.17145782\n",
      "Iteration 1697, loss = 0.17135474\n",
      "Iteration 1698, loss = 0.17124669\n",
      "Iteration 1699, loss = 0.17110911\n",
      "Iteration 1700, loss = 0.17101603\n",
      "Iteration 1701, loss = 0.17091535\n",
      "Iteration 1702, loss = 0.17079730\n",
      "Iteration 1703, loss = 0.17069376\n",
      "Iteration 1704, loss = 0.17057843\n",
      "Iteration 1705, loss = 0.17049372\n",
      "Iteration 1706, loss = 0.17040011\n",
      "Iteration 1707, loss = 0.17030388\n",
      "Iteration 1708, loss = 0.17020985\n",
      "Iteration 1709, loss = 0.17010119\n",
      "Iteration 1710, loss = 0.17000904\n",
      "Iteration 1711, loss = 0.16994508\n",
      "Iteration 1712, loss = 0.16975609\n",
      "Iteration 1713, loss = 0.16964215\n",
      "Iteration 1714, loss = 0.16957576\n",
      "Iteration 1715, loss = 0.16962907\n",
      "Iteration 1716, loss = 0.16933472\n",
      "Iteration 1717, loss = 0.16924001\n",
      "Iteration 1718, loss = 0.16913346\n",
      "Iteration 1719, loss = 0.16901594\n",
      "Iteration 186, loss = 0.38604778\n",
      "Iteration 187, loss = 0.38568496\n",
      "Iteration 188, loss = 0.38531255\n",
      "Iteration 189, loss = 0.38497612\n",
      "Iteration 190, loss = 0.38460902\n",
      "Iteration 191, loss = 0.38422802\n",
      "Iteration 192, loss = 0.38388194\n",
      "Iteration 193, loss = 0.38351573\n",
      "Iteration 194, loss = 0.38316014\n",
      "Iteration 195, loss = 0.38281365\n",
      "Iteration 196, loss = 0.38250928\n",
      "Iteration 197, loss = 0.38217972\n",
      "Iteration 198, loss = 0.38180929\n",
      "Iteration 199, loss = 0.38145725\n",
      "Iteration 200, loss = 0.38115119\n",
      "Iteration 201, loss = 0.38081337\n",
      "Iteration 202, loss = 0.38046825\n",
      "Iteration 203, loss = 0.38015881\n",
      "Iteration 204, loss = 0.37980278\n",
      "Iteration 205, loss = 0.37951660\n",
      "Iteration 206, loss = 0.37921145\n",
      "Iteration 207, loss = 0.37886941\n",
      "Iteration 208, loss = 0.37862599\n",
      "Iteration 209, loss = 0.37825683\n",
      "Iteration 210, loss = 0.37794057\n",
      "Iteration 211, loss = 0.37765524\n",
      "Iteration 212, loss = 0.37733353\n",
      "Iteration 213, loss = 0.37703340\n",
      "Iteration 214, loss = 0.37675686\n",
      "Iteration 215, loss = 0.37644221\n",
      "Iteration 216, loss = 0.37613477\n",
      "Iteration 217, loss = 0.37584862\n",
      "Iteration 218, loss = 0.37555050\n",
      "Iteration 219, loss = 0.37526185\n",
      "Iteration 220, loss = 0.37498330\n",
      "Iteration 221, loss = 0.37470523\n",
      "Iteration 222, loss = 0.37440822\n",
      "Iteration 223, loss = 0.37416745\n",
      "Iteration 224, loss = 0.37385387\n",
      "Iteration 225, loss = 0.37358569\n",
      "Iteration 226, loss = 0.37330535\n",
      "Iteration 227, loss = 0.37303042\n",
      "Iteration 228, loss = 0.37278792\n",
      "Iteration 229, loss = 0.37249385\n",
      "Iteration 230, loss = 0.37221617\n",
      "Iteration 231, loss = 0.37195291\n",
      "Iteration 232, loss = 0.37170323\n",
      "Iteration 233, loss = 0.37145785\n",
      "Iteration 234, loss = 0.37118488\n",
      "Iteration 235, loss = 0.37095094\n",
      "Iteration 236, loss = 0.37067828\n",
      "Iteration 237, loss = 0.37044606\n",
      "Iteration 238, loss = 0.37017306\n",
      "Iteration 239, loss = 0.36993366\n",
      "Iteration 240, loss = 0.36971713\n",
      "Iteration 241, loss = 0.36945181\n",
      "Iteration 242, loss = 0.36925247\n",
      "Iteration 243, loss = 0.36896689\n",
      "Iteration 244, loss = 0.36872862\n",
      "Iteration 245, loss = 0.36853087\n",
      "Iteration 246, loss = 0.36827771\n",
      "Iteration 247, loss = 0.36805924\n",
      "Iteration 248, loss = 0.36781891\n",
      "Iteration 249, loss = 0.36760008\n",
      "Iteration 250, loss = 0.36737059\n",
      "Iteration 251, loss = 0.36713072\n",
      "Iteration 252, loss = 0.36690706\n",
      "Iteration 253, loss = 0.36670952\n",
      "Iteration 254, loss = 0.36651725\n",
      "Iteration 255, loss = 0.36624458\n",
      "Iteration 256, loss = 0.36603246\n",
      "Iteration 257, loss = 0.36583447\n",
      "Iteration 258, loss = 0.36559203\n",
      "Iteration 259, loss = 0.36537118\n",
      "Iteration 260, loss = 0.36518317\n",
      "Iteration 261, loss = 0.36495236\n",
      "Iteration 262, loss = 0.36476419\n",
      "Iteration 263, loss = 0.36453731\n",
      "Iteration 264, loss = 0.36434440\n",
      "Iteration 265, loss = 0.36414300\n",
      "Iteration 266, loss = 0.36391766\n",
      "Iteration 267, loss = 0.36371539\n",
      "Iteration 268, loss = 0.36352426\n",
      "Iteration 269, loss = 0.36333858\n",
      "Iteration 270, loss = 0.36313286\n",
      "Iteration 271, loss = 0.36292264\n",
      "Iteration 272, loss = 0.36274304\n",
      "Iteration 273, loss = 0.36257155\n",
      "Iteration 274, loss = 0.36233550\n",
      "Iteration 275, loss = 0.36216871\n",
      "Iteration 276, loss = 0.36195133\n",
      "Iteration 277, loss = 0.36176924\n",
      "Iteration 278, loss = 0.36156340\n",
      "Iteration 279, loss = 0.36140009\n",
      "Iteration 280, loss = 0.36119258\n",
      "Iteration 281, loss = 0.36100433\n",
      "Iteration 282, loss = 0.36079719\n",
      "Iteration 283, loss = 0.36062471\n",
      "Iteration 284, loss = 0.36041307\n",
      "Iteration 285, loss = 0.36023727\n",
      "Iteration 286, loss = 0.36004708\n",
      "Iteration 287, loss = 0.35987321\n",
      "Iteration 288, loss = 0.35970943\n",
      "Iteration 289, loss = 0.35950547\n",
      "Iteration 290, loss = 0.35931078\n",
      "Iteration 291, loss = 0.35913423\n",
      "Iteration 292, loss = 0.35896378\n",
      "Iteration 293, loss = 0.35876047\n",
      "Iteration 294, loss = 0.35858655\n",
      "Iteration 295, loss = 0.35842563\n",
      "Iteration 296, loss = 0.35824373\n",
      "Iteration 297, loss = 0.35805291\n",
      "Iteration 298, loss = 0.35788633\n",
      "Iteration 299, loss = 0.35772384\n",
      "Iteration 300, loss = 0.35753726\n",
      "Iteration 301, loss = 0.35738765\n",
      "Iteration 302, loss = 0.35719345\n",
      "Iteration 303, loss = 0.35702547\n",
      "Iteration 304, loss = 0.35683822\n",
      "Iteration 305, loss = 0.35665448\n",
      "Iteration 306, loss = 0.35650021\n",
      "Iteration 307, loss = 0.35633754\n",
      "Iteration 308, loss = 0.35613466\n",
      "Iteration 309, loss = 0.35597331\n",
      "Iteration 310, loss = 0.35580317\n",
      "Iteration 311, loss = 0.35561863\n",
      "Iteration 312, loss = 0.35545860\n",
      "Iteration 313, loss = 0.35530345\n",
      "Iteration 314, loss = 0.35513056\n",
      "Iteration 315, loss = 0.35496017\n",
      "Iteration 316, loss = 0.35482931\n",
      "Iteration 317, loss = 0.35462023\n",
      "Iteration 318, loss = 0.35444934\n",
      "Iteration 319, loss = 0.35428451\n",
      "Iteration 320, loss = 0.35412431\n",
      "Iteration 321, loss = 0.35395839\n",
      "Iteration 322, loss = 0.35383694\n",
      "Iteration 323, loss = 0.35364670\n",
      "Iteration 324, loss = 0.35345954\n",
      "Iteration 325, loss = 0.35332317\n",
      "Iteration 326, loss = 0.35315688\n",
      "Iteration 327, loss = 0.35298101\n",
      "Iteration 328, loss = 0.35281739\n",
      "Iteration 329, loss = 0.35267085\n",
      "Iteration 330, loss = 0.35249796\n",
      "Iteration 331, loss = 0.35234068\n",
      "Iteration 332, loss = 0.35221122\n",
      "Iteration 333, loss = 0.35201996\n",
      "Iteration 334, loss = 0.35187579\n",
      "Iteration 335, loss = 0.35171032\n",
      "Iteration 336, loss = 0.35155931\n",
      "Iteration 337, loss = 0.35138961\n",
      "Iteration 338, loss = 0.35122636\n",
      "Iteration 339, loss = 0.35111381\n",
      "Iteration 340, loss = 0.35093043\n",
      "Iteration 341, loss = 0.35077094\n",
      "Iteration 342, loss = 0.35062586\n",
      "Iteration 343, loss = 0.35046238\n",
      "Iteration 344, loss = 0.35036040\n",
      "Iteration 345, loss = 0.35019457\n",
      "Iteration 346, loss = 0.35011451\n",
      "Iteration 347, loss = 0.34984627\n",
      "Iteration 348, loss = 0.34976252\n",
      "Iteration 349, loss = 0.34955265\n",
      "Iteration 350, loss = 0.34941367\n",
      "Iteration 351, loss = 0.34925890\n",
      "Iteration 352, loss = 0.34915810\n",
      "Iteration 353, loss = 0.34903554\n",
      "Iteration 354, loss = 0.34881159\n",
      "Iteration 355, loss = 0.34864096\n",
      "Iteration 356, loss = 0.34850447\n",
      "Iteration 357, loss = 0.34835836\n",
      "Iteration 358, loss = 0.34820319\n",
      "Iteration 359, loss = 0.34806107\n",
      "Iteration 360, loss = 0.34790398\n",
      "Iteration 361, loss = 0.34776355\n",
      "Iteration 362, loss = 0.34761257\n",
      "Iteration 363, loss = 0.34747195\n",
      "Iteration 364, loss = 0.34733734\n",
      "Iteration 365, loss = 0.34716882\n",
      "Iteration 366, loss = 0.34703983\n",
      "Iteration 367, loss = 0.34690050\n",
      "Iteration 368, loss = 0.34674323\n",
      "Iteration 369, loss = 0.34662541\n",
      "Iteration 370, loss = 0.34648434\n",
      "Iteration 371, loss = 0.34632862\n",
      "Iteration 372, loss = 0.34619072\n",
      "Iteration 373, loss = 0.34606970\n",
      "Iteration 374, loss = 0.34586115\n",
      "Iteration 375, loss = 0.34573492\n",
      "Iteration 376, loss = 0.34561185\n",
      "Iteration 377, loss = 0.34544270\n",
      "Iteration 378, loss = 0.34528305\n",
      "Iteration 379, loss = 0.34514949\n",
      "Iteration 380, loss = 0.34501828\n",
      "Iteration 381, loss = 0.34488080\n",
      "Iteration 382, loss = 0.34473188\n",
      "Iteration 383, loss = 0.34465957\n",
      "Iteration 384, loss = 0.34444263\n",
      "Iteration 385, loss = 0.34430475\n",
      "Iteration 386, loss = 0.34418998\n",
      "Iteration 387, loss = 0.34408060\n",
      "Iteration 388, loss = 0.34388029\n",
      "Iteration 389, loss = 0.34378193\n",
      "Iteration 390, loss = 0.34363536\n",
      "Iteration 391, loss = 0.34346254\n",
      "Iteration 392, loss = 0.34332096\n",
      "Iteration 393, loss = 0.34319260\n",
      "Iteration 394, loss = 0.34307324\n",
      "Iteration 395, loss = 0.34290434\n",
      "Iteration 396, loss = 0.34276380\n",
      "Iteration 397, loss = 0.34265996\n",
      "Iteration 398, loss = 0.34250297\n",
      "Iteration 399, loss = 0.34237381\n",
      "Iteration 400, loss = 0.34222414\n",
      "Iteration 401, loss = 0.34211826\n",
      "Iteration 402, loss = 0.34196158\n",
      "Iteration 403, loss = 0.34182805\n",
      "Iteration 404, loss = 0.34170336\n",
      "Iteration 405, loss = 0.34155347\n",
      "Iteration 406, loss = 0.34142929\n",
      "Iteration 407, loss = 0.34130318\n",
      "Iteration 408, loss = 0.34113515\n",
      "Iteration 409, loss = 0.34101029\n",
      "Iteration 410, loss = 0.34086827\n",
      "Iteration 411, loss = 0.34075960\n",
      "Iteration 412, loss = 0.34062391\n",
      "Iteration 413, loss = 0.34047737\n",
      "Iteration 414, loss = 0.34034710\n",
      "Iteration 415, loss = 0.34020608\n",
      "Iteration 416, loss = 0.34008360\n",
      "Iteration 417, loss = 0.34006848\n",
      "Iteration 418, loss = 0.33985349\n",
      "Iteration 419, loss = 0.33973191\n",
      "Iteration 420, loss = 0.33957145\n",
      "Iteration 421, loss = 0.33942937\n",
      "Iteration 422, loss = 0.33933500\n",
      "Iteration 423, loss = 0.33918008\n",
      "Iteration 424, loss = 0.33909347\n",
      "Iteration 425, loss = 0.33891860\n",
      "Iteration 426, loss = 0.33878007\n",
      "Iteration 427, loss = 0.33865119\n",
      "Iteration 428, loss = 0.33854509\n",
      "Iteration 429, loss = 0.33846790\n",
      "Iteration 430, loss = 0.33829932\n",
      "Iteration 431, loss = 0.33813558\n",
      "Iteration 432, loss = 0.33807981\n",
      "Iteration 433, loss = 0.33789239\n",
      "Iteration 434, loss = 0.33776517\n",
      "Iteration 435, loss = 0.33764721\n",
      "Iteration 436, loss = 0.33753526\n",
      "Iteration 437, loss = 0.33744463\n",
      "Iteration 438, loss = 0.33727776\n",
      "Iteration 439, loss = 0.33713997\n",
      "Iteration 440, loss = 0.33700507\n",
      "Iteration 441, loss = 0.33690523\n",
      "Iteration 442, loss = 0.33676003\n",
      "Iteration 443, loss = 0.33665201\n",
      "Iteration 444, loss = 0.33650784\n",
      "Iteration 445, loss = 0.33643784\n",
      "Iteration 446, loss = 0.33624053\n",
      "Iteration 447, loss = 0.33613921\n",
      "Iteration 448, loss = 0.33602146\n",
      "Iteration 449, loss = 0.33590408\n",
      "Iteration 450, loss = 0.33576476\n",
      "Iteration 451, loss = 0.33565923\n",
      "Iteration 452, loss = 0.33549051\n",
      "Iteration 453, loss = 0.33540120\n",
      "Iteration 454, loss = 0.33526482\n",
      "Iteration 455, loss = 0.33511575\n",
      "Iteration 456, loss = 0.33500243\n",
      "Iteration 457, loss = 0.33485871\n",
      "Iteration 458, loss = 0.33478517\n",
      "Iteration 459, loss = 0.33459819\n",
      "Iteration 460, loss = 0.33454480\n",
      "Iteration 461, loss = 0.33435002\n",
      "Iteration 462, loss = 0.33423349\n",
      "Iteration 463, loss = 0.33410940\n",
      "Iteration 464, loss = 0.33398242\n",
      "Iteration 465, loss = 0.33385084\n",
      "Iteration 466, loss = 0.33371473\n",
      "Iteration 467, loss = 0.33359791\n",
      "Iteration 468, loss = 0.33350485\n",
      "Iteration 469, loss = 0.33337844\n",
      "Iteration 470, loss = 0.33322313\n",
      "Iteration 471, loss = 0.33309651\n",
      "Iteration 472, loss = 0.33297758\n",
      "Iteration 473, loss = 0.33285136\n",
      "Iteration 474, loss = 0.33283993\n",
      "Iteration 475, loss = 0.33259873\n",
      "Iteration 476, loss = 0.33247855\n",
      "Iteration 477, loss = 0.33235923\n",
      "Iteration 478, loss = 0.33225063\n",
      "Iteration 479, loss = 0.33210224\n",
      "Iteration 480, loss = 0.33197363\n",
      "Iteration 481, loss = 0.33185063\n",
      "Iteration 482, loss = 0.33173867\n",
      "Iteration 483, loss = 0.33163110\n",
      "Iteration 484, loss = 0.33149341\n",
      "Iteration 485, loss = 0.33136006\n",
      "Iteration 486, loss = 0.33123912\n",
      "Iteration 487, loss = 0.33112220\n",
      "Iteration 488, loss = 0.33098634\n",
      "Iteration 489, loss = 0.33086560\n",
      "Iteration 490, loss = 0.33075069\n",
      "Iteration 491, loss = 0.33063126\n",
      "Iteration 492, loss = 0.33051094\n",
      "Iteration 493, loss = 0.33041219\n",
      "Iteration 494, loss = 0.33024733\n",
      "Iteration 495, loss = 0.33011986\n",
      "Iteration 496, loss = 0.33000230\n",
      "Iteration 497, loss = 0.32990694\n",
      "Iteration 498, loss = 0.32977155\n",
      "Iteration 499, loss = 0.32962835\n",
      "Iteration 500, loss = 0.32953088\n",
      "Iteration 501, loss = 0.32939605\n",
      "Iteration 502, loss = 0.32929971\n",
      "Iteration 503, loss = 0.32917256\n",
      "Iteration 504, loss = 0.32908766\n",
      "Iteration 505, loss = 0.32890841\n",
      "Iteration 506, loss = 0.32880491\n",
      "Iteration 507, loss = 0.32867545\n",
      "Iteration 508, loss = 0.32854868\n",
      "Iteration 509, loss = 0.32842277\n",
      "Iteration 510, loss = 0.32830805\n",
      "Iteration 511, loss = 0.32818700\n",
      "Iteration 512, loss = 0.32810038\n",
      "Iteration 513, loss = 0.32793761\n",
      "Iteration 514, loss = 0.32780742\n",
      "Iteration 515, loss = 0.32768357\n",
      "Iteration 516, loss = 0.32759812\n",
      "Iteration 517, loss = 0.32748379\n",
      "Iteration 518, loss = 0.32736105\n",
      "Iteration 519, loss = 0.32724486\n",
      "Iteration 520, loss = 0.32708494\n",
      "Iteration 521, loss = 0.32696466\n",
      "Iteration 522, loss = 0.32686007\n",
      "Iteration 523, loss = 0.32677941\n",
      "Iteration 524, loss = 0.32660990\n",
      "Iteration 525, loss = 0.32647300\n",
      "Iteration 526, loss = 0.32638123\n",
      "Iteration 527, loss = 0.32623642\n",
      "Iteration 528, loss = 0.32612525\n",
      "Iteration 529, loss = 0.32600590\n",
      "Iteration 530, loss = 0.32590240\n",
      "Iteration 531, loss = 0.32573309\n",
      "Iteration 532, loss = 0.32561459\n",
      "Iteration 533, loss = 0.32550689\n",
      "Iteration 534, loss = 0.32536683\n",
      "Iteration 535, loss = 0.32526538\n",
      "Iteration 536, loss = 0.32514837\n",
      "Iteration 537, loss = 0.32502224\n",
      "Iteration 538, loss = 0.32488640\n",
      "Iteration 539, loss = 0.32481536\n",
      "Iteration 540, loss = 0.32464125\n",
      "Iteration 541, loss = 0.32455263\n",
      "Iteration 542, loss = 0.32442001\n",
      "Iteration 543, loss = 0.32430065\n",
      "Iteration 544, loss = 0.32414753\n",
      "Iteration 545, loss = 0.32401777\n",
      "Iteration 546, loss = 0.32391515\n",
      "Iteration 547, loss = 0.32378240\n",
      "Iteration 548, loss = 0.32366976\n",
      "Iteration 549, loss = 0.32353446\n",
      "Iteration 550, loss = 0.32341445\n",
      "Iteration 551, loss = 0.32328121\n",
      "Iteration 552, loss = 0.32314781\n",
      "Iteration 553, loss = 0.32307032\n",
      "Iteration 554, loss = 0.32290912\n",
      "Iteration 555, loss = 0.32277573\n",
      "Iteration 556, loss = 0.32272417\n",
      "Iteration 557, loss = 0.32253603\n",
      "Iteration 558, loss = 0.32241157\n",
      "Iteration 559, loss = 0.32230497\n",
      "Iteration 560, loss = 0.32216310\n",
      "Iteration 561, loss = 0.32203759\n",
      "Iteration 562, loss = 0.32191321\n",
      "Iteration 563, loss = 0.32178929\n",
      "Iteration 564, loss = 0.32169034\n",
      "Iteration 565, loss = 0.32158440\n",
      "Iteration 566, loss = 0.32143739\n",
      "Iteration 567, loss = 0.32134576\n",
      "Iteration 568, loss = 0.32119711\n",
      "Iteration 569, loss = 0.32108699\n",
      "Iteration 570, loss = 0.32096252\n",
      "Iteration 571, loss = 0.32085667\n",
      "Iteration 572, loss = 0.32071644\n",
      "Iteration 573, loss = 0.32065412\n",
      "Iteration 574, loss = 0.32046046\n",
      "Iteration 575, loss = 0.32034341\n",
      "Iteration 576, loss = 0.32022291\n",
      "Iteration 577, loss = 0.32009401\n",
      "Iteration 578, loss = 0.31997699\n",
      "Iteration 579, loss = 0.31985742\n",
      "Iteration 580, loss = 0.31976200\n",
      "Iteration 581, loss = 0.31961561\n",
      "Iteration 582, loss = 0.31952729\n",
      "Iteration 583, loss = 0.31939012\n",
      "Iteration 584, loss = 0.31925744\n",
      "Iteration 585, loss = 0.31914104\n",
      "Iteration 586, loss = 0.31898260\n",
      "Iteration 587, loss = 0.31892214\n",
      "Iteration 588, loss = 0.31880684\n",
      "Iteration 589, loss = 0.31866316\n",
      "Iteration 590, loss = 0.31850264\n",
      "Iteration 591, loss = 0.31838658\n",
      "Iteration 592, loss = 0.31827504\n",
      "Iteration 593, loss = 0.31814993\n",
      "Iteration 594, loss = 0.31804043\n",
      "Iteration 595, loss = 0.31791252\n",
      "Iteration 596, loss = 0.31778716\n",
      "Iteration 597, loss = 0.31769337\n",
      "Iteration 598, loss = 0.31765742\n",
      "Iteration 599, loss = 0.31741935\n",
      "Iteration 600, loss = 0.31733948\n",
      "Iteration 601, loss = 0.31717631\n",
      "Iteration 602, loss = 0.31705988\n",
      "Iteration 603, loss = 0.31695122\n",
      "Iteration 604, loss = 0.31681542\n",
      "Iteration 605, loss = 0.31672357\n",
      "Iteration 606, loss = 0.31660360\n",
      "Iteration 607, loss = 0.31647192\n",
      "Iteration 608, loss = 0.31637645\n",
      "Iteration 609, loss = 0.31624776\n",
      "Iteration 610, loss = 0.31613669\n",
      "Iteration 611, loss = 0.31604526\n",
      "Iteration 612, loss = 0.31591214\n",
      "Iteration 613, loss = 0.31574800\n",
      "Iteration 614, loss = 0.31561650\n",
      "Iteration 615, loss = 0.31553758\n",
      "Iteration 616, loss = 0.31538540\n",
      "Iteration 617, loss = 0.31532272\n",
      "Iteration 618, loss = 0.31518619\n",
      "Iteration 619, loss = 0.31504267\n",
      "Iteration 620, loss = 0.31497367\n",
      "Iteration 621, loss = 0.31479928\n",
      "Iteration 622, loss = 0.31473070\n",
      "Iteration 623, loss = 0.31456771\n",
      "Iteration 624, loss = 0.31443533\n",
      "Iteration 625, loss = 0.31430353\n",
      "Iteration 626, loss = 0.31419048\n",
      "Iteration 627, loss = 0.31407526\n",
      "Iteration 628, loss = 0.31395295\n",
      "Iteration 629, loss = 0.31388185\n",
      "Iteration 630, loss = 0.31374425\n",
      "Iteration 631, loss = 0.31363253\n",
      "Iteration 632, loss = 0.31345695\n",
      "Iteration 633, loss = 0.31338706\n",
      "Iteration 634, loss = 0.31319991\n",
      "Iteration 635, loss = 0.31311217\n",
      "Iteration 636, loss = 0.31298324\n",
      "Iteration 637, loss = 0.31288200\n",
      "Iteration 638, loss = 0.31279946\n",
      "Iteration 639, loss = 0.31262555\n",
      "Iteration 640, loss = 0.31250408\n",
      "Iteration 641, loss = 0.31242309\n",
      "Iteration 642, loss = 0.31224380\n",
      "Iteration 643, loss = 0.31214481\n",
      "Iteration 644, loss = 0.31203580\n",
      "Iteration 645, loss = 0.31190426\n",
      "Iteration 646, loss = 0.31178144\n",
      "Iteration 647, loss = 0.31165576\n",
      "Iteration 648, loss = 0.31153913\n",
      "Iteration 649, loss = 0.31144039\n",
      "Iteration 650, loss = 0.31133014\n",
      "Iteration 651, loss = 0.31121658\n",
      "Iteration 652, loss = 0.31105534\n",
      "Iteration 653, loss = 0.31098443\n",
      "Iteration 654, loss = 0.31084631\n",
      "Iteration 655, loss = 0.31073657\n",
      "Iteration 656, loss = 0.31061321\n",
      "Iteration 657, loss = 0.31051033\n",
      "Iteration 658, loss = 0.31036709\n",
      "Iteration 659, loss = 0.31026127\n",
      "Iteration 660, loss = 0.31011806\n",
      "Iteration 661, loss = 0.31003904\n",
      "Iteration 662, loss = 0.30994328\n",
      "Iteration 663, loss = 0.30976659\n",
      "Iteration 664, loss = 0.30964651\n",
      "Iteration 665, loss = 0.30952441\n",
      "Iteration 666, loss = 0.30941625\n",
      "Iteration 667, loss = 0.30927426\n",
      "Iteration 668, loss = 0.30918365\n",
      "Iteration 669, loss = 0.30908154\n",
      "Iteration 670, loss = 0.30891582\n",
      "Iteration 671, loss = 0.30881792\n",
      "Iteration 672, loss = 0.30868030\n",
      "Iteration 673, loss = 0.30858503\n",
      "Iteration 674, loss = 0.30848341\n",
      "Iteration 675, loss = 0.30840595\n",
      "Iteration 676, loss = 0.30824543\n",
      "Iteration 677, loss = 0.30812198\n",
      "Iteration 678, loss = 0.30797534\n",
      "Iteration 679, loss = 0.30789900\n",
      "Iteration 680, loss = 0.30775770\n",
      "Iteration 681, loss = 0.30760425\n",
      "Iteration 1718, loss = 0.20664049\n",
      "Iteration 1719, loss = 0.20655559\n",
      "Iteration 1720, loss = 0.20639462\n",
      "Iteration 1721, loss = 0.20625414\n",
      "Iteration 1722, loss = 0.20611918\n",
      "Iteration 1723, loss = 0.20608607\n",
      "Iteration 1724, loss = 0.20588878\n",
      "Iteration 1725, loss = 0.20584644\n",
      "Iteration 1726, loss = 0.20562933\n",
      "Iteration 1727, loss = 0.20555827\n",
      "Iteration 1728, loss = 0.20534932\n",
      "Iteration 1729, loss = 0.20525606\n",
      "Iteration 1730, loss = 0.20510490\n",
      "Iteration 1731, loss = 0.20503187\n",
      "Iteration 1732, loss = 0.20485657\n",
      "Iteration 1733, loss = 0.20475621\n",
      "Iteration 1734, loss = 0.20459147\n",
      "Iteration 1735, loss = 0.20445239\n",
      "Iteration 1736, loss = 0.20437449\n",
      "Iteration 1737, loss = 0.20423958\n",
      "Iteration 1738, loss = 0.20414407\n",
      "Iteration 1739, loss = 0.20400512\n",
      "Iteration 1740, loss = 0.20385189\n",
      "Iteration 1741, loss = 0.20374966\n",
      "Iteration 1742, loss = 0.20359861\n",
      "Iteration 1743, loss = 0.20360729\n",
      "Iteration 1744, loss = 0.20336091\n",
      "Iteration 1745, loss = 0.20337626\n",
      "Iteration 1746, loss = 0.20332416\n",
      "Iteration 1747, loss = 0.20300326\n",
      "Iteration 1748, loss = 0.20282496\n",
      "Iteration 1749, loss = 0.20291139\n",
      "Iteration 1750, loss = 0.20262000\n",
      "Iteration 1751, loss = 0.20247128\n",
      "Iteration 1752, loss = 0.20245022\n",
      "Iteration 1753, loss = 0.20223033\n",
      "Iteration 1754, loss = 0.20220432\n",
      "Iteration 1755, loss = 0.20197687\n",
      "Iteration 1756, loss = 0.20187076\n",
      "Iteration 1757, loss = 0.20172990\n",
      "Iteration 1758, loss = 0.20158943\n",
      "Iteration 1759, loss = 0.20160302\n",
      "Iteration 1760, loss = 0.20142735\n",
      "Iteration 1761, loss = 0.20124797\n",
      "Iteration 1762, loss = 0.20110019\n",
      "Iteration 1763, loss = 0.20108123\n",
      "Iteration 1764, loss = 0.20088736\n",
      "Iteration 1765, loss = 0.20074311\n",
      "Iteration 1766, loss = 0.20060251\n",
      "Iteration 1767, loss = 0.20045277\n",
      "Iteration 1768, loss = 0.20031183\n",
      "Iteration 1769, loss = 0.20035003\n",
      "Iteration 1770, loss = 0.20009467\n",
      "Iteration 1771, loss = 0.19998892\n",
      "Iteration 1772, loss = 0.19984476\n",
      "Iteration 1773, loss = 0.19971316\n",
      "Iteration 1774, loss = 0.19964917\n",
      "Iteration 1775, loss = 0.19944397\n",
      "Iteration 1776, loss = 0.19940304\n",
      "Iteration 1777, loss = 0.19919724\n",
      "Iteration 1778, loss = 0.19915570\n",
      "Iteration 1779, loss = 0.19907386\n",
      "Iteration 1780, loss = 0.19890073\n",
      "Iteration 1781, loss = 0.19876007\n",
      "Iteration 1782, loss = 0.19862539\n",
      "Iteration 1783, loss = 0.19853842\n",
      "Iteration 1784, loss = 0.19840789\n",
      "Iteration 1785, loss = 0.19826072\n",
      "Iteration 1786, loss = 0.19817621\n",
      "Iteration 1787, loss = 0.19801004\n",
      "Iteration 1788, loss = 0.19786359\n",
      "Iteration 1789, loss = 0.19778655\n",
      "Iteration 1790, loss = 0.19766042\n",
      "Iteration 1791, loss = 0.19754608\n",
      "Iteration 1792, loss = 0.19738043\n",
      "Iteration 1793, loss = 0.19726587\n",
      "Iteration 1794, loss = 0.19721954\n",
      "Iteration 1795, loss = 0.19724468\n",
      "Iteration 1796, loss = 0.19691903\n",
      "Iteration 1797, loss = 0.19680971\n",
      "Iteration 1798, loss = 0.19665089\n",
      "Iteration 1799, loss = 0.19653300\n",
      "Iteration 1800, loss = 0.19643264\n",
      "Iteration 1801, loss = 0.19634249\n",
      "Iteration 1802, loss = 0.19626098\n",
      "Iteration 1803, loss = 0.19633351\n",
      "Iteration 1804, loss = 0.19602232\n",
      "Iteration 1805, loss = 0.19583702\n",
      "Iteration 1806, loss = 0.19574781\n",
      "Iteration 1807, loss = 0.19563124\n",
      "Iteration 1808, loss = 0.19542552\n",
      "Iteration 1809, loss = 0.19533542\n",
      "Iteration 1810, loss = 0.19519810\n",
      "Iteration 1811, loss = 0.19512723\n",
      "Iteration 1812, loss = 0.19503551\n",
      "Iteration 1813, loss = 0.19486208\n",
      "Iteration 1814, loss = 0.19476164\n",
      "Iteration 1815, loss = 0.19463474\n",
      "Iteration 1816, loss = 0.19451456\n",
      "Iteration 1817, loss = 0.19440753\n",
      "Iteration 1818, loss = 0.19428699\n",
      "Iteration 1819, loss = 0.19435479\n",
      "Iteration 1820, loss = 0.19405183\n",
      "Iteration 1821, loss = 0.19391658\n",
      "Iteration 1822, loss = 0.19374215\n",
      "Iteration 1823, loss = 0.19363048\n",
      "Iteration 1824, loss = 0.19354383\n",
      "Iteration 1825, loss = 0.19352329\n",
      "Iteration 1826, loss = 0.19335325\n",
      "Iteration 1827, loss = 0.19341371\n",
      "Iteration 1828, loss = 0.19314682\n",
      "Iteration 1829, loss = 0.19301751\n",
      "Iteration 1830, loss = 0.19288312\n",
      "Iteration 1831, loss = 0.19270086\n",
      "Iteration 1832, loss = 0.19262267\n",
      "Iteration 1833, loss = 0.19252125\n",
      "Iteration 1834, loss = 0.19235145\n",
      "Iteration 1835, loss = 0.19227516\n",
      "Iteration 1836, loss = 0.19216030\n",
      "Iteration 1837, loss = 0.19199405\n",
      "Iteration 1838, loss = 0.19197645\n",
      "Iteration 1839, loss = 0.19195416\n",
      "Iteration 1840, loss = 0.19167592\n",
      "Iteration 1841, loss = 0.19156388\n",
      "Iteration 1842, loss = 0.19147181\n",
      "Iteration 1843, loss = 0.19129517\n",
      "Iteration 1844, loss = 0.19117267\n",
      "Iteration 1845, loss = 0.19106807\n",
      "Iteration 1846, loss = 0.19101541\n",
      "Iteration 1847, loss = 0.19082054\n",
      "Iteration 1848, loss = 0.19070015\n",
      "Iteration 1849, loss = 0.19068460\n",
      "Iteration 1850, loss = 0.19050333\n",
      "Iteration 1851, loss = 0.19030463\n",
      "Iteration 1852, loss = 0.19020048\n",
      "Iteration 1853, loss = 0.19007897\n",
      "Iteration 1854, loss = 0.18999553\n",
      "Iteration 1855, loss = 0.18992304\n",
      "Iteration 1856, loss = 0.18972930\n",
      "Iteration 1857, loss = 0.18967020\n",
      "Iteration 1858, loss = 0.18950787\n",
      "Iteration 1859, loss = 0.18945577\n",
      "Iteration 1860, loss = 0.18926196\n",
      "Iteration 1861, loss = 0.18928102\n",
      "Iteration 1862, loss = 0.18902748\n",
      "Iteration 1863, loss = 0.18892670\n",
      "Iteration 1864, loss = 0.18898052\n",
      "Iteration 1865, loss = 0.18868776\n",
      "Iteration 1866, loss = 0.18858116\n",
      "Iteration 1867, loss = 0.18847151\n",
      "Iteration 1868, loss = 0.18837342\n",
      "Iteration 1869, loss = 0.18820302\n",
      "Iteration 1870, loss = 0.18810376\n",
      "Iteration 1871, loss = 0.18801320\n",
      "Iteration 1872, loss = 0.18791364\n",
      "Iteration 1873, loss = 0.18788644\n",
      "Iteration 1874, loss = 0.18770760\n",
      "Iteration 1875, loss = 0.18754713\n",
      "Iteration 1876, loss = 0.18748429\n",
      "Iteration 1877, loss = 0.18740461\n",
      "Iteration 1878, loss = 0.18723523\n",
      "Iteration 1879, loss = 0.18710663\n",
      "Iteration 1880, loss = 0.18695648\n",
      "Iteration 1881, loss = 0.18688449\n",
      "Iteration 1882, loss = 0.18673548\n",
      "Iteration 1883, loss = 0.18670528\n",
      "Iteration 1884, loss = 0.18650275\n",
      "Iteration 1885, loss = 0.18640750\n",
      "Iteration 1886, loss = 0.18629341\n",
      "Iteration 1887, loss = 0.18612477\n",
      "Iteration 1888, loss = 0.18606082\n",
      "Iteration 1889, loss = 0.18595800\n",
      "Iteration 1890, loss = 0.18579220\n",
      "Iteration 1891, loss = 0.18566628\n",
      "Iteration 1892, loss = 0.18555070\n",
      "Iteration 1893, loss = 0.18543513\n",
      "Iteration 1894, loss = 0.18530494\n",
      "Iteration 1895, loss = 0.18523073\n",
      "Iteration 1896, loss = 0.18510092\n",
      "Iteration 1897, loss = 0.18500208\n",
      "Iteration 1898, loss = 0.18504717\n",
      "Iteration 1899, loss = 0.18474052\n",
      "Iteration 1900, loss = 0.18463557\n",
      "Iteration 1901, loss = 0.18456058\n",
      "Iteration 1902, loss = 0.18448694\n",
      "Iteration 1903, loss = 0.18430228\n",
      "Iteration 1904, loss = 0.18420185\n",
      "Iteration 1905, loss = 0.18410342\n",
      "Iteration 1906, loss = 0.18399778\n",
      "Iteration 1907, loss = 0.18404472\n",
      "Iteration 1908, loss = 0.18386186\n",
      "Iteration 1909, loss = 0.18363858\n",
      "Iteration 1910, loss = 0.18355693\n",
      "Iteration 1911, loss = 0.18338765\n",
      "Iteration 1912, loss = 0.18332399\n",
      "Iteration 1913, loss = 0.18318086\n",
      "Iteration 1914, loss = 0.18305442\n",
      "Iteration 1915, loss = 0.18293914\n",
      "Iteration 1916, loss = 0.18281366\n",
      "Iteration 1917, loss = 0.18276042\n",
      "Iteration 1918, loss = 0.18253946\n",
      "Iteration 1919, loss = 0.18246225\n",
      "Iteration 1920, loss = 0.18244312\n",
      "Iteration 1921, loss = 0.18224727\n",
      "Iteration 1922, loss = 0.18228417\n",
      "Iteration 1923, loss = 0.18208390\n",
      "Iteration 1924, loss = 0.18202928\n",
      "Iteration 1925, loss = 0.18187127\n",
      "Iteration 1926, loss = 0.18172899\n",
      "Iteration 1927, loss = 0.18157161\n",
      "Iteration 1928, loss = 0.18155231\n",
      "Iteration 1929, loss = 0.18142096\n",
      "Iteration 1930, loss = 0.18125267\n",
      "Iteration 1931, loss = 0.18119572\n",
      "Iteration 1932, loss = 0.18105748\n",
      "Iteration 1933, loss = 0.18087506\n",
      "Iteration 1934, loss = 0.18083832\n",
      "Iteration 1935, loss = 0.18075124\n",
      "Iteration 1936, loss = 0.18060555\n",
      "Iteration 1937, loss = 0.18060309\n",
      "Iteration 1938, loss = 0.18033606\n",
      "Iteration 1939, loss = 0.18023915\n",
      "Iteration 1940, loss = 0.18013071\n",
      "Iteration 1941, loss = 0.18006966\n",
      "Iteration 1942, loss = 0.17992141\n",
      "Iteration 1943, loss = 0.17978228\n",
      "Iteration 1944, loss = 0.17966932\n",
      "Iteration 1945, loss = 0.17957791\n",
      "Iteration 1946, loss = 0.17948754\n",
      "Iteration 1947, loss = 0.17936801\n",
      "Iteration 1948, loss = 0.17925537\n",
      "Iteration 1949, loss = 0.17916234\n",
      "Iteration 1950, loss = 0.17898537\n",
      "Iteration 1951, loss = 0.17896674\n",
      "Iteration 1952, loss = 0.17881553\n",
      "Iteration 1953, loss = 0.17872624\n",
      "Iteration 1954, loss = 0.17859606\n",
      "Iteration 1955, loss = 0.17849801\n",
      "Iteration 1956, loss = 0.17834304\n",
      "Iteration 1957, loss = 0.17835224\n",
      "Iteration 1958, loss = 0.17813759\n",
      "Iteration 1959, loss = 0.17806421\n",
      "Iteration 1960, loss = 0.17791436\n",
      "Iteration 1961, loss = 0.17787777\n",
      "Iteration 1962, loss = 0.17779295\n",
      "Iteration 1963, loss = 0.17763933\n",
      "Iteration 1964, loss = 0.17749935\n",
      "Iteration 1965, loss = 0.17741491\n",
      "Iteration 1966, loss = 0.17733221\n",
      "Iteration 1967, loss = 0.17724926\n",
      "Iteration 1968, loss = 0.17712343\n",
      "Iteration 1969, loss = 0.17702295\n",
      "Iteration 1970, loss = 0.17684435\n",
      "Iteration 1971, loss = 0.17679724\n",
      "Iteration 1972, loss = 0.17661887\n",
      "Iteration 1973, loss = 0.17671753\n",
      "Iteration 1974, loss = 0.17640559\n",
      "Iteration 1975, loss = 0.17635502\n",
      "Iteration 1976, loss = 0.17620900\n",
      "Iteration 1977, loss = 0.17605769\n",
      "Iteration 1978, loss = 0.17595738\n",
      "Iteration 1979, loss = 0.17586847\n",
      "Iteration 1980, loss = 0.17580289\n",
      "Iteration 1981, loss = 0.17567833\n",
      "Iteration 1982, loss = 0.17553650\n",
      "Iteration 1983, loss = 0.17550882\n",
      "Iteration 1984, loss = 0.17538565\n",
      "Iteration 1985, loss = 0.17524642\n",
      "Iteration 1986, loss = 0.17509850\n",
      "Iteration 1987, loss = 0.17500883\n",
      "Iteration 1988, loss = 0.17499317\n",
      "Iteration 1989, loss = 0.17477724\n",
      "Iteration 1990, loss = 0.17467953\n",
      "Iteration 1991, loss = 0.17457822\n",
      "Iteration 1992, loss = 0.17456089\n",
      "Iteration 1993, loss = 0.17436417\n",
      "Iteration 1994, loss = 0.17425315\n",
      "Iteration 1995, loss = 0.17419350\n",
      "Iteration 1996, loss = 0.17406371\n",
      "Iteration 1997, loss = 0.17395663\n",
      "Iteration 1998, loss = 0.17387444\n",
      "Iteration 1999, loss = 0.17374242\n",
      "Iteration 2000, loss = 0.17360085\n",
      "Iteration 2001, loss = 0.17356786\n",
      "Iteration 2002, loss = 0.17352488\n",
      "Iteration 2003, loss = 0.17329781\n",
      "Iteration 2004, loss = 0.17324222\n",
      "Iteration 2005, loss = 0.17311283\n",
      "Iteration 2006, loss = 0.17304715\n",
      "Iteration 2007, loss = 0.17286885\n",
      "Iteration 2008, loss = 0.17285547\n",
      "Iteration 2009, loss = 0.17272422\n",
      "Iteration 2010, loss = 0.17255539\n",
      "Iteration 2011, loss = 0.17244994\n",
      "Iteration 2012, loss = 0.17237920\n",
      "Iteration 2013, loss = 0.17232458\n",
      "Iteration 2014, loss = 0.17214024\n",
      "Iteration 2015, loss = 0.17203839\n",
      "Iteration 2016, loss = 0.17206469\n",
      "Iteration 2017, loss = 0.17182915\n",
      "Iteration 2018, loss = 0.17174636\n",
      "Iteration 2019, loss = 0.17167560\n",
      "Iteration 2020, loss = 0.17159063\n",
      "Iteration 2021, loss = 0.17149549\n",
      "Iteration 2022, loss = 0.17136144\n",
      "Iteration 2023, loss = 0.17125583\n",
      "Iteration 2024, loss = 0.17126033\n",
      "Iteration 2025, loss = 0.17106481\n",
      "Iteration 2026, loss = 0.17097017\n",
      "Iteration 2027, loss = 0.17083833\n",
      "Iteration 2028, loss = 0.17068649\n",
      "Iteration 2029, loss = 0.17067022\n",
      "Iteration 2030, loss = 0.17053040\n",
      "Iteration 2031, loss = 0.17040847\n",
      "Iteration 2032, loss = 0.17039187\n",
      "Iteration 2033, loss = 0.17023079\n",
      "Iteration 2034, loss = 0.17015986\n",
      "Iteration 2035, loss = 0.17008865\n",
      "Iteration 2036, loss = 0.17008647\n",
      "Iteration 2037, loss = 0.16983648\n",
      "Iteration 2038, loss = 0.16975044\n",
      "Iteration 2039, loss = 0.16971354\n",
      "Iteration 2040, loss = 0.16951873\n",
      "Iteration 2041, loss = 0.16946031\n",
      "Iteration 2042, loss = 0.16938307\n",
      "Iteration 2043, loss = 0.16923470\n",
      "Iteration 2044, loss = 0.16916409\n",
      "Iteration 2045, loss = 0.16910374\n",
      "Iteration 2046, loss = 0.16894450\n",
      "Iteration 2047, loss = 0.16889777\n",
      "Iteration 2048, loss = 0.16876539\n",
      "Iteration 2049, loss = 0.16867260\n",
      "Iteration 2050, loss = 0.16851940\n",
      "Iteration 2051, loss = 0.16847354\n",
      "Iteration 2052, loss = 0.16832019\n",
      "Iteration 2053, loss = 0.16820541\n",
      "Iteration 2054, loss = 0.16813737\n",
      "Iteration 2055, loss = 0.16809400\n",
      "Iteration 2056, loss = 0.16792372\n",
      "Iteration 2057, loss = 0.16785296\n",
      "Iteration 2058, loss = 0.16778600\n",
      "Iteration 2059, loss = 0.16762417\n",
      "Iteration 2060, loss = 0.16766284\n",
      "Iteration 2061, loss = 0.16742018\n",
      "Iteration 2062, loss = 0.16734773\n",
      "Iteration 2063, loss = 0.16729758\n",
      "Iteration 2064, loss = 0.16717081\n",
      "Iteration 2065, loss = 0.16707992\n",
      "Iteration 2066, loss = 0.16695995\n",
      "Iteration 2067, loss = 0.16687671\n",
      "Iteration 2068, loss = 0.16683915\n",
      "Iteration 2069, loss = 0.16672447\n",
      "Iteration 2070, loss = 0.16656634\n",
      "Iteration 2071, loss = 0.16644871\n",
      "Iteration 2072, loss = 0.16641638\n",
      "Iteration 2073, loss = 0.16629774\n",
      "Iteration 2074, loss = 0.16630960\n",
      "Iteration 2075, loss = 0.16611729\n",
      "Iteration 2076, loss = 0.16601891\n",
      "Iteration 2077, loss = 0.16600418\n",
      "Iteration 2078, loss = 0.16591745\n",
      "Iteration 2079, loss = 0.16575363\n",
      "Iteration 2080, loss = 0.16575969\n",
      "Iteration 2081, loss = 0.16552390\n",
      "Iteration 2082, loss = 0.16554197\n",
      "Iteration 2083, loss = 0.16536674\n",
      "Iteration 2084, loss = 0.16525312\n",
      "Iteration 2085, loss = 0.16539166\n",
      "Iteration 2086, loss = 0.16509643\n",
      "Iteration 2087, loss = 0.16493925\n",
      "Iteration 2088, loss = 0.16492994\n",
      "Iteration 2089, loss = 0.16482682\n",
      "Iteration 2090, loss = 0.16469176\n",
      "Iteration 2091, loss = 0.16475977\n",
      "Iteration 2092, loss = 0.16448701\n",
      "Iteration 2093, loss = 0.16440084\n",
      "Iteration 2094, loss = 0.16431601\n",
      "Iteration 2095, loss = 0.16419864\n",
      "Iteration 2096, loss = 0.16409722\n",
      "Iteration 2097, loss = 0.16411273\n",
      "Iteration 2098, loss = 0.16392580\n",
      "Iteration 2099, loss = 0.16382020\n",
      "Iteration 2100, loss = 0.16371137\n",
      "Iteration 2101, loss = 0.16364931\n",
      "Iteration 2102, loss = 0.16364656\n",
      "Iteration 2103, loss = 0.16342125\n",
      "Iteration 2104, loss = 0.16334170\n",
      "Iteration 2105, loss = 0.16325815\n",
      "Iteration 2106, loss = 0.16314815\n",
      "Iteration 2107, loss = 0.16309924\n",
      "Iteration 2108, loss = 0.16297615\n",
      "Iteration 2109, loss = 0.16289264\n",
      "Iteration 2110, loss = 0.16286079\n",
      "Iteration 2111, loss = 0.16278855\n",
      "Iteration 2112, loss = 0.16278881\n",
      "Iteration 2113, loss = 0.16253895\n",
      "Iteration 2114, loss = 0.16245830\n",
      "Iteration 2115, loss = 0.16237232\n",
      "Iteration 2116, loss = 0.16222981\n",
      "Iteration 2117, loss = 0.16215973\n",
      "Iteration 2118, loss = 0.16201703\n",
      "Iteration 2119, loss = 0.16196374\n",
      "Iteration 2120, loss = 0.16186042\n",
      "Iteration 2121, loss = 0.16177344\n",
      "Iteration 2122, loss = 0.16164685\n",
      "Iteration 2123, loss = 0.16156631\n",
      "Iteration 2124, loss = 0.16151163\n",
      "Iteration 2125, loss = 0.16140004\n",
      "Iteration 2126, loss = 0.16136116\n",
      "Iteration 2127, loss = 0.16121364\n",
      "Iteration 2128, loss = 0.16127462\n",
      "Iteration 2129, loss = 0.16102569\n",
      "Iteration 2130, loss = 0.16095456\n",
      "Iteration 2131, loss = 0.16086718\n",
      "Iteration 2132, loss = 0.16077257\n",
      "Iteration 2133, loss = 0.16067937\n",
      "Iteration 2134, loss = 0.16064511\n",
      "Iteration 2135, loss = 0.16055784\n",
      "Iteration 2136, loss = 0.16045505\n",
      "Iteration 2137, loss = 0.16030091\n",
      "Iteration 2138, loss = 0.16028995\n",
      "Iteration 2139, loss = 0.16011238\n",
      "Iteration 2140, loss = 0.16004249\n",
      "Iteration 2141, loss = 0.15999844\n",
      "Iteration 2142, loss = 0.15983479\n",
      "Iteration 2143, loss = 0.15977853\n",
      "Iteration 2144, loss = 0.15967516\n",
      "Iteration 2145, loss = 0.15966763\n",
      "Iteration 2146, loss = 0.15970657\n",
      "Iteration 2147, loss = 0.15941205\n",
      "Iteration 2148, loss = 0.15937621\n",
      "Iteration 2149, loss = 0.15928943\n",
      "Iteration 2150, loss = 0.15919587\n",
      "Iteration 2151, loss = 0.15908412\n",
      "Iteration 2152, loss = 0.15899698\n",
      "Iteration 2153, loss = 0.15903153\n",
      "Iteration 2154, loss = 0.15883199\n",
      "Iteration 2155, loss = 0.15875543\n",
      "Iteration 2156, loss = 0.15867339\n",
      "Iteration 2157, loss = 0.15861453\n",
      "Iteration 2158, loss = 0.15846922\n",
      "Iteration 2159, loss = 0.15856215\n",
      "Iteration 2160, loss = 0.15829915\n",
      "Iteration 2161, loss = 0.15819000\n",
      "Iteration 2162, loss = 0.15813244\n",
      "Iteration 2163, loss = 0.15800054\n",
      "Iteration 2164, loss = 0.15792819\n",
      "Iteration 2165, loss = 0.15782744\n",
      "Iteration 2166, loss = 0.15777370\n",
      "Iteration 2167, loss = 0.15766856\n",
      "Iteration 2168, loss = 0.15758851\n",
      "Iteration 2169, loss = 0.15751018\n",
      "Iteration 2170, loss = 0.15744609\n",
      "Iteration 2171, loss = 0.15736300\n",
      "Iteration 2172, loss = 0.15722460\n",
      "Iteration 2173, loss = 0.15712157\n",
      "Iteration 2174, loss = 0.15704868\n",
      "Iteration 2175, loss = 0.15699491\n",
      "Iteration 2176, loss = 0.15697066\n",
      "Iteration 2177, loss = 0.15678370\n",
      "Iteration 2178, loss = 0.15671468\n",
      "Iteration 2179, loss = 0.15675444\n",
      "Iteration 2180, loss = 0.15663395\n",
      "Iteration 2181, loss = 0.15652696\n",
      "Iteration 2182, loss = 0.15638932\n",
      "Iteration 2183, loss = 0.15649743\n",
      "Iteration 2184, loss = 0.15633929\n",
      "Iteration 2185, loss = 0.15614137\n",
      "Iteration 2186, loss = 0.15603438\n",
      "Iteration 2187, loss = 0.15598142\n",
      "Iteration 2188, loss = 0.15588743\n",
      "Iteration 2189, loss = 0.15590622\n",
      "Iteration 2190, loss = 0.15573218\n",
      "Iteration 2191, loss = 0.15560539\n",
      "Iteration 2192, loss = 0.15555097\n",
      "Iteration 2193, loss = 0.15546347\n",
      "Iteration 2194, loss = 0.15542319\n",
      "Iteration 2195, loss = 0.15531438\n",
      "Iteration 2196, loss = 0.15515683\n",
      "Iteration 2197, loss = 0.15513175\n",
      "Iteration 680, loss = 0.30204474\n",
      "Iteration 681, loss = 0.30195905\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86138010\n",
      "Iteration 2, loss = 0.85240879\n",
      "Iteration 3, loss = 0.83935458\n",
      "Iteration 4, loss = 0.82426470\n",
      "Iteration 5, loss = 0.80892981\n",
      "Iteration 6, loss = 0.79344171\n",
      "Iteration 7, loss = 0.77907912\n",
      "Iteration 8, loss = 0.76519899\n",
      "Iteration 9, loss = 0.75335725\n",
      "Iteration 10, loss = 0.74273748\n",
      "Iteration 11, loss = 0.73251568\n",
      "Iteration 12, loss = 0.72373020\n",
      "Iteration 13, loss = 0.71556578\n",
      "Iteration 14, loss = 0.70796471\n",
      "Iteration 15, loss = 0.70122870\n",
      "Iteration 16, loss = 0.69485239\n",
      "Iteration 17, loss = 0.68870737\n",
      "Iteration 18, loss = 0.68336405\n",
      "Iteration 19, loss = 0.67806731\n",
      "Iteration 20, loss = 0.67276080\n",
      "Iteration 21, loss = 0.66794883\n",
      "Iteration 22, loss = 0.66307373\n",
      "Iteration 23, loss = 0.65851813\n",
      "Iteration 24, loss = 0.65396835\n",
      "Iteration 25, loss = 0.64948426\n",
      "Iteration 26, loss = 0.64505043\n",
      "Iteration 27, loss = 0.64061451\n",
      "Iteration 28, loss = 0.63625870\n",
      "Iteration 29, loss = 0.63193865\n",
      "Iteration 30, loss = 0.62764852\n",
      "Iteration 31, loss = 0.62341007\n",
      "Iteration 32, loss = 0.61921919\n",
      "Iteration 33, loss = 0.61494290\n",
      "Iteration 34, loss = 0.61071522\n",
      "Iteration 35, loss = 0.60645408\n",
      "Iteration 36, loss = 0.60234886\n",
      "Iteration 37, loss = 0.59815923\n",
      "Iteration 38, loss = 0.59388408\n",
      "Iteration 39, loss = 0.58970867\n",
      "Iteration 40, loss = 0.58565566\n",
      "Iteration 41, loss = 0.58145283\n",
      "Iteration 42, loss = 0.57729568\n",
      "Iteration 43, loss = 0.57306729\n",
      "Iteration 44, loss = 0.56897289\n",
      "Iteration 45, loss = 0.56486707\n",
      "Iteration 46, loss = 0.56075595\n",
      "Iteration 47, loss = 0.55678134\n",
      "Iteration 48, loss = 0.55261239\n",
      "Iteration 49, loss = 0.54861115\n",
      "Iteration 50, loss = 0.54464960\n",
      "Iteration 51, loss = 0.54064340\n",
      "Iteration 52, loss = 0.53672270\n",
      "Iteration 53, loss = 0.53283224\n",
      "Iteration 54, loss = 0.52896627\n",
      "Iteration 55, loss = 0.52521748\n",
      "Iteration 56, loss = 0.52136931\n",
      "Iteration 57, loss = 0.51773559\n",
      "Iteration 58, loss = 0.51411750\n",
      "Iteration 59, loss = 0.51044786\n",
      "Iteration 60, loss = 0.50691659\n",
      "Iteration 61, loss = 0.50335729\n",
      "Iteration 62, loss = 0.49993045\n",
      "Iteration 63, loss = 0.49665869\n",
      "Iteration 64, loss = 0.49322472\n",
      "Iteration 65, loss = 0.48992813\n",
      "Iteration 66, loss = 0.48680107\n",
      "Iteration 67, loss = 0.48353242\n",
      "Iteration 68, loss = 0.48047062\n",
      "Iteration 69, loss = 0.47749053\n",
      "Iteration 70, loss = 0.47459855\n",
      "Iteration 71, loss = 0.47170253\n",
      "Iteration 72, loss = 0.46896048\n",
      "Iteration 73, loss = 0.46601205\n",
      "Iteration 74, loss = 0.46349329\n",
      "Iteration 75, loss = 0.46092799\n",
      "Iteration 76, loss = 0.45828358\n",
      "Iteration 77, loss = 0.45585158\n",
      "Iteration 78, loss = 0.45336543\n",
      "Iteration 79, loss = 0.45112717\n",
      "Iteration 80, loss = 0.44879207\n",
      "Iteration 81, loss = 0.44655100\n",
      "Iteration 82, loss = 0.44443092\n",
      "Iteration 83, loss = 0.44230578\n",
      "Iteration 84, loss = 0.44030295\n",
      "Iteration 85, loss = 0.43825967\n",
      "Iteration 86, loss = 0.43637811\n",
      "Iteration 87, loss = 0.43452530\n",
      "Iteration 88, loss = 0.43263542\n",
      "Iteration 89, loss = 0.43089465\n",
      "Iteration 90, loss = 0.42910441\n",
      "Iteration 91, loss = 0.42752893\n",
      "Iteration 92, loss = 0.42594902\n",
      "Iteration 93, loss = 0.42437184\n",
      "Iteration 94, loss = 0.42277877\n",
      "Iteration 95, loss = 0.42132752\n",
      "Iteration 96, loss = 0.41992396\n",
      "Iteration 97, loss = 0.41848773\n",
      "Iteration 98, loss = 0.41713758\n",
      "Iteration 99, loss = 0.41585621\n",
      "Iteration 100, loss = 0.41452058\n",
      "Iteration 101, loss = 0.41327433\n",
      "Iteration 102, loss = 0.41215340\n",
      "Iteration 103, loss = 0.41096346\n",
      "Iteration 104, loss = 0.40974556\n",
      "Iteration 105, loss = 0.40866378\n",
      "Iteration 106, loss = 0.40759812\n",
      "Iteration 107, loss = 0.40655625\n",
      "Iteration 108, loss = 0.40549538\n",
      "Iteration 109, loss = 0.40456134\n",
      "Iteration 110, loss = 0.40353418\n",
      "Iteration 111, loss = 0.40254622\n",
      "Iteration 112, loss = 0.40160848\n",
      "Iteration 113, loss = 0.40070378\n",
      "Iteration 114, loss = 0.39979640\n",
      "Iteration 115, loss = 0.39900194\n",
      "Iteration 116, loss = 0.39815004\n",
      "Iteration 117, loss = 0.39737043\n",
      "Iteration 118, loss = 0.39656478\n",
      "Iteration 119, loss = 0.39575863\n",
      "Iteration 120, loss = 0.39498283\n",
      "Iteration 121, loss = 0.39423396\n",
      "Iteration 122, loss = 0.39350464\n",
      "Iteration 123, loss = 0.39279994\n",
      "Iteration 124, loss = 0.39212674\n",
      "Iteration 125, loss = 0.39144085\n",
      "Iteration 126, loss = 0.39076454\n",
      "Iteration 127, loss = 0.39011791\n",
      "Iteration 128, loss = 0.38947819\n",
      "Iteration 129, loss = 0.38883854\n",
      "Iteration 130, loss = 0.38821839\n",
      "Iteration 131, loss = 0.38768058\n",
      "Iteration 132, loss = 0.38704732\n",
      "Iteration 133, loss = 0.38648111\n",
      "Iteration 134, loss = 0.38589410\n",
      "Iteration 135, loss = 0.38534317\n",
      "Iteration 136, loss = 0.38476215\n",
      "Iteration 137, loss = 0.38421073\n",
      "Iteration 138, loss = 0.38371641\n",
      "Iteration 139, loss = 0.38316257\n",
      "Iteration 140, loss = 0.38265642\n",
      "Iteration 141, loss = 0.38214216\n",
      "Iteration 142, loss = 0.38170521\n",
      "Iteration 143, loss = 0.38117423\n",
      "Iteration 144, loss = 0.38066848\n",
      "Iteration 145, loss = 0.38016443\n",
      "Iteration 146, loss = 0.37967632\n",
      "Iteration 147, loss = 0.37920390\n",
      "Iteration 148, loss = 0.37873006\n",
      "Iteration 149, loss = 0.37830401\n",
      "Iteration 150, loss = 0.37784053\n",
      "Iteration 151, loss = 0.37746976\n",
      "Iteration 152, loss = 0.37697236\n",
      "Iteration 153, loss = 0.37654306\n",
      "Iteration 154, loss = 0.37608677\n",
      "Iteration 155, loss = 0.37572030\n",
      "Iteration 156, loss = 0.37525152\n",
      "Iteration 157, loss = 0.37488846\n",
      "Iteration 158, loss = 0.37449194\n",
      "Iteration 159, loss = 0.37405485\n",
      "Iteration 160, loss = 0.37364213\n",
      "Iteration 161, loss = 0.37324153\n",
      "Iteration 162, loss = 0.37284236\n",
      "Iteration 163, loss = 0.37245733\n",
      "Iteration 164, loss = 0.37205633\n",
      "Iteration 165, loss = 0.37171067\n",
      "Iteration 166, loss = 0.37135191\n",
      "Iteration 167, loss = 0.37093380\n",
      "Iteration 168, loss = 0.37057659\n",
      "Iteration 169, loss = 0.37020073\n",
      "Iteration 170, loss = 0.36984585\n",
      "Iteration 171, loss = 0.36948118\n",
      "Iteration 172, loss = 0.36917564\n",
      "Iteration 173, loss = 0.36876568\n",
      "Iteration 174, loss = 0.36844384\n",
      "Iteration 175, loss = 0.36807149\n",
      "Iteration 176, loss = 0.36778320\n",
      "Iteration 177, loss = 0.36737728\n",
      "Iteration 178, loss = 0.36702830\n",
      "Iteration 179, loss = 0.36671186\n",
      "Iteration 180, loss = 0.36640220\n",
      "Iteration 181, loss = 0.36603705\n",
      "Iteration 182, loss = 0.36571441\n",
      "Iteration 183, loss = 0.36538707\n",
      "Iteration 184, loss = 0.36511226\n",
      "Iteration 185, loss = 0.36475014\n",
      "Iteration 186, loss = 0.36443878\n",
      "Iteration 187, loss = 0.36410374\n",
      "Iteration 188, loss = 0.36379666\n",
      "Iteration 189, loss = 0.36346971\n",
      "Iteration 190, loss = 0.36319450\n",
      "Iteration 191, loss = 0.36288892\n",
      "Iteration 192, loss = 0.36259010\n",
      "Iteration 193, loss = 0.36224532\n",
      "Iteration 194, loss = 0.36192488\n",
      "Iteration 195, loss = 0.36162339\n",
      "Iteration 196, loss = 0.36134664\n",
      "Iteration 197, loss = 0.36105638\n",
      "Iteration 198, loss = 0.36073771\n",
      "Iteration 199, loss = 0.36047283\n",
      "Iteration 200, loss = 0.36015463\n",
      "Iteration 201, loss = 0.35987531\n",
      "Iteration 202, loss = 0.35956395\n",
      "Iteration 203, loss = 0.35931633\n",
      "Iteration 204, loss = 0.35898882\n",
      "Iteration 205, loss = 0.35870047\n",
      "Iteration 206, loss = 0.35841110\n",
      "Iteration 207, loss = 0.35816870\n",
      "Iteration 208, loss = 0.35784166\n",
      "Iteration 209, loss = 0.35760192\n",
      "Iteration 210, loss = 0.35727963\n",
      "Iteration 211, loss = 0.35701764\n",
      "Iteration 212, loss = 0.35674729\n",
      "Iteration 213, loss = 0.35648948\n",
      "Iteration 214, loss = 0.35620241\n",
      "Iteration 215, loss = 0.35596185\n",
      "Iteration 216, loss = 0.35570026\n",
      "Iteration 217, loss = 0.35541525\n",
      "Iteration 218, loss = 0.35517405\n",
      "Iteration 219, loss = 0.35489826\n",
      "Iteration 220, loss = 0.35472539\n",
      "Iteration 221, loss = 0.35437096\n",
      "Iteration 222, loss = 0.35413161\n",
      "Iteration 223, loss = 0.35386834\n",
      "Iteration 224, loss = 0.35365234\n",
      "Iteration 225, loss = 0.35337989\n",
      "Iteration 226, loss = 0.35313741\n",
      "Iteration 227, loss = 0.35288532\n",
      "Iteration 228, loss = 0.35263756\n",
      "Iteration 229, loss = 0.35240592\n",
      "Iteration 230, loss = 0.35215611\n",
      "Iteration 231, loss = 0.35194764\n",
      "Iteration 232, loss = 0.35167928\n",
      "Iteration 233, loss = 0.35146148\n",
      "Iteration 234, loss = 0.35122080\n",
      "Iteration 235, loss = 0.35097968\n",
      "Iteration 236, loss = 0.35074088\n",
      "Iteration 237, loss = 0.35052529\n",
      "Iteration 238, loss = 0.35027017\n",
      "Iteration 239, loss = 0.35002358\n",
      "Iteration 240, loss = 0.34986424\n",
      "Iteration 241, loss = 0.34957812\n",
      "Iteration 242, loss = 0.34936457\n",
      "Iteration 243, loss = 0.34913580\n",
      "Iteration 244, loss = 0.34890315\n",
      "Iteration 245, loss = 0.34869262\n",
      "Iteration 246, loss = 0.34845353\n",
      "Iteration 247, loss = 0.34820064\n",
      "Iteration 248, loss = 0.34801385\n",
      "Iteration 249, loss = 0.34781958\n",
      "Iteration 250, loss = 0.34754221\n",
      "Iteration 251, loss = 0.34732147\n",
      "Iteration 252, loss = 0.34710647\n",
      "Iteration 253, loss = 0.34688526\n",
      "Iteration 254, loss = 0.34668385\n",
      "Iteration 255, loss = 0.34644578\n",
      "Iteration 256, loss = 0.34622949\n",
      "Iteration 257, loss = 0.34599744\n",
      "Iteration 258, loss = 0.34581234\n",
      "Iteration 259, loss = 0.34559573\n",
      "Iteration 260, loss = 0.34537111\n",
      "Iteration 261, loss = 0.34514152\n",
      "Iteration 262, loss = 0.34493001\n",
      "Iteration 263, loss = 0.34471739\n",
      "Iteration 264, loss = 0.34451859\n",
      "Iteration 265, loss = 0.34430492\n",
      "Iteration 266, loss = 0.34411393\n",
      "Iteration 267, loss = 0.34388397\n",
      "Iteration 268, loss = 0.34368709\n",
      "Iteration 269, loss = 0.34349006\n",
      "Iteration 270, loss = 0.34326837\n",
      "Iteration 271, loss = 0.34309618\n",
      "Iteration 272, loss = 0.34287867\n",
      "Iteration 273, loss = 0.34271285\n",
      "Iteration 274, loss = 0.34247544\n",
      "Iteration 275, loss = 0.34230126\n",
      "Iteration 276, loss = 0.34208161\n",
      "Iteration 277, loss = 0.34188115\n",
      "Iteration 278, loss = 0.34168977\n",
      "Iteration 279, loss = 0.34149453\n",
      "Iteration 280, loss = 0.34131401\n",
      "Iteration 281, loss = 0.34110825\n",
      "Iteration 282, loss = 0.34096500\n",
      "Iteration 283, loss = 0.34072096\n",
      "Iteration 284, loss = 0.34052047\n",
      "Iteration 285, loss = 0.34032869\n",
      "Iteration 286, loss = 0.34012915\n",
      "Iteration 287, loss = 0.33995648\n",
      "Iteration 288, loss = 0.33977056\n",
      "Iteration 289, loss = 0.33958084\n",
      "Iteration 290, loss = 0.33937691\n",
      "Iteration 291, loss = 0.33918912\n",
      "Iteration 292, loss = 0.33900842\n",
      "Iteration 293, loss = 0.33881413\n",
      "Iteration 294, loss = 0.33865384\n",
      "Iteration 295, loss = 0.33847215\n",
      "Iteration 296, loss = 0.33828446\n",
      "Iteration 297, loss = 0.33807751\n",
      "Iteration 298, loss = 0.33789548\n",
      "Iteration 299, loss = 0.33774522\n",
      "Iteration 300, loss = 0.33757373\n",
      "Iteration 301, loss = 0.33735996\n",
      "Iteration 302, loss = 0.33716640\n",
      "Iteration 303, loss = 0.33702397\n",
      "Iteration 304, loss = 0.33682741\n",
      "Iteration 305, loss = 0.33664106\n",
      "Iteration 306, loss = 0.33644180\n",
      "Iteration 307, loss = 0.33627754\n",
      "Iteration 308, loss = 0.33615196\n",
      "Iteration 309, loss = 0.33590607\n",
      "Iteration 310, loss = 0.33573943\n",
      "Iteration 311, loss = 0.33555704\n",
      "Iteration 312, loss = 0.33536774\n",
      "Iteration 313, loss = 0.33520708\n",
      "Iteration 314, loss = 0.33502435\n",
      "Iteration 315, loss = 0.33487188\n",
      "Iteration 316, loss = 0.33469484\n",
      "Iteration 317, loss = 0.33456538\n",
      "Iteration 318, loss = 0.33432706\n",
      "Iteration 319, loss = 0.33412873\n",
      "Iteration 320, loss = 0.33397301\n",
      "Iteration 321, loss = 0.33384317\n",
      "Iteration 322, loss = 0.33362719\n",
      "Iteration 323, loss = 0.33342464\n",
      "Iteration 324, loss = 0.33326621\n",
      "Iteration 325, loss = 0.33310119\n",
      "Iteration 326, loss = 0.33291987\n",
      "Iteration 327, loss = 0.33273705\n",
      "Iteration 328, loss = 0.33255566\n",
      "Iteration 329, loss = 0.33241216\n",
      "Iteration 330, loss = 0.33222918\n",
      "Iteration 331, loss = 0.33206785\n",
      "Iteration 332, loss = 0.33185779\n",
      "Iteration 333, loss = 0.33173016\n",
      "Iteration 334, loss = 0.33151664\n",
      "Iteration 335, loss = 0.33135576\n",
      "Iteration 336, loss = 0.33115565\n",
      "Iteration 337, loss = 0.33102753\n",
      "Iteration 338, loss = 0.33084680\n",
      "Iteration 339, loss = 0.33069872\n",
      "Iteration 340, loss = 0.33046774\n",
      "Iteration 341, loss = 0.33033113\n",
      "Iteration 342, loss = 0.33014741\n",
      "Iteration 343, loss = 0.32995799\n",
      "Iteration 344, loss = 0.32980537\n",
      "Iteration 345, loss = 0.32961628\n",
      "Iteration 346, loss = 0.32945995\n",
      "Iteration 347, loss = 0.32926783\n",
      "Iteration 348, loss = 0.32909815\n",
      "Iteration 349, loss = 0.32894127\n",
      "Iteration 350, loss = 0.32876661\n",
      "Iteration 351, loss = 0.32862351\n",
      "Iteration 352, loss = 0.32842287\n",
      "Iteration 353, loss = 0.32826998\n",
      "Iteration 354, loss = 0.32807516\n",
      "Iteration 355, loss = 0.32794219\n",
      "Iteration 356, loss = 0.32774266\n",
      "Iteration 357, loss = 0.32758801\n",
      "Iteration 358, loss = 0.32740418\n",
      "Iteration 359, loss = 0.32723403\n",
      "Iteration 360, loss = 0.32706771\n",
      "Iteration 361, loss = 0.32690180\n",
      "Iteration 362, loss = 0.32671693\n",
      "Iteration 363, loss = 0.32657617\n",
      "Iteration 364, loss = 0.32643267\n",
      "Iteration 365, loss = 0.32622364\n",
      "Iteration 366, loss = 0.32604414\n",
      "Iteration 367, loss = 0.32587326\n",
      "Iteration 368, loss = 0.32571249\n",
      "Iteration 369, loss = 0.32557710\n",
      "Iteration 370, loss = 0.32535784\n",
      "Iteration 371, loss = 0.32523173\n",
      "Iteration 372, loss = 0.32503282\n",
      "Iteration 373, loss = 0.32487804\n",
      "Iteration 374, loss = 0.32469105\n",
      "Iteration 375, loss = 0.32458899\n",
      "Iteration 376, loss = 0.32436818\n",
      "Iteration 377, loss = 0.32420217\n",
      "Iteration 378, loss = 0.32404280\n",
      "Iteration 379, loss = 0.32387549\n",
      "Iteration 380, loss = 0.32372470\n",
      "Iteration 381, loss = 0.32357749\n",
      "Iteration 382, loss = 0.32338697\n",
      "Iteration 383, loss = 0.32327160\n",
      "Iteration 384, loss = 0.32307251\n",
      "Iteration 385, loss = 0.32293935\n",
      "Iteration 386, loss = 0.32272834\n",
      "Iteration 387, loss = 0.32255908\n",
      "Iteration 388, loss = 0.32242938\n",
      "Iteration 389, loss = 0.32224854\n",
      "Iteration 390, loss = 0.32208811\n",
      "Iteration 391, loss = 0.32192103\n",
      "Iteration 392, loss = 0.32176582\n",
      "Iteration 393, loss = 0.32161289\n",
      "Iteration 394, loss = 0.32143061\n",
      "Iteration 395, loss = 0.32128205\n",
      "Iteration 396, loss = 0.32113558\n",
      "Iteration 397, loss = 0.32094873\n",
      "Iteration 398, loss = 0.32080840\n",
      "Iteration 399, loss = 0.32063378\n",
      "Iteration 400, loss = 0.32047801\n",
      "Iteration 401, loss = 0.32033500\n",
      "Iteration 402, loss = 0.32015291\n",
      "Iteration 403, loss = 0.32000511\n",
      "Iteration 404, loss = 0.31982553\n",
      "Iteration 405, loss = 0.31966210\n",
      "Iteration 406, loss = 0.31949789\n",
      "Iteration 407, loss = 0.31934885\n",
      "Iteration 408, loss = 0.31917394\n",
      "Iteration 409, loss = 0.31904414\n",
      "Iteration 410, loss = 0.31887560\n",
      "Iteration 411, loss = 0.31872838\n",
      "Iteration 412, loss = 0.31853204\n",
      "Iteration 413, loss = 0.31840725\n",
      "Iteration 414, loss = 0.31821512\n",
      "Iteration 415, loss = 0.31806641\n",
      "Iteration 416, loss = 0.31790466\n",
      "Iteration 417, loss = 0.31775585\n",
      "Iteration 418, loss = 0.31769085\n",
      "Iteration 419, loss = 0.31741224\n",
      "Iteration 420, loss = 0.31726072\n",
      "Iteration 421, loss = 0.31711883\n",
      "Iteration 422, loss = 0.31694374\n",
      "Iteration 423, loss = 0.31683720\n",
      "Iteration 424, loss = 0.31662386\n",
      "Iteration 425, loss = 0.31651770\n",
      "Iteration 426, loss = 0.31636311\n",
      "Iteration 427, loss = 0.31615761\n",
      "Iteration 428, loss = 0.31601182\n",
      "Iteration 429, loss = 0.31583640\n",
      "Iteration 430, loss = 0.31570037\n",
      "Iteration 431, loss = 0.31552445\n",
      "Iteration 432, loss = 0.31536938\n",
      "Iteration 433, loss = 0.31524425\n",
      "Iteration 434, loss = 0.31509939\n",
      "Iteration 435, loss = 0.31491071\n",
      "Iteration 436, loss = 0.31476278\n",
      "Iteration 437, loss = 0.31460930\n",
      "Iteration 438, loss = 0.31442535\n",
      "Iteration 439, loss = 0.31427732\n",
      "Iteration 440, loss = 0.31413789\n",
      "Iteration 441, loss = 0.31398302\n",
      "Iteration 442, loss = 0.31382462\n",
      "Iteration 443, loss = 0.31365891\n",
      "Iteration 444, loss = 0.31349392\n",
      "Iteration 445, loss = 0.31335836\n",
      "Iteration 446, loss = 0.31320004\n",
      "Iteration 447, loss = 0.31304157\n",
      "Iteration 448, loss = 0.31290711\n",
      "Iteration 449, loss = 0.31272388\n",
      "Iteration 450, loss = 0.31259924\n",
      "Iteration 451, loss = 0.31242955\n",
      "Iteration 452, loss = 0.31225890\n",
      "Iteration 453, loss = 0.31212043\n",
      "Iteration 454, loss = 0.31196987\n",
      "Iteration 455, loss = 0.31182995\n",
      "Iteration 456, loss = 0.31164041\n",
      "Iteration 457, loss = 0.31150098\n",
      "Iteration 458, loss = 0.31139843\n",
      "Iteration 459, loss = 0.31120113\n",
      "Iteration 460, loss = 0.31104705\n",
      "Iteration 461, loss = 0.31088321\n",
      "Iteration 462, loss = 0.31073881\n",
      "Iteration 463, loss = 0.31056695\n",
      "Iteration 464, loss = 0.31043855\n",
      "Iteration 465, loss = 0.31027649\n",
      "Iteration 466, loss = 0.31012513\n",
      "Iteration 467, loss = 0.30997689\n",
      "Iteration 468, loss = 0.30982565\n",
      "Iteration 469, loss = 0.30967316\n",
      "Iteration 470, loss = 0.30950478\n",
      "Iteration 471, loss = 0.30936637\n",
      "Iteration 472, loss = 0.30927349\n",
      "Iteration 473, loss = 0.30907179\n",
      "Iteration 474, loss = 0.30888698\n",
      "Iteration 475, loss = 0.30873877\n",
      "Iteration 476, loss = 0.30861342\n",
      "Iteration 477, loss = 0.30844524\n",
      "Iteration 478, loss = 0.30829917\n",
      "Iteration 479, loss = 0.30819618\n",
      "Iteration 480, loss = 0.30799772\n",
      "Iteration 481, loss = 0.30785197\n",
      "Iteration 482, loss = 0.30770536\n",
      "Iteration 483, loss = 0.30754380\n",
      "Iteration 484, loss = 0.30740170\n",
      "Iteration 485, loss = 0.30727191\n",
      "Iteration 486, loss = 0.30709762\n",
      "Iteration 487, loss = 0.30696956\n",
      "Iteration 488, loss = 0.30685163\n",
      "Iteration 489, loss = 0.30664723\n",
      "Iteration 490, loss = 0.30650548\n",
      "Iteration 491, loss = 0.30637494\n",
      "Iteration 492, loss = 0.30620060\n",
      "Iteration 493, loss = 0.30609002\n",
      "Iteration 494, loss = 0.30592750\n",
      "Iteration 122, loss = 0.44538084\n",
      "Iteration 123, loss = 0.44349252\n",
      "Iteration 124, loss = 0.44152291\n",
      "Iteration 125, loss = 0.43962566\n",
      "Iteration 126, loss = 0.43786095\n",
      "Iteration 127, loss = 0.43610349\n",
      "Iteration 128, loss = 0.43432696\n",
      "Iteration 129, loss = 0.43271965\n",
      "Iteration 130, loss = 0.43103854\n",
      "Iteration 131, loss = 0.42945037\n",
      "Iteration 132, loss = 0.42798283\n",
      "Iteration 133, loss = 0.42642767\n",
      "Iteration 134, loss = 0.42493198\n",
      "Iteration 135, loss = 0.42353496\n",
      "Iteration 136, loss = 0.42212167\n",
      "Iteration 137, loss = 0.42086495\n",
      "Iteration 138, loss = 0.41948363\n",
      "Iteration 139, loss = 0.41828583\n",
      "Iteration 140, loss = 0.41703501\n",
      "Iteration 141, loss = 0.41585430\n",
      "Iteration 142, loss = 0.41466857\n",
      "Iteration 143, loss = 0.41354165\n",
      "Iteration 144, loss = 0.41247021\n",
      "Iteration 145, loss = 0.41129214\n",
      "Iteration 146, loss = 0.41031476\n",
      "Iteration 147, loss = 0.40932702\n",
      "Iteration 148, loss = 0.40828852\n",
      "Iteration 149, loss = 0.40729317\n",
      "Iteration 150, loss = 0.40634411\n",
      "Iteration 151, loss = 0.40543191\n",
      "Iteration 152, loss = 0.40453307\n",
      "Iteration 153, loss = 0.40369586\n",
      "Iteration 154, loss = 0.40276225\n",
      "Iteration 155, loss = 0.40193985\n",
      "Iteration 156, loss = 0.40114698\n",
      "Iteration 157, loss = 0.40037323\n",
      "Iteration 158, loss = 0.39954187\n",
      "Iteration 159, loss = 0.39875969\n",
      "Iteration 160, loss = 0.39807694\n",
      "Iteration 161, loss = 0.39727227\n",
      "Iteration 162, loss = 0.39661024\n",
      "Iteration 163, loss = 0.39588990\n",
      "Iteration 164, loss = 0.39516678\n",
      "Iteration 165, loss = 0.39449446\n",
      "Iteration 166, loss = 0.39385745\n",
      "Iteration 167, loss = 0.39319672\n",
      "Iteration 168, loss = 0.39251363\n",
      "Iteration 169, loss = 0.39189805\n",
      "Iteration 170, loss = 0.39126722\n",
      "Iteration 171, loss = 0.39071988\n",
      "Iteration 172, loss = 0.39009588\n",
      "Iteration 173, loss = 0.38955741\n",
      "Iteration 174, loss = 0.38887813\n",
      "Iteration 175, loss = 0.38827152\n",
      "Iteration 176, loss = 0.38773221\n",
      "Iteration 177, loss = 0.38715099\n",
      "Iteration 178, loss = 0.38656353\n",
      "Iteration 179, loss = 0.38602511\n",
      "Iteration 180, loss = 0.38554750\n",
      "Iteration 181, loss = 0.38493609\n",
      "Iteration 182, loss = 0.38442496\n",
      "Iteration 183, loss = 0.38391110\n",
      "Iteration 184, loss = 0.38348362\n",
      "Iteration 185, loss = 0.38287265\n",
      "Iteration 186, loss = 0.38239821\n",
      "Iteration 187, loss = 0.38188709\n",
      "Iteration 188, loss = 0.38133789\n",
      "Iteration 189, loss = 0.38086821\n",
      "Iteration 190, loss = 0.38044869\n",
      "Iteration 191, loss = 0.37991129\n",
      "Iteration 192, loss = 0.37957675\n",
      "Iteration 193, loss = 0.37898319\n",
      "Iteration 194, loss = 0.37851434\n",
      "Iteration 195, loss = 0.37805210\n",
      "Iteration 196, loss = 0.37761103\n",
      "Iteration 197, loss = 0.37715155\n",
      "Iteration 198, loss = 0.37676134\n",
      "Iteration 199, loss = 0.37630426\n",
      "Iteration 200, loss = 0.37588232\n",
      "Iteration 201, loss = 0.37541116\n",
      "Iteration 202, loss = 0.37497184\n",
      "Iteration 203, loss = 0.37458555\n",
      "Iteration 204, loss = 0.37413575\n",
      "Iteration 205, loss = 0.37371582\n",
      "Iteration 206, loss = 0.37329193\n",
      "Iteration 207, loss = 0.37289230\n",
      "Iteration 208, loss = 0.37248205\n",
      "Iteration 209, loss = 0.37205764\n",
      "Iteration 210, loss = 0.37167138\n",
      "Iteration 211, loss = 0.37127995\n",
      "Iteration 212, loss = 0.37086363\n",
      "Iteration 213, loss = 0.37047951\n",
      "Iteration 214, loss = 0.37013980\n",
      "Iteration 215, loss = 0.36976176\n",
      "Iteration 216, loss = 0.36930362\n",
      "Iteration 217, loss = 0.36893548\n",
      "Iteration 218, loss = 0.36854684\n",
      "Iteration 219, loss = 0.36816505\n",
      "Iteration 220, loss = 0.36787758\n",
      "Iteration 221, loss = 0.36744988\n",
      "Iteration 222, loss = 0.36708532\n",
      "Iteration 223, loss = 0.36668292\n",
      "Iteration 224, loss = 0.36627593\n",
      "Iteration 225, loss = 0.36601514\n",
      "Iteration 226, loss = 0.36563110\n",
      "Iteration 227, loss = 0.36519988\n",
      "Iteration 228, loss = 0.36489226\n",
      "Iteration 229, loss = 0.36451421\n",
      "Iteration 230, loss = 0.36420060\n",
      "Iteration 231, loss = 0.36380426\n",
      "Iteration 232, loss = 0.36348933\n",
      "Iteration 233, loss = 0.36309313\n",
      "Iteration 234, loss = 0.36278591\n",
      "Iteration 235, loss = 0.36240743\n",
      "Iteration 236, loss = 0.36204218\n",
      "Iteration 237, loss = 0.36170653\n",
      "Iteration 238, loss = 0.36135036\n",
      "Iteration 239, loss = 0.36113290\n",
      "Iteration 240, loss = 0.36068007\n",
      "Iteration 241, loss = 0.36036433\n",
      "Iteration 242, loss = 0.36005437\n",
      "Iteration 243, loss = 0.35970773\n",
      "Iteration 244, loss = 0.35940649\n",
      "Iteration 245, loss = 0.35905281\n",
      "Iteration 246, loss = 0.35875479\n",
      "Iteration 247, loss = 0.35839733\n",
      "Iteration 248, loss = 0.35813275\n",
      "Iteration 249, loss = 0.35784598\n",
      "Iteration 250, loss = 0.35755204\n",
      "Iteration 251, loss = 0.35720142\n",
      "Iteration 252, loss = 0.35691318\n",
      "Iteration 253, loss = 0.35660690\n",
      "Iteration 254, loss = 0.35630495\n",
      "Iteration 255, loss = 0.35603360\n",
      "Iteration 256, loss = 0.35573044\n",
      "Iteration 257, loss = 0.35544925\n",
      "Iteration 258, loss = 0.35523563\n",
      "Iteration 259, loss = 0.35486765\n",
      "Iteration 260, loss = 0.35461183\n",
      "Iteration 261, loss = 0.35439049\n",
      "Iteration 262, loss = 0.35405684\n",
      "Iteration 263, loss = 0.35385933\n",
      "Iteration 264, loss = 0.35354378\n",
      "Iteration 265, loss = 0.35324067\n",
      "Iteration 266, loss = 0.35303165\n",
      "Iteration 267, loss = 0.35269873\n",
      "Iteration 268, loss = 0.35240948\n",
      "Iteration 269, loss = 0.35228226\n",
      "Iteration 270, loss = 0.35189420\n",
      "Iteration 271, loss = 0.35165471\n",
      "Iteration 272, loss = 0.35140657\n",
      "Iteration 273, loss = 0.35110181\n",
      "Iteration 274, loss = 0.35089528\n",
      "Iteration 275, loss = 0.35058588\n",
      "Iteration 276, loss = 0.35043595\n",
      "Iteration 277, loss = 0.35010739\n",
      "Iteration 278, loss = 0.34992287\n",
      "Iteration 279, loss = 0.34972716\n",
      "Iteration 280, loss = 0.34935241\n",
      "Iteration 281, loss = 0.34912115\n",
      "Iteration 282, loss = 0.34887562\n",
      "Iteration 283, loss = 0.34862040\n",
      "Iteration 284, loss = 0.34840130\n",
      "Iteration 285, loss = 0.34813924\n",
      "Iteration 286, loss = 0.34790639\n",
      "Iteration 287, loss = 0.34776032\n",
      "Iteration 288, loss = 0.34744874\n",
      "Iteration 289, loss = 0.34721573\n",
      "Iteration 290, loss = 0.34702548\n",
      "Iteration 291, loss = 0.34670048\n",
      "Iteration 292, loss = 0.34652935\n",
      "Iteration 293, loss = 0.34621875\n",
      "Iteration 294, loss = 0.34597172\n",
      "Iteration 295, loss = 0.34577661\n",
      "Iteration 296, loss = 0.34554071\n",
      "Iteration 297, loss = 0.34529156\n",
      "Iteration 298, loss = 0.34514345\n",
      "Iteration 299, loss = 0.34488144\n",
      "Iteration 300, loss = 0.34467524\n",
      "Iteration 301, loss = 0.34437631\n",
      "Iteration 302, loss = 0.34420042\n",
      "Iteration 303, loss = 0.34398925\n",
      "Iteration 304, loss = 0.34375047\n",
      "Iteration 305, loss = 0.34350495\n",
      "Iteration 306, loss = 0.34329155\n",
      "Iteration 307, loss = 0.34310138\n",
      "Iteration 308, loss = 0.34289521\n",
      "Iteration 309, loss = 0.34272419\n",
      "Iteration 310, loss = 0.34245800\n",
      "Iteration 311, loss = 0.34227176\n",
      "Iteration 312, loss = 0.34203897\n",
      "Iteration 313, loss = 0.34191707\n",
      "Iteration 314, loss = 0.34161040\n",
      "Iteration 315, loss = 0.34140291\n",
      "Iteration 316, loss = 0.34119088\n",
      "Iteration 317, loss = 0.34099662\n",
      "Iteration 318, loss = 0.34076966\n",
      "Iteration 319, loss = 0.34056687\n",
      "Iteration 320, loss = 0.34039394\n",
      "Iteration 321, loss = 0.34022304\n",
      "Iteration 322, loss = 0.34005505\n",
      "Iteration 323, loss = 0.33975478\n",
      "Iteration 324, loss = 0.33955177\n",
      "Iteration 325, loss = 0.33936575\n",
      "Iteration 326, loss = 0.33917042\n",
      "Iteration 327, loss = 0.33900664\n",
      "Iteration 328, loss = 0.33875848\n",
      "Iteration 329, loss = 0.33855393\n",
      "Iteration 330, loss = 0.33841362\n",
      "Iteration 331, loss = 0.33823287\n",
      "Iteration 332, loss = 0.33800270\n",
      "Iteration 333, loss = 0.33781388\n",
      "Iteration 334, loss = 0.33760578\n",
      "Iteration 335, loss = 0.33742236\n",
      "Iteration 336, loss = 0.33726885\n",
      "Iteration 337, loss = 0.33706179\n",
      "Iteration 338, loss = 0.33682490\n",
      "Iteration 339, loss = 0.33663721\n",
      "Iteration 340, loss = 0.33648728\n",
      "Iteration 341, loss = 0.33627529\n",
      "Iteration 342, loss = 0.33607810\n",
      "Iteration 343, loss = 0.33590939\n",
      "Iteration 344, loss = 0.33575066\n",
      "Iteration 345, loss = 0.33554903\n",
      "Iteration 346, loss = 0.33533682\n",
      "Iteration 347, loss = 0.33516472\n",
      "Iteration 348, loss = 0.33499703\n",
      "Iteration 349, loss = 0.33483964\n",
      "Iteration 350, loss = 0.33462210\n",
      "Iteration 351, loss = 0.33445264\n",
      "Iteration 352, loss = 0.33426836\n",
      "Iteration 353, loss = 0.33411612\n",
      "Iteration 354, loss = 0.33394533\n",
      "Iteration 355, loss = 0.33380927\n",
      "Iteration 356, loss = 0.33360475\n",
      "Iteration 357, loss = 0.33342937\n",
      "Iteration 358, loss = 0.33327762\n",
      "Iteration 359, loss = 0.33308145\n",
      "Iteration 360, loss = 0.33295487\n",
      "Iteration 361, loss = 0.33276522\n",
      "Iteration 362, loss = 0.33253724\n",
      "Iteration 363, loss = 0.33238959\n",
      "Iteration 364, loss = 0.33221060\n",
      "Iteration 365, loss = 0.33205481\n",
      "Iteration 366, loss = 0.33185420\n",
      "Iteration 367, loss = 0.33168125\n",
      "Iteration 368, loss = 0.33150914\n",
      "Iteration 369, loss = 0.33133701\n",
      "Iteration 370, loss = 0.33119484\n",
      "Iteration 371, loss = 0.33103494\n",
      "Iteration 372, loss = 0.33085109\n",
      "Iteration 373, loss = 0.33068751\n",
      "Iteration 374, loss = 0.33051547\n",
      "Iteration 375, loss = 0.33041422\n",
      "Iteration 376, loss = 0.33019099\n",
      "Iteration 377, loss = 0.33004852\n",
      "Iteration 378, loss = 0.32985379\n",
      "Iteration 379, loss = 0.32968863\n",
      "Iteration 380, loss = 0.32953808\n",
      "Iteration 381, loss = 0.32935759\n",
      "Iteration 382, loss = 0.32920650\n",
      "Iteration 383, loss = 0.32916131\n",
      "Iteration 384, loss = 0.32893108\n",
      "Iteration 385, loss = 0.32871043\n",
      "Iteration 386, loss = 0.32857746\n",
      "Iteration 387, loss = 0.32850380\n",
      "Iteration 388, loss = 0.32826535\n",
      "Iteration 389, loss = 0.32805447\n",
      "Iteration 390, loss = 0.32798149\n",
      "Iteration 391, loss = 0.32773975\n",
      "Iteration 392, loss = 0.32761598\n",
      "Iteration 393, loss = 0.32746874\n",
      "Iteration 394, loss = 0.32726579\n",
      "Iteration 395, loss = 0.32710263\n",
      "Iteration 396, loss = 0.32696080\n",
      "Iteration 397, loss = 0.32683777\n",
      "Iteration 398, loss = 0.32662437\n",
      "Iteration 399, loss = 0.32658512\n",
      "Iteration 400, loss = 0.32631755\n",
      "Iteration 401, loss = 0.32622386\n",
      "Iteration 402, loss = 0.32607433\n",
      "Iteration 403, loss = 0.32586185\n",
      "Iteration 404, loss = 0.32571334\n",
      "Iteration 405, loss = 0.32555788\n",
      "Iteration 406, loss = 0.32536708\n",
      "Iteration 407, loss = 0.32519717\n",
      "Iteration 408, loss = 0.32505528\n",
      "Iteration 409, loss = 0.32499180\n",
      "Iteration 410, loss = 0.32473256\n",
      "Iteration 411, loss = 0.32470987\n",
      "Iteration 412, loss = 0.32443030\n",
      "Iteration 413, loss = 0.32431572\n",
      "Iteration 414, loss = 0.32423433\n",
      "Iteration 415, loss = 0.32398219\n",
      "Iteration 416, loss = 0.32381203\n",
      "Iteration 417, loss = 0.32368440\n",
      "Iteration 418, loss = 0.32350914\n",
      "Iteration 419, loss = 0.32338687\n",
      "Iteration 420, loss = 0.32327785\n",
      "Iteration 421, loss = 0.32315307\n",
      "Iteration 422, loss = 0.32296487\n",
      "Iteration 423, loss = 0.32275605\n",
      "Iteration 424, loss = 0.32261923\n",
      "Iteration 425, loss = 0.32247250\n",
      "Iteration 426, loss = 0.32234304\n",
      "Iteration 427, loss = 0.32218684\n",
      "Iteration 428, loss = 0.32201497\n",
      "Iteration 429, loss = 0.32186841\n",
      "Iteration 430, loss = 0.32171984\n",
      "Iteration 431, loss = 0.32163110\n",
      "Iteration 432, loss = 0.32143137\n",
      "Iteration 433, loss = 0.32127119\n",
      "Iteration 434, loss = 0.32112691\n",
      "Iteration 435, loss = 0.32103512\n",
      "Iteration 436, loss = 0.32083857\n",
      "Iteration 437, loss = 0.32067121\n",
      "Iteration 438, loss = 0.32056182\n",
      "Iteration 439, loss = 0.32045251\n",
      "Iteration 440, loss = 0.32025775\n",
      "Iteration 441, loss = 0.32008509\n",
      "Iteration 442, loss = 0.31993367\n",
      "Iteration 443, loss = 0.31981695\n",
      "Iteration 444, loss = 0.31967594\n",
      "Iteration 445, loss = 0.31955805\n",
      "Iteration 446, loss = 0.31939459\n",
      "Iteration 447, loss = 0.31926301\n",
      "Iteration 448, loss = 0.31913829\n",
      "Iteration 449, loss = 0.31896363\n",
      "Iteration 450, loss = 0.31883135\n",
      "Iteration 451, loss = 0.31868767\n",
      "Iteration 452, loss = 0.31850941\n",
      "Iteration 453, loss = 0.31839942\n",
      "Iteration 454, loss = 0.31824468\n",
      "Iteration 455, loss = 0.31807914\n",
      "Iteration 456, loss = 0.31793081\n",
      "Iteration 457, loss = 0.31785031\n",
      "Iteration 458, loss = 0.31764503\n",
      "Iteration 459, loss = 0.31752646\n",
      "Iteration 460, loss = 0.31747687\n",
      "Iteration 461, loss = 0.31732762\n",
      "Iteration 462, loss = 0.31713293\n",
      "Iteration 463, loss = 0.31695459\n",
      "Iteration 464, loss = 0.31681983\n",
      "Iteration 465, loss = 0.31672707\n",
      "Iteration 466, loss = 0.31655574\n",
      "Iteration 467, loss = 0.31638766\n",
      "Iteration 468, loss = 0.31624607\n",
      "Iteration 469, loss = 0.31612934\n",
      "Iteration 470, loss = 0.31597161\n",
      "Iteration 471, loss = 0.31581759\n",
      "Iteration 472, loss = 0.31576201\n",
      "Iteration 473, loss = 0.31563774\n",
      "Iteration 474, loss = 0.31539650\n",
      "Iteration 475, loss = 0.31527452\n",
      "Iteration 476, loss = 0.31515347\n",
      "Iteration 477, loss = 0.31503633\n",
      "Iteration 478, loss = 0.31485741\n",
      "Iteration 479, loss = 0.31474382\n",
      "Iteration 480, loss = 0.31456069\n",
      "Iteration 481, loss = 0.31443519\n",
      "Iteration 482, loss = 0.31435071\n",
      "Iteration 483, loss = 0.31424491\n",
      "Iteration 484, loss = 0.31401979\n",
      "Iteration 485, loss = 0.31393222\n",
      "Iteration 486, loss = 0.31377027\n",
      "Iteration 487, loss = 0.31362302\n",
      "Iteration 488, loss = 0.31351273\n",
      "Iteration 489, loss = 0.31335197\n",
      "Iteration 490, loss = 0.31329031\n",
      "Iteration 491, loss = 0.31306130\n",
      "Iteration 492, loss = 0.31302886\n",
      "Iteration 493, loss = 0.31282805\n",
      "Iteration 494, loss = 0.31265532\n",
      "Iteration 495, loss = 0.31248069\n",
      "Iteration 496, loss = 0.31244844\n",
      "Iteration 497, loss = 0.31221031\n",
      "Iteration 498, loss = 0.31209545\n",
      "Iteration 499, loss = 0.31194194\n",
      "Iteration 500, loss = 0.31181419\n",
      "Iteration 501, loss = 0.31166427\n",
      "Iteration 502, loss = 0.31150871\n",
      "Iteration 503, loss = 0.31138257\n",
      "Iteration 504, loss = 0.31129137\n",
      "Iteration 505, loss = 0.31118939\n",
      "Iteration 506, loss = 0.31098541\n",
      "Iteration 507, loss = 0.31085807\n",
      "Iteration 508, loss = 0.31069046\n",
      "Iteration 509, loss = 0.31054175\n",
      "Iteration 510, loss = 0.31041716\n",
      "Iteration 511, loss = 0.31031177\n",
      "Iteration 512, loss = 0.31015286\n",
      "Iteration 513, loss = 0.31005275\n",
      "Iteration 514, loss = 0.30987505\n",
      "Iteration 515, loss = 0.30972275\n",
      "Iteration 516, loss = 0.30963220\n",
      "Iteration 517, loss = 0.30945283\n",
      "Iteration 518, loss = 0.30943139\n",
      "Iteration 519, loss = 0.30920408\n",
      "Iteration 520, loss = 0.30920024\n",
      "Iteration 521, loss = 0.30893365\n",
      "Iteration 522, loss = 0.30879145\n",
      "Iteration 523, loss = 0.30866124\n",
      "Iteration 524, loss = 0.30849836\n",
      "Iteration 525, loss = 0.30834925\n",
      "Iteration 526, loss = 0.30822232\n",
      "Iteration 527, loss = 0.30813516\n",
      "Iteration 528, loss = 0.30798418\n",
      "Iteration 529, loss = 0.30788260\n",
      "Iteration 530, loss = 0.30771035\n",
      "Iteration 531, loss = 0.30756802\n",
      "Iteration 532, loss = 0.30752020\n",
      "Iteration 533, loss = 0.30730991\n",
      "Iteration 534, loss = 0.30715207\n",
      "Iteration 535, loss = 0.30705776\n",
      "Iteration 536, loss = 0.30690171\n",
      "Iteration 537, loss = 0.30675544\n",
      "Iteration 538, loss = 0.30663209\n",
      "Iteration 539, loss = 0.30651991\n",
      "Iteration 540, loss = 0.30636704\n",
      "Iteration 541, loss = 0.30634821\n",
      "Iteration 542, loss = 0.30610318\n",
      "Iteration 543, loss = 0.30597104\n",
      "Iteration 544, loss = 0.30584499\n",
      "Iteration 545, loss = 0.30575556\n",
      "Iteration 546, loss = 0.30557840\n",
      "Iteration 547, loss = 0.30550475\n",
      "Iteration 548, loss = 0.30531087\n",
      "Iteration 549, loss = 0.30522703\n",
      "Iteration 550, loss = 0.30503220\n",
      "Iteration 551, loss = 0.30489653\n",
      "Iteration 552, loss = 0.30492097\n",
      "Iteration 553, loss = 0.30462411\n",
      "Iteration 554, loss = 0.30454911\n",
      "Iteration 555, loss = 0.30456814\n",
      "Iteration 556, loss = 0.30427946\n",
      "Iteration 557, loss = 0.30415826\n",
      "Iteration 558, loss = 0.30396015\n",
      "Iteration 559, loss = 0.30385551\n",
      "Iteration 560, loss = 0.30377404\n",
      "Iteration 561, loss = 0.30359331\n",
      "Iteration 562, loss = 0.30347581\n",
      "Iteration 563, loss = 0.30334106\n",
      "Iteration 564, loss = 0.30319674\n",
      "Iteration 565, loss = 0.30305642\n",
      "Iteration 566, loss = 0.30291264\n",
      "Iteration 567, loss = 0.30276362\n",
      "Iteration 568, loss = 0.30272190\n",
      "Iteration 569, loss = 0.30247335\n",
      "Iteration 570, loss = 0.30232479\n",
      "Iteration 571, loss = 0.30219270\n",
      "Iteration 572, loss = 0.30205259\n",
      "Iteration 573, loss = 0.30191579\n",
      "Iteration 574, loss = 0.30188189\n",
      "Iteration 575, loss = 0.30160685\n",
      "Iteration 576, loss = 0.30162872\n",
      "Iteration 577, loss = 0.30136600\n",
      "Iteration 578, loss = 0.30119752\n",
      "Iteration 579, loss = 0.30107609\n",
      "Iteration 580, loss = 0.30093935\n",
      "Iteration 581, loss = 0.30088377\n",
      "Iteration 582, loss = 0.30074928\n",
      "Iteration 583, loss = 0.30051575\n",
      "Iteration 584, loss = 0.30035731\n",
      "Iteration 585, loss = 0.30023230\n",
      "Iteration 586, loss = 0.30013823\n",
      "Iteration 587, loss = 0.29991904\n",
      "Iteration 588, loss = 0.29979305\n",
      "Iteration 589, loss = 0.29965070\n",
      "Iteration 590, loss = 0.29953207\n",
      "Iteration 591, loss = 0.29942864\n",
      "Iteration 592, loss = 0.29923045\n",
      "Iteration 593, loss = 0.29909060\n",
      "Iteration 594, loss = 0.29892998\n",
      "Iteration 595, loss = 0.29880590\n",
      "Iteration 596, loss = 0.29865994\n",
      "Iteration 597, loss = 0.29852570\n",
      "Iteration 598, loss = 0.29838700\n",
      "Iteration 599, loss = 0.29823551\n",
      "Iteration 600, loss = 0.29818239\n",
      "Iteration 601, loss = 0.29800554\n",
      "Iteration 602, loss = 0.29787998\n",
      "Iteration 603, loss = 0.29776501\n",
      "Iteration 604, loss = 0.29753349\n",
      "Iteration 605, loss = 0.29739978\n",
      "Iteration 606, loss = 0.29728541\n",
      "Iteration 607, loss = 0.29718103\n",
      "Iteration 608, loss = 0.29714090\n",
      "Iteration 609, loss = 0.29689019\n",
      "Iteration 610, loss = 0.29673549\n",
      "Iteration 611, loss = 0.29660664\n",
      "Iteration 612, loss = 0.29644577\n",
      "Iteration 613, loss = 0.29639427\n",
      "Iteration 614, loss = 0.29613207\n",
      "Iteration 615, loss = 0.29602736\n",
      "Iteration 616, loss = 0.29587907\n",
      "Iteration 617, loss = 0.29576742\n",
      "Iteration 1720, loss = 0.16896025\n",
      "Iteration 1721, loss = 0.16882162\n",
      "Iteration 1722, loss = 0.16874864\n",
      "Iteration 1723, loss = 0.16868717\n",
      "Iteration 1724, loss = 0.16852786\n",
      "Iteration 1725, loss = 0.16839834\n",
      "Iteration 1726, loss = 0.16826008\n",
      "Iteration 1727, loss = 0.16820944\n",
      "Iteration 1728, loss = 0.16806220\n",
      "Iteration 1729, loss = 0.16797820\n",
      "Iteration 1730, loss = 0.16786507\n",
      "Iteration 1731, loss = 0.16783455\n",
      "Iteration 1732, loss = 0.16765074\n",
      "Iteration 1733, loss = 0.16755860\n",
      "Iteration 1734, loss = 0.16743069\n",
      "Iteration 1735, loss = 0.16734677\n",
      "Iteration 1736, loss = 0.16724173\n",
      "Iteration 1737, loss = 0.16715446\n",
      "Iteration 1738, loss = 0.16712993\n",
      "Iteration 1739, loss = 0.16693075\n",
      "Iteration 1740, loss = 0.16681874\n",
      "Iteration 1741, loss = 0.16670228\n",
      "Iteration 1742, loss = 0.16658856\n",
      "Iteration 1743, loss = 0.16651366\n",
      "Iteration 1744, loss = 0.16640222\n",
      "Iteration 1745, loss = 0.16632390\n",
      "Iteration 1746, loss = 0.16620672\n",
      "Iteration 1747, loss = 0.16609190\n",
      "Iteration 1748, loss = 0.16598797\n",
      "Iteration 1749, loss = 0.16590467\n",
      "Iteration 1750, loss = 0.16584194\n",
      "Iteration 1751, loss = 0.16570656\n",
      "Iteration 1752, loss = 0.16557247\n",
      "Iteration 1753, loss = 0.16547385\n",
      "Iteration 1754, loss = 0.16538903\n",
      "Iteration 1755, loss = 0.16529226\n",
      "Iteration 1756, loss = 0.16519332\n",
      "Iteration 1757, loss = 0.16507233\n",
      "Iteration 1758, loss = 0.16493721\n",
      "Iteration 1759, loss = 0.16485448\n",
      "Iteration 1760, loss = 0.16480249\n",
      "Iteration 1761, loss = 0.16462766\n",
      "Iteration 1762, loss = 0.16455080\n",
      "Iteration 1763, loss = 0.16444036\n",
      "Iteration 1764, loss = 0.16435036\n",
      "Iteration 1765, loss = 0.16422336\n",
      "Iteration 1766, loss = 0.16416094\n",
      "Iteration 1767, loss = 0.16401798\n",
      "Iteration 1768, loss = 0.16390364\n",
      "Iteration 1769, loss = 0.16388167\n",
      "Iteration 1770, loss = 0.16374304\n",
      "Iteration 1771, loss = 0.16357069\n",
      "Iteration 1772, loss = 0.16354391\n",
      "Iteration 1773, loss = 0.16344156\n",
      "Iteration 1774, loss = 0.16328228\n",
      "Iteration 1775, loss = 0.16320690\n",
      "Iteration 1776, loss = 0.16309626\n",
      "Iteration 1777, loss = 0.16298682\n",
      "Iteration 1778, loss = 0.16293075\n",
      "Iteration 1779, loss = 0.16278486\n",
      "Iteration 1780, loss = 0.16269553\n",
      "Iteration 1781, loss = 0.16259325\n",
      "Iteration 1782, loss = 0.16247089\n",
      "Iteration 1783, loss = 0.16238873\n",
      "Iteration 1784, loss = 0.16230374\n",
      "Iteration 1785, loss = 0.16217569\n",
      "Iteration 1786, loss = 0.16215758\n",
      "Iteration 1787, loss = 0.16194626\n",
      "Iteration 1788, loss = 0.16191556\n",
      "Iteration 1789, loss = 0.16179348\n",
      "Iteration 1790, loss = 0.16166676\n",
      "Iteration 1791, loss = 0.16155631\n",
      "Iteration 1792, loss = 0.16150972\n",
      "Iteration 1793, loss = 0.16136268\n",
      "Iteration 1794, loss = 0.16124405\n",
      "Iteration 1795, loss = 0.16112965\n",
      "Iteration 1796, loss = 0.16105339\n",
      "Iteration 1797, loss = 0.16099405\n",
      "Iteration 1798, loss = 0.16082696\n",
      "Iteration 1799, loss = 0.16072012\n",
      "Iteration 1800, loss = 0.16061820\n",
      "Iteration 1801, loss = 0.16059286\n",
      "Iteration 1802, loss = 0.16044906\n",
      "Iteration 1803, loss = 0.16036361\n",
      "Iteration 1804, loss = 0.16023949\n",
      "Iteration 1805, loss = 0.16016859\n",
      "Iteration 1806, loss = 0.16008105\n",
      "Iteration 1807, loss = 0.15995353\n",
      "Iteration 1808, loss = 0.15979415\n",
      "Iteration 1809, loss = 0.15971241\n",
      "Iteration 1810, loss = 0.15964635\n",
      "Iteration 1811, loss = 0.15952055\n",
      "Iteration 1812, loss = 0.15941512\n",
      "Iteration 1813, loss = 0.15930002\n",
      "Iteration 1814, loss = 0.15920342\n",
      "Iteration 1815, loss = 0.15908509\n",
      "Iteration 1816, loss = 0.15900112\n",
      "Iteration 1817, loss = 0.15899562\n",
      "Iteration 1818, loss = 0.15885375\n",
      "Iteration 1819, loss = 0.15876044\n",
      "Iteration 1820, loss = 0.15861992\n",
      "Iteration 1821, loss = 0.15850141\n",
      "Iteration 1822, loss = 0.15841002\n",
      "Iteration 1823, loss = 0.15831778\n",
      "Iteration 1824, loss = 0.15821240\n",
      "Iteration 1825, loss = 0.15814103\n",
      "Iteration 1826, loss = 0.15805143\n",
      "Iteration 1827, loss = 0.15790079\n",
      "Iteration 1828, loss = 0.15781665\n",
      "Iteration 1829, loss = 0.15772152\n",
      "Iteration 1830, loss = 0.15767167\n",
      "Iteration 1831, loss = 0.15751954\n",
      "Iteration 1832, loss = 0.15738331\n",
      "Iteration 1833, loss = 0.15730979\n",
      "Iteration 1834, loss = 0.15720055\n",
      "Iteration 1835, loss = 0.15712568\n",
      "Iteration 1836, loss = 0.15700741\n",
      "Iteration 1837, loss = 0.15688462\n",
      "Iteration 1838, loss = 0.15682366\n",
      "Iteration 1839, loss = 0.15668045\n",
      "Iteration 1840, loss = 0.15662449\n",
      "Iteration 1841, loss = 0.15649246\n",
      "Iteration 1842, loss = 0.15644198\n",
      "Iteration 1843, loss = 0.15628991\n",
      "Iteration 1844, loss = 0.15622066\n",
      "Iteration 1845, loss = 0.15610790\n",
      "Iteration 1846, loss = 0.15602318\n",
      "Iteration 1847, loss = 0.15594545\n",
      "Iteration 1848, loss = 0.15588677\n",
      "Iteration 1849, loss = 0.15571380\n",
      "Iteration 1850, loss = 0.15561891\n",
      "Iteration 1851, loss = 0.15558520\n",
      "Iteration 1852, loss = 0.15546045\n",
      "Iteration 1853, loss = 0.15538011\n",
      "Iteration 1854, loss = 0.15524506\n",
      "Iteration 1855, loss = 0.15516705\n",
      "Iteration 1856, loss = 0.15508289\n",
      "Iteration 1857, loss = 0.15495499\n",
      "Iteration 1858, loss = 0.15486188\n",
      "Iteration 1859, loss = 0.15486566\n",
      "Iteration 1860, loss = 0.15463184\n",
      "Iteration 1861, loss = 0.15453916\n",
      "Iteration 1862, loss = 0.15447900\n",
      "Iteration 1863, loss = 0.15433186\n",
      "Iteration 1864, loss = 0.15424347\n",
      "Iteration 1865, loss = 0.15421525\n",
      "Iteration 1866, loss = 0.15407496\n",
      "Iteration 1867, loss = 0.15394225\n",
      "Iteration 1868, loss = 0.15387326\n",
      "Iteration 1869, loss = 0.15377630\n",
      "Iteration 1870, loss = 0.15374001\n",
      "Iteration 1871, loss = 0.15356534\n",
      "Iteration 1872, loss = 0.15346275\n",
      "Iteration 1873, loss = 0.15337518\n",
      "Iteration 1874, loss = 0.15328765\n",
      "Iteration 1875, loss = 0.15317761\n",
      "Iteration 1876, loss = 0.15309026\n",
      "Iteration 1877, loss = 0.15303056\n",
      "Iteration 1878, loss = 0.15288217\n",
      "Iteration 1879, loss = 0.15281807\n",
      "Iteration 1880, loss = 0.15268228\n",
      "Iteration 1881, loss = 0.15267876\n",
      "Iteration 1882, loss = 0.15249619\n",
      "Iteration 1883, loss = 0.15244247\n",
      "Iteration 1884, loss = 0.15236948\n",
      "Iteration 1885, loss = 0.15220998\n",
      "Iteration 1886, loss = 0.15213081\n",
      "Iteration 1887, loss = 0.15202053\n",
      "Iteration 1888, loss = 0.15195536\n",
      "Iteration 1889, loss = 0.15189498\n",
      "Iteration 1890, loss = 0.15173827\n",
      "Iteration 1891, loss = 0.15167183\n",
      "Iteration 1892, loss = 0.15154498\n",
      "Iteration 1893, loss = 0.15143391\n",
      "Iteration 1894, loss = 0.15132974\n",
      "Iteration 1895, loss = 0.15126183\n",
      "Iteration 1896, loss = 0.15116689\n",
      "Iteration 1897, loss = 0.15108466\n",
      "Iteration 1898, loss = 0.15100173\n",
      "Iteration 1899, loss = 0.15088204\n",
      "Iteration 1900, loss = 0.15079521\n",
      "Iteration 1901, loss = 0.15067083\n",
      "Iteration 1902, loss = 0.15059487\n",
      "Iteration 1903, loss = 0.15049891\n",
      "Iteration 1904, loss = 0.15048072\n",
      "Iteration 1905, loss = 0.15031077\n",
      "Iteration 1906, loss = 0.15025730\n",
      "Iteration 1907, loss = 0.15012601\n",
      "Iteration 1908, loss = 0.15002631\n",
      "Iteration 1909, loss = 0.14996718\n",
      "Iteration 1910, loss = 0.14983597\n",
      "Iteration 1911, loss = 0.14977297\n",
      "Iteration 1912, loss = 0.14968821\n",
      "Iteration 1913, loss = 0.14952232\n",
      "Iteration 1914, loss = 0.14944870\n",
      "Iteration 1915, loss = 0.14934648\n",
      "Iteration 1916, loss = 0.14933514\n",
      "Iteration 1917, loss = 0.14922239\n",
      "Iteration 1918, loss = 0.14904290\n",
      "Iteration 1919, loss = 0.14896608\n",
      "Iteration 1920, loss = 0.14886471\n",
      "Iteration 1921, loss = 0.14880633\n",
      "Iteration 1922, loss = 0.14868263\n",
      "Iteration 1923, loss = 0.14859272\n",
      "Iteration 1924, loss = 0.14854371\n",
      "Iteration 1925, loss = 0.14841655\n",
      "Iteration 1926, loss = 0.14832941\n",
      "Iteration 1927, loss = 0.14822221\n",
      "Iteration 1928, loss = 0.14814230\n",
      "Iteration 1929, loss = 0.14803927\n",
      "Iteration 1930, loss = 0.14794030\n",
      "Iteration 1931, loss = 0.14785971\n",
      "Iteration 1932, loss = 0.14782566\n",
      "Iteration 1933, loss = 0.14770095\n",
      "Iteration 1934, loss = 0.14755982\n",
      "Iteration 1935, loss = 0.14749916\n",
      "Iteration 1936, loss = 0.14737284\n",
      "Iteration 1937, loss = 0.14724210\n",
      "Iteration 1938, loss = 0.14716794\n",
      "Iteration 1939, loss = 0.14706368\n",
      "Iteration 1940, loss = 0.14696937\n",
      "Iteration 1941, loss = 0.14691326\n",
      "Iteration 1942, loss = 0.14682919\n",
      "Iteration 1943, loss = 0.14674047\n",
      "Iteration 1944, loss = 0.14665176\n",
      "Iteration 1945, loss = 0.14660412\n",
      "Iteration 1946, loss = 0.14642611\n",
      "Iteration 1947, loss = 0.14635500\n",
      "Iteration 1948, loss = 0.14624656\n",
      "Iteration 1949, loss = 0.14625262\n",
      "Iteration 1950, loss = 0.14606680\n",
      "Iteration 1951, loss = 0.14612298\n",
      "Iteration 1952, loss = 0.14587922\n",
      "Iteration 1953, loss = 0.14578322\n",
      "Iteration 1954, loss = 0.14569973\n",
      "Iteration 1955, loss = 0.14564350\n",
      "Iteration 1956, loss = 0.14548828\n",
      "Iteration 1957, loss = 0.14543214\n",
      "Iteration 1958, loss = 0.14529738\n",
      "Iteration 1959, loss = 0.14520649\n",
      "Iteration 1960, loss = 0.14515270\n",
      "Iteration 1961, loss = 0.14515547\n",
      "Iteration 1962, loss = 0.14513231\n",
      "Iteration 1963, loss = 0.14500712\n",
      "Iteration 1964, loss = 0.14477507\n",
      "Iteration 1965, loss = 0.14466447\n",
      "Iteration 1966, loss = 0.14456709\n",
      "Iteration 1967, loss = 0.14447097\n",
      "Iteration 1968, loss = 0.14438363\n",
      "Iteration 1969, loss = 0.14426478\n",
      "Iteration 1970, loss = 0.14419479\n",
      "Iteration 1971, loss = 0.14413721\n",
      "Iteration 1972, loss = 0.14400354\n",
      "Iteration 1973, loss = 0.14405547\n",
      "Iteration 1974, loss = 0.14384610\n",
      "Iteration 1975, loss = 0.14376944\n",
      "Iteration 1976, loss = 0.14363446\n",
      "Iteration 1977, loss = 0.14353965\n",
      "Iteration 1978, loss = 0.14344657\n",
      "Iteration 1979, loss = 0.14335905\n",
      "Iteration 1980, loss = 0.14326464\n",
      "Iteration 1981, loss = 0.14322903\n",
      "Iteration 1982, loss = 0.14311327\n",
      "Iteration 1983, loss = 0.14303584\n",
      "Iteration 1984, loss = 0.14292929\n",
      "Iteration 1985, loss = 0.14280992\n",
      "Iteration 1986, loss = 0.14273896\n",
      "Iteration 1987, loss = 0.14265433\n",
      "Iteration 1988, loss = 0.14256613\n",
      "Iteration 1989, loss = 0.14247401\n",
      "Iteration 1990, loss = 0.14238096\n",
      "Iteration 1991, loss = 0.14232273\n",
      "Iteration 1992, loss = 0.14221312\n",
      "Iteration 1993, loss = 0.14208271\n",
      "Iteration 1994, loss = 0.14203489\n",
      "Iteration 1995, loss = 0.14202909\n",
      "Iteration 1996, loss = 0.14191110\n",
      "Iteration 1997, loss = 0.14183965\n",
      "Iteration 1998, loss = 0.14168264\n",
      "Iteration 1999, loss = 0.14157488\n",
      "Iteration 2000, loss = 0.14150131\n",
      "Iteration 2001, loss = 0.14142601\n",
      "Iteration 2002, loss = 0.14132954\n",
      "Iteration 2003, loss = 0.14125163\n",
      "Iteration 2004, loss = 0.14115649\n",
      "Iteration 2005, loss = 0.14104365\n",
      "Iteration 2006, loss = 0.14096896\n",
      "Iteration 2007, loss = 0.14085947\n",
      "Iteration 2008, loss = 0.14079560\n",
      "Iteration 2009, loss = 0.14066531\n",
      "Iteration 2010, loss = 0.14071333\n",
      "Iteration 2011, loss = 0.14057426\n",
      "Iteration 2012, loss = 0.14042165\n",
      "Iteration 2013, loss = 0.14035652\n",
      "Iteration 2014, loss = 0.14023674\n",
      "Iteration 2015, loss = 0.14018678\n",
      "Iteration 2016, loss = 0.14005858\n",
      "Iteration 2017, loss = 0.14006726\n",
      "Iteration 2018, loss = 0.13989235\n",
      "Iteration 2019, loss = 0.13993849\n",
      "Iteration 2020, loss = 0.13980512\n",
      "Iteration 2021, loss = 0.13962771\n",
      "Iteration 2022, loss = 0.13956895\n",
      "Iteration 2023, loss = 0.13945627\n",
      "Iteration 2024, loss = 0.13934797\n",
      "Iteration 2025, loss = 0.13926462\n",
      "Iteration 2026, loss = 0.13921684\n",
      "Iteration 2027, loss = 0.13909358\n",
      "Iteration 2028, loss = 0.13915737\n",
      "Iteration 2029, loss = 0.13894664\n",
      "Iteration 2030, loss = 0.13882386\n",
      "Iteration 2031, loss = 0.13876257\n",
      "Iteration 2032, loss = 0.13869976\n",
      "Iteration 2033, loss = 0.13854490\n",
      "Iteration 2034, loss = 0.13850771\n",
      "Iteration 2035, loss = 0.13845604\n",
      "Iteration 2036, loss = 0.13833854\n",
      "Iteration 2037, loss = 0.13830865\n",
      "Iteration 2038, loss = 0.13816091\n",
      "Iteration 2039, loss = 0.13803524\n",
      "Iteration 2040, loss = 0.13799974\n",
      "Iteration 2041, loss = 0.13788988\n",
      "Iteration 2042, loss = 0.13779140\n",
      "Iteration 2043, loss = 0.13769071\n",
      "Iteration 2044, loss = 0.13766687\n",
      "Iteration 2045, loss = 0.13751862\n",
      "Iteration 2046, loss = 0.13753324\n",
      "Iteration 2047, loss = 0.13741844\n",
      "Iteration 2048, loss = 0.13725367\n",
      "Iteration 2049, loss = 0.13719414\n",
      "Iteration 2050, loss = 0.13712842\n",
      "Iteration 2051, loss = 0.13701135\n",
      "Iteration 2052, loss = 0.13695031\n",
      "Iteration 2053, loss = 0.13685590\n",
      "Iteration 2054, loss = 0.13678396\n",
      "Iteration 2055, loss = 0.13671169\n",
      "Iteration 2056, loss = 0.13670668\n",
      "Iteration 2057, loss = 0.13649568\n",
      "Iteration 2058, loss = 0.13644695\n",
      "Iteration 2059, loss = 0.13634578\n",
      "Iteration 2060, loss = 0.13625052\n",
      "Iteration 2061, loss = 0.13618013\n",
      "Iteration 2062, loss = 0.13606676\n",
      "Iteration 2063, loss = 0.13607067\n",
      "Iteration 2064, loss = 0.13592360\n",
      "Iteration 2065, loss = 0.13588578\n",
      "Iteration 2066, loss = 0.13577143\n",
      "Iteration 2067, loss = 0.13567735\n",
      "Iteration 2068, loss = 0.13560543\n",
      "Iteration 2069, loss = 0.13550978\n",
      "Iteration 2070, loss = 0.13548436\n",
      "Iteration 2071, loss = 0.13533799\n",
      "Iteration 2072, loss = 0.13530379\n",
      "Iteration 2073, loss = 0.13523407\n",
      "Iteration 2074, loss = 0.13510018\n",
      "Iteration 2075, loss = 0.13500790\n",
      "Iteration 2076, loss = 0.13493999\n",
      "Iteration 2077, loss = 0.13490911\n",
      "Iteration 2078, loss = 0.13479883\n",
      "Iteration 2079, loss = 0.13467073\n",
      "Iteration 2080, loss = 0.13460053\n",
      "Iteration 2081, loss = 0.13449897\n",
      "Iteration 2082, loss = 0.13442258\n",
      "Iteration 2083, loss = 0.13433651\n",
      "Iteration 2084, loss = 0.13426417\n",
      "Iteration 2085, loss = 0.13427505\n",
      "Iteration 2086, loss = 0.13416439\n",
      "Iteration 2087, loss = 0.13401535\n",
      "Iteration 2088, loss = 0.13395338\n",
      "Iteration 2089, loss = 0.13390182\n",
      "Iteration 2090, loss = 0.13377522\n",
      "Iteration 2091, loss = 0.13368932\n",
      "Iteration 2092, loss = 0.13373944\n",
      "Iteration 2093, loss = 0.13352468\n",
      "Iteration 2094, loss = 0.13343466\n",
      "Iteration 2095, loss = 0.13339023\n",
      "Iteration 2096, loss = 0.13327622\n",
      "Iteration 2097, loss = 0.13324305\n",
      "Iteration 2098, loss = 0.13313699\n",
      "Iteration 2099, loss = 0.13303758\n",
      "Iteration 2100, loss = 0.13295937\n",
      "Iteration 2101, loss = 0.13288336\n",
      "Iteration 2102, loss = 0.13281324\n",
      "Iteration 2103, loss = 0.13278449\n",
      "Iteration 2104, loss = 0.13271004\n",
      "Iteration 2105, loss = 0.13258701\n",
      "Iteration 2106, loss = 0.13249598\n",
      "Iteration 2107, loss = 0.13242686\n",
      "Iteration 2108, loss = 0.13234911\n",
      "Iteration 2109, loss = 0.13226070\n",
      "Iteration 2110, loss = 0.13213009\n",
      "Iteration 2111, loss = 0.13209754\n",
      "Iteration 2112, loss = 0.13197966\n",
      "Iteration 2113, loss = 0.13189829\n",
      "Iteration 2114, loss = 0.13184676\n",
      "Iteration 2115, loss = 0.13183734\n",
      "Iteration 2116, loss = 0.13170014\n",
      "Iteration 2117, loss = 0.13164934\n",
      "Iteration 2118, loss = 0.13158465\n",
      "Iteration 2119, loss = 0.13145762\n",
      "Iteration 2120, loss = 0.13137198\n",
      "Iteration 2121, loss = 0.13134166\n",
      "Iteration 2122, loss = 0.13121523\n",
      "Iteration 2123, loss = 0.13129663\n",
      "Iteration 2124, loss = 0.13106784\n",
      "Iteration 2125, loss = 0.13093174\n",
      "Iteration 2126, loss = 0.13096492\n",
      "Iteration 2127, loss = 0.13085172\n",
      "Iteration 2128, loss = 0.13073690\n",
      "Iteration 2129, loss = 0.13064798\n",
      "Iteration 2130, loss = 0.13057090\n",
      "Iteration 2131, loss = 0.13052195\n",
      "Iteration 2132, loss = 0.13040705\n",
      "Iteration 2133, loss = 0.13036334\n",
      "Iteration 2134, loss = 0.13029467\n",
      "Iteration 2135, loss = 0.13020111\n",
      "Iteration 2136, loss = 0.13013992\n",
      "Iteration 2137, loss = 0.13005074\n",
      "Iteration 2138, loss = 0.12996509\n",
      "Iteration 2139, loss = 0.12988217\n",
      "Iteration 2140, loss = 0.12979345\n",
      "Iteration 2141, loss = 0.12972674\n",
      "Iteration 2142, loss = 0.12965855\n",
      "Iteration 2143, loss = 0.12961508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78469387\n",
      "Iteration 2, loss = 0.78023621\n",
      "Iteration 3, loss = 0.77329910\n",
      "Iteration 4, loss = 0.76495771\n",
      "Iteration 5, loss = 0.75573005\n",
      "Iteration 6, loss = 0.74576883\n",
      "Iteration 7, loss = 0.73532259\n",
      "Iteration 8, loss = 0.72499627\n",
      "Iteration 9, loss = 0.71442788\n",
      "Iteration 10, loss = 0.70446764\n",
      "Iteration 11, loss = 0.69429993\n",
      "Iteration 12, loss = 0.68456790\n",
      "Iteration 13, loss = 0.67521748\n",
      "Iteration 14, loss = 0.66594601\n",
      "Iteration 15, loss = 0.65732051\n",
      "Iteration 16, loss = 0.64856602\n",
      "Iteration 17, loss = 0.64020425\n",
      "Iteration 18, loss = 0.63247278\n",
      "Iteration 19, loss = 0.62479824\n",
      "Iteration 20, loss = 0.61728868\n",
      "Iteration 21, loss = 0.61020208\n",
      "Iteration 22, loss = 0.60335165\n",
      "Iteration 23, loss = 0.59671266\n",
      "Iteration 24, loss = 0.59026796\n",
      "Iteration 25, loss = 0.58409758\n",
      "Iteration 26, loss = 0.57829470\n",
      "Iteration 27, loss = 0.57245354\n",
      "Iteration 28, loss = 0.56699809\n",
      "Iteration 29, loss = 0.56157508\n",
      "Iteration 30, loss = 0.55640732\n",
      "Iteration 31, loss = 0.55143429\n",
      "Iteration 32, loss = 0.54657327\n",
      "Iteration 33, loss = 0.54188158\n",
      "Iteration 34, loss = 0.53734946\n",
      "Iteration 35, loss = 0.53299105\n",
      "Iteration 36, loss = 0.52870900\n",
      "Iteration 37, loss = 0.52466594\n",
      "Iteration 38, loss = 0.52059390\n",
      "Iteration 39, loss = 0.51679799\n",
      "Iteration 40, loss = 0.51313570\n",
      "Iteration 41, loss = 0.50953973\n",
      "Iteration 42, loss = 0.50599185\n",
      "Iteration 43, loss = 0.50265301\n",
      "Iteration 44, loss = 0.49937165\n",
      "Iteration 45, loss = 0.49623576\n",
      "Iteration 46, loss = 0.49314337\n",
      "Iteration 47, loss = 0.49011712\n",
      "Iteration 48, loss = 0.48716490\n",
      "Iteration 49, loss = 0.48442391\n",
      "Iteration 50, loss = 0.48166266\n",
      "Iteration 51, loss = 0.47896435\n",
      "Iteration 52, loss = 0.47640386\n",
      "Iteration 53, loss = 0.47400289\n",
      "Iteration 54, loss = 0.47148613\n",
      "Iteration 55, loss = 0.46907519\n",
      "Iteration 56, loss = 0.46679025\n",
      "Iteration 57, loss = 0.46454248\n",
      "Iteration 58, loss = 0.46237895Iteration 2198, loss = 0.15509923\n",
      "Iteration 2199, loss = 0.15494466\n",
      "Iteration 2200, loss = 0.15505014\n",
      "Iteration 2201, loss = 0.15488560\n",
      "Iteration 2202, loss = 0.15466281\n",
      "Iteration 2203, loss = 0.15460287\n",
      "Iteration 2204, loss = 0.15453835\n",
      "Iteration 2205, loss = 0.15444984\n",
      "Iteration 2206, loss = 0.15435605\n",
      "Iteration 2207, loss = 0.15427885\n",
      "Iteration 2208, loss = 0.15419012\n",
      "Iteration 2209, loss = 0.15411026\n",
      "Iteration 2210, loss = 0.15404329\n",
      "Iteration 2211, loss = 0.15397783\n",
      "Iteration 2212, loss = 0.15391521\n",
      "Iteration 2213, loss = 0.15382593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82003957\n",
      "Iteration 2, loss = 0.81162999\n",
      "Iteration 3, loss = 0.79907233\n",
      "Iteration 4, loss = 0.78394778\n",
      "Iteration 5, loss = 0.76707621\n",
      "Iteration 6, loss = 0.75063758\n",
      "Iteration 7, loss = 0.73350407\n",
      "Iteration 8, loss = 0.71705298\n",
      "Iteration 9, loss = 0.70153969\n",
      "Iteration 10, loss = 0.68699500\n",
      "Iteration 11, loss = 0.67254146\n",
      "Iteration 12, loss = 0.65955174\n",
      "Iteration 13, loss = 0.64820613\n",
      "Iteration 14, loss = 0.63676587\n",
      "Iteration 15, loss = 0.62593587\n",
      "Iteration 16, loss = 0.61688711\n",
      "Iteration 17, loss = 0.60755664\n",
      "Iteration 18, loss = 0.59957973\n",
      "Iteration 19, loss = 0.59154579\n",
      "Iteration 20, loss = 0.58403928\n",
      "Iteration 21, loss = 0.57750905\n",
      "Iteration 22, loss = 0.57100319\n",
      "Iteration 23, loss = 0.56505215\n",
      "Iteration 24, loss = 0.55913534\n",
      "Iteration 25, loss = 0.55363937\n",
      "Iteration 26, loss = 0.54859161\n",
      "Iteration 27, loss = 0.54353099\n",
      "Iteration 28, loss = 0.53900372\n",
      "Iteration 29, loss = 0.53427766\n",
      "Iteration 30, loss = 0.53004056\n",
      "Iteration 31, loss = 0.52576599\n",
      "Iteration 32, loss = 0.52174570\n",
      "Iteration 33, loss = 0.51782995\n",
      "Iteration 34, loss = 0.51418181\n",
      "Iteration 35, loss = 0.51066568\n",
      "Iteration 36, loss = 0.50723053\n",
      "Iteration 37, loss = 0.50380068\n",
      "Iteration 38, loss = 0.50064901\n",
      "Iteration 39, loss = 0.49766506\n",
      "Iteration 40, loss = 0.49449663\n",
      "Iteration 41, loss = 0.49160884\n",
      "Iteration 42, loss = 0.48869964\n",
      "Iteration 43, loss = 0.48600849\n",
      "Iteration 44, loss = 0.48337931\n",
      "Iteration 45, loss = 0.48076274\n",
      "Iteration 46, loss = 0.47829736\n",
      "Iteration 47, loss = 0.47575029\n",
      "Iteration 48, loss = 0.47342183\n",
      "Iteration 49, loss = 0.47109695\n",
      "Iteration 50, loss = 0.46884280\n",
      "Iteration 51, loss = 0.46664368\n",
      "Iteration 52, loss = 0.46446951\n",
      "Iteration 53, loss = 0.46244842\n",
      "Iteration 54, loss = 0.46044181\n",
      "Iteration 55, loss = 0.45843282\n",
      "Iteration 56, loss = 0.45651321\n",
      "Iteration 57, loss = 0.45464154\n",
      "Iteration 58, loss = 0.45282998\n",
      "Iteration 59, loss = 0.45099461\n",
      "Iteration 60, loss = 0.44928853\n",
      "Iteration 61, loss = 0.44754760\n",
      "Iteration 62, loss = 0.44591556\n",
      "Iteration 63, loss = 0.44429641\n",
      "Iteration 64, loss = 0.44273186\n",
      "Iteration 65, loss = 0.44116615\n",
      "Iteration 66, loss = 0.43971396\n",
      "Iteration 67, loss = 0.43823571\n",
      "Iteration 68, loss = 0.43675953\n",
      "Iteration 69, loss = 0.43533165\n",
      "Iteration 70, loss = 0.43400464\n",
      "Iteration 71, loss = 0.43259884\n",
      "Iteration 72, loss = 0.43128792\n",
      "Iteration 73, loss = 0.43001761\n",
      "Iteration 74, loss = 0.42871715\n",
      "Iteration 75, loss = 0.42755603\n",
      "Iteration 76, loss = 0.42629172\n",
      "Iteration 77, loss = 0.42515581\n",
      "Iteration 78, loss = 0.42403641\n",
      "Iteration 79, loss = 0.42283983\n",
      "Iteration 80, loss = 0.42175720\n",
      "Iteration 81, loss = 0.42071153\n",
      "Iteration 82, loss = 0.41963382\n",
      "Iteration 83, loss = 0.41860459\n",
      "Iteration 84, loss = 0.41756096\n",
      "Iteration 85, loss = 0.41662632\n",
      "Iteration 86, loss = 0.41562511\n",
      "Iteration 87, loss = 0.41467033\n",
      "Iteration 88, loss = 0.41372868\n",
      "Iteration 89, loss = 0.41282656\n",
      "Iteration 90, loss = 0.41191456\n",
      "Iteration 91, loss = 0.41105620\n",
      "Iteration 92, loss = 0.41021985\n",
      "Iteration 93, loss = 0.40936023\n",
      "Iteration 94, loss = 0.40850224\n",
      "Iteration 95, loss = 0.40771952\n",
      "Iteration 96, loss = 0.40691288\n",
      "Iteration 97, loss = 0.40615364\n",
      "Iteration 98, loss = 0.40538867\n",
      "Iteration 99, loss = 0.40461935\n",
      "Iteration 100, loss = 0.40388853\n",
      "Iteration 101, loss = 0.40317421\n",
      "Iteration 102, loss = 0.40246579\n",
      "Iteration 103, loss = 0.40174435\n",
      "Iteration 104, loss = 0.40106773\n",
      "Iteration 105, loss = 0.40040243\n",
      "Iteration 106, loss = 0.39978774\n",
      "Iteration 107, loss = 0.39908139\n",
      "Iteration 108, loss = 0.39846607\n",
      "Iteration 109, loss = 0.39784257\n",
      "Iteration 110, loss = 0.39721502\n",
      "Iteration 111, loss = 0.39661665\n",
      "Iteration 112, loss = 0.39603138\n",
      "Iteration 113, loss = 0.39542480\n",
      "Iteration 114, loss = 0.39484711\n",
      "Iteration 115, loss = 0.39432111\n",
      "Iteration 116, loss = 0.39376425\n",
      "Iteration 117, loss = 0.39317780\n",
      "Iteration 118, loss = 0.39268141\n",
      "Iteration 119, loss = 0.39214668\n",
      "Iteration 120, loss = 0.39161980\n",
      "Iteration 121, loss = 0.39111770\n",
      "Iteration 122, loss = 0.39060335\n",
      "Iteration 123, loss = 0.39011678\n",
      "Iteration 124, loss = 0.38963477\n",
      "Iteration 125, loss = 0.38915783\n",
      "Iteration 126, loss = 0.38865911\n",
      "Iteration 127, loss = 0.38819212\n",
      "Iteration 128, loss = 0.38774718\n",
      "Iteration 129, loss = 0.38729132\n",
      "Iteration 130, loss = 0.38686383\n",
      "Iteration 131, loss = 0.38641310\n",
      "Iteration 132, loss = 0.38598247\n",
      "Iteration 133, loss = 0.38555869\n",
      "Iteration 134, loss = 0.38511321\n",
      "Iteration 135, loss = 0.38469710\n",
      "Iteration 136, loss = 0.38427598\n",
      "Iteration 137, loss = 0.38389582\n",
      "Iteration 138, loss = 0.38348869\n",
      "Iteration 139, loss = 0.38309732\n",
      "Iteration 140, loss = 0.38270741\n",
      "Iteration 141, loss = 0.38231613\n",
      "Iteration 142, loss = 0.38197568\n",
      "Iteration 143, loss = 0.38155680\n",
      "Iteration 144, loss = 0.38120529\n",
      "Iteration 145, loss = 0.38083211\n",
      "Iteration 146, loss = 0.38046805\n",
      "Iteration 147, loss = 0.38008810\n",
      "Iteration 148, loss = 0.37974467\n",
      "Iteration 149, loss = 0.37941603\n",
      "Iteration 150, loss = 0.37906510\n",
      "Iteration 151, loss = 0.37870732\n",
      "Iteration 152, loss = 0.37840066\n",
      "Iteration 153, loss = 0.37804921\n",
      "Iteration 154, loss = 0.37772269\n",
      "Iteration 155, loss = 0.37738288\n",
      "Iteration 156, loss = 0.37708332\n",
      "Iteration 157, loss = 0.37674937\n",
      "Iteration 158, loss = 0.37644732\n",
      "Iteration 159, loss = 0.37614025\n",
      "Iteration 160, loss = 0.37582446\n",
      "Iteration 161, loss = 0.37552041\n",
      "Iteration 162, loss = 0.37522387\n",
      "Iteration 163, loss = 0.37493254\n",
      "Iteration 164, loss = 0.37465032\n",
      "Iteration 165, loss = 0.37433955\n",
      "Iteration 166, loss = 0.37406396\n",
      "Iteration 167, loss = 0.37377952\n",
      "Iteration 168, loss = 0.37349198\n",
      "Iteration 169, loss = 0.37320782\n",
      "Iteration 170, loss = 0.37293444\n",
      "Iteration 171, loss = 0.37267075\n",
      "Iteration 172, loss = 0.37239257\n",
      "Iteration 173, loss = 0.37212318\n",
      "Iteration 174, loss = 0.37186871\n",
      "Iteration 175, loss = 0.37158874\n",
      "Iteration 176, loss = 0.37134949\n",
      "Iteration 177, loss = 0.37108617\n",
      "Iteration 178, loss = 0.37081890\n",
      "Iteration 179, loss = 0.37056828\n",
      "Iteration 180, loss = 0.37032570\n",
      "Iteration 181, loss = 0.37006704\n",
      "Iteration 182, loss = 0.36983477\n",
      "Iteration 183, loss = 0.36958554\n",
      "Iteration 184, loss = 0.36932940\n",
      "Iteration 185, loss = 0.36911484\n",
      "Iteration 186, loss = 0.36885513\n",
      "Iteration 187, loss = 0.36861871\n",
      "Iteration 188, loss = 0.36838098\n",
      "Iteration 189, loss = 0.36816159\n",
      "Iteration 190, loss = 0.36791490\n",
      "Iteration 191, loss = 0.36770084\n",
      "Iteration 192, loss = 0.36748681\n",
      "Iteration 193, loss = 0.36724321\n",
      "Iteration 194, loss = 0.36703235\n",
      "Iteration 195, loss = 0.36680369\n",
      "Iteration 196, loss = 0.36658791\n",
      "Iteration 197, loss = 0.36637410\n",
      "Iteration 198, loss = 0.36616594\n",
      "Iteration 199, loss = 0.36594072\n",
      "Iteration 200, loss = 0.36573261\n",
      "Iteration 201, loss = 0.36551910\n",
      "Iteration 202, loss = 0.36531671\n",
      "Iteration 203, loss = 0.36511725\n",
      "Iteration 204, loss = 0.36491738\n",
      "Iteration 205, loss = 0.36469757\n",
      "Iteration 206, loss = 0.36452048\n",
      "Iteration 207, loss = 0.36430180\n",
      "Iteration 208, loss = 0.36411451\n",
      "Iteration 209, loss = 0.36391661\n",
      "Iteration 210, loss = 0.36371709\n",
      "Iteration 211, loss = 0.36353072\n",
      "Iteration 212, loss = 0.36333733\n",
      "Iteration 213, loss = 0.36314647\n",
      "Iteration 214, loss = 0.36296038\n",
      "Iteration 215, loss = 0.36278753\n",
      "Iteration 216, loss = 0.36256907\n",
      "Iteration 217, loss = 0.36239833\n",
      "Iteration 218, loss = 0.36223385\n",
      "Iteration 219, loss = 0.36204766\n",
      "Iteration 220, loss = 0.36185864\n",
      "Iteration 221, loss = 0.36168612\n",
      "Iteration 222, loss = 0.36149661\n",
      "Iteration 223, loss = 0.36133479\n",
      "Iteration 224, loss = 0.36113472\n",
      "Iteration 225, loss = 0.36097703\n",
      "Iteration 226, loss = 0.36080210\n",
      "Iteration 227, loss = 0.36061587\n",
      "Iteration 228, loss = 0.36046463\n",
      "Iteration 229, loss = 0.36027763\n",
      "Iteration 230, loss = 0.36011760\n",
      "Iteration 231, loss = 0.35994816\n",
      "Iteration 232, loss = 0.35976988\n",
      "Iteration 233, loss = 0.35961873\n",
      "Iteration 234, loss = 0.35943999\n",
      "Iteration 235, loss = 0.35927659\n",
      "Iteration 236, loss = 0.35911992\n",
      "Iteration 237, loss = 0.35896901\n",
      "Iteration 238, loss = 0.35879317\n",
      "Iteration 239, loss = 0.35862616\n",
      "Iteration 240, loss = 0.35846859\n",
      "Iteration 241, loss = 0.35831544\n",
      "Iteration 242, loss = 0.35816633\n",
      "Iteration 243, loss = 0.35799713\n",
      "Iteration 244, loss = 0.35785044\n",
      "Iteration 245, loss = 0.35768840\n",
      "Iteration 246, loss = 0.35753681\n",
      "Iteration 247, loss = 0.35738985\n",
      "Iteration 248, loss = 0.35724946\n",
      "Iteration 249, loss = 0.35708518\n",
      "Iteration 250, loss = 0.35693627\n",
      "Iteration 251, loss = 0.35678542\n",
      "Iteration 252, loss = 0.35663674\n",
      "Iteration 253, loss = 0.35648553\n",
      "Iteration 254, loss = 0.35634898\n",
      "Iteration 255, loss = 0.35619854\n",
      "Iteration 256, loss = 0.35605549\n",
      "Iteration 257, loss = 0.35590263\n",
      "Iteration 258, loss = 0.35576750\n",
      "Iteration 259, loss = 0.35563830\n",
      "Iteration 260, loss = 0.35547706\n",
      "Iteration 261, loss = 0.35534655\n",
      "Iteration 262, loss = 0.35520671\n",
      "Iteration 263, loss = 0.35507479\n",
      "Iteration 264, loss = 0.35492238\n",
      "Iteration 265, loss = 0.35477934\n",
      "Iteration 266, loss = 0.35464051\n",
      "Iteration 267, loss = 0.35450606\n",
      "Iteration 268, loss = 0.35437545\n",
      "Iteration 269, loss = 0.35423300\n",
      "Iteration 270, loss = 0.35409064\n",
      "Iteration 271, loss = 0.35397759\n",
      "Iteration 272, loss = 0.35382298\n",
      "Iteration 273, loss = 0.35369154\n",
      "Iteration 274, loss = 0.35355820\n",
      "Iteration 275, loss = 0.35342640\n",
      "Iteration 276, loss = 0.35329906\n",
      "Iteration 277, loss = 0.35316935\n",
      "Iteration 278, loss = 0.35303555\n",
      "Iteration 279, loss = 0.35290649\n",
      "Iteration 280, loss = 0.35276955\n",
      "Iteration 281, loss = 0.35265027\n",
      "Iteration 282, loss = 0.35250941\n",
      "Iteration 283, loss = 0.35239502\n",
      "Iteration 284, loss = 0.35226847\n",
      "Iteration 285, loss = 0.35213308\n",
      "Iteration 286, loss = 0.35202100\n",
      "Iteration 287, loss = 0.35187448\n",
      "Iteration 288, loss = 0.35174946\n",
      "Iteration 289, loss = 0.35163126\n",
      "Iteration 290, loss = 0.35152103\n",
      "Iteration 291, loss = 0.35138075\n",
      "Iteration 292, loss = 0.35125551\n",
      "Iteration 293, loss = 0.35114022\n",
      "Iteration 294, loss = 0.35101814\n",
      "Iteration 295, loss = 0.35088986\n",
      "Iteration 296, loss = 0.35077457\n",
      "Iteration 297, loss = 0.35064874\n",
      "Iteration 298, loss = 0.35053321\n",
      "Iteration 299, loss = 0.35041435\n",
      "Iteration 300, loss = 0.35028996\n",
      "Iteration 301, loss = 0.35017860\n",
      "Iteration 302, loss = 0.35005194\n",
      "Iteration 303, loss = 0.34993637\n",
      "Iteration 304, loss = 0.34982564\n",
      "Iteration 305, loss = 0.34970337\n",
      "Iteration 306, loss = 0.34959129\n",
      "Iteration 307, loss = 0.34947228\n",
      "Iteration 308, loss = 0.34935530\n",
      "Iteration 309, loss = 0.34924269\n",
      "Iteration 310, loss = 0.34912200\n",
      "Iteration 311, loss = 0.34901096\n",
      "Iteration 312, loss = 0.34889658\n",
      "Iteration 313, loss = 0.34879202\n",
      "Iteration 314, loss = 0.34866824\n",
      "Iteration 315, loss = 0.34856063\n",
      "Iteration 316, loss = 0.34844865\n",
      "Iteration 317, loss = 0.34833444\n",
      "Iteration 318, loss = 0.34824041\n",
      "Iteration 319, loss = 0.34811758\n",
      "Iteration 320, loss = 0.34801464\n",
      "Iteration 321, loss = 0.34791917\n",
      "Iteration 322, loss = 0.34778886\n",
      "Iteration 323, loss = 0.34768768\n",
      "Iteration 324, loss = 0.34756454\n",
      "Iteration 325, loss = 0.34748376\n",
      "Iteration 326, loss = 0.34736075\n",
      "Iteration 327, loss = 0.34725665\n",
      "Iteration 328, loss = 0.34714588\n",
      "Iteration 329, loss = 0.34704982\n",
      "Iteration 330, loss = 0.34693898\n",
      "Iteration 331, loss = 0.34685018\n",
      "Iteration 332, loss = 0.34672400\n",
      "Iteration 333, loss = 0.34662965\n",
      "Iteration 334, loss = 0.34651720\n",
      "Iteration 335, loss = 0.34641647\n",
      "Iteration 336, loss = 0.34630094\n",
      "Iteration 337, loss = 0.34620579\n",
      "Iteration 338, loss = 0.34610849\n",
      "Iteration 339, loss = 0.34599997\n",
      "Iteration 340, loss = 0.34589913\n",
      "Iteration 341, loss = 0.34580015\n",
      "Iteration 342, loss = 0.34570313\n",
      "Iteration 343, loss = 0.34559631\n",
      "Iteration 344, loss = 0.34549251\n",
      "Iteration 345, loss = 0.34539668\n",
      "Iteration 346, loss = 0.34529980\n",
      "Iteration 347, loss = 0.34519882\n",
      "Iteration 348, loss = 0.34510185\n",
      "Iteration 349, loss = 0.34499377\n",
      "Iteration 350, loss = 0.34489635\n",
      "Iteration 351, loss = 0.34479604\n",
      "Iteration 352, loss = 0.34470005\n",
      "Iteration 353, loss = 0.34460444\n",
      "Iteration 354, loss = 0.34450641\n",
      "Iteration 355, loss = 0.34440254\n",
      "Iteration 356, loss = 0.34430780\n",
      "Iteration 357, loss = 0.34421042\n",
      "Iteration 358, loss = 0.34412281\n",
      "Iteration 359, loss = 0.34401522\n",
      "Iteration 360, loss = 0.34392881\n",
      "Iteration 361, loss = 0.34384229\n",
      "Iteration 362, loss = 0.34374684\n",
      "Iteration 363, loss = 0.34364897\n",
      "Iteration 364, loss = 0.34356997\n",
      "Iteration 365, loss = 0.34344672\n",
      "Iteration 366, loss = 0.34336589\n",
      "Iteration 367, loss = 0.34326508\n",
      "Iteration 368, loss = 0.34316744\n",
      "Iteration 369, loss = 0.34307437\n",
      "Iteration 370, loss = 0.34298999\n",
      "Iteration 371, loss = 0.34289987\n",
      "Iteration 372, loss = 0.34280649\n",
      "Iteration 373, loss = 0.34270591\n",
      "Iteration 374, loss = 0.34261639\n",
      "Iteration 375, loss = 0.34253719\n",
      "Iteration 376, loss = 0.34243608\n",
      "Iteration 377, loss = 0.34234830\n",
      "Iteration 378, loss = 0.34226740\n",
      "Iteration 379, loss = 0.34216097\n",
      "Iteration 380, loss = 0.34207506\n",
      "Iteration 381, loss = 0.34198730\n",
      "Iteration 382, loss = 0.34189940\n",
      "Iteration 383, loss = 0.34182303\n",
      "Iteration 384, loss = 0.34172077\n",
      "Iteration 385, loss = 0.34163526\n",
      "Iteration 386, loss = 0.34154932\n",
      "Iteration 387, loss = 0.34146410\n",
      "Iteration 388, loss = 0.34137651\n",
      "Iteration 389, loss = 0.34129614\n",
      "Iteration 390, loss = 0.34119759\n",
      "Iteration 391, loss = 0.34111477\n",
      "Iteration 392, loss = 0.34103060\n",
      "Iteration 393, loss = 0.34094086\n",
      "Iteration 394, loss = 0.34086364\n",
      "Iteration 395, loss = 0.34077028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74637023\n",
      "Iteration 2, loss = 0.74210414\n",
      "Iteration 3, loss = 0.73583287\n",
      "Iteration 4, loss = 0.72783418\n",
      "Iteration 5, loss = 0.71924743\n",
      "Iteration 6, loss = 0.70989334\n",
      "Iteration 7, loss = 0.70075811\n",
      "Iteration 8, loss = 0.69121744\n",
      "Iteration 9, loss = 0.68182439\n",
      "Iteration 10, loss = 0.67295505\n",
      "Iteration 11, loss = 0.66395839\n",
      "Iteration 12, loss = 0.65549161\n",
      "Iteration 13, loss = 0.64715913\n",
      "Iteration 14, loss = 0.63924730\n",
      "Iteration 15, loss = 0.63173771\n",
      "Iteration 16, loss = 0.62420189\n",
      "Iteration 17, loss = 0.61727387\n",
      "Iteration 18, loss = 0.61045928\n",
      "Iteration 19, loss = 0.60386240\n",
      "Iteration 20, loss = 0.59761804\n",
      "Iteration 21, loss = 0.59153231\n",
      "Iteration 22, loss = 0.58569837\n",
      "Iteration 23, loss = 0.58010434\n",
      "Iteration 24, loss = 0.57465608\n",
      "Iteration 25, loss = 0.56934603\n",
      "Iteration 26, loss = 0.56443160\n",
      "Iteration 27, loss = 0.55938699\n",
      "Iteration 28, loss = 0.55465737\n",
      "Iteration 29, loss = 0.55008674\n",
      "Iteration 30, loss = 0.54556839\n",
      "Iteration 31, loss = 0.54131793\n",
      "Iteration 32, loss = 0.53711433\n",
      "Iteration 33, loss = 0.53311164\n",
      "Iteration 34, loss = 0.52911448\n",
      "Iteration 35, loss = 0.52530060\n",
      "Iteration 36, loss = 0.52160615\n",
      "Iteration 37, loss = 0.51799438\n",
      "Iteration 38, loss = 0.51444942\n",
      "Iteration 39, loss = 0.51112021\n",
      "Iteration 40, loss = 0.50781785\n",
      "Iteration 41, loss = 0.50455177\n",
      "Iteration 42, loss = 0.50135593\n",
      "Iteration 43, loss = 0.49836405\n",
      "Iteration 44, loss = 0.49542330\n",
      "Iteration 45, loss = 0.49255814\n",
      "Iteration 46, loss = 0.48965401\n",
      "Iteration 47, loss = 0.48699722\n",
      "Iteration 48, loss = 0.48425171\n",
      "Iteration 49, loss = 0.48173021\n",
      "Iteration 50, loss = 0.47909371\n",
      "Iteration 51, loss = 0.47664167\n",
      "Iteration 52, loss = 0.47419287\n",
      "Iteration 53, loss = 0.47193907\n",
      "Iteration 54, loss = 0.46957320\n",
      "Iteration 55, loss = 0.46736632\n",
      "Iteration 56, loss = 0.46510943\n",
      "Iteration 57, loss = 0.46307862\n",
      "Iteration 58, loss = 0.46095583\n",
      "Iteration 59, loss = 0.45888048\n",
      "Iteration 60, loss = 0.45691272\n",
      "Iteration 61, loss = 0.45499970\n",
      "Iteration 62, loss = 0.45314143\n",
      "Iteration 63, loss = 0.45126883\n",
      "Iteration 64, loss = 0.44947558\n",
      "Iteration 65, loss = 0.44767327\n",
      "Iteration 66, loss = 0.44598747\n",
      "Iteration 67, loss = 0.44427856\n",
      "Iteration 68, loss = 0.44262752\n",
      "Iteration 69, loss = 0.44098926\n",
      "Iteration 70, loss = 0.43940643\n",
      "Iteration 71, loss = 0.43787559\n",
      "Iteration 72, loss = 0.43636370\n",
      "Iteration 73, loss = 0.43491031\n",
      "Iteration 74, loss = 0.43342462\n",
      "Iteration 75, loss = 0.43202638\n",
      "Iteration 76, loss = 0.43063160\n",
      "Iteration 77, loss = 0.42926437\n",
      "Iteration 78, loss = 0.42792469\n",
      "Iteration 79, loss = 0.42661972\n",
      "Iteration 80, loss = 0.42536744\n",
      "Iteration 81, loss = 0.42409843\n",
      "Iteration 82, loss = 0.42284959\n",
      "Iteration 83, loss = 0.42166975\n",
      "Iteration 84, loss = 0.42048039\n",
      "Iteration 85, loss = 0.41930916\n",
      "Iteration 491, loss = 0.32554576\n",
      "Iteration 492, loss = 0.32542352\n",
      "Iteration 493, loss = 0.32527382\n",
      "Iteration 494, loss = 0.32514431\n",
      "Iteration 495, loss = 0.32501214\n",
      "Iteration 496, loss = 0.32488961\n",
      "Iteration 497, loss = 0.32479094\n",
      "Iteration 498, loss = 0.32463133\n",
      "Iteration 499, loss = 0.32452494\n",
      "Iteration 500, loss = 0.32436572\n",
      "Iteration 501, loss = 0.32426993\n",
      "Iteration 502, loss = 0.32411256\n",
      "Iteration 503, loss = 0.32396499\n",
      "Iteration 504, loss = 0.32386443\n",
      "Iteration 505, loss = 0.32375271\n",
      "Iteration 506, loss = 0.32360368\n",
      "Iteration 507, loss = 0.32353029\n",
      "Iteration 508, loss = 0.32335580\n",
      "Iteration 509, loss = 0.32321516\n",
      "Iteration 510, loss = 0.32308517\n",
      "Iteration 511, loss = 0.32296116\n",
      "Iteration 512, loss = 0.32282105\n",
      "Iteration 513, loss = 0.32268987\n",
      "Iteration 514, loss = 0.32258097\n",
      "Iteration 515, loss = 0.32245871\n",
      "Iteration 516, loss = 0.32231591\n",
      "Iteration 517, loss = 0.32217025\n",
      "Iteration 518, loss = 0.32205951\n",
      "Iteration 519, loss = 0.32193450\n",
      "Iteration 520, loss = 0.32192172\n",
      "Iteration 521, loss = 0.32166169\n",
      "Iteration 522, loss = 0.32153585\n",
      "Iteration 523, loss = 0.32142183\n",
      "Iteration 524, loss = 0.32130469\n",
      "Iteration 525, loss = 0.32117879\n",
      "Iteration 526, loss = 0.32105033\n",
      "Iteration 527, loss = 0.32093691\n",
      "Iteration 528, loss = 0.32078245\n",
      "Iteration 529, loss = 0.32066336\n",
      "Iteration 530, loss = 0.32056124\n",
      "Iteration 531, loss = 0.32040555\n",
      "Iteration 532, loss = 0.32032904\n",
      "Iteration 533, loss = 0.32018266\n",
      "Iteration 534, loss = 0.32007065\n",
      "Iteration 535, loss = 0.31991410\n",
      "Iteration 536, loss = 0.31981349\n",
      "Iteration 537, loss = 0.31967786\n",
      "Iteration 538, loss = 0.31955282\n",
      "Iteration 539, loss = 0.31941317\n",
      "Iteration 540, loss = 0.31928911\n",
      "Iteration 541, loss = 0.31917934\n",
      "Iteration 542, loss = 0.31906262\n",
      "Iteration 543, loss = 0.31890960\n",
      "Iteration 544, loss = 0.31879828\n",
      "Iteration 545, loss = 0.31868420\n",
      "Iteration 546, loss = 0.31856238\n",
      "Iteration 547, loss = 0.31844670\n",
      "Iteration 548, loss = 0.31828449\n",
      "Iteration 549, loss = 0.31823066\n",
      "Iteration 550, loss = 0.31806655\n",
      "Iteration 551, loss = 0.31795284\n",
      "Iteration 552, loss = 0.31787514\n",
      "Iteration 553, loss = 0.31770026\n",
      "Iteration 554, loss = 0.31766331\n",
      "Iteration 555, loss = 0.31745373\n",
      "Iteration 556, loss = 0.31732270\n",
      "Iteration 557, loss = 0.31722077\n",
      "Iteration 558, loss = 0.31708651\n",
      "Iteration 559, loss = 0.31697598\n",
      "Iteration 560, loss = 0.31682464\n",
      "Iteration 561, loss = 0.31679115\n",
      "Iteration 562, loss = 0.31659599\n",
      "Iteration 563, loss = 0.31652103\n",
      "Iteration 564, loss = 0.31634952\n",
      "Iteration 565, loss = 0.31622519\n",
      "Iteration 566, loss = 0.31608979\n",
      "Iteration 567, loss = 0.31602795\n",
      "Iteration 568, loss = 0.31585598\n",
      "Iteration 569, loss = 0.31574951\n",
      "Iteration 570, loss = 0.31566370\n",
      "Iteration 571, loss = 0.31549719\n",
      "Iteration 572, loss = 0.31537954\n",
      "Iteration 573, loss = 0.31526939\n",
      "Iteration 574, loss = 0.31514173\n",
      "Iteration 575, loss = 0.31501235\n",
      "Iteration 576, loss = 0.31488198\n",
      "Iteration 577, loss = 0.31479245\n",
      "Iteration 578, loss = 0.31466458\n",
      "Iteration 579, loss = 0.31453250\n",
      "Iteration 580, loss = 0.31442953\n",
      "Iteration 581, loss = 0.31430722\n",
      "Iteration 582, loss = 0.31418994\n",
      "Iteration 583, loss = 0.31412118\n",
      "Iteration 584, loss = 0.31397437\n",
      "Iteration 585, loss = 0.31380162\n",
      "Iteration 586, loss = 0.31376163\n",
      "Iteration 587, loss = 0.31356935\n",
      "Iteration 588, loss = 0.31349164\n",
      "Iteration 589, loss = 0.31334153\n",
      "Iteration 590, loss = 0.31325615\n",
      "Iteration 591, loss = 0.31310089\n",
      "Iteration 592, loss = 0.31303015\n",
      "Iteration 593, loss = 0.31289118\n",
      "Iteration 594, loss = 0.31274004\n",
      "Iteration 595, loss = 0.31263659\n",
      "Iteration 596, loss = 0.31251753\n",
      "Iteration 597, loss = 0.31238456\n",
      "Iteration 598, loss = 0.31228109\n",
      "Iteration 599, loss = 0.31214851\n",
      "Iteration 600, loss = 0.31204217\n",
      "Iteration 601, loss = 0.31189710\n",
      "Iteration 602, loss = 0.31179848\n",
      "Iteration 603, loss = 0.31171750\n",
      "Iteration 604, loss = 0.31156227\n",
      "Iteration 605, loss = 0.31142687\n",
      "Iteration 606, loss = 0.31131530\n",
      "Iteration 607, loss = 0.31122411\n",
      "Iteration 608, loss = 0.31112262\n",
      "Iteration 609, loss = 0.31095272\n",
      "Iteration 610, loss = 0.31085874\n",
      "Iteration 611, loss = 0.31073215\n",
      "Iteration 612, loss = 0.31060932\n",
      "Iteration 613, loss = 0.31048744\n",
      "Iteration 614, loss = 0.31045173\n",
      "Iteration 615, loss = 0.31025589\n",
      "Iteration 616, loss = 0.31014460\n",
      "Iteration 617, loss = 0.31002117\n",
      "Iteration 618, loss = 0.30994838\n",
      "Iteration 619, loss = 0.30979331\n",
      "Iteration 620, loss = 0.30968437\n",
      "Iteration 621, loss = 0.30955258\n",
      "Iteration 622, loss = 0.30943761\n",
      "Iteration 623, loss = 0.30933265\n",
      "Iteration 624, loss = 0.30919167\n",
      "Iteration 625, loss = 0.30909442\n",
      "Iteration 626, loss = 0.30898348\n",
      "Iteration 627, loss = 0.30886647\n",
      "Iteration 628, loss = 0.30877906\n",
      "Iteration 629, loss = 0.30864742\n",
      "Iteration 630, loss = 0.30851173\n",
      "Iteration 631, loss = 0.30842597\n",
      "Iteration 632, loss = 0.30828964\n",
      "Iteration 633, loss = 0.30818217\n",
      "Iteration 634, loss = 0.30808475\n",
      "Iteration 635, loss = 0.30796348\n",
      "Iteration 636, loss = 0.30784399\n",
      "Iteration 637, loss = 0.30777467\n",
      "Iteration 638, loss = 0.30760298\n",
      "Iteration 639, loss = 0.30750432\n",
      "Iteration 640, loss = 0.30739467\n",
      "Iteration 641, loss = 0.30725951\n",
      "Iteration 642, loss = 0.30716030\n",
      "Iteration 643, loss = 0.30705516\n",
      "Iteration 644, loss = 0.30693203\n",
      "Iteration 645, loss = 0.30690654\n",
      "Iteration 646, loss = 0.30670189\n",
      "Iteration 647, loss = 0.30659504\n",
      "Iteration 648, loss = 0.30648058\n",
      "Iteration 649, loss = 0.30635406\n",
      "Iteration 650, loss = 0.30627095\n",
      "Iteration 651, loss = 0.30614295\n",
      "Iteration 652, loss = 0.30602362\n",
      "Iteration 653, loss = 0.30591659\n",
      "Iteration 654, loss = 0.30583123\n",
      "Iteration 655, loss = 0.30567873\n",
      "Iteration 656, loss = 0.30560026\n",
      "Iteration 657, loss = 0.30546059\n",
      "Iteration 658, loss = 0.30535373\n",
      "Iteration 659, loss = 0.30524240\n",
      "Iteration 660, loss = 0.30516202\n",
      "Iteration 661, loss = 0.30502278\n",
      "Iteration 662, loss = 0.30491579\n",
      "Iteration 663, loss = 0.30481347\n",
      "Iteration 664, loss = 0.30475363\n",
      "Iteration 665, loss = 0.30457695\n",
      "Iteration 666, loss = 0.30446830\n",
      "Iteration 667, loss = 0.30436690\n",
      "Iteration 668, loss = 0.30425607\n",
      "Iteration 669, loss = 0.30416848\n",
      "Iteration 670, loss = 0.30404103\n",
      "Iteration 671, loss = 0.30393413\n",
      "Iteration 672, loss = 0.30381419\n",
      "Iteration 673, loss = 0.30368786\n",
      "Iteration 674, loss = 0.30361293\n",
      "Iteration 675, loss = 0.30347259\n",
      "Iteration 676, loss = 0.30337374\n",
      "Iteration 677, loss = 0.30333163\n",
      "Iteration 678, loss = 0.30315226\n",
      "Iteration 679, loss = 0.30307574\n",
      "Iteration 680, loss = 0.30292338\n",
      "Iteration 681, loss = 0.30280876\n",
      "Iteration 682, loss = 0.30276654\n",
      "Iteration 683, loss = 0.30259677\n",
      "Iteration 684, loss = 0.30245813\n",
      "Iteration 685, loss = 0.30237497\n",
      "Iteration 686, loss = 0.30228041\n",
      "Iteration 687, loss = 0.30212786\n",
      "Iteration 688, loss = 0.30203600\n",
      "Iteration 689, loss = 0.30197007\n",
      "Iteration 690, loss = 0.30182726\n",
      "Iteration 691, loss = 0.30169296\n",
      "Iteration 692, loss = 0.30160762\n",
      "Iteration 693, loss = 0.30150268\n",
      "Iteration 694, loss = 0.30137729\n",
      "Iteration 695, loss = 0.30126942\n",
      "Iteration 696, loss = 0.30122923\n",
      "Iteration 697, loss = 0.30103502\n",
      "Iteration 698, loss = 0.30093471\n",
      "Iteration 699, loss = 0.30081236\n",
      "Iteration 700, loss = 0.30071588\n",
      "Iteration 701, loss = 0.30059569\n",
      "Iteration 702, loss = 0.30055430\n",
      "Iteration 703, loss = 0.30036605\n",
      "Iteration 704, loss = 0.30031750\n",
      "Iteration 705, loss = 0.30015067\n",
      "Iteration 706, loss = 0.30005119\n",
      "Iteration 707, loss = 0.29994792\n",
      "Iteration 708, loss = 0.29983308\n",
      "Iteration 709, loss = 0.29972843\n",
      "Iteration 710, loss = 0.29960518\n",
      "Iteration 711, loss = 0.29952116\n",
      "Iteration 712, loss = 0.29939031\n",
      "Iteration 713, loss = 0.29927610\n",
      "Iteration 714, loss = 0.29917531\n",
      "Iteration 715, loss = 0.29909857\n",
      "Iteration 716, loss = 0.29896653\n",
      "Iteration 717, loss = 0.29887290\n",
      "Iteration 718, loss = 0.29874741\n",
      "Iteration 719, loss = 0.29864135\n",
      "Iteration 720, loss = 0.29852629\n",
      "Iteration 721, loss = 0.29841903\n",
      "Iteration 722, loss = 0.29833846\n",
      "Iteration 723, loss = 0.29818498\n",
      "Iteration 724, loss = 0.29810847\n",
      "Iteration 725, loss = 0.29799301\n",
      "Iteration 726, loss = 0.29788697\n",
      "Iteration 727, loss = 0.29776794\n",
      "Iteration 728, loss = 0.29766167\n",
      "Iteration 729, loss = 0.29758997\n",
      "Iteration 730, loss = 0.29745688\n",
      "Iteration 731, loss = 0.29733895\n",
      "Iteration 732, loss = 0.29726147\n",
      "Iteration 733, loss = 0.29721701\n",
      "Iteration 734, loss = 0.29701047\n",
      "Iteration 735, loss = 0.29692734\n",
      "Iteration 736, loss = 0.29680251\n",
      "Iteration 737, loss = 0.29670809\n",
      "Iteration 738, loss = 0.29658912\n",
      "Iteration 739, loss = 0.29647386\n",
      "Iteration 740, loss = 0.29636106\n",
      "Iteration 741, loss = 0.29627537\n",
      "Iteration 742, loss = 0.29618137\n",
      "Iteration 743, loss = 0.29605170\n",
      "Iteration 744, loss = 0.29597940\n",
      "Iteration 745, loss = 0.29582365\n",
      "Iteration 746, loss = 0.29574494\n",
      "Iteration 747, loss = 0.29564507\n",
      "Iteration 748, loss = 0.29552277\n",
      "Iteration 749, loss = 0.29539422\n",
      "Iteration 750, loss = 0.29527890\n",
      "Iteration 751, loss = 0.29518915\n",
      "Iteration 752, loss = 0.29507620\n",
      "Iteration 753, loss = 0.29500024\n",
      "Iteration 754, loss = 0.29485597\n",
      "Iteration 755, loss = 0.29477856\n",
      "Iteration 756, loss = 0.29467093\n",
      "Iteration 757, loss = 0.29453961\n",
      "Iteration 758, loss = 0.29444559\n",
      "Iteration 759, loss = 0.29430301\n",
      "Iteration 760, loss = 0.29422317\n",
      "Iteration 761, loss = 0.29411705\n",
      "Iteration 762, loss = 0.29398154\n",
      "Iteration 763, loss = 0.29387263\n",
      "Iteration 764, loss = 0.29380224\n",
      "Iteration 765, loss = 0.29365643\n",
      "Iteration 766, loss = 0.29359907\n",
      "Iteration 767, loss = 0.29346440\n",
      "Iteration 768, loss = 0.29341293\n",
      "Iteration 769, loss = 0.29321894\n",
      "Iteration 770, loss = 0.29312830\n",
      "Iteration 771, loss = 0.29302471\n",
      "Iteration 772, loss = 0.29291196\n",
      "Iteration 773, loss = 0.29279818\n",
      "Iteration 774, loss = 0.29273157\n",
      "Iteration 775, loss = 0.29259775\n",
      "Iteration 776, loss = 0.29250133\n",
      "Iteration 777, loss = 0.29236747\n",
      "Iteration 778, loss = 0.29231145\n",
      "Iteration 779, loss = 0.29218171\n",
      "Iteration 780, loss = 0.29206224\n",
      "Iteration 781, loss = 0.29194706\n",
      "Iteration 782, loss = 0.29184026\n",
      "Iteration 783, loss = 0.29175597\n",
      "Iteration 784, loss = 0.29162499\n",
      "Iteration 785, loss = 0.29154384\n",
      "Iteration 786, loss = 0.29139867\n",
      "Iteration 787, loss = 0.29130214\n",
      "Iteration 788, loss = 0.29118635\n",
      "Iteration 789, loss = 0.29107492\n",
      "Iteration 790, loss = 0.29098245\n",
      "Iteration 791, loss = 0.29092151\n",
      "Iteration 792, loss = 0.29078067\n",
      "Iteration 793, loss = 0.29067434\n",
      "Iteration 794, loss = 0.29056862\n",
      "Iteration 795, loss = 0.29045163\n",
      "Iteration 796, loss = 0.29035224\n",
      "Iteration 797, loss = 0.29022933\n",
      "Iteration 798, loss = 0.29014966\n",
      "Iteration 799, loss = 0.29001763\n",
      "Iteration 800, loss = 0.28993245\n",
      "Iteration 801, loss = 0.28984479\n",
      "Iteration 802, loss = 0.28973007\n",
      "Iteration 803, loss = 0.28960154\n",
      "Iteration 804, loss = 0.28953860\n",
      "Iteration 805, loss = 0.28942802\n",
      "Iteration 806, loss = 0.28934616\n",
      "Iteration 807, loss = 0.28920205\n",
      "Iteration 808, loss = 0.28915656\n",
      "Iteration 809, loss = 0.28899253\n",
      "Iteration 810, loss = 0.28891036\n",
      "Iteration 811, loss = 0.28878482\n",
      "Iteration 812, loss = 0.28865915\n",
      "Iteration 813, loss = 0.28862860\n",
      "Iteration 814, loss = 0.28844680\n",
      "Iteration 815, loss = 0.28833988\n",
      "Iteration 816, loss = 0.28828614\n",
      "Iteration 817, loss = 0.28815664\n",
      "Iteration 818, loss = 0.28805412\n",
      "Iteration 819, loss = 0.28793476\n",
      "Iteration 820, loss = 0.28784201\n",
      "Iteration 821, loss = 0.28772037\n",
      "Iteration 822, loss = 0.28764230\n",
      "Iteration 823, loss = 0.28751713\n",
      "Iteration 824, loss = 0.28740489\n",
      "Iteration 825, loss = 0.28731929\n",
      "Iteration 826, loss = 0.28722140\n",
      "Iteration 827, loss = 0.28723873\n",
      "Iteration 828, loss = 0.28698101\n",
      "Iteration 829, loss = 0.28690260\n",
      "Iteration 830, loss = 0.28677976\n",
      "Iteration 831, loss = 0.28672015\n",
      "Iteration 832, loss = 0.28657305\n",
      "Iteration 833, loss = 0.28649353\n",
      "Iteration 834, loss = 0.28641107\n",
      "Iteration 835, loss = 0.28628103\n",
      "Iteration 836, loss = 0.28613925\n",
      "Iteration 837, loss = 0.28607369\n",
      "Iteration 838, loss = 0.28599234\n",
      "Iteration 839, loss = 0.28584851\n",
      "Iteration 840, loss = 0.28575744\n",
      "Iteration 841, loss = 0.28563321\n",
      "Iteration 842, loss = 0.28552101\n",
      "Iteration 843, loss = 0.28542769\n",
      "Iteration 844, loss = 0.28532900\n",
      "Iteration 845, loss = 0.28526475\n",
      "Iteration 846, loss = 0.28511398\n",
      "Iteration 847, loss = 0.28503050\n",
      "Iteration 848, loss = 0.28490395\n",
      "Iteration 849, loss = 0.28482371\n",
      "Iteration 850, loss = 0.28470198\n",
      "Iteration 851, loss = 0.28459044\n",
      "Iteration 852, loss = 0.28449434\n",
      "Iteration 853, loss = 0.28446374\n",
      "Iteration 854, loss = 0.28428136\n",
      "Iteration 855, loss = 0.28428730\n",
      "Iteration 856, loss = 0.28407400\n",
      "Iteration 857, loss = 0.28397062\n",
      "Iteration 858, loss = 0.28386439\n",
      "Iteration 859, loss = 0.28375147\n",
      "Iteration 860, loss = 0.28366239\n",
      "Iteration 861, loss = 0.28359938\n",
      "Iteration 862, loss = 0.28347877\n",
      "Iteration 863, loss = 0.28334853\n",
      "Iteration 864, loss = 0.28325505\n",
      "Iteration 865, loss = 0.28313425\n",
      "Iteration 866, loss = 0.28303739\n",
      "Iteration 867, loss = 0.28291974\n",
      "Iteration 868, loss = 0.28285189\n",
      "Iteration 869, loss = 0.28273877\n",
      "Iteration 870, loss = 0.28262278\n",
      "Iteration 871, loss = 0.28250838\n",
      "Iteration 872, loss = 0.28240806\n",
      "Iteration 873, loss = 0.28229380\n",
      "Iteration 874, loss = 0.28222809\n",
      "Iteration 875, loss = 0.28210226\n",
      "Iteration 876, loss = 0.28197469\n",
      "Iteration 877, loss = 0.28195507\n",
      "Iteration 878, loss = 0.28176637\n",
      "Iteration 879, loss = 0.28171548\n",
      "Iteration 880, loss = 0.28156685\n",
      "Iteration 881, loss = 0.28145014\n",
      "Iteration 882, loss = 0.28135388\n",
      "Iteration 883, loss = 0.28127979\n",
      "Iteration 884, loss = 0.28114744\n",
      "Iteration 885, loss = 0.28109031\n",
      "Iteration 886, loss = 0.28095118\n",
      "Iteration 887, loss = 0.28081058\n",
      "Iteration 888, loss = 0.28071747\n",
      "Iteration 889, loss = 0.28062070\n",
      "Iteration 890, loss = 0.28049444\n",
      "Iteration 891, loss = 0.28040381\n",
      "Iteration 892, loss = 0.28033286\n",
      "Iteration 893, loss = 0.28019471\n",
      "Iteration 894, loss = 0.28009605\n",
      "Iteration 895, loss = 0.27997582\n",
      "Iteration 896, loss = 0.27987654\n",
      "Iteration 897, loss = 0.27977182\n",
      "Iteration 898, loss = 0.27967355\n",
      "Iteration 899, loss = 0.27954927\n",
      "Iteration 900, loss = 0.27945464\n",
      "Iteration 901, loss = 0.27934538\n",
      "Iteration 902, loss = 0.27923453\n",
      "Iteration 903, loss = 0.27913248\n",
      "Iteration 904, loss = 0.27901793\n",
      "Iteration 905, loss = 0.27895345\n",
      "Iteration 906, loss = 0.27881848\n",
      "Iteration 907, loss = 0.27874898\n",
      "Iteration 908, loss = 0.27861968\n",
      "Iteration 909, loss = 0.27849920\n",
      "Iteration 910, loss = 0.27838341\n",
      "Iteration 911, loss = 0.27831968\n",
      "Iteration 912, loss = 0.27819463\n",
      "Iteration 913, loss = 0.27807583\n",
      "Iteration 914, loss = 0.27801264\n",
      "Iteration 915, loss = 0.27786348\n",
      "Iteration 916, loss = 0.27775960\n",
      "Iteration 917, loss = 0.27771802\n",
      "Iteration 918, loss = 0.27760307\n",
      "Iteration 919, loss = 0.27744911\n",
      "Iteration 920, loss = 0.27735026\n",
      "Iteration 921, loss = 0.27728659\n",
      "Iteration 922, loss = 0.27714210\n",
      "Iteration 923, loss = 0.27705763\n",
      "Iteration 924, loss = 0.27692995\n",
      "Iteration 925, loss = 0.27683708\n",
      "Iteration 926, loss = 0.27671888\n",
      "Iteration 927, loss = 0.27663423\n",
      "Iteration 928, loss = 0.27652927\n",
      "Iteration 929, loss = 0.27641242\n",
      "Iteration 930, loss = 0.27629469\n",
      "Iteration 931, loss = 0.27620425\n",
      "Iteration 932, loss = 0.27608587\n",
      "Iteration 933, loss = 0.27595922\n",
      "Iteration 934, loss = 0.27587986\n",
      "Iteration 935, loss = 0.27575213\n",
      "Iteration 936, loss = 0.27567572\n",
      "Iteration 937, loss = 0.27558969\n",
      "Iteration 938, loss = 0.27554764\n",
      "Iteration 939, loss = 0.27532469\n",
      "Iteration 940, loss = 0.27524638\n",
      "Iteration 941, loss = 0.27522467\n",
      "Iteration 942, loss = 0.27501683\n",
      "Iteration 943, loss = 0.27491475\n",
      "Iteration 944, loss = 0.27480233\n",
      "Iteration 945, loss = 0.27471273\n",
      "Iteration 946, loss = 0.27458010\n",
      "Iteration 947, loss = 0.27450835\n",
      "Iteration 948, loss = 0.27438368\n",
      "Iteration 949, loss = 0.27429498\n",
      "Iteration 950, loss = 0.27416495\n",
      "Iteration 951, loss = 0.27407528\n",
      "Iteration 952, loss = 0.27400327\n",
      "Iteration 953, loss = 0.27389760\n",
      "Iteration 954, loss = 0.27375905\n",
      "Iteration 955, loss = 0.27369794\n",
      "Iteration 956, loss = 0.27356755\n",
      "Iteration 957, loss = 0.27343246\n",
      "Iteration 958, loss = 0.27338191\n",
      "Iteration 959, loss = 0.27322362\n",
      "Iteration 960, loss = 0.27312765\n",
      "Iteration 961, loss = 0.27303414\n",
      "Iteration 962, loss = 0.27293916\n",
      "Iteration 963, loss = 0.27281986\n",
      "Iteration 964, loss = 0.27270154\n",
      "Iteration 965, loss = 0.27263434\n",
      "Iteration 966, loss = 0.27251455\n",
      "Iteration 967, loss = 0.27238033\n",
      "Iteration 968, loss = 0.27229949\n",
      "Iteration 969, loss = 0.27218867\n",
      "Iteration 970, loss = 0.27210959\n",
      "Iteration 971, loss = 0.27195695\n",
      "Iteration 972, loss = 0.27186018\n",
      "Iteration 973, loss = 0.27179343\n",
      "Iteration 974, loss = 0.27169358\n",
      "Iteration 975, loss = 0.27156921\n",
      "Iteration 976, loss = 0.27145866\n",
      "Iteration 977, loss = 0.27133642\n",
      "Iteration 978, loss = 0.27125884\n",
      "Iteration 979, loss = 0.27115825\n",
      "Iteration 980, loss = 0.27103846\n",
      "Iteration 981, loss = 0.27093004\n",
      "Iteration 982, loss = 0.27080503\n",
      "Iteration 983, loss = 0.27072640\n",
      "Iteration 984, loss = 0.27059006\n",
      "Iteration 985, loss = 0.27046662\n",
      "Iteration 986, loss = 0.27041612\n",
      "\n",
      "Iteration 59, loss = 0.46027268\n",
      "Iteration 60, loss = 0.45817643\n",
      "Iteration 61, loss = 0.45614980\n",
      "Iteration 62, loss = 0.45425273\n",
      "Iteration 63, loss = 0.45237047\n",
      "Iteration 64, loss = 0.45045856\n",
      "Iteration 65, loss = 0.44859479\n",
      "Iteration 66, loss = 0.44683888\n",
      "Iteration 67, loss = 0.44520231\n",
      "Iteration 68, loss = 0.44345427\n",
      "Iteration 69, loss = 0.44182549\n",
      "Iteration 70, loss = 0.44022999\n",
      "Iteration 71, loss = 0.43871552\n",
      "Iteration 72, loss = 0.43712908\n",
      "Iteration 73, loss = 0.43571361\n",
      "Iteration 74, loss = 0.43421869\n",
      "Iteration 75, loss = 0.43284529\n",
      "Iteration 76, loss = 0.43148058\n",
      "Iteration 77, loss = 0.43007582\n",
      "Iteration 78, loss = 0.42877974\n",
      "Iteration 79, loss = 0.42753126\n",
      "Iteration 80, loss = 0.42625468\n",
      "Iteration 81, loss = 0.42499540\n",
      "Iteration 82, loss = 0.42380905\n",
      "Iteration 83, loss = 0.42261193\n",
      "Iteration 84, loss = 0.42148874\n",
      "Iteration 85, loss = 0.42038412\n",
      "Iteration 86, loss = 0.41929790\n",
      "Iteration 87, loss = 0.41821985\n",
      "Iteration 88, loss = 0.41708973\n",
      "Iteration 89, loss = 0.41613167\n",
      "Iteration 90, loss = 0.41510453\n",
      "Iteration 91, loss = 0.41411374\n",
      "Iteration 92, loss = 0.41317049\n",
      "Iteration 93, loss = 0.41219064\n",
      "Iteration 94, loss = 0.41128195\n",
      "Iteration 95, loss = 0.41034804\n",
      "Iteration 96, loss = 0.40945874\n",
      "Iteration 97, loss = 0.40858758\n",
      "Iteration 98, loss = 0.40776431\n",
      "Iteration 99, loss = 0.40690369\n",
      "Iteration 100, loss = 0.40609243\n",
      "Iteration 101, loss = 0.40528582\n",
      "Iteration 102, loss = 0.40446252\n",
      "Iteration 103, loss = 0.40369333\n",
      "Iteration 104, loss = 0.40292536\n",
      "Iteration 105, loss = 0.40220782\n",
      "Iteration 106, loss = 0.40140651\n",
      "Iteration 107, loss = 0.40069480\n",
      "Iteration 108, loss = 0.40002334\n",
      "Iteration 109, loss = 0.39931023\n",
      "Iteration 110, loss = 0.39865258\n",
      "Iteration 111, loss = 0.39795973\n",
      "Iteration 112, loss = 0.39730006\n",
      "Iteration 113, loss = 0.39665219\n",
      "Iteration 114, loss = 0.39597268\n",
      "Iteration 115, loss = 0.39536496\n",
      "Iteration 116, loss = 0.39473669\n",
      "Iteration 117, loss = 0.39412887\n",
      "Iteration 118, loss = 0.39351954\n",
      "Iteration 119, loss = 0.39295054\n",
      "Iteration 120, loss = 0.39234368\n",
      "Iteration 121, loss = 0.39177159\n",
      "Iteration 122, loss = 0.39120581\n",
      "Iteration 123, loss = 0.39066052\n",
      "Iteration 124, loss = 0.39011770\n",
      "Iteration 125, loss = 0.38956526\n",
      "Iteration 126, loss = 0.38905109\n",
      "Iteration 127, loss = 0.38850990\n",
      "Iteration 128, loss = 0.38802435\n",
      "Iteration 129, loss = 0.38747781\n",
      "Iteration 130, loss = 0.38696645\n",
      "Iteration 131, loss = 0.38646926\n",
      "Iteration 132, loss = 0.38599551\n",
      "Iteration 133, loss = 0.38551470\n",
      "Iteration 134, loss = 0.38505068\n",
      "Iteration 135, loss = 0.38458037\n",
      "Iteration 136, loss = 0.38409843\n",
      "Iteration 137, loss = 0.38365550\n",
      "Iteration 138, loss = 0.38321285\n",
      "Iteration 139, loss = 0.38279220\n",
      "Iteration 140, loss = 0.38233540\n",
      "Iteration 141, loss = 0.38188579\n",
      "Iteration 142, loss = 0.38147726\n",
      "Iteration 143, loss = 0.38105877\n",
      "Iteration 144, loss = 0.38064754\n",
      "Iteration 145, loss = 0.38022041\n",
      "Iteration 146, loss = 0.37982454\n",
      "Iteration 147, loss = 0.37943083\n",
      "Iteration 148, loss = 0.37900351\n",
      "Iteration 149, loss = 0.37862786\n",
      "Iteration 150, loss = 0.37827700\n",
      "Iteration 151, loss = 0.37786377\n",
      "Iteration 152, loss = 0.37748650\n",
      "Iteration 153, loss = 0.37712383\n",
      "Iteration 154, loss = 0.37675419\n",
      "Iteration 155, loss = 0.37638579\n",
      "Iteration 156, loss = 0.37603129\n",
      "Iteration 157, loss = 0.37569389\n",
      "Iteration 158, loss = 0.37532019\n",
      "Iteration 159, loss = 0.37498669\n",
      "Iteration 160, loss = 0.37461168\n",
      "Iteration 161, loss = 0.37430913\n",
      "Iteration 162, loss = 0.37397954\n",
      "Iteration 163, loss = 0.37362017\n",
      "Iteration 164, loss = 0.37327984\n",
      "Iteration 165, loss = 0.37297342\n",
      "Iteration 166, loss = 0.37265148\n",
      "Iteration 167, loss = 0.37234126\n",
      "Iteration 168, loss = 0.37202335\n",
      "Iteration 169, loss = 0.37169882\n",
      "Iteration 170, loss = 0.37139853\n",
      "Iteration 171, loss = 0.37110189\n",
      "Iteration 172, loss = 0.37078030\n",
      "Iteration 173, loss = 0.37048856\n",
      "Iteration 174, loss = 0.37019171\n",
      "Iteration 175, loss = 0.36989339\n",
      "Iteration 176, loss = 0.36961179\n",
      "Iteration 177, loss = 0.36934795\n",
      "Iteration 178, loss = 0.36902221\n",
      "Iteration 179, loss = 0.36874249\n",
      "Iteration 180, loss = 0.36844743\n",
      "Iteration 181, loss = 0.36818681\n",
      "Iteration 182, loss = 0.36791057\n",
      "Iteration 183, loss = 0.36763023\n",
      "Iteration 184, loss = 0.36738012\n",
      "Iteration 185, loss = 0.36708714\n",
      "Iteration 186, loss = 0.36683164\n",
      "Iteration 187, loss = 0.36656362\n",
      "Iteration 188, loss = 0.36629217\n",
      "Iteration 189, loss = 0.36601684\n",
      "Iteration 190, loss = 0.36578688\n",
      "Iteration 191, loss = 0.36553089\n",
      "Iteration 192, loss = 0.36528037\n",
      "Iteration 193, loss = 0.36502933\n",
      "Iteration 194, loss = 0.36476442\n",
      "Iteration 195, loss = 0.36451635\n",
      "Iteration 196, loss = 0.36428216\n",
      "Iteration 197, loss = 0.36404985\n",
      "Iteration 198, loss = 0.36380020\n",
      "Iteration 199, loss = 0.36355505\n",
      "Iteration 200, loss = 0.36332214\n",
      "Iteration 201, loss = 0.36310671\n",
      "Iteration 202, loss = 0.36286744\n",
      "Iteration 203, loss = 0.36262072\n",
      "Iteration 204, loss = 0.36240737\n",
      "Iteration 205, loss = 0.36217502\n",
      "Iteration 206, loss = 0.36194904\n",
      "Iteration 207, loss = 0.36172309\n",
      "Iteration 208, loss = 0.36149015\n",
      "Iteration 209, loss = 0.36126934\n",
      "Iteration 210, loss = 0.36105772\n",
      "Iteration 211, loss = 0.36084479\n",
      "Iteration 212, loss = 0.36061896\n",
      "Iteration 213, loss = 0.36039873\n",
      "Iteration 214, loss = 0.36019743\n",
      "Iteration 215, loss = 0.35998010\n",
      "Iteration 216, loss = 0.35977481\n",
      "Iteration 217, loss = 0.35956140\n",
      "Iteration 218, loss = 0.35936261\n",
      "Iteration 219, loss = 0.35914930\n",
      "Iteration 220, loss = 0.35894431\n",
      "Iteration 221, loss = 0.35873381\n",
      "Iteration 222, loss = 0.35854849\n",
      "Iteration 223, loss = 0.35832592\n",
      "Iteration 224, loss = 0.35815419\n",
      "Iteration 225, loss = 0.35794253\n",
      "Iteration 226, loss = 0.35773610\n",
      "Iteration 227, loss = 0.35756011\n",
      "Iteration 228, loss = 0.35734945\n",
      "Iteration 229, loss = 0.35717093\n",
      "Iteration 230, loss = 0.35696508\n",
      "Iteration 231, loss = 0.35677112\n",
      "Iteration 232, loss = 0.35660485\n",
      "Iteration 233, loss = 0.35642399\n",
      "Iteration 234, loss = 0.35622092\n",
      "Iteration 235, loss = 0.35602827\n",
      "Iteration 236, loss = 0.35584858\n",
      "Iteration 237, loss = 0.35566861\n",
      "Iteration 238, loss = 0.35548584\n",
      "Iteration 239, loss = 0.35530407\n",
      "Iteration 240, loss = 0.35512295\n",
      "Iteration 241, loss = 0.35493445\n",
      "Iteration 242, loss = 0.35476739\n",
      "Iteration 243, loss = 0.35459437\n",
      "Iteration 244, loss = 0.35441525\n",
      "Iteration 245, loss = 0.35423726\n",
      "Iteration 246, loss = 0.35406874\n",
      "Iteration 247, loss = 0.35388479\n",
      "Iteration 248, loss = 0.35372348\n",
      "Iteration 249, loss = 0.35354907\n",
      "Iteration 250, loss = 0.35337116\n",
      "Iteration 251, loss = 0.35321872\n",
      "Iteration 252, loss = 0.35303620\n",
      "Iteration 253, loss = 0.35287928\n",
      "Iteration 254, loss = 0.35271817\n",
      "Iteration 255, loss = 0.35254084\n",
      "Iteration 256, loss = 0.35237459\n",
      "Iteration 257, loss = 0.35222513\n",
      "Iteration 258, loss = 0.35206032\n",
      "Iteration 259, loss = 0.35189019\n",
      "Iteration 260, loss = 0.35173326\n",
      "Iteration 261, loss = 0.35158220\n",
      "Iteration 262, loss = 0.35143152\n",
      "Iteration 263, loss = 0.35124857\n",
      "Iteration 264, loss = 0.35109883\n",
      "Iteration 265, loss = 0.35094005\n",
      "Iteration 266, loss = 0.35078672\n",
      "Iteration 267, loss = 0.35063672\n",
      "Iteration 268, loss = 0.35048208\n",
      "Iteration 269, loss = 0.35032490\n",
      "Iteration 270, loss = 0.35017086\n",
      "Iteration 271, loss = 0.35002430\n",
      "Iteration 272, loss = 0.34986895\n",
      "Iteration 273, loss = 0.34972003\n",
      "Iteration 274, loss = 0.34956508\n",
      "Iteration 275, loss = 0.34942266\n",
      "Iteration 276, loss = 0.34928221\n",
      "Iteration 277, loss = 0.34912355\n",
      "Iteration 278, loss = 0.34898296\n",
      "Iteration 279, loss = 0.34882986\n",
      "Iteration 280, loss = 0.34869424\n",
      "Iteration 281, loss = 0.34855177\n",
      "Iteration 282, loss = 0.34840738\n",
      "Iteration 283, loss = 0.34825501\n",
      "Iteration 284, loss = 0.34811831\n",
      "Iteration 285, loss = 0.34798631\n",
      "Iteration 286, loss = 0.34783910\n",
      "Iteration 287, loss = 0.34769570\n",
      "Iteration 288, loss = 0.34756615\n",
      "Iteration 289, loss = 0.34741784\n",
      "Iteration 290, loss = 0.34728588\n",
      "Iteration 291, loss = 0.34714238\n",
      "Iteration 292, loss = 0.34702071\n",
      "Iteration 293, loss = 0.34687016\n",
      "Iteration 294, loss = 0.34673958\n",
      "Iteration 295, loss = 0.34660199\n",
      "Iteration 296, loss = 0.34647402\n",
      "Iteration 297, loss = 0.34633488\n",
      "Iteration 298, loss = 0.34620720\n",
      "Iteration 299, loss = 0.34606703\n",
      "Iteration 300, loss = 0.34594137\n",
      "Iteration 301, loss = 0.34579681\n",
      "Iteration 302, loss = 0.34568016\n",
      "Iteration 303, loss = 0.34554171\n",
      "Iteration 304, loss = 0.34542223\n",
      "Iteration 305, loss = 0.34529064\n",
      "Iteration 306, loss = 0.34515633\n",
      "Iteration 307, loss = 0.34504177\n",
      "Iteration 308, loss = 0.34491865\n",
      "Iteration 309, loss = 0.34478109\n",
      "Iteration 310, loss = 0.34466270\n",
      "Iteration 311, loss = 0.34453069\n",
      "Iteration 312, loss = 0.34440639\n",
      "Iteration 313, loss = 0.34428045\n",
      "Iteration 314, loss = 0.34418014\n",
      "Iteration 315, loss = 0.34404386\n",
      "Iteration 316, loss = 0.34391290\n",
      "Iteration 317, loss = 0.34378545\n",
      "Iteration 318, loss = 0.34367245\n",
      "Iteration 319, loss = 0.34355805\n",
      "Iteration 320, loss = 0.34343280\n",
      "Iteration 321, loss = 0.34330712\n",
      "Iteration 322, loss = 0.34318902\n",
      "Iteration 323, loss = 0.34306800\n",
      "Iteration 324, loss = 0.34295343\n",
      "Iteration 325, loss = 0.34285435\n",
      "Iteration 326, loss = 0.34271974\n",
      "Iteration 327, loss = 0.34261834\n",
      "Iteration 328, loss = 0.34249215\n",
      "Iteration 329, loss = 0.34237344\n",
      "Iteration 330, loss = 0.34226446\n",
      "Iteration 331, loss = 0.34214737\n",
      "Iteration 332, loss = 0.34201891\n",
      "Iteration 333, loss = 0.34191511\n",
      "Iteration 334, loss = 0.34180172\n",
      "Iteration 335, loss = 0.34169484\n",
      "Iteration 336, loss = 0.34157219\n",
      "Iteration 337, loss = 0.34146329\n",
      "Iteration 338, loss = 0.34135034\n",
      "Iteration 339, loss = 0.34123344\n",
      "Iteration 340, loss = 0.34112513\n",
      "Iteration 341, loss = 0.34101206\n",
      "Iteration 342, loss = 0.34090242\n",
      "Iteration 343, loss = 0.34079195\n",
      "Iteration 344, loss = 0.34068917\n",
      "Iteration 345, loss = 0.34057393\n",
      "Iteration 346, loss = 0.34046628\n",
      "Iteration 347, loss = 0.34035715\n",
      "Iteration 348, loss = 0.34024309\n",
      "Iteration 349, loss = 0.34013645\n",
      "Iteration 350, loss = 0.34003653\n",
      "Iteration 351, loss = 0.33992887\n",
      "Iteration 352, loss = 0.33982522\n",
      "Iteration 353, loss = 0.33970949\n",
      "Iteration 354, loss = 0.33960509\n",
      "Iteration 355, loss = 0.33949689\n",
      "Iteration 356, loss = 0.33939111\n",
      "Iteration 357, loss = 0.33931320\n",
      "Iteration 358, loss = 0.33918126\n",
      "Iteration 359, loss = 0.33908366\n",
      "Iteration 360, loss = 0.33898989\n",
      "Iteration 361, loss = 0.33887346\n",
      "Iteration 362, loss = 0.33876963\n",
      "Iteration 363, loss = 0.33866552\n",
      "Iteration 364, loss = 0.33857351\n",
      "Iteration 365, loss = 0.33846604\n",
      "Iteration 366, loss = 0.33837031\n",
      "Iteration 367, loss = 0.33827110\n",
      "Iteration 368, loss = 0.33816898\n",
      "Iteration 369, loss = 0.33807018\n",
      "Iteration 370, loss = 0.33795505\n",
      "Iteration 371, loss = 0.33786736\n",
      "Iteration 372, loss = 0.33777266\n",
      "Iteration 373, loss = 0.33767024\n",
      "Iteration 374, loss = 0.33757218\n",
      "Iteration 375, loss = 0.33747940\n",
      "Iteration 376, loss = 0.33737138\n",
      "Iteration 377, loss = 0.33726883\n",
      "Iteration 378, loss = 0.33719309\n",
      "Iteration 379, loss = 0.33707848\n",
      "Iteration 380, loss = 0.33698925\n",
      "Iteration 381, loss = 0.33688410\n",
      "Iteration 382, loss = 0.33679722\n",
      "Iteration 383, loss = 0.33669546\n",
      "Iteration 384, loss = 0.33660599\n",
      "Iteration 385, loss = 0.33652334\n",
      "Iteration 386, loss = 0.33641767\n",
      "Iteration 387, loss = 0.33633131\n",
      "Iteration 388, loss = 0.33622992\n",
      "Iteration 389, loss = 0.33614228\n",
      "Iteration 390, loss = 0.33604351\n",
      "Iteration 391, loss = 0.33595992\n",
      "Iteration 392, loss = 0.33585321\n",
      "Iteration 393, loss = 0.33577564\n",
      "Iteration 394, loss = 0.33567266\n",
      "Iteration 395, loss = 0.33558141\n",
      "Iteration 396, loss = 0.33549171\n",
      "Iteration 397, loss = 0.33539931\n",
      "Iteration 398, loss = 0.33530329\n",
      "Iteration 399, loss = 0.33521763\n",
      "Iteration 400, loss = 0.33511675\n",
      "Iteration 401, loss = 0.33504406\n",
      "Iteration 402, loss = 0.33493560\n",
      "Iteration 403, loss = 0.33485690\n",
      "Iteration 404, loss = 0.33476165\n",
      "Iteration 405, loss = 0.33467132\n",
      "Iteration 406, loss = 0.33458580\n",
      "Iteration 407, loss = 0.33449730\n",
      "Iteration 408, loss = 0.33440878\n",
      "Iteration 409, loss = 0.33431951\n",
      "Iteration 410, loss = 0.33423074\n",
      "Iteration 411, loss = 0.33414538\n",
      "Iteration 412, loss = 0.33407138\n",
      "Iteration 413, loss = 0.33397957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69973953\n",
      "Iteration 2, loss = 0.69752967\n",
      "Iteration 3, loss = 0.69411697\n",
      "Iteration 4, loss = 0.69006132\n",
      "Iteration 5, loss = 0.68538254\n",
      "Iteration 6, loss = 0.68046806\n",
      "Iteration 7, loss = 0.67511374\n",
      "Iteration 8, loss = 0.66998184\n",
      "Iteration 9, loss = 0.66466087\n",
      "Iteration 10, loss = 0.65948264\n",
      "Iteration 11, loss = 0.65425466\n",
      "Iteration 12, loss = 0.64916198\n",
      "Iteration 13, loss = 0.64410518\n",
      "Iteration 14, loss = 0.63922072\n",
      "Iteration 15, loss = 0.63440554\n",
      "Iteration 16, loss = 0.62959846\n",
      "Iteration 17, loss = 0.62509426\n",
      "Iteration 18, loss = 0.62057477\n",
      "Iteration 19, loss = 0.61617090\n",
      "Iteration 20, loss = 0.61175481\n",
      "Iteration 21, loss = 0.60752656\n",
      "Iteration 22, loss = 0.60332333\n",
      "Iteration 23, loss = 0.59915736\n",
      "Iteration 24, loss = 0.59521807\n",
      "Iteration 25, loss = 0.59114675\n",
      "Iteration 26, loss = 0.58725291\n",
      "Iteration 27, loss = 0.58336859\n",
      "Iteration 28, loss = 0.57955505\n",
      "Iteration 29, loss = 0.57587139\n",
      "Iteration 30, loss = 0.57214972\n",
      "Iteration 31, loss = 0.56851302\n",
      "Iteration 32, loss = 0.56497795\n",
      "Iteration 33, loss = 0.56141042\n",
      "Iteration 34, loss = 0.55793683\n",
      "Iteration 35, loss = 0.55446543\n",
      "Iteration 36, loss = 0.55112841\n",
      "Iteration 37, loss = 0.54772033\n",
      "Iteration 38, loss = 0.54440683\n",
      "Iteration 39, loss = 0.54119498\n",
      "Iteration 40, loss = 0.53800420\n",
      "Iteration 41, loss = 0.53480592\n",
      "Iteration 42, loss = 0.53155536\n",
      "Iteration 43, loss = 0.52849239\n",
      "Iteration 44, loss = 0.52550898\n",
      "Iteration 45, loss = 0.52260808\n",
      "Iteration 46, loss = 0.51941357\n",
      "Iteration 47, loss = 0.51661885\n",
      "Iteration 48, loss = 0.51363177\n",
      "Iteration 49, loss = 0.51079534\n",
      "Iteration 50, loss = 0.50809460\n",
      "Iteration 51, loss = 0.50520778\n",
      "Iteration 52, loss = 0.50253590\n",
      "Iteration 53, loss = 0.49985180\n",
      "Iteration 54, loss = 0.49725740\n",
      "Iteration 55, loss = 0.49460129\n",
      "Iteration 56, loss = 0.49208631\n",
      "Iteration 57, loss = 0.48955226\n",
      "Iteration 58, loss = 0.48701963\n",
      "Iteration 59, loss = 0.48457641\n",
      "Iteration 60, loss = 0.48220353\n",
      "Iteration 61, loss = 0.47988196\n",
      "Iteration 62, loss = 0.47745803\n",
      "Iteration 63, loss = 0.47523708\n",
      "Iteration 64, loss = 0.47300417\n",
      "Iteration 65, loss = 0.47074008\n",
      "Iteration 66, loss = 0.46856828\n",
      "Iteration 67, loss = 0.46644623\n",
      "Iteration 68, loss = 0.46435711\n",
      "Iteration 69, loss = 0.46226796\n",
      "Iteration 70, loss = 0.46019863\n",
      "Iteration 71, loss = 0.45834266\n",
      "Iteration 72, loss = 0.45633681\n",
      "Iteration 73, loss = 0.45437040\n",
      "Iteration 74, loss = 0.45258045\n",
      "Iteration 75, loss = 0.45066556\n",
      "Iteration 76, loss = 0.44886168\n",
      "Iteration 77, loss = 0.44708702\n",
      "Iteration 78, loss = 0.44535920\n",
      "Iteration 79, loss = 0.44364563\n",
      "Iteration 80, loss = 0.44194164\n",
      "Iteration 81, loss = 0.44037107\n",
      "Iteration 82, loss = 0.43866621\n",
      "Iteration 83, loss = 0.43710747\n",
      "Iteration 84, loss = 0.43562201\n",
      "Iteration 85, loss = 0.43408484\n",
      "Iteration 86, loss = 0.43255094\n",
      "Iteration 87, loss = 0.43121911\n",
      "Iteration 88, loss = 0.42977229\n",
      "Iteration 89, loss = 0.42830287\n",
      "Iteration 90, loss = 0.42696172\n",
      "Iteration 91, loss = 0.42559255\n",
      "Iteration 92, loss = 0.42431807\n",
      "Iteration 93, loss = 0.42304848\n",
      "Iteration 94, loss = 0.42174136\n",
      "Iteration 95, loss = 0.42059758\n",
      "Iteration 96, loss = 0.41941059\n",
      "Iteration 97, loss = 0.41815694\n",
      "Iteration 98, loss = 0.41702335\n",
      "Iteration 99, loss = 0.41593292\n",
      "Iteration 100, loss = 0.41485450\n",
      "Iteration 101, loss = 0.41373845\n",
      "Iteration 102, loss = 0.41265231\n",
      "Iteration 103, loss = 0.41163399\n",
      "Iteration 104, loss = 0.41059694\n",
      "Iteration 105, loss = 0.40968334\n",
      "Iteration 106, loss = 0.40866748\n",
      "Iteration 107, loss = 0.40769894\n",
      "Iteration 108, loss = 0.40676461\n",
      "Iteration 109, loss = 0.40584943\n",
      "Iteration 110, loss = 0.40496457\n",
      "Iteration 111, loss = 0.40408258\n",
      "Iteration 112, loss = 0.40320539\n",
      "Iteration 113, loss = 0.40237905\n",
      "Iteration 114, loss = 0.40150276\n",
      "Iteration 115, loss = 0.40069259\n",
      "Iteration 116, loss = 0.39994966\n",
      "Iteration 117, loss = 0.39914563\n",
      "Iteration 118, loss = 0.39840945\n",
      "Iteration 119, loss = 0.39758265\n",
      "Iteration 120, loss = 0.39680987\n",
      "Iteration 121, loss = 0.39611686\n",
      "Iteration 122, loss = 0.39543605\n",
      "Iteration 123, loss = 0.39472059\n",
      "Iteration 124, loss = 0.39397858\n",
      "Iteration 125, loss = 0.39333681\n",
      "Iteration 126, loss = 0.39270756\n",
      "Iteration 127, loss = 0.39199518\n",
      "Iteration 128, loss = 0.39138587\n",
      "Iteration 129, loss = 0.39073461\n",
      "Iteration 130, loss = 0.39009054\n",
      "Iteration 131, loss = 0.38948665\n",
      "Iteration 132, loss = 0.38890814\n",
      "Iteration 133, loss = 0.38831941\n",
      "Iteration 134, loss = 0.38773027\n",
      "Iteration 135, loss = 0.38712830\n",
      "Iteration 136, loss = 0.38657073\n",
      "Iteration 137, loss = 0.38601243\n",
      "Iteration 138, loss = 0.38550702\n",
      "Iteration 139, loss = 0.38491640\n",
      "Iteration 140, loss = 0.38440912\n",
      "Iteration 141, loss = 0.38392524\n",
      "Iteration 142, loss = 0.38338838\n",
      "Iteration 1064, loss = 0.27098952\n",
      "Iteration 1065, loss = 0.27084494\n",
      "Iteration 1066, loss = 0.27075305\n",
      "Iteration 1067, loss = 0.27061482\n",
      "Iteration 1068, loss = 0.27050399\n",
      "Iteration 1069, loss = 0.27040696\n",
      "Iteration 1070, loss = 0.27022990\n",
      "Iteration 1071, loss = 0.27011926\n",
      "Iteration 1072, loss = 0.26998309\n",
      "Iteration 1073, loss = 0.26988450\n",
      "Iteration 1074, loss = 0.26974134\n",
      "Iteration 1075, loss = 0.26961958\n",
      "Iteration 1076, loss = 0.26948292\n",
      "Iteration 1077, loss = 0.26940317\n",
      "Iteration 1078, loss = 0.26933891\n",
      "Iteration 1079, loss = 0.26915752\n",
      "Iteration 1080, loss = 0.26899225\n",
      "Iteration 1081, loss = 0.26891216\n",
      "Iteration 1082, loss = 0.26873938\n",
      "Iteration 1083, loss = 0.26863855\n",
      "Iteration 1084, loss = 0.26853790\n",
      "Iteration 1085, loss = 0.26841918\n",
      "Iteration 1086, loss = 0.26824338\n",
      "Iteration 1087, loss = 0.26813600\n",
      "Iteration 1088, loss = 0.26800614\n",
      "Iteration 1089, loss = 0.26789387\n",
      "Iteration 1090, loss = 0.26775110\n",
      "Iteration 1091, loss = 0.26764766\n",
      "Iteration 1092, loss = 0.26753150\n",
      "Iteration 1093, loss = 0.26737440\n",
      "Iteration 1094, loss = 0.26728385\n",
      "Iteration 1095, loss = 0.26712729\n",
      "Iteration 1096, loss = 0.26700161\n",
      "Iteration 1097, loss = 0.26690420\n",
      "Iteration 1098, loss = 0.26686236\n",
      "Iteration 1099, loss = 0.26665412\n",
      "Iteration 1100, loss = 0.26652199\n",
      "Iteration 1101, loss = 0.26640285\n",
      "Iteration 1102, loss = 0.26628220\n",
      "Iteration 1103, loss = 0.26616092\n",
      "Iteration 1104, loss = 0.26600350\n",
      "Iteration 1105, loss = 0.26589527\n",
      "Iteration 1106, loss = 0.26578495\n",
      "Iteration 1107, loss = 0.26563264\n",
      "Iteration 1108, loss = 0.26554440\n",
      "Iteration 1109, loss = 0.26544082\n",
      "Iteration 1110, loss = 0.26528068\n",
      "Iteration 1111, loss = 0.26516497\n",
      "Iteration 1112, loss = 0.26503995\n",
      "Iteration 1113, loss = 0.26499477\n",
      "Iteration 1114, loss = 0.26480411\n",
      "Iteration 1115, loss = 0.26466807\n",
      "Iteration 1116, loss = 0.26456382\n",
      "Iteration 1117, loss = 0.26442404\n",
      "Iteration 1118, loss = 0.26428978\n",
      "Iteration 1119, loss = 0.26420983\n",
      "Iteration 1120, loss = 0.26408552\n",
      "Iteration 1121, loss = 0.26392821\n",
      "Iteration 1122, loss = 0.26384956\n",
      "Iteration 1123, loss = 0.26366963\n",
      "Iteration 1124, loss = 0.26359963\n",
      "Iteration 1125, loss = 0.26359802\n",
      "Iteration 1126, loss = 0.26330263\n",
      "Iteration 1127, loss = 0.26320962\n",
      "Iteration 1128, loss = 0.26313265\n",
      "Iteration 1129, loss = 0.26295043\n",
      "Iteration 1130, loss = 0.26282712\n",
      "Iteration 1131, loss = 0.26268363\n",
      "Iteration 1132, loss = 0.26256084\n",
      "Iteration 1133, loss = 0.26245726\n",
      "Iteration 1134, loss = 0.26232658\n",
      "Iteration 1135, loss = 0.26222455\n",
      "Iteration 1136, loss = 0.26209542\n",
      "Iteration 1137, loss = 0.26198955\n",
      "Iteration 1138, loss = 0.26192650\n",
      "Iteration 1139, loss = 0.26170418\n",
      "Iteration 1140, loss = 0.26160558\n",
      "Iteration 1141, loss = 0.26152269\n",
      "Iteration 1142, loss = 0.26134326\n",
      "Iteration 1143, loss = 0.26123768\n",
      "Iteration 1144, loss = 0.26109731\n",
      "Iteration 1145, loss = 0.26097164\n",
      "Iteration 1146, loss = 0.26086947\n",
      "Iteration 1147, loss = 0.26074220\n",
      "Iteration 1148, loss = 0.26075295\n",
      "Iteration 1149, loss = 0.26045442\n",
      "Iteration 1150, loss = 0.26034910\n",
      "Iteration 1151, loss = 0.26024606\n",
      "Iteration 1152, loss = 0.26008958\n",
      "Iteration 1153, loss = 0.25997445\n",
      "Iteration 1154, loss = 0.25991110\n",
      "Iteration 1155, loss = 0.25972405\n",
      "Iteration 1156, loss = 0.25959506\n",
      "Iteration 1157, loss = 0.25949179\n",
      "Iteration 1158, loss = 0.25937229\n",
      "Iteration 1159, loss = 0.25921237\n",
      "Iteration 1160, loss = 0.25916739\n",
      "Iteration 1161, loss = 0.25901641\n",
      "Iteration 1162, loss = 0.25887577\n",
      "Iteration 1163, loss = 0.25884765\n",
      "Iteration 1164, loss = 0.25862399\n",
      "Iteration 1165, loss = 0.25849338\n",
      "Iteration 1166, loss = 0.25837353\n",
      "Iteration 1167, loss = 0.25828729\n",
      "Iteration 1168, loss = 0.25821085\n",
      "Iteration 1169, loss = 0.25803769\n",
      "Iteration 1170, loss = 0.25793534\n",
      "Iteration 1171, loss = 0.25779593\n",
      "Iteration 1172, loss = 0.25765287\n",
      "Iteration 1173, loss = 0.25755355\n",
      "Iteration 1174, loss = 0.25738322\n",
      "Iteration 1175, loss = 0.25728712\n",
      "Iteration 1176, loss = 0.25714495\n",
      "Iteration 1177, loss = 0.25700490\n",
      "Iteration 1178, loss = 0.25691211\n",
      "Iteration 1179, loss = 0.25679184\n",
      "Iteration 1180, loss = 0.25668533\n",
      "Iteration 1181, loss = 0.25654639\n",
      "Iteration 1182, loss = 0.25647381\n",
      "Iteration 1183, loss = 0.25631253\n",
      "Iteration 1184, loss = 0.25616748\n",
      "Iteration 1185, loss = 0.25609160\n",
      "Iteration 1186, loss = 0.25593133\n",
      "Iteration 1187, loss = 0.25582952\n",
      "Iteration 1188, loss = 0.25566871\n",
      "Iteration 1189, loss = 0.25555402\n",
      "Iteration 1190, loss = 0.25544852\n",
      "Iteration 1191, loss = 0.25535085\n",
      "Iteration 1192, loss = 0.25522498\n",
      "Iteration 1193, loss = 0.25505335\n",
      "Iteration 1194, loss = 0.25495899\n",
      "Iteration 1195, loss = 0.25485843\n",
      "Iteration 1196, loss = 0.25472566\n",
      "Iteration 1197, loss = 0.25458887\n",
      "Iteration 1198, loss = 0.25444983\n",
      "Iteration 1199, loss = 0.25433973\n",
      "Iteration 1200, loss = 0.25424188\n",
      "Iteration 1201, loss = 0.25410505\n",
      "Iteration 1202, loss = 0.25395438\n",
      "Iteration 1203, loss = 0.25385543\n",
      "Iteration 1204, loss = 0.25373161\n",
      "Iteration 1205, loss = 0.25359971\n",
      "Iteration 1206, loss = 0.25345975\n",
      "Iteration 1207, loss = 0.25337347\n",
      "Iteration 1208, loss = 0.25326528\n",
      "Iteration 1209, loss = 0.25311039\n",
      "Iteration 1210, loss = 0.25302526\n",
      "Iteration 1211, loss = 0.25286919\n",
      "Iteration 1212, loss = 0.25276717\n",
      "Iteration 1213, loss = 0.25263632\n",
      "Iteration 1214, loss = 0.25250165\n",
      "Iteration 1215, loss = 0.25253194\n",
      "Iteration 1216, loss = 0.25227297\n",
      "Iteration 1217, loss = 0.25213097\n",
      "Iteration 1218, loss = 0.25202847\n",
      "Iteration 1219, loss = 0.25188197\n",
      "Iteration 1220, loss = 0.25179494\n",
      "Iteration 1221, loss = 0.25165204\n",
      "Iteration 1222, loss = 0.25158495\n",
      "Iteration 1223, loss = 0.25137067\n",
      "Iteration 1224, loss = 0.25128511\n",
      "Iteration 1225, loss = 0.25113364\n",
      "Iteration 1226, loss = 0.25107038\n",
      "Iteration 1227, loss = 0.25097071\n",
      "Iteration 1228, loss = 0.25078612\n",
      "Iteration 1229, loss = 0.25065437\n",
      "Iteration 1230, loss = 0.25054638\n",
      "Iteration 1231, loss = 0.25042882\n",
      "Iteration 1232, loss = 0.25027026\n",
      "Iteration 1233, loss = 0.25019175\n",
      "Iteration 1234, loss = 0.25008349\n",
      "Iteration 1235, loss = 0.24992824\n",
      "Iteration 1236, loss = 0.24980137\n",
      "Iteration 1237, loss = 0.24968326\n",
      "Iteration 1238, loss = 0.24954252\n",
      "Iteration 1239, loss = 0.24946292\n",
      "Iteration 1240, loss = 0.24932795\n",
      "Iteration 1241, loss = 0.24919746\n",
      "Iteration 1242, loss = 0.24907145\n",
      "Iteration 1243, loss = 0.24896268\n",
      "Iteration 1244, loss = 0.24888410\n",
      "Iteration 1245, loss = 0.24870070\n",
      "Iteration 1246, loss = 0.24858311\n",
      "Iteration 1247, loss = 0.24851210\n",
      "Iteration 1248, loss = 0.24835364\n",
      "Iteration 1249, loss = 0.24829029\n",
      "Iteration 1250, loss = 0.24812294\n",
      "Iteration 1251, loss = 0.24799241\n",
      "Iteration 1252, loss = 0.24785632\n",
      "Iteration 1253, loss = 0.24775813\n",
      "Iteration 1254, loss = 0.24763578\n",
      "Iteration 1255, loss = 0.24749727\n",
      "Iteration 1256, loss = 0.24736538\n",
      "Iteration 1257, loss = 0.24728037\n",
      "Iteration 1258, loss = 0.24715610\n",
      "Iteration 1259, loss = 0.24705186\n",
      "Iteration 1260, loss = 0.24690403\n",
      "Iteration 1261, loss = 0.24679019\n",
      "Iteration 1262, loss = 0.24667516\n",
      "Iteration 1263, loss = 0.24655801\n",
      "Iteration 1264, loss = 0.24642829\n",
      "Iteration 1265, loss = 0.24634230\n",
      "Iteration 1266, loss = 0.24619304\n",
      "Iteration 1267, loss = 0.24607121\n",
      "Iteration 1268, loss = 0.24599149\n",
      "Iteration 1269, loss = 0.24582644\n",
      "Iteration 1270, loss = 0.24584321\n",
      "Iteration 1271, loss = 0.24560246\n",
      "Iteration 1272, loss = 0.24547590\n",
      "Iteration 1273, loss = 0.24534872\n",
      "Iteration 1274, loss = 0.24522554\n",
      "Iteration 1275, loss = 0.24513747\n",
      "Iteration 1276, loss = 0.24502881\n",
      "Iteration 1277, loss = 0.24487827\n",
      "Iteration 1278, loss = 0.24479325\n",
      "Iteration 1279, loss = 0.24462977\n",
      "Iteration 1280, loss = 0.24457547\n",
      "Iteration 1281, loss = 0.24437924\n",
      "Iteration 1282, loss = 0.24432574\n",
      "Iteration 1283, loss = 0.24419926\n",
      "Iteration 1284, loss = 0.24403791\n",
      "Iteration 1285, loss = 0.24394369\n",
      "Iteration 1286, loss = 0.24379639\n",
      "Iteration 1287, loss = 0.24376674\n",
      "Iteration 1288, loss = 0.24361165\n",
      "Iteration 1289, loss = 0.24347530\n",
      "Iteration 1290, loss = 0.24336506\n",
      "Iteration 1291, loss = 0.24321233\n",
      "Iteration 1292, loss = 0.24312953\n",
      "Iteration 1293, loss = 0.24297785\n",
      "Iteration 1294, loss = 0.24289005\n",
      "Iteration 1295, loss = 0.24287160\n",
      "Iteration 1296, loss = 0.24260006\n",
      "Iteration 1297, loss = 0.24251931\n",
      "Iteration 1298, loss = 0.24244678\n",
      "Iteration 1299, loss = 0.24240873\n",
      "Iteration 1300, loss = 0.24218360\n",
      "Iteration 1301, loss = 0.24205861\n",
      "Iteration 1302, loss = 0.24191693\n",
      "Iteration 1303, loss = 0.24182454\n",
      "Iteration 1304, loss = 0.24168899\n",
      "Iteration 1305, loss = 0.24157926\n",
      "Iteration 1306, loss = 0.24144800\n",
      "Iteration 1307, loss = 0.24131477\n",
      "Iteration 1308, loss = 0.24123387\n",
      "Iteration 1309, loss = 0.24112136\n",
      "Iteration 1310, loss = 0.24097773\n",
      "Iteration 1311, loss = 0.24088099\n",
      "Iteration 1312, loss = 0.24077269\n",
      "Iteration 1313, loss = 0.24061764\n",
      "Iteration 1314, loss = 0.24050590\n",
      "Iteration 1315, loss = 0.24041075\n",
      "Iteration 1316, loss = 0.24030481\n",
      "Iteration 1317, loss = 0.24018333\n",
      "Iteration 1318, loss = 0.24009104\n",
      "Iteration 1319, loss = 0.24000188\n",
      "Iteration 1320, loss = 0.23980086\n",
      "Iteration 1321, loss = 0.23971069\n",
      "Iteration 1322, loss = 0.23959572\n",
      "Iteration 1323, loss = 0.23948102\n",
      "Iteration 1324, loss = 0.23934506\n",
      "Iteration 1325, loss = 0.23922809\n",
      "Iteration 1326, loss = 0.23912450\n",
      "Iteration 1327, loss = 0.23898184\n",
      "Iteration 1328, loss = 0.23892946\n",
      "Iteration 1329, loss = 0.23879508\n",
      "Iteration 1330, loss = 0.23866883\n",
      "Iteration 1331, loss = 0.23853381\n",
      "Iteration 1332, loss = 0.23842033\n",
      "Iteration 1333, loss = 0.23827317\n",
      "Iteration 1334, loss = 0.23819379\n",
      "Iteration 1335, loss = 0.23805555\n",
      "Iteration 1336, loss = 0.23794510\n",
      "Iteration 1337, loss = 0.23780843\n",
      "Iteration 1338, loss = 0.23772177\n",
      "Iteration 1339, loss = 0.23758139\n",
      "Iteration 1340, loss = 0.23746920\n",
      "Iteration 1341, loss = 0.23734325\n",
      "Iteration 1342, loss = 0.23724366\n",
      "Iteration 1343, loss = 0.23711589\n",
      "Iteration 1344, loss = 0.23700666\n",
      "Iteration 1345, loss = 0.23691878\n",
      "Iteration 1346, loss = 0.23677445\n",
      "Iteration 1347, loss = 0.23665017\n",
      "Iteration 1348, loss = 0.23651314\n",
      "Iteration 1349, loss = 0.23641396\n",
      "Iteration 1350, loss = 0.23630818\n",
      "Iteration 1351, loss = 0.23618716\n",
      "Iteration 1352, loss = 0.23609107\n",
      "Iteration 1353, loss = 0.23594523\n",
      "Iteration 1354, loss = 0.23580687\n",
      "Iteration 1355, loss = 0.23569717\n",
      "Iteration 1356, loss = 0.23559771\n",
      "Iteration 1357, loss = 0.23552468\n",
      "Iteration 1358, loss = 0.23537811\n",
      "Iteration 1359, loss = 0.23523710\n",
      "Iteration 1360, loss = 0.23513593\n",
      "Iteration 1361, loss = 0.23498558\n",
      "Iteration 1362, loss = 0.23488067\n",
      "Iteration 1363, loss = 0.23479726\n",
      "Iteration 1364, loss = 0.23465365\n",
      "Iteration 1365, loss = 0.23454635\n",
      "Iteration 1366, loss = 0.23441051\n",
      "Iteration 1367, loss = 0.23427754\n",
      "Iteration 1368, loss = 0.23420053\n",
      "Iteration 1369, loss = 0.23410799\n",
      "Iteration 1370, loss = 0.23394501\n",
      "Iteration 1371, loss = 0.23380686\n",
      "Iteration 1372, loss = 0.23380543\n",
      "Iteration 1373, loss = 0.23362107\n",
      "Iteration 1374, loss = 0.23348149\n",
      "Iteration 1375, loss = 0.23335273\n",
      "Iteration 1376, loss = 0.23327515\n",
      "Iteration 1377, loss = 0.23310793\n",
      "Iteration 1378, loss = 0.23302639\n",
      "Iteration 1379, loss = 0.23290693\n",
      "Iteration 1380, loss = 0.23280398\n",
      "Iteration 1381, loss = 0.23266372\n",
      "Iteration 1382, loss = 0.23259492\n",
      "Iteration 1383, loss = 0.23248203\n",
      "Iteration 1384, loss = 0.23235823\n",
      "Iteration 1385, loss = 0.23222261\n",
      "Iteration 1386, loss = 0.23213803\n",
      "Iteration 1387, loss = 0.23196192\n",
      "Iteration 1388, loss = 0.23185068\n",
      "Iteration 1389, loss = 0.23179711\n",
      "Iteration 1390, loss = 0.23163789\n",
      "Iteration 1391, loss = 0.23153873\n",
      "Iteration 1392, loss = 0.23137719\n",
      "Iteration 1393, loss = 0.23134054\n",
      "Iteration 1394, loss = 0.23121661\n",
      "Iteration 1395, loss = 0.23108371\n",
      "Iteration 1396, loss = 0.23093770\n",
      "Iteration 1397, loss = 0.23085852\n",
      "Iteration 1398, loss = 0.23072636\n",
      "Iteration 1399, loss = 0.23057697\n",
      "Iteration 1400, loss = 0.23054132\n",
      "Iteration 1401, loss = 0.23037982\n",
      "Iteration 1402, loss = 0.23027317\n",
      "Iteration 1403, loss = 0.23017676\n",
      "Iteration 1404, loss = 0.23000280\n",
      "Iteration 1405, loss = 0.22990628\n",
      "Iteration 1406, loss = 0.22982212\n",
      "Iteration 1407, loss = 0.22974134\n",
      "Iteration 1408, loss = 0.22964928\n",
      "Iteration 1409, loss = 0.22942801\n",
      "Iteration 1410, loss = 0.22934797\n",
      "Iteration 1411, loss = 0.22923752\n",
      "Iteration 1412, loss = 0.22918661\n",
      "Iteration 1413, loss = 0.22904994\n",
      "Iteration 1414, loss = 0.22890286\n",
      "Iteration 1415, loss = 0.22881615\n",
      "Iteration 1416, loss = 0.22868865\n",
      "Iteration 1417, loss = 0.22855338\n",
      "Iteration 1418, loss = 0.22847258\n",
      "Iteration 1419, loss = 0.22834263\n",
      "Iteration 1420, loss = 0.22822950\n",
      "Iteration 1421, loss = 0.22808679\n",
      "Iteration 1422, loss = 0.22798585\n",
      "Iteration 1423, loss = 0.22788232\n",
      "Iteration 1424, loss = 0.22778063\n",
      "Iteration 1425, loss = 0.22770026\n",
      "Iteration 1426, loss = 0.22751807\n",
      "Iteration 1427, loss = 0.22745586\n",
      "Iteration 1428, loss = 0.22735954\n",
      "Iteration 1429, loss = 0.22723424\n",
      "Iteration 1430, loss = 0.22723582\n",
      "Iteration 1431, loss = 0.22705271\n",
      "Iteration 1432, loss = 0.22686165\n",
      "Iteration 1433, loss = 0.22678241\n",
      "Iteration 1434, loss = 0.22662863\n",
      "Iteration 1435, loss = 0.22656704\n",
      "Iteration 1436, loss = 0.22644394\n",
      "Iteration 1437, loss = 0.22632757\n",
      "Iteration 1438, loss = 0.22620939\n",
      "Iteration 1439, loss = 0.22609395\n",
      "Iteration 1440, loss = 0.22594151\n",
      "Iteration 1441, loss = 0.22585825\n",
      "Iteration 1442, loss = 0.22572176\n",
      "Iteration 1443, loss = 0.22562203\n",
      "Iteration 1444, loss = 0.22551822\n",
      "Iteration 1445, loss = 0.22544586\n",
      "Iteration 1446, loss = 0.22528924\n",
      "Iteration 1447, loss = 0.22513165\n",
      "Iteration 1448, loss = 0.22506372\n",
      "Iteration 1449, loss = 0.22493478\n",
      "Iteration 1450, loss = 0.22483558\n",
      "Iteration 1451, loss = 0.22472304\n",
      "Iteration 1452, loss = 0.22457164\n",
      "Iteration 1453, loss = 0.22445685\n",
      "Iteration 1454, loss = 0.22433483\n",
      "Iteration 1455, loss = 0.22421956\n",
      "Iteration 1456, loss = 0.22413879\n",
      "Iteration 1457, loss = 0.22401200\n",
      "Iteration 1458, loss = 0.22388801\n",
      "Iteration 1459, loss = 0.22380744\n",
      "Iteration 1460, loss = 0.22370538\n",
      "Iteration 1461, loss = 0.22355877\n",
      "Iteration 1462, loss = 0.22346831\n",
      "Iteration 1463, loss = 0.22334323\n",
      "Iteration 1464, loss = 0.22327704\n",
      "Iteration 1465, loss = 0.22312078\n",
      "Iteration 1466, loss = 0.22302681\n",
      "Iteration 1467, loss = 0.22301786\n",
      "Iteration 1468, loss = 0.22280513\n",
      "Iteration 1469, loss = 0.22269641\n",
      "Iteration 1470, loss = 0.22267593\n",
      "Iteration 1471, loss = 0.22247358\n",
      "Iteration 1472, loss = 0.22237121\n",
      "Iteration 1473, loss = 0.22229421\n",
      "Iteration 1474, loss = 0.22211334\n",
      "Iteration 1475, loss = 0.22205603\n",
      "Iteration 1476, loss = 0.22190108\n",
      "Iteration 1477, loss = 0.22178943\n",
      "Iteration 1478, loss = 0.22168248\n",
      "Iteration 1479, loss = 0.22157569\n",
      "Iteration 1480, loss = 0.22144806\n",
      "Iteration 1481, loss = 0.22134261\n",
      "Iteration 1482, loss = 0.22125154\n",
      "Iteration 1483, loss = 0.22114348\n",
      "Iteration 1484, loss = 0.22099683\n",
      "Iteration 1485, loss = 0.22098858\n",
      "Iteration 1486, loss = 0.22077637\n",
      "Iteration 1487, loss = 0.22064893\n",
      "Iteration 1488, loss = 0.22056203\n",
      "Iteration 1489, loss = 0.22045175\n",
      "Iteration 1490, loss = 0.22033679\n",
      "Iteration 1491, loss = 0.22023897\n",
      "Iteration 1492, loss = 0.22010183\n",
      "Iteration 1493, loss = 0.21997934\n",
      "Iteration 1494, loss = 0.21990473\n",
      "Iteration 1495, loss = 0.21979490\n",
      "Iteration 1496, loss = 0.21967380\n",
      "Iteration 1497, loss = 0.21955250\n",
      "Iteration 1498, loss = 0.21945410\n",
      "Iteration 1499, loss = 0.21938728\n",
      "Iteration 1500, loss = 0.21920534\n",
      "Iteration 1501, loss = 0.21909899\n",
      "Iteration 1502, loss = 0.21899365\n",
      "Iteration 1503, loss = 0.21887939\n",
      "Iteration 1504, loss = 0.21875474\n",
      "Iteration 1505, loss = 0.21863181\n",
      "Iteration 1506, loss = 0.21854272\n",
      "Iteration 1507, loss = 0.21843091\n",
      "Iteration 1508, loss = 0.21831900\n",
      "Iteration 1509, loss = 0.21825968\n",
      "Iteration 1510, loss = 0.21811144\n",
      "Iteration 1511, loss = 0.21799656\n",
      "Iteration 1512, loss = 0.21787190\n",
      "Iteration 1513, loss = 0.21775341\n",
      "Iteration 1514, loss = 0.21773817\n",
      "Iteration 1515, loss = 0.21755024\n",
      "Iteration 1516, loss = 0.21743677\n",
      "Iteration 1517, loss = 0.21733731\n",
      "Iteration 1518, loss = 0.21719958\n",
      "Iteration 1519, loss = 0.21714443\n",
      "Iteration 1520, loss = 0.21699181\n",
      "Iteration 1521, loss = 0.21696211\n",
      "Iteration 1522, loss = 0.21678350\n",
      "Iteration 1523, loss = 0.21666370\n",
      "Iteration 1524, loss = 0.21655897\n",
      "Iteration 1525, loss = 0.21640956\n",
      "Iteration 1526, loss = 0.21631077\n",
      "Iteration 1527, loss = 0.21624427\n",
      "Iteration 1528, loss = 0.21613321\n",
      "Iteration 1529, loss = 0.21597411\n",
      "Iteration 1530, loss = 0.21589855\n",
      "Iteration 1531, loss = 0.21576984\n",
      "Iteration 1532, loss = 0.21565665\n",
      "Iteration 1533, loss = 0.21555411\n",
      "Iteration 1534, loss = 0.21544904\n",
      "Iteration 1535, loss = 0.21531331\n",
      "Iteration 1536, loss = 0.21522746\n",
      "Iteration 1537, loss = 0.21511594\n",
      "Iteration 1538, loss = 0.21512954\n",
      "Iteration 1539, loss = 0.21488998\n",
      "Iteration 1540, loss = 0.21479253\n",
      "Iteration 1541, loss = 0.21478864\n",
      "Iteration 1542, loss = 0.21459038\n",
      "Iteration 1543, loss = 0.21447887\n",
      "Iteration 575, loss = 0.30189403\n",
      "Iteration 576, loss = 0.30179640\n",
      "Iteration 577, loss = 0.30174701\n",
      "Iteration 578, loss = 0.30159688\n",
      "Iteration 579, loss = 0.30150585\n",
      "Iteration 580, loss = 0.30142177\n",
      "Iteration 581, loss = 0.30133979\n",
      "Iteration 582, loss = 0.30124275\n",
      "Iteration 583, loss = 0.30116650\n",
      "Iteration 584, loss = 0.30103571\n",
      "Iteration 585, loss = 0.30093869\n",
      "Iteration 586, loss = 0.30085602\n",
      "Iteration 587, loss = 0.30075024\n",
      "Iteration 588, loss = 0.30065464\n",
      "Iteration 589, loss = 0.30057647\n",
      "Iteration 590, loss = 0.30049450\n",
      "Iteration 591, loss = 0.30040059\n",
      "Iteration 592, loss = 0.30031123\n",
      "Iteration 593, loss = 0.30021502\n",
      "Iteration 594, loss = 0.30010329\n",
      "Iteration 595, loss = 0.30001949\n",
      "Iteration 596, loss = 0.29994137\n",
      "Iteration 597, loss = 0.29983404\n",
      "Iteration 598, loss = 0.29976074\n",
      "Iteration 599, loss = 0.29965576\n",
      "Iteration 600, loss = 0.29956208\n",
      "Iteration 601, loss = 0.29950024\n",
      "Iteration 602, loss = 0.29934283\n",
      "Iteration 603, loss = 0.29927015\n",
      "Iteration 604, loss = 0.29917989\n",
      "Iteration 605, loss = 0.29911723\n",
      "Iteration 606, loss = 0.29897463\n",
      "Iteration 607, loss = 0.29891305\n",
      "Iteration 608, loss = 0.29880768\n",
      "Iteration 609, loss = 0.29869112\n",
      "Iteration 610, loss = 0.29862759\n",
      "Iteration 611, loss = 0.29851332\n",
      "Iteration 612, loss = 0.29839583\n",
      "Iteration 613, loss = 0.29837113\n",
      "Iteration 614, loss = 0.29824369\n",
      "Iteration 615, loss = 0.29814814\n",
      "Iteration 616, loss = 0.29805288\n",
      "Iteration 617, loss = 0.29793010\n",
      "Iteration 618, loss = 0.29785884\n",
      "Iteration 619, loss = 0.29777064\n",
      "Iteration 620, loss = 0.29768532\n",
      "Iteration 621, loss = 0.29756204\n",
      "Iteration 622, loss = 0.29747610\n",
      "Iteration 623, loss = 0.29738739\n",
      "Iteration 624, loss = 0.29730457\n",
      "Iteration 625, loss = 0.29722089\n",
      "Iteration 626, loss = 0.29710448\n",
      "Iteration 627, loss = 0.29699333\n",
      "Iteration 628, loss = 0.29691416\n",
      "Iteration 629, loss = 0.29682066\n",
      "Iteration 630, loss = 0.29673497\n",
      "Iteration 631, loss = 0.29666135\n",
      "Iteration 632, loss = 0.29652514\n",
      "Iteration 633, loss = 0.29645286\n",
      "Iteration 634, loss = 0.29635164\n",
      "Iteration 635, loss = 0.29625274\n",
      "Iteration 636, loss = 0.29618850\n",
      "Iteration 637, loss = 0.29606626\n",
      "Iteration 638, loss = 0.29595958\n",
      "Iteration 639, loss = 0.29591678\n",
      "Iteration 640, loss = 0.29579890\n",
      "Iteration 641, loss = 0.29569291\n",
      "Iteration 642, loss = 0.29560976\n",
      "Iteration 643, loss = 0.29553363\n",
      "Iteration 644, loss = 0.29542852\n",
      "Iteration 645, loss = 0.29533507\n",
      "Iteration 646, loss = 0.29522719\n",
      "Iteration 647, loss = 0.29515512\n",
      "Iteration 648, loss = 0.29505446\n",
      "Iteration 649, loss = 0.29503927\n",
      "Iteration 650, loss = 0.29485391\n",
      "Iteration 651, loss = 0.29476414\n",
      "Iteration 652, loss = 0.29474375\n",
      "Iteration 653, loss = 0.29459684\n",
      "Iteration 654, loss = 0.29447868\n",
      "Iteration 655, loss = 0.29442280\n",
      "Iteration 656, loss = 0.29431022\n",
      "Iteration 657, loss = 0.29427620\n",
      "Iteration 658, loss = 0.29413586\n",
      "Iteration 659, loss = 0.29405207\n",
      "Iteration 660, loss = 0.29398271\n",
      "Iteration 661, loss = 0.29386102\n",
      "Iteration 662, loss = 0.29378694\n",
      "Iteration 663, loss = 0.29370287\n",
      "Iteration 664, loss = 0.29358090\n",
      "Iteration 665, loss = 0.29350342\n",
      "Iteration 666, loss = 0.29344377\n",
      "Iteration 667, loss = 0.29330579\n",
      "Iteration 668, loss = 0.29322786\n",
      "Iteration 669, loss = 0.29313289\n",
      "Iteration 670, loss = 0.29303701\n",
      "Iteration 671, loss = 0.29296807\n",
      "Iteration 672, loss = 0.29286590\n",
      "Iteration 673, loss = 0.29280661\n",
      "Iteration 674, loss = 0.29269606\n",
      "Iteration 675, loss = 0.29261398\n",
      "Iteration 676, loss = 0.29252701\n",
      "Iteration 677, loss = 0.29241776\n",
      "Iteration 678, loss = 0.29231833\n",
      "Iteration 679, loss = 0.29230902\n",
      "Iteration 680, loss = 0.29214737\n",
      "Iteration 681, loss = 0.29205936\n",
      "Iteration 682, loss = 0.29196512\n",
      "Iteration 683, loss = 0.29194335\n",
      "Iteration 684, loss = 0.29181562\n",
      "Iteration 685, loss = 0.29172425\n",
      "Iteration 686, loss = 0.29162527\n",
      "Iteration 687, loss = 0.29154798\n",
      "Iteration 688, loss = 0.29149157\n",
      "Iteration 689, loss = 0.29135967\n",
      "Iteration 690, loss = 0.29126427\n",
      "Iteration 691, loss = 0.29124996\n",
      "Iteration 692, loss = 0.29108613\n",
      "Iteration 693, loss = 0.29105544\n",
      "Iteration 694, loss = 0.29091552\n",
      "Iteration 695, loss = 0.29081517\n",
      "Iteration 696, loss = 0.29075072\n",
      "Iteration 697, loss = 0.29063389\n",
      "Iteration 698, loss = 0.29053946\n",
      "Iteration 699, loss = 0.29046572\n",
      "Iteration 700, loss = 0.29036314\n",
      "Iteration 701, loss = 0.29027969\n",
      "Iteration 702, loss = 0.29022852\n",
      "Iteration 703, loss = 0.29012418\n",
      "Iteration 704, loss = 0.29003115\n",
      "Iteration 705, loss = 0.28992371\n",
      "Iteration 706, loss = 0.28990862\n",
      "Iteration 707, loss = 0.28976302\n",
      "Iteration 708, loss = 0.28967964\n",
      "Iteration 709, loss = 0.28972385\n",
      "Iteration 710, loss = 0.28948022\n",
      "Iteration 711, loss = 0.28939471\n",
      "Iteration 712, loss = 0.28931562\n",
      "Iteration 713, loss = 0.28921565\n",
      "Iteration 714, loss = 0.28912558\n",
      "Iteration 715, loss = 0.28904180\n",
      "Iteration 716, loss = 0.28895051\n",
      "Iteration 717, loss = 0.28885317\n",
      "Iteration 718, loss = 0.28876420\n",
      "Iteration 719, loss = 0.28867918\n",
      "Iteration 720, loss = 0.28859113\n",
      "Iteration 721, loss = 0.28853759\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.84230520\n",
      "Iteration 2, loss = 0.83738873\n",
      "Iteration 3, loss = 0.82991095\n",
      "Iteration 4, loss = 0.82148913\n",
      "Iteration 5, loss = 0.81191993\n",
      "Iteration 6, loss = 0.80226141\n",
      "Iteration 7, loss = 0.79268079\n",
      "Iteration 8, loss = 0.78411447\n",
      "Iteration 9, loss = 0.77565201\n",
      "Iteration 10, loss = 0.76773603\n",
      "Iteration 11, loss = 0.76019527\n",
      "Iteration 12, loss = 0.75358700\n",
      "Iteration 13, loss = 0.74741820\n",
      "Iteration 14, loss = 0.74125333\n",
      "Iteration 15, loss = 0.73594466\n",
      "Iteration 16, loss = 0.73069663\n",
      "Iteration 17, loss = 0.72576299\n",
      "Iteration 18, loss = 0.72089631\n",
      "Iteration 19, loss = 0.71636169\n",
      "Iteration 20, loss = 0.71221356\n",
      "Iteration 21, loss = 0.70795108\n",
      "Iteration 22, loss = 0.70390491\n",
      "Iteration 23, loss = 0.70012071\n",
      "Iteration 24, loss = 0.69622338\n",
      "Iteration 25, loss = 0.69250960\n",
      "Iteration 26, loss = 0.68880283\n",
      "Iteration 27, loss = 0.68524590\n",
      "Iteration 28, loss = 0.68153233\n",
      "Iteration 29, loss = 0.67792686\n",
      "Iteration 30, loss = 0.67439561\n",
      "Iteration 31, loss = 0.67094466\n",
      "Iteration 32, loss = 0.66735569\n",
      "Iteration 33, loss = 0.66381194\n",
      "Iteration 34, loss = 0.66018520\n",
      "Iteration 35, loss = 0.65686805\n",
      "Iteration 36, loss = 0.65317211\n",
      "Iteration 37, loss = 0.64979713\n",
      "Iteration 38, loss = 0.64621535\n",
      "Iteration 39, loss = 0.64272890\n",
      "Iteration 40, loss = 0.63920159\n",
      "Iteration 41, loss = 0.63567326\n",
      "Iteration 42, loss = 0.63218653\n",
      "Iteration 43, loss = 0.62864435\n",
      "Iteration 44, loss = 0.62515496\n",
      "Iteration 45, loss = 0.62156572\n",
      "Iteration 46, loss = 0.61802715\n",
      "Iteration 47, loss = 0.61453897\n",
      "Iteration 48, loss = 0.61099838\n",
      "Iteration 49, loss = 0.60740825\n",
      "Iteration 50, loss = 0.60386568\n",
      "Iteration 51, loss = 0.60034610\n",
      "Iteration 52, loss = 0.59673558\n",
      "Iteration 53, loss = 0.59322663\n",
      "Iteration 54, loss = 0.58974719\n",
      "Iteration 55, loss = 0.58616588\n",
      "Iteration 56, loss = 0.58278346\n",
      "Iteration 57, loss = 0.57924707\n",
      "Iteration 58, loss = 0.57581524\n",
      "Iteration 59, loss = 0.57235470\n",
      "Iteration 60, loss = 0.56892918\n",
      "Iteration 61, loss = 0.56560235\n",
      "Iteration 62, loss = 0.56222318\n",
      "Iteration 63, loss = 0.55884762\n",
      "Iteration 64, loss = 0.55555292\n",
      "Iteration 65, loss = 0.55223750\n",
      "Iteration 66, loss = 0.54908964\n",
      "Iteration 67, loss = 0.54599792\n",
      "Iteration 68, loss = 0.54264495\n",
      "Iteration 69, loss = 0.53964787\n",
      "Iteration 70, loss = 0.53654223\n",
      "Iteration 71, loss = 0.53359790\n",
      "Iteration 72, loss = 0.53049106\n",
      "Iteration 73, loss = 0.52755124\n",
      "Iteration 74, loss = 0.52463853\n",
      "Iteration 75, loss = 0.52187400\n",
      "Iteration 76, loss = 0.51907320\n",
      "Iteration 77, loss = 0.51620360\n",
      "Iteration 78, loss = 0.51356455\n",
      "Iteration 79, loss = 0.51086192\n",
      "Iteration 80, loss = 0.50831651\n",
      "Iteration 81, loss = 0.50568444\n",
      "Iteration 82, loss = 0.50315581\n",
      "Iteration 83, loss = 0.50079637\n",
      "Iteration 84, loss = 0.49832199\n",
      "Iteration 85, loss = 0.49595207\n",
      "Iteration 86, loss = 0.49362789\n",
      "Iteration 87, loss = 0.49129231\n",
      "Iteration 88, loss = 0.48914799\n",
      "Iteration 89, loss = 0.48691547\n",
      "Iteration 90, loss = 0.48486615\n",
      "Iteration 91, loss = 0.48279611\n",
      "Iteration 92, loss = 0.48071594\n",
      "Iteration 93, loss = 0.47877819\n",
      "Iteration 94, loss = 0.47683458\n",
      "Iteration 95, loss = 0.47489796\n",
      "Iteration 96, loss = 0.47310303\n",
      "Iteration 97, loss = 0.47131841\n",
      "Iteration 98, loss = 0.46960376\n",
      "Iteration 99, loss = 0.46786700\n",
      "Iteration 100, loss = 0.46614670\n",
      "Iteration 101, loss = 0.46463185\n",
      "Iteration 102, loss = 0.46291596\n",
      "Iteration 103, loss = 0.46146740\n",
      "Iteration 104, loss = 0.45991429\n",
      "Iteration 105, loss = 0.45840371\n",
      "Iteration 106, loss = 0.45700614\n",
      "Iteration 107, loss = 0.45560299\n",
      "Iteration 108, loss = 0.45427912\n",
      "Iteration 109, loss = 0.45293515\n",
      "Iteration 110, loss = 0.45161877\n",
      "Iteration 111, loss = 0.45038834\n",
      "Iteration 112, loss = 0.44911985\n",
      "Iteration 113, loss = 0.44793159\n",
      "Iteration 114, loss = 0.44676860\n",
      "Iteration 115, loss = 0.44568851\n",
      "Iteration 116, loss = 0.44445081\n",
      "Iteration 117, loss = 0.44340328\n",
      "Iteration 118, loss = 0.44230286\n",
      "Iteration 119, loss = 0.44131804\n",
      "Iteration 120, loss = 0.44023462\n",
      "Iteration 121, loss = 0.43926206\n",
      "Iteration 122, loss = 0.43830500\n",
      "Iteration 123, loss = 0.43739239\n",
      "Iteration 124, loss = 0.43638814\n",
      "Iteration 125, loss = 0.43544824\n",
      "Iteration 126, loss = 0.43463646\n",
      "Iteration 127, loss = 0.43372705\n",
      "Iteration 128, loss = 0.43289833\n",
      "Iteration 129, loss = 0.43198214\n",
      "Iteration 130, loss = 0.43118005\n",
      "Iteration 131, loss = 0.43035597\n",
      "Iteration 132, loss = 0.42956339\n",
      "Iteration 133, loss = 0.42881703\n",
      "Iteration 134, loss = 0.42803617\n",
      "Iteration 135, loss = 0.42731739\n",
      "Iteration 136, loss = 0.42654068\n",
      "Iteration 137, loss = 0.42585758\n",
      "Iteration 138, loss = 0.42510048\n",
      "Iteration 139, loss = 0.42441375\n",
      "Iteration 140, loss = 0.42381180\n",
      "Iteration 141, loss = 0.42306848\n",
      "Iteration 142, loss = 0.42241453\n",
      "Iteration 143, loss = 0.42176400\n",
      "Iteration 144, loss = 0.42108523\n",
      "Iteration 145, loss = 0.42048308\n",
      "Iteration 146, loss = 0.41988885\n",
      "Iteration 147, loss = 0.41930454\n",
      "Iteration 148, loss = 0.41864880\n",
      "Iteration 149, loss = 0.41813053\n",
      "Iteration 150, loss = 0.41749765\n",
      "Iteration 151, loss = 0.41692076\n",
      "Iteration 152, loss = 0.41641525\n",
      "Iteration 153, loss = 0.41580215\n",
      "Iteration 154, loss = 0.41524155\n",
      "Iteration 155, loss = 0.41473750\n",
      "Iteration 156, loss = 0.41418163\n",
      "Iteration 157, loss = 0.41366032\n",
      "Iteration 158, loss = 0.41316498\n",
      "Iteration 159, loss = 0.41263414\n",
      "Iteration 160, loss = 0.41220181\n",
      "Iteration 161, loss = 0.41164703\n",
      "Iteration 162, loss = 0.41113599\n",
      "Iteration 163, loss = 0.41067738\n",
      "Iteration 164, loss = 0.41019192\n",
      "Iteration 165, loss = 0.40969039\n",
      "Iteration 166, loss = 0.40922194\n",
      "Iteration 167, loss = 0.40871447\n",
      "Iteration 168, loss = 0.40827837\n",
      "Iteration 169, loss = 0.40782962\n",
      "Iteration 170, loss = 0.40734884\n",
      "Iteration 171, loss = 0.40691840\n",
      "Iteration 172, loss = 0.40649980\n",
      "Iteration 173, loss = 0.40602348\n",
      "Iteration 174, loss = 0.40558808\n",
      "Iteration 175, loss = 0.40516090\n",
      "Iteration 176, loss = 0.40475326\n",
      "Iteration 177, loss = 0.40430871\n",
      "Iteration 178, loss = 0.40388848\n",
      "Iteration 179, loss = 0.40346336\n",
      "Iteration 180, loss = 0.40308804\n",
      "Iteration 181, loss = 0.40266412\n",
      "Iteration 182, loss = 0.40225459\n",
      "Iteration 183, loss = 0.40186024\n",
      "Iteration 184, loss = 0.40146323\n",
      "Iteration 185, loss = 0.40108258\n",
      "Iteration 186, loss = 0.40068736\n",
      "Iteration 187, loss = 0.40029333\n",
      "Iteration 188, loss = 0.39995267\n",
      "Iteration 189, loss = 0.39954923\n",
      "Iteration 190, loss = 0.39917082\n",
      "Iteration 191, loss = 0.39879272\n",
      "Iteration 192, loss = 0.39845291\n",
      "Iteration 193, loss = 0.39807553\n",
      "Iteration 194, loss = 0.39768546\n",
      "Iteration 195, loss = 0.39738459\n",
      "Iteration 196, loss = 0.39703208\n",
      "Iteration 197, loss = 0.39665528\n",
      "Iteration 198, loss = 0.39632044\n",
      "Iteration 199, loss = 0.39596119\n",
      "Iteration 200, loss = 0.39559620\n",
      "Iteration 201, loss = 0.39525953\n",
      "Iteration 202, loss = 0.39493442\n",
      "Iteration 203, loss = 0.39459306\n",
      "Iteration 204, loss = 0.39427018\n",
      "Iteration 205, loss = 0.39393246\n",
      "Iteration 206, loss = 0.39359209\n",
      "Iteration 207, loss = 0.39325320\n",
      "Iteration 208, loss = 0.39296122\n",
      "Iteration 209, loss = 0.39260266\n",
      "Iteration 210, loss = 0.39231837\n",
      "Iteration 211, loss = 0.39196582\n",
      "Iteration 212, loss = 0.39163371\n",
      "Iteration 213, loss = 0.39136150\n",
      "Iteration 214, loss = 0.39103240\n",
      "Iteration 215, loss = 0.39075233\n",
      "Iteration 216, loss = 0.39042883\n",
      "Iteration 217, loss = 0.39015962\n",
      "Iteration 218, loss = 0.38980336\n",
      "Iteration 219, loss = 0.38948469\n",
      "Iteration 220, loss = 0.38919169\n",
      "Iteration 221, loss = 0.38893715\n",
      "Iteration 222, loss = 0.38858432\n",
      "Iteration 223, loss = 0.38830815\n",
      "Iteration 224, loss = 0.38799815\n",
      "Iteration 225, loss = 0.38773951\n",
      "Iteration 226, loss = 0.38743014\n",
      "Iteration 227, loss = 0.38715368\n",
      "Iteration 228, loss = 0.38686085\n",
      "Iteration 229, loss = 0.38655528\n",
      "Iteration 230, loss = 0.38628194\n",
      "Iteration 231, loss = 0.38604402\n",
      "Iteration 232, loss = 0.38573839\n",
      "Iteration 233, loss = 0.38545854\n",
      "Iteration 234, loss = 0.38516471\n",
      "Iteration 235, loss = 0.38491361\n",
      "Iteration 236, loss = 0.38463503\n",
      "Iteration 237, loss = 0.38435708\n",
      "Iteration 238, loss = 0.38412721\n",
      "Iteration 239, loss = 0.38383769\n",
      "Iteration 240, loss = 0.38355166\n",
      "Iteration 241, loss = 0.38329732\n",
      "Iteration 242, loss = 0.38302471\n",
      "Iteration 243, loss = 0.38276098\n",
      "Iteration 244, loss = 0.38254282\n",
      "Iteration 245, loss = 0.38223327\n",
      "Iteration 246, loss = 0.38199232\n",
      "Iteration 247, loss = 0.38179236\n",
      "Iteration 248, loss = 0.38146815\n",
      "Iteration 249, loss = 0.38123582\n",
      "Iteration 250, loss = 0.38098034\n",
      "Iteration 251, loss = 0.38072705\n",
      "Iteration 252, loss = 0.38047048\n",
      "Iteration 253, loss = 0.38022800\n",
      "Iteration 254, loss = 0.38000792\n",
      "Iteration 255, loss = 0.37973318\n",
      "Iteration 256, loss = 0.37950791\n",
      "Iteration 257, loss = 0.37925804\n",
      "Iteration 258, loss = 0.37905657\n",
      "Iteration 259, loss = 0.37883100\n",
      "Iteration 260, loss = 0.37853671\n",
      "Iteration 261, loss = 0.37831381\n",
      "Iteration 262, loss = 0.37807145\n",
      "Iteration 263, loss = 0.37781210\n",
      "Iteration 264, loss = 0.37760646\n",
      "Iteration 265, loss = 0.37736709\n",
      "Iteration 266, loss = 0.37712264\n",
      "Iteration 267, loss = 0.37688356\n",
      "Iteration 268, loss = 0.37664983\n",
      "Iteration 269, loss = 0.37645449\n",
      "Iteration 270, loss = 0.37624491\n",
      "Iteration 271, loss = 0.37597877\n",
      "Iteration 272, loss = 0.37573325\n",
      "Iteration 273, loss = 0.37551755\n",
      "Iteration 274, loss = 0.37528470\n",
      "Iteration 275, loss = 0.37508360\n",
      "Iteration 276, loss = 0.37485660\n",
      "Iteration 277, loss = 0.37462405\n",
      "Iteration 278, loss = 0.37443302\n",
      "Iteration 279, loss = 0.37422057\n",
      "Iteration 280, loss = 0.37400703\n",
      "Iteration 281, loss = 0.37381648\n",
      "Iteration 282, loss = 0.37354581\n",
      "Iteration 283, loss = 0.37331618\n",
      "Iteration 284, loss = 0.37310033\n",
      "Iteration 285, loss = 0.37287965\n",
      "Iteration 286, loss = 0.37267097\n",
      "Iteration 287, loss = 0.37246767\n",
      "Iteration 288, loss = 0.37225071\n",
      "Iteration 289, loss = 0.37204121\n",
      "Iteration 290, loss = 0.37182635\n",
      "Iteration 291, loss = 0.37163178\n",
      "Iteration 292, loss = 0.37143205\n",
      "Iteration 293, loss = 0.37123620\n",
      "Iteration 294, loss = 0.37099485\n",
      "Iteration 295, loss = 0.37081070\n",
      "Iteration 296, loss = 0.37059791\n",
      "Iteration 297, loss = 0.37040150\n",
      "Iteration 298, loss = 0.37018164\n",
      "Iteration 299, loss = 0.36998624\n",
      "Iteration 300, loss = 0.36983264\n",
      "Iteration 301, loss = 0.36961845\n",
      "Iteration 302, loss = 0.36938930\n",
      "Iteration 303, loss = 0.36918439\n",
      "Iteration 304, loss = 0.36898904\n",
      "Iteration 305, loss = 0.36884376\n",
      "Iteration 306, loss = 0.36862019\n",
      "Iteration 307, loss = 0.36839375\n",
      "Iteration 308, loss = 0.36821096\n",
      "Iteration 309, loss = 0.36799123\n",
      "Iteration 310, loss = 0.36780767\n",
      "Iteration 311, loss = 0.36763303\n",
      "Iteration 312, loss = 0.36741042\n",
      "Iteration 313, loss = 0.36724908\n",
      "Iteration 314, loss = 0.36702658\n",
      "Iteration 315, loss = 0.36686062\n",
      "Iteration 316, loss = 0.36666373\n",
      "Iteration 317, loss = 0.36646589\n",
      "Iteration 318, loss = 0.36628927\n",
      "Iteration 319, loss = 0.36610098\n",
      "Iteration 320, loss = 0.36591331\n",
      "Iteration 321, loss = 0.36572505\n",
      "Iteration 322, loss = 0.36553027\n",
      "Iteration 323, loss = 0.36536169\n",
      "Iteration 324, loss = 0.36517899\n",
      "Iteration 325, loss = 0.36497535\n",
      "Iteration 326, loss = 0.36481526\n",
      "Iteration 327, loss = 0.36459048\n",
      "Iteration 328, loss = 0.36444593\n",
      "Iteration 329, loss = 0.36426661\n",
      "Iteration 330, loss = 0.36406070\n",
      "Iteration 331, loss = 0.36394957\n",
      "Iteration 332, loss = 0.36370663\n",
      "Iteration 333, loss = 0.36352083\n",
      "Iteration 334, loss = 0.36334540\n",
      "Iteration 335, loss = 0.36315631\n",
      "Iteration 336, loss = 0.36298779\n",
      "Iteration 337, loss = 0.36282387\n",
      "Iteration 338, loss = 0.36264176\n",
      "Iteration 339, loss = 0.36246410\n",
      "Iteration 340, loss = 0.36230137\n",
      "Iteration 341, loss = 0.36209058\n",
      "Iteration 342, loss = 0.36193037\n",
      "Iteration 343, loss = 0.36179113\n",
      "Iteration 344, loss = 0.36157399\n",
      "Iteration 345, loss = 0.36140631\n",
      "Iteration 346, loss = 0.36124043\n",
      "Iteration 347, loss = 0.36105195\n",
      "Iteration 348, loss = 0.36089390\n",
      "Iteration 349, loss = 0.36070087\n",
      "Iteration 682, loss = 0.30752653\n",
      "Iteration 683, loss = 0.30736700\n",
      "Iteration 684, loss = 0.30727159\n",
      "Iteration 685, loss = 0.30716059\n",
      "Iteration 686, loss = 0.30702212\n",
      "Iteration 687, loss = 0.30688805\n",
      "Iteration 688, loss = 0.30682968\n",
      "Iteration 689, loss = 0.30672439\n",
      "Iteration 690, loss = 0.30653253\n",
      "Iteration 691, loss = 0.30642164\n",
      "Iteration 692, loss = 0.30629788\n",
      "Iteration 693, loss = 0.30616138\n",
      "Iteration 694, loss = 0.30607342\n",
      "Iteration 695, loss = 0.30593153\n",
      "Iteration 696, loss = 0.30580637\n",
      "Iteration 697, loss = 0.30568721\n",
      "Iteration 698, loss = 0.30556612\n",
      "Iteration 699, loss = 0.30544257\n",
      "Iteration 700, loss = 0.30533523\n",
      "Iteration 701, loss = 0.30526428\n",
      "Iteration 702, loss = 0.30510328\n",
      "Iteration 703, loss = 0.30499808\n",
      "Iteration 704, loss = 0.30486338\n",
      "Iteration 705, loss = 0.30474838\n",
      "Iteration 706, loss = 0.30466162\n",
      "Iteration 707, loss = 0.30449542\n",
      "Iteration 708, loss = 0.30436566\n",
      "Iteration 709, loss = 0.30424459\n",
      "Iteration 710, loss = 0.30412330\n",
      "Iteration 711, loss = 0.30401759\n",
      "Iteration 712, loss = 0.30387541\n",
      "Iteration 713, loss = 0.30379134\n",
      "Iteration 714, loss = 0.30367780\n",
      "Iteration 715, loss = 0.30352812\n",
      "Iteration 716, loss = 0.30347088\n",
      "Iteration 717, loss = 0.30327439\n",
      "Iteration 718, loss = 0.30315316\n",
      "Iteration 719, loss = 0.30302978\n",
      "Iteration 720, loss = 0.30291540\n",
      "Iteration 721, loss = 0.30279101\n",
      "Iteration 722, loss = 0.30269580\n",
      "Iteration 723, loss = 0.30265118\n",
      "Iteration 724, loss = 0.30242494\n",
      "Iteration 725, loss = 0.30230709\n",
      "Iteration 726, loss = 0.30218324\n",
      "Iteration 727, loss = 0.30207024\n",
      "Iteration 728, loss = 0.30194144\n",
      "Iteration 729, loss = 0.30182552\n",
      "Iteration 730, loss = 0.30166907\n",
      "Iteration 731, loss = 0.30157307\n",
      "Iteration 732, loss = 0.30148129\n",
      "Iteration 733, loss = 0.30131214\n",
      "Iteration 734, loss = 0.30119503\n",
      "Iteration 735, loss = 0.30107361\n",
      "Iteration 736, loss = 0.30094856\n",
      "Iteration 737, loss = 0.30082585\n",
      "Iteration 738, loss = 0.30073984\n",
      "Iteration 739, loss = 0.30058422\n",
      "Iteration 740, loss = 0.30048953\n",
      "Iteration 741, loss = 0.30033943\n",
      "Iteration 742, loss = 0.30022039\n",
      "Iteration 743, loss = 0.30010769\n",
      "Iteration 744, loss = 0.29995966\n",
      "Iteration 745, loss = 0.29984944\n",
      "Iteration 746, loss = 0.29973199\n",
      "Iteration 747, loss = 0.29959205\n",
      "Iteration 748, loss = 0.29949071\n",
      "Iteration 749, loss = 0.29935594\n",
      "Iteration 750, loss = 0.29925939\n",
      "Iteration 751, loss = 0.29911456\n",
      "Iteration 752, loss = 0.29901192\n",
      "Iteration 753, loss = 0.29886830\n",
      "Iteration 754, loss = 0.29876044\n",
      "Iteration 755, loss = 0.29868629\n",
      "Iteration 756, loss = 0.29851902\n",
      "Iteration 757, loss = 0.29842235\n",
      "Iteration 758, loss = 0.29825208\n",
      "Iteration 759, loss = 0.29817558\n",
      "Iteration 760, loss = 0.29803931\n",
      "Iteration 761, loss = 0.29790787\n",
      "Iteration 762, loss = 0.29780534\n",
      "Iteration 763, loss = 0.29770065\n",
      "Iteration 764, loss = 0.29758922\n",
      "Iteration 765, loss = 0.29750540\n",
      "Iteration 766, loss = 0.29732376\n",
      "Iteration 767, loss = 0.29716987\n",
      "Iteration 768, loss = 0.29710471\n",
      "Iteration 769, loss = 0.29696788\n",
      "Iteration 770, loss = 0.29680432\n",
      "Iteration 771, loss = 0.29679643\n",
      "Iteration 772, loss = 0.29656592\n",
      "Iteration 773, loss = 0.29645497\n",
      "Iteration 774, loss = 0.29634088\n",
      "Iteration 775, loss = 0.29623577\n",
      "Iteration 776, loss = 0.29607045\n",
      "Iteration 777, loss = 0.29600239\n",
      "Iteration 778, loss = 0.29585763\n",
      "Iteration 779, loss = 0.29574380\n",
      "Iteration 780, loss = 0.29561675\n",
      "Iteration 781, loss = 0.29547690\n",
      "Iteration 782, loss = 0.29543181\n",
      "Iteration 783, loss = 0.29521317\n",
      "Iteration 784, loss = 0.29511650\n",
      "Iteration 785, loss = 0.29497708\n",
      "Iteration 786, loss = 0.29486330\n",
      "Iteration 787, loss = 0.29478206\n",
      "Iteration 788, loss = 0.29464189\n",
      "Iteration 789, loss = 0.29450891\n",
      "Iteration 790, loss = 0.29437043\n",
      "Iteration 791, loss = 0.29425477\n",
      "Iteration 792, loss = 0.29415585\n",
      "Iteration 793, loss = 0.29403595\n",
      "Iteration 794, loss = 0.29389038\n",
      "Iteration 795, loss = 0.29376691\n",
      "Iteration 796, loss = 0.29362041\n",
      "Iteration 797, loss = 0.29352864\n",
      "Iteration 798, loss = 0.29339429\n",
      "Iteration 799, loss = 0.29325231\n",
      "Iteration 800, loss = 0.29319267\n",
      "Iteration 801, loss = 0.29300682\n",
      "Iteration 802, loss = 0.29288381\n",
      "Iteration 803, loss = 0.29280268\n",
      "Iteration 804, loss = 0.29265316\n",
      "Iteration 805, loss = 0.29254575\n",
      "Iteration 806, loss = 0.29245569\n",
      "Iteration 807, loss = 0.29227330\n",
      "Iteration 808, loss = 0.29216389\n",
      "Iteration 809, loss = 0.29205120\n",
      "Iteration 810, loss = 0.29189304\n",
      "Iteration 811, loss = 0.29176898\n",
      "Iteration 812, loss = 0.29164012\n",
      "Iteration 813, loss = 0.29164931\n",
      "Iteration 814, loss = 0.29140190\n",
      "Iteration 815, loss = 0.29128463\n",
      "Iteration 816, loss = 0.29115458\n",
      "Iteration 817, loss = 0.29103775\n",
      "Iteration 818, loss = 0.29092591\n",
      "Iteration 819, loss = 0.29081645\n",
      "Iteration 820, loss = 0.29067483\n",
      "Iteration 821, loss = 0.29052859\n",
      "Iteration 822, loss = 0.29041531\n",
      "Iteration 823, loss = 0.29028744\n",
      "Iteration 824, loss = 0.29017927\n",
      "Iteration 825, loss = 0.29004017\n",
      "Iteration 826, loss = 0.28993480\n",
      "Iteration 827, loss = 0.28981613\n",
      "Iteration 828, loss = 0.28968722\n",
      "Iteration 829, loss = 0.28956370\n",
      "Iteration 830, loss = 0.28941929\n",
      "Iteration 831, loss = 0.28930437\n",
      "Iteration 832, loss = 0.28922573\n",
      "Iteration 833, loss = 0.28905508\n",
      "Iteration 834, loss = 0.28892852\n",
      "Iteration 835, loss = 0.28884860\n",
      "Iteration 836, loss = 0.28868836\n",
      "Iteration 837, loss = 0.28858013\n",
      "Iteration 838, loss = 0.28841658\n",
      "Iteration 839, loss = 0.28830692\n",
      "Iteration 840, loss = 0.28822286\n",
      "Iteration 841, loss = 0.28805412\n",
      "Iteration 842, loss = 0.28793446\n",
      "Iteration 843, loss = 0.28786560\n",
      "Iteration 844, loss = 0.28771192\n",
      "Iteration 845, loss = 0.28756669\n",
      "Iteration 846, loss = 0.28744037\n",
      "Iteration 847, loss = 0.28734955\n",
      "Iteration 848, loss = 0.28724849\n",
      "Iteration 849, loss = 0.28707385\n",
      "Iteration 850, loss = 0.28700329\n",
      "Iteration 851, loss = 0.28684493\n",
      "Iteration 852, loss = 0.28669494\n",
      "Iteration 853, loss = 0.28657727\n",
      "Iteration 854, loss = 0.28645387\n",
      "Iteration 855, loss = 0.28633599\n",
      "Iteration 856, loss = 0.28627926\n",
      "Iteration 857, loss = 0.28610143\n",
      "Iteration 858, loss = 0.28596802\n",
      "Iteration 859, loss = 0.28587372\n",
      "Iteration 860, loss = 0.28574874\n",
      "Iteration 861, loss = 0.28574982\n",
      "Iteration 862, loss = 0.28546032\n",
      "Iteration 863, loss = 0.28537087\n",
      "Iteration 864, loss = 0.28529191\n",
      "Iteration 865, loss = 0.28515030\n",
      "Iteration 866, loss = 0.28497924\n",
      "Iteration 867, loss = 0.28486725\n",
      "Iteration 868, loss = 0.28475064\n",
      "Iteration 869, loss = 0.28463533\n",
      "Iteration 870, loss = 0.28449557\n",
      "Iteration 871, loss = 0.28440157\n",
      "Iteration 872, loss = 0.28428020\n",
      "Iteration 873, loss = 0.28411849\n",
      "Iteration 874, loss = 0.28401793\n",
      "Iteration 875, loss = 0.28394968\n",
      "Iteration 876, loss = 0.28378385\n",
      "Iteration 877, loss = 0.28363363\n",
      "Iteration 878, loss = 0.28350560\n",
      "Iteration 879, loss = 0.28338670\n",
      "Iteration 880, loss = 0.28346005\n",
      "Iteration 881, loss = 0.28314399\n",
      "Iteration 882, loss = 0.28305592\n",
      "Iteration 883, loss = 0.28291778\n",
      "Iteration 884, loss = 0.28278744\n",
      "Iteration 885, loss = 0.28265748\n",
      "Iteration 886, loss = 0.28256962\n",
      "Iteration 887, loss = 0.28243489\n",
      "Iteration 888, loss = 0.28230170\n",
      "Iteration 889, loss = 0.28220839\n",
      "Iteration 890, loss = 0.28211579\n",
      "Iteration 891, loss = 0.28193086\n",
      "Iteration 892, loss = 0.28180087\n",
      "Iteration 893, loss = 0.28169751\n",
      "Iteration 894, loss = 0.28157084\n",
      "Iteration 895, loss = 0.28146880\n",
      "Iteration 896, loss = 0.28133936\n",
      "Iteration 897, loss = 0.28120205\n",
      "Iteration 898, loss = 0.28111163\n",
      "Iteration 899, loss = 0.28100949\n",
      "Iteration 900, loss = 0.28084845\n",
      "Iteration 901, loss = 0.28076281\n",
      "Iteration 902, loss = 0.28060559\n",
      "Iteration 903, loss = 0.28052801\n",
      "Iteration 904, loss = 0.28039865\n",
      "Iteration 905, loss = 0.28024764\n",
      "Iteration 906, loss = 0.28017556\n",
      "Iteration 907, loss = 0.28002375\n",
      "Iteration 908, loss = 0.27988782\n",
      "Iteration 909, loss = 0.27981827\n",
      "Iteration 910, loss = 0.27963427\n",
      "Iteration 911, loss = 0.27962653\n",
      "Iteration 912, loss = 0.27936700\n",
      "Iteration 913, loss = 0.27928412\n",
      "Iteration 914, loss = 0.27916871\n",
      "Iteration 915, loss = 0.27905870\n",
      "Iteration 916, loss = 0.27894304\n",
      "Iteration 917, loss = 0.27880291\n",
      "Iteration 918, loss = 0.27870751\n",
      "Iteration 919, loss = 0.27852170\n",
      "Iteration 920, loss = 0.27839244\n",
      "Iteration 921, loss = 0.27828122\n",
      "Iteration 922, loss = 0.27822426\n",
      "Iteration 923, loss = 0.27816635\n",
      "Iteration 924, loss = 0.27791856\n",
      "Iteration 925, loss = 0.27784259\n",
      "Iteration 926, loss = 0.27767862\n",
      "Iteration 927, loss = 0.27756339\n",
      "Iteration 928, loss = 0.27742459\n",
      "Iteration 929, loss = 0.27733037\n",
      "Iteration 930, loss = 0.27716768\n",
      "Iteration 931, loss = 0.27711776\n",
      "Iteration 932, loss = 0.27693429\n",
      "Iteration 933, loss = 0.27686224\n",
      "Iteration 934, loss = 0.27669227\n",
      "Iteration 935, loss = 0.27660387\n",
      "Iteration 936, loss = 0.27644183\n",
      "Iteration 937, loss = 0.27635761\n",
      "Iteration 938, loss = 0.27619478\n",
      "Iteration 939, loss = 0.27605790\n",
      "Iteration 940, loss = 0.27592606\n",
      "Iteration 941, loss = 0.27581169\n",
      "Iteration 942, loss = 0.27571628\n",
      "Iteration 943, loss = 0.27560875\n",
      "Iteration 944, loss = 0.27547722\n",
      "Iteration 945, loss = 0.27532856\n",
      "Iteration 946, loss = 0.27522368\n",
      "Iteration 947, loss = 0.27508854\n",
      "Iteration 948, loss = 0.27497182\n",
      "Iteration 949, loss = 0.27487274\n",
      "Iteration 950, loss = 0.27478888\n",
      "Iteration 951, loss = 0.27460877\n",
      "Iteration 952, loss = 0.27449234\n",
      "Iteration 953, loss = 0.27436693\n",
      "Iteration 954, loss = 0.27434523\n",
      "Iteration 955, loss = 0.27411133\n",
      "Iteration 956, loss = 0.27407429\n",
      "Iteration 957, loss = 0.27389589\n",
      "Iteration 958, loss = 0.27383728\n",
      "Iteration 959, loss = 0.27363020\n",
      "Iteration 960, loss = 0.27350382\n",
      "Iteration 961, loss = 0.27338012\n",
      "Iteration 962, loss = 0.27324250\n",
      "Iteration 963, loss = 0.27314777\n",
      "Iteration 964, loss = 0.27298396\n",
      "Iteration 965, loss = 0.27296791\n",
      "Iteration 966, loss = 0.27275825\n",
      "Iteration 967, loss = 0.27261237\n",
      "Iteration 968, loss = 0.27249399\n",
      "Iteration 969, loss = 0.27236975\n",
      "Iteration 970, loss = 0.27224653\n",
      "Iteration 971, loss = 0.27216103\n",
      "Iteration 972, loss = 0.27202251\n",
      "Iteration 973, loss = 0.27189988\n",
      "Iteration 974, loss = 0.27178388\n",
      "Iteration 975, loss = 0.27164714\n",
      "Iteration 976, loss = 0.27155727\n",
      "Iteration 977, loss = 0.27139960\n",
      "Iteration 978, loss = 0.27129236\n",
      "Iteration 979, loss = 0.27119083\n",
      "Iteration 980, loss = 0.27103493\n",
      "Iteration 981, loss = 0.27091867\n",
      "Iteration 982, loss = 0.27079511\n",
      "Iteration 983, loss = 0.27069001\n",
      "Iteration 984, loss = 0.27052163\n",
      "Iteration 985, loss = 0.27041096\n",
      "Iteration 986, loss = 0.27028312\n",
      "Iteration 987, loss = 0.27015782\n",
      "Iteration 988, loss = 0.27005657\n",
      "Iteration 989, loss = 0.26991068\n",
      "Iteration 990, loss = 0.26977151\n",
      "Iteration 991, loss = 0.26967819\n",
      "Iteration 992, loss = 0.26954511\n",
      "Iteration 993, loss = 0.26947403\n",
      "Iteration 994, loss = 0.26932519\n",
      "Iteration 995, loss = 0.26914858\n",
      "Iteration 996, loss = 0.26912276\n",
      "Iteration 997, loss = 0.26900607\n",
      "Iteration 998, loss = 0.26880400\n",
      "Iteration 999, loss = 0.26866280\n",
      "Iteration 1000, loss = 0.26854784\n",
      "Iteration 1001, loss = 0.26852171\n",
      "Iteration 1002, loss = 0.26827986\n",
      "Iteration 1003, loss = 0.26823241\n",
      "Iteration 1004, loss = 0.26815729\n",
      "Iteration 1005, loss = 0.26792945\n",
      "Iteration 1006, loss = 0.26782225\n",
      "Iteration 1007, loss = 0.26767650\n",
      "Iteration 1008, loss = 0.26757410\n",
      "Iteration 1009, loss = 0.26748569\n",
      "Iteration 1010, loss = 0.26736020\n",
      "Iteration 1011, loss = 0.26719440\n",
      "Iteration 1012, loss = 0.26705403\n",
      "Iteration 1013, loss = 0.26693647\n",
      "Iteration 1014, loss = 0.26686628\n",
      "Iteration 1015, loss = 0.26670271\n",
      "Iteration 1016, loss = 0.26666734\n",
      "Iteration 1017, loss = 0.26646241\n",
      "Iteration 1018, loss = 0.26632351\n",
      "Iteration 1019, loss = 0.26625146\n",
      "Iteration 1020, loss = 0.26610431\n",
      "Iteration 1021, loss = 0.26606626\n",
      "Iteration 1022, loss = 0.26581949\n",
      "Iteration 1023, loss = 0.26571656\n",
      "Iteration 1024, loss = 0.26561064\n",
      "Iteration 1025, loss = 0.26546435\n",
      "Iteration 1026, loss = 0.26533886\n",
      "Iteration 1027, loss = 0.26525853\n",
      "Iteration 1028, loss = 0.26511339\n",
      "Iteration 1029, loss = 0.26500792\n",
      "Iteration 1030, loss = 0.26486209\n",
      "Iteration 1031, loss = 0.26476793\n",
      "Iteration 1032, loss = 0.26462931\n",
      "Iteration 1033, loss = 0.26450502\n",
      "Iteration 1034, loss = 0.26444801\n",
      "Iteration 1035, loss = 0.26427607\n",
      "Iteration 1036, loss = 0.26414810\n",
      "Iteration 1037, loss = 0.26399974\n",
      "Iteration 1038, loss = 0.26389158\n",
      "Iteration 1039, loss = 0.26375514\n",
      "Iteration 1040, loss = 0.26373558\n",
      "Iteration 1041, loss = 0.26357826\n",
      "Iteration 1042, loss = 0.26342502\n",
      "Iteration 1043, loss = 0.26337797\n",
      "Iteration 1044, loss = 0.26320885\n",
      "Iteration 1045, loss = 0.26303531\n",
      "Iteration 1046, loss = 0.26296100\n",
      "Iteration 1047, loss = 0.26280949\n",
      "Iteration 1048, loss = 0.26269721\n",
      "Iteration 1049, loss = 0.26256681\n",
      "Iteration 1050, loss = 0.26247404\n",
      "Iteration 1051, loss = 0.26234026\n",
      "Iteration 1052, loss = 0.26222835\n",
      "Iteration 1053, loss = 0.26210511\n",
      "Iteration 1054, loss = 0.26196701\n",
      "Iteration 1055, loss = 0.26194627\n",
      "Iteration 1056, loss = 0.26172885\n",
      "Iteration 1057, loss = 0.26164848\n",
      "Iteration 1058, loss = 0.26150820\n",
      "Iteration 1059, loss = 0.26144957\n",
      "Iteration 1060, loss = 0.26128149\n",
      "Iteration 1061, loss = 0.26115230\n",
      "Iteration 1062, loss = 0.26101522\n",
      "Iteration 1063, loss = 0.26092217\n",
      "Iteration 1064, loss = 0.26080669\n",
      "Iteration 1065, loss = 0.26064326\n",
      "Iteration 1066, loss = 0.26055876\n",
      "Iteration 1067, loss = 0.26042272\n",
      "Iteration 1068, loss = 0.26031585\n",
      "Iteration 1069, loss = 0.26018144\n",
      "Iteration 1070, loss = 0.26016380\n",
      "Iteration 1071, loss = 0.26002876\n",
      "Iteration 1072, loss = 0.25989916\n",
      "Iteration 1073, loss = 0.25971635\n",
      "Iteration 1074, loss = 0.25962988\n",
      "Iteration 1075, loss = 0.25952157\n",
      "Iteration 1076, loss = 0.25939968\n",
      "Iteration 1077, loss = 0.25928555\n",
      "Iteration 1078, loss = 0.25911953\n",
      "Iteration 1079, loss = 0.25908538\n",
      "Iteration 1080, loss = 0.25888212\n",
      "Iteration 1081, loss = 0.25875951\n",
      "Iteration 1082, loss = 0.25863925\n",
      "Iteration 1083, loss = 0.25852882\n",
      "Iteration 1084, loss = 0.25841250\n",
      "Iteration 1085, loss = 0.25834765\n",
      "Iteration 1086, loss = 0.25821649\n",
      "Iteration 1087, loss = 0.25811930\n",
      "Iteration 1088, loss = 0.25796819\n",
      "Iteration 1089, loss = 0.25786662\n",
      "Iteration 1090, loss = 0.25772379\n",
      "Iteration 1091, loss = 0.25758752\n",
      "Iteration 1092, loss = 0.25749200\n",
      "Iteration 1093, loss = 0.25736271\n",
      "Iteration 1094, loss = 0.25723857\n",
      "Iteration 1095, loss = 0.25715840\n",
      "Iteration 1096, loss = 0.25699360\n",
      "Iteration 1097, loss = 0.25687643\n",
      "Iteration 1098, loss = 0.25676854\n",
      "Iteration 1099, loss = 0.25664946\n",
      "Iteration 1100, loss = 0.25653397\n",
      "Iteration 1101, loss = 0.25639823\n",
      "Iteration 1102, loss = 0.25629246\n",
      "Iteration 1103, loss = 0.25619237\n",
      "Iteration 1104, loss = 0.25608783\n",
      "Iteration 1105, loss = 0.25594784\n",
      "Iteration 1106, loss = 0.25587434\n",
      "Iteration 1107, loss = 0.25571437\n",
      "Iteration 1108, loss = 0.25562850\n",
      "Iteration 1109, loss = 0.25549252\n",
      "Iteration 1110, loss = 0.25536004\n",
      "Iteration 1111, loss = 0.25528518\n",
      "Iteration 1112, loss = 0.25510108\n",
      "Iteration 1113, loss = 0.25504836\n",
      "Iteration 1114, loss = 0.25490134\n",
      "Iteration 1115, loss = 0.25474578\n",
      "Iteration 1116, loss = 0.25465163\n",
      "Iteration 1117, loss = 0.25462404\n",
      "Iteration 1118, loss = 0.25447881\n",
      "Iteration 1119, loss = 0.25432914\n",
      "Iteration 1120, loss = 0.25420353\n",
      "Iteration 1121, loss = 0.25407622\n",
      "Iteration 1122, loss = 0.25403983\n",
      "Iteration 1123, loss = 0.25387366\n",
      "Iteration 1124, loss = 0.25373635\n",
      "Iteration 1125, loss = 0.25359675\n",
      "Iteration 1126, loss = 0.25352199\n",
      "Iteration 1127, loss = 0.25334826\n",
      "Iteration 1128, loss = 0.25325100\n",
      "Iteration 1129, loss = 0.25312092\n",
      "Iteration 1130, loss = 0.25307349\n",
      "Iteration 1131, loss = 0.25289873\n",
      "Iteration 1132, loss = 0.25280908\n",
      "Iteration 1133, loss = 0.25273728\n",
      "Iteration 1134, loss = 0.25258341\n",
      "Iteration 1135, loss = 0.25245169\n",
      "Iteration 1136, loss = 0.25234569\n",
      "Iteration 1137, loss = 0.25221713\n",
      "Iteration 1138, loss = 0.25206125\n",
      "Iteration 1139, loss = 0.25201140\n",
      "Iteration 1140, loss = 0.25195005\n",
      "Iteration 1141, loss = 0.25174667\n",
      "Iteration 1142, loss = 0.25161782\n",
      "Iteration 1143, loss = 0.25147934\n",
      "Iteration 1144, loss = 0.25140299\n",
      "Iteration 1145, loss = 0.25140411\n",
      "Iteration 1146, loss = 0.25116553\n",
      "Iteration 1147, loss = 0.25108328\n",
      "Iteration 1148, loss = 0.25096060\n",
      "Iteration 1149, loss = 0.25080002\n",
      "Iteration 1150, loss = 0.25074884\n",
      "Iteration 1151, loss = 0.25065529\n",
      "Iteration 1152, loss = 0.25050442\n",
      "Iteration 1153, loss = 0.25031834\n",
      "Iteration 1154, loss = 0.25019627\n",
      "Iteration 1155, loss = 0.25010155\n",
      "Iteration 1156, loss = 0.25003354\n",
      "Iteration 1157, loss = 0.24991022\n",
      "Iteration 1158, loss = 0.24981089\n",
      "Iteration 1159, loss = 0.24968202\n",
      "Iteration 1160, loss = 0.24959495\n",
      "Iteration 1161, loss = 0.24947452\n",
      "Iteration 1162, loss = 0.24929512\n",
      "Iteration 1163, loss = 0.24917108\n",
      "Iteration 1164, loss = 0.24908751\n",
      "Iteration 1165, loss = 0.24895186\n",
      "Iteration 1166, loss = 0.24886435\n",
      "Iteration 1167, loss = 0.24881025\n",
      "Iteration 1168, loss = 0.24860259\n",
      "Iteration 1169, loss = 0.24846364\n",
      "Iteration 1170, loss = 0.24836754\n",
      "Iteration 1171, loss = 0.24823523\n",
      "Iteration 1172, loss = 0.24810586\n",
      "Iteration 618, loss = 0.29557244\n",
      "Iteration 619, loss = 0.29549493\n",
      "Iteration 620, loss = 0.29551909\n",
      "Iteration 621, loss = 0.29528397\n",
      "Iteration 622, loss = 0.29504518\n",
      "Iteration 623, loss = 0.29490676\n",
      "Iteration 624, loss = 0.29477281\n",
      "Iteration 625, loss = 0.29463143\n",
      "Iteration 626, loss = 0.29463383\n",
      "Iteration 627, loss = 0.29438517\n",
      "Iteration 628, loss = 0.29424969\n",
      "Iteration 629, loss = 0.29407984\n",
      "Iteration 630, loss = 0.29393953\n",
      "Iteration 631, loss = 0.29380553\n",
      "Iteration 632, loss = 0.29375645\n",
      "Iteration 633, loss = 0.29351033\n",
      "Iteration 634, loss = 0.29338052\n",
      "Iteration 635, loss = 0.29323255\n",
      "Iteration 636, loss = 0.29311356\n",
      "Iteration 637, loss = 0.29299395\n",
      "Iteration 638, loss = 0.29281643\n",
      "Iteration 639, loss = 0.29266371\n",
      "Iteration 640, loss = 0.29256984\n",
      "Iteration 641, loss = 0.29244000\n",
      "Iteration 642, loss = 0.29235304\n",
      "Iteration 643, loss = 0.29211676\n",
      "Iteration 644, loss = 0.29196977\n",
      "Iteration 645, loss = 0.29191240\n",
      "Iteration 646, loss = 0.29176395\n",
      "Iteration 647, loss = 0.29161132\n",
      "Iteration 648, loss = 0.29143876\n",
      "Iteration 649, loss = 0.29127178\n",
      "Iteration 650, loss = 0.29118004\n",
      "Iteration 651, loss = 0.29098863\n",
      "Iteration 652, loss = 0.29083679\n",
      "Iteration 653, loss = 0.29072382\n",
      "Iteration 654, loss = 0.29056149\n",
      "Iteration 655, loss = 0.29045538\n",
      "Iteration 656, loss = 0.29029994\n",
      "Iteration 657, loss = 0.29021186\n",
      "Iteration 658, loss = 0.29026912\n",
      "Iteration 659, loss = 0.28988273\n",
      "Iteration 660, loss = 0.28973857\n",
      "Iteration 661, loss = 0.28963052\n",
      "Iteration 662, loss = 0.28946542\n",
      "Iteration 663, loss = 0.28928511\n",
      "Iteration 664, loss = 0.28929538\n",
      "Iteration 665, loss = 0.28903754\n",
      "Iteration 666, loss = 0.28885037\n",
      "Iteration 667, loss = 0.28872789\n",
      "Iteration 668, loss = 0.28863667\n",
      "Iteration 669, loss = 0.28841252\n",
      "Iteration 670, loss = 0.28846088\n",
      "Iteration 671, loss = 0.28831199\n",
      "Iteration 672, loss = 0.28801841\n",
      "Iteration 673, loss = 0.28794321\n",
      "Iteration 674, loss = 0.28792560\n",
      "Iteration 675, loss = 0.28768518\n",
      "Iteration 676, loss = 0.28747414\n",
      "Iteration 677, loss = 0.28735735\n",
      "Iteration 678, loss = 0.28717201\n",
      "Iteration 679, loss = 0.28697942\n",
      "Iteration 680, loss = 0.28687638\n",
      "Iteration 681, loss = 0.28671358\n",
      "Iteration 682, loss = 0.28654670\n",
      "Iteration 683, loss = 0.28651852\n",
      "Iteration 684, loss = 0.28627823\n",
      "Iteration 685, loss = 0.28611147\n",
      "Iteration 686, loss = 0.28600644\n",
      "Iteration 687, loss = 0.28585858\n",
      "Iteration 688, loss = 0.28572291\n",
      "Iteration 689, loss = 0.28555917\n",
      "Iteration 690, loss = 0.28542676\n",
      "Iteration 691, loss = 0.28530283\n",
      "Iteration 692, loss = 0.28518444\n",
      "Iteration 693, loss = 0.28500120\n",
      "Iteration 694, loss = 0.28489034\n",
      "Iteration 695, loss = 0.28470501\n",
      "Iteration 696, loss = 0.28458187\n",
      "Iteration 697, loss = 0.28444800\n",
      "Iteration 698, loss = 0.28427555\n",
      "Iteration 699, loss = 0.28423998\n",
      "Iteration 700, loss = 0.28401373\n",
      "Iteration 701, loss = 0.28387981\n",
      "Iteration 702, loss = 0.28382064\n",
      "Iteration 703, loss = 0.28361374\n",
      "Iteration 704, loss = 0.28345384\n",
      "Iteration 705, loss = 0.28340938\n",
      "Iteration 706, loss = 0.28323646\n",
      "Iteration 707, loss = 0.28308083\n",
      "Iteration 708, loss = 0.28292032\n",
      "Iteration 709, loss = 0.28292298\n",
      "Iteration 710, loss = 0.28267967\n",
      "Iteration 711, loss = 0.28250542\n",
      "Iteration 712, loss = 0.28237387\n",
      "Iteration 713, loss = 0.28222666\n",
      "Iteration 714, loss = 0.28205981\n",
      "Iteration 715, loss = 0.28198444\n",
      "Iteration 716, loss = 0.28180899\n",
      "Iteration 717, loss = 0.28172483\n",
      "Iteration 718, loss = 0.28156472\n",
      "Iteration 719, loss = 0.28136261\n",
      "Iteration 720, loss = 0.28121791\n",
      "Iteration 721, loss = 0.28110913\n",
      "Iteration 722, loss = 0.28093257\n",
      "Iteration 723, loss = 0.28082600\n",
      "Iteration 724, loss = 0.28074693\n",
      "Iteration 725, loss = 0.28058782\n",
      "Iteration 726, loss = 0.28042402\n",
      "Iteration 727, loss = 0.28022516\n",
      "Iteration 728, loss = 0.28011322\n",
      "Iteration 729, loss = 0.27995784\n",
      "Iteration 730, loss = 0.27984165\n",
      "Iteration 731, loss = 0.27984933\n",
      "Iteration 732, loss = 0.27957641\n",
      "Iteration 733, loss = 0.27944536\n",
      "Iteration 734, loss = 0.27935115\n",
      "Iteration 735, loss = 0.27914791\n",
      "Iteration 736, loss = 0.27895512\n",
      "Iteration 737, loss = 0.27881206\n",
      "Iteration 738, loss = 0.27866322\n",
      "Iteration 739, loss = 0.27869477\n",
      "Iteration 740, loss = 0.27843863\n",
      "Iteration 741, loss = 0.27821489\n",
      "Iteration 742, loss = 0.27809706\n",
      "Iteration 743, loss = 0.27799267\n",
      "Iteration 744, loss = 0.27776404\n",
      "Iteration 745, loss = 0.27768503\n",
      "Iteration 746, loss = 0.27751085\n",
      "Iteration 747, loss = 0.27735890\n",
      "Iteration 748, loss = 0.27722346\n",
      "Iteration 749, loss = 0.27707716\n",
      "Iteration 750, loss = 0.27698662\n",
      "Iteration 751, loss = 0.27683177\n",
      "Iteration 752, loss = 0.27667586\n",
      "Iteration 753, loss = 0.27657350\n",
      "Iteration 754, loss = 0.27633379\n",
      "Iteration 755, loss = 0.27616200\n",
      "Iteration 756, loss = 0.27609893\n",
      "Iteration 757, loss = 0.27588582\n",
      "Iteration 758, loss = 0.27570114\n",
      "Iteration 759, loss = 0.27556601\n",
      "Iteration 760, loss = 0.27540600\n",
      "Iteration 761, loss = 0.27533186\n",
      "Iteration 762, loss = 0.27511650\n",
      "Iteration 763, loss = 0.27500323\n",
      "Iteration 764, loss = 0.27484247\n",
      "Iteration 765, loss = 0.27467535\n",
      "Iteration 766, loss = 0.27454439\n",
      "Iteration 767, loss = 0.27446571\n",
      "Iteration 768, loss = 0.27429031\n",
      "Iteration 769, loss = 0.27410619\n",
      "Iteration 770, loss = 0.27398022\n",
      "Iteration 771, loss = 0.27382611\n",
      "Iteration 772, loss = 0.27372332\n",
      "Iteration 773, loss = 0.27358930\n",
      "Iteration 774, loss = 0.27342322\n",
      "Iteration 775, loss = 0.27329489\n",
      "Iteration 776, loss = 0.27325822\n",
      "Iteration 777, loss = 0.27298014\n",
      "Iteration 778, loss = 0.27284177\n",
      "Iteration 779, loss = 0.27271102\n",
      "Iteration 780, loss = 0.27261024\n",
      "Iteration 781, loss = 0.27241568\n",
      "Iteration 782, loss = 0.27229285\n",
      "Iteration 783, loss = 0.27217551\n",
      "Iteration 784, loss = 0.27195924\n",
      "Iteration 785, loss = 0.27187775\n",
      "Iteration 786, loss = 0.27182229\n",
      "Iteration 787, loss = 0.27151875\n",
      "Iteration 788, loss = 0.27139025\n",
      "Iteration 789, loss = 0.27123289\n",
      "Iteration 790, loss = 0.27109342\n",
      "Iteration 791, loss = 0.27100556\n",
      "Iteration 792, loss = 0.27084238\n",
      "Iteration 793, loss = 0.27069504\n",
      "Iteration 794, loss = 0.27054644\n",
      "Iteration 795, loss = 0.27035531\n",
      "Iteration 796, loss = 0.27023198\n",
      "Iteration 797, loss = 0.27009255\n",
      "Iteration 798, loss = 0.26994341\n",
      "Iteration 799, loss = 0.26982348\n",
      "Iteration 800, loss = 0.26966982\n",
      "Iteration 801, loss = 0.26944390\n",
      "Iteration 802, loss = 0.26941056\n",
      "Iteration 803, loss = 0.26920768\n",
      "Iteration 804, loss = 0.26907920\n",
      "Iteration 805, loss = 0.26897317\n",
      "Iteration 806, loss = 0.26878100\n",
      "Iteration 807, loss = 0.26876722\n",
      "Iteration 808, loss = 0.26847045\n",
      "Iteration 809, loss = 0.26838019\n",
      "Iteration 810, loss = 0.26824654\n",
      "Iteration 811, loss = 0.26803424\n",
      "Iteration 812, loss = 0.26792997\n",
      "Iteration 813, loss = 0.26779990\n",
      "Iteration 814, loss = 0.26773274\n",
      "Iteration 815, loss = 0.26746533\n",
      "Iteration 816, loss = 0.26736930\n",
      "Iteration 817, loss = 0.26718180\n",
      "Iteration 818, loss = 0.26708113\n",
      "Iteration 819, loss = 0.26692728\n",
      "Iteration 820, loss = 0.26679940\n",
      "Iteration 821, loss = 0.26675561\n",
      "Iteration 822, loss = 0.26650026\n",
      "Iteration 823, loss = 0.26638557\n",
      "Iteration 824, loss = 0.26617353\n",
      "Iteration 825, loss = 0.26608935\n",
      "Iteration 826, loss = 0.26601860\n",
      "Iteration 827, loss = 0.26579613\n",
      "Iteration 828, loss = 0.26562847\n",
      "Iteration 829, loss = 0.26550230\n",
      "Iteration 830, loss = 0.26533484\n",
      "Iteration 831, loss = 0.26522546\n",
      "Iteration 832, loss = 0.26514338\n",
      "Iteration 833, loss = 0.26499588\n",
      "Iteration 834, loss = 0.26478360\n",
      "Iteration 835, loss = 0.26466016\n",
      "Iteration 836, loss = 0.26454956\n",
      "Iteration 837, loss = 0.26438179\n",
      "Iteration 838, loss = 0.26422022\n",
      "Iteration 839, loss = 0.26408312\n",
      "Iteration 840, loss = 0.26410493\n",
      "Iteration 841, loss = 0.26384587\n",
      "Iteration 842, loss = 0.26371829\n",
      "Iteration 843, loss = 0.26360444\n",
      "Iteration 844, loss = 0.26340728\n",
      "Iteration 845, loss = 0.26324325\n",
      "Iteration 846, loss = 0.26309765\n",
      "Iteration 847, loss = 0.26297029\n",
      "Iteration 848, loss = 0.26301062\n",
      "Iteration 849, loss = 0.26271068\n",
      "Iteration 850, loss = 0.26263664\n",
      "Iteration 851, loss = 0.26251975\n",
      "Iteration 852, loss = 0.26237162\n",
      "Iteration 853, loss = 0.26218038\n",
      "Iteration 854, loss = 0.26198694\n",
      "Iteration 855, loss = 0.26184893\n",
      "Iteration 856, loss = 0.26175224\n",
      "Iteration 857, loss = 0.26166066\n",
      "Iteration 858, loss = 0.26143313\n",
      "Iteration 859, loss = 0.26134911\n",
      "Iteration 860, loss = 0.26118381\n",
      "Iteration 861, loss = 0.26115732\n",
      "Iteration 862, loss = 0.26094616\n",
      "Iteration 863, loss = 0.26073154\n",
      "Iteration 864, loss = 0.26063147\n",
      "Iteration 865, loss = 0.26048023\n",
      "Iteration 866, loss = 0.26041544\n",
      "Iteration 867, loss = 0.26031605\n",
      "Iteration 868, loss = 0.26010645\n",
      "Iteration 869, loss = 0.25992404\n",
      "Iteration 870, loss = 0.25974599\n",
      "Iteration 871, loss = 0.25976713\n",
      "Iteration 872, loss = 0.25947116\n",
      "Iteration 873, loss = 0.25934451\n",
      "Iteration 874, loss = 0.25932238\n",
      "Iteration 875, loss = 0.25907091\n",
      "Iteration 876, loss = 0.25888361\n",
      "Iteration 877, loss = 0.25880421\n",
      "Iteration 878, loss = 0.25873639\n",
      "Iteration 879, loss = 0.25857955\n",
      "Iteration 880, loss = 0.25837422\n",
      "Iteration 881, loss = 0.25823421\n",
      "Iteration 882, loss = 0.25807396\n",
      "Iteration 883, loss = 0.25798771\n",
      "Iteration 884, loss = 0.25784792\n",
      "Iteration 885, loss = 0.25763797\n",
      "Iteration 886, loss = 0.25751563\n",
      "Iteration 887, loss = 0.25736354\n",
      "Iteration 888, loss = 0.25723609\n",
      "Iteration 889, loss = 0.25707125\n",
      "Iteration 890, loss = 0.25696331\n",
      "Iteration 891, loss = 0.25674985\n",
      "Iteration 892, loss = 0.25673734\n",
      "Iteration 893, loss = 0.25649106\n",
      "Iteration 894, loss = 0.25649902\n",
      "Iteration 895, loss = 0.25624079\n",
      "Iteration 896, loss = 0.25605411\n",
      "Iteration 897, loss = 0.25587712\n",
      "Iteration 898, loss = 0.25573763\n",
      "Iteration 899, loss = 0.25562298\n",
      "Iteration 900, loss = 0.25548403\n",
      "Iteration 901, loss = 0.25531746\n",
      "Iteration 902, loss = 0.25537173\n",
      "Iteration 903, loss = 0.25506873\n",
      "Iteration 904, loss = 0.25487747\n",
      "Iteration 905, loss = 0.25474902\n",
      "Iteration 906, loss = 0.25459908\n",
      "Iteration 907, loss = 0.25441264\n",
      "Iteration 908, loss = 0.25427515\n",
      "Iteration 909, loss = 0.25412555\n",
      "Iteration 910, loss = 0.25397803\n",
      "Iteration 911, loss = 0.25386614\n",
      "Iteration 912, loss = 0.25369607\n",
      "Iteration 913, loss = 0.25353153\n",
      "Iteration 914, loss = 0.25341108\n",
      "Iteration 915, loss = 0.25327219\n",
      "Iteration 916, loss = 0.25315753\n",
      "Iteration 917, loss = 0.25303654\n",
      "Iteration 918, loss = 0.25292192\n",
      "Iteration 919, loss = 0.25276044\n",
      "Iteration 920, loss = 0.25267855\n",
      "Iteration 921, loss = 0.25245328\n",
      "Iteration 922, loss = 0.25220859\n",
      "Iteration 923, loss = 0.25211327\n",
      "Iteration 924, loss = 0.25193523\n",
      "Iteration 925, loss = 0.25194322\n",
      "Iteration 926, loss = 0.25171655\n",
      "Iteration 927, loss = 0.25149732\n",
      "Iteration 928, loss = 0.25132851\n",
      "Iteration 929, loss = 0.25125066\n",
      "Iteration 930, loss = 0.25110251\n",
      "Iteration 931, loss = 0.25091001\n",
      "Iteration 932, loss = 0.25075415\n",
      "Iteration 933, loss = 0.25059702\n",
      "Iteration 934, loss = 0.25044569\n",
      "Iteration 935, loss = 0.25034380\n",
      "Iteration 936, loss = 0.25018101\n",
      "Iteration 937, loss = 0.25007341\n",
      "Iteration 938, loss = 0.24988532\n",
      "Iteration 939, loss = 0.24974429\n",
      "Iteration 940, loss = 0.24954936\n",
      "Iteration 941, loss = 0.24940827\n",
      "Iteration 942, loss = 0.24930973\n",
      "Iteration 943, loss = 0.24913861\n",
      "Iteration 944, loss = 0.24897116\n",
      "Iteration 945, loss = 0.24878799\n",
      "Iteration 946, loss = 0.24871949\n",
      "Iteration 947, loss = 0.24851111\n",
      "Iteration 948, loss = 0.24838698\n",
      "Iteration 949, loss = 0.24823322\n",
      "Iteration 950, loss = 0.24811816\n",
      "Iteration 951, loss = 0.24790531\n",
      "Iteration 952, loss = 0.24788923\n",
      "Iteration 953, loss = 0.24765859\n",
      "Iteration 954, loss = 0.24748078\n",
      "Iteration 955, loss = 0.24729180\n",
      "Iteration 956, loss = 0.24717585\n",
      "Iteration 957, loss = 0.24702087\n",
      "Iteration 958, loss = 0.24686523\n",
      "Iteration 959, loss = 0.24696747\n",
      "Iteration 960, loss = 0.24657871\n",
      "Iteration 961, loss = 0.24642486\n",
      "Iteration 962, loss = 0.24636838\n",
      "Iteration 963, loss = 0.24619339\n",
      "Iteration 964, loss = 0.24602426\n",
      "Iteration 965, loss = 0.24581805\n",
      "Iteration 966, loss = 0.24571014\n",
      "Iteration 967, loss = 0.24552355\n",
      "Iteration 968, loss = 0.24539755\n",
      "Iteration 969, loss = 0.24526987\n",
      "Iteration 970, loss = 0.24506302\n",
      "Iteration 971, loss = 0.24495782\n",
      "Iteration 972, loss = 0.24481822\n",
      "Iteration 973, loss = 0.24460565\n",
      "Iteration 974, loss = 0.24447821\n",
      "Iteration 975, loss = 0.24440838\n",
      "Iteration 976, loss = 0.24428704\n",
      "Iteration 977, loss = 0.24407703\n",
      "Iteration 978, loss = 0.24391840\n",
      "Iteration 979, loss = 0.24374875\n",
      "Iteration 980, loss = 0.24359964\n",
      "Iteration 981, loss = 0.24348596\n",
      "Iteration 982, loss = 0.24330572\n",
      "Iteration 983, loss = 0.24316191\n",
      "Iteration 984, loss = 0.24298937\n",
      "Iteration 985, loss = 0.24284375\n",
      "Iteration 986, loss = 0.24271150\n",
      "Iteration 987, loss = 0.24264992\n",
      "Iteration 988, loss = 0.24259470\n",
      "Iteration 989, loss = 0.24238527\n",
      "Iteration 990, loss = 0.24213267\n",
      "Iteration 991, loss = 0.24191930\n",
      "Iteration 992, loss = 0.24180383\n",
      "Iteration 993, loss = 0.24161629\n",
      "Iteration 994, loss = 0.24146364\n",
      "Iteration 995, loss = 0.24133661\n",
      "Iteration 996, loss = 0.24133519\n",
      "Iteration 997, loss = 0.24101038\n",
      "Iteration 998, loss = 0.24091223\n",
      "Iteration 999, loss = 0.24074919\n",
      "Iteration 1000, loss = 0.24059507\n",
      "Iteration 1001, loss = 0.24042655\n",
      "Iteration 1002, loss = 0.24036332\n",
      "Iteration 1003, loss = 0.24010654\n",
      "Iteration 1004, loss = 0.24009364\n",
      "Iteration 1005, loss = 0.23980224\n",
      "Iteration 1006, loss = 0.23961707\n",
      "Iteration 1007, loss = 0.23946919\n",
      "Iteration 1008, loss = 0.23935552\n",
      "Iteration 1009, loss = 0.23916841\n",
      "Iteration 1010, loss = 0.23899440\n",
      "Iteration 1011, loss = 0.23901665\n",
      "Iteration 1012, loss = 0.23871696\n",
      "Iteration 1013, loss = 0.23862465\n",
      "Iteration 1014, loss = 0.23850578\n",
      "Iteration 1015, loss = 0.23826737\n",
      "Iteration 1016, loss = 0.23816493\n",
      "Iteration 1017, loss = 0.23791884\n",
      "Iteration 1018, loss = 0.23789979\n",
      "Iteration 1019, loss = 0.23760736\n",
      "Iteration 1020, loss = 0.23757909\n",
      "Iteration 1021, loss = 0.23733289\n",
      "Iteration 1022, loss = 0.23716918\n",
      "Iteration 1023, loss = 0.23704754\n",
      "Iteration 1024, loss = 0.23694505\n",
      "Iteration 1025, loss = 0.23667726\n",
      "Iteration 1026, loss = 0.23655507\n",
      "Iteration 1027, loss = 0.23647398\n",
      "Iteration 1028, loss = 0.23638847\n",
      "Iteration 1029, loss = 0.23620627\n",
      "Iteration 1030, loss = 0.23600699\n",
      "Iteration 1031, loss = 0.23587498\n",
      "Iteration 1032, loss = 0.23560776\n",
      "Iteration 1033, loss = 0.23562359\n",
      "Iteration 1034, loss = 0.23530123\n",
      "Iteration 1035, loss = 0.23513895\n",
      "Iteration 1036, loss = 0.23498586\n",
      "Iteration 1037, loss = 0.23489773\n",
      "Iteration 1038, loss = 0.23477001\n",
      "Iteration 1039, loss = 0.23452792\n",
      "Iteration 1040, loss = 0.23451545\n",
      "Iteration 1041, loss = 0.23437798\n",
      "Iteration 1042, loss = 0.23416523\n",
      "Iteration 1043, loss = 0.23395010\n",
      "Iteration 1044, loss = 0.23378708\n",
      "Iteration 1045, loss = 0.23359616\n",
      "Iteration 1046, loss = 0.23345197\n",
      "Iteration 1047, loss = 0.23339524\n",
      "Iteration 1048, loss = 0.23318759\n",
      "Iteration 1049, loss = 0.23305868\n",
      "Iteration 1050, loss = 0.23288004\n",
      "Iteration 1051, loss = 0.23271230\n",
      "Iteration 1052, loss = 0.23253407\n",
      "Iteration 1053, loss = 0.23245079\n",
      "Iteration 1054, loss = 0.23223577\n",
      "Iteration 1055, loss = 0.23209249\n",
      "Iteration 1056, loss = 0.23196615\n",
      "Iteration 1057, loss = 0.23178977\n",
      "Iteration 1058, loss = 0.23167874\n",
      "Iteration 1059, loss = 0.23150772\n",
      "Iteration 1060, loss = 0.23138279\n",
      "Iteration 1061, loss = 0.23117846\n",
      "Iteration 1062, loss = 0.23105233\n",
      "Iteration 1063, loss = 0.23087027\n",
      "Iteration 1064, loss = 0.23077480\n",
      "Iteration 1065, loss = 0.23064860\n",
      "Iteration 1066, loss = 0.23044531\n",
      "Iteration 1067, loss = 0.23035483\n",
      "Iteration 1068, loss = 0.23012984\n",
      "Iteration 1069, loss = 0.23012379\n",
      "Iteration 1070, loss = 0.22983058\n",
      "Iteration 1071, loss = 0.22969221\n",
      "Iteration 1072, loss = 0.22947730\n",
      "Iteration 1073, loss = 0.22933741\n",
      "Iteration 1074, loss = 0.22927203\n",
      "Iteration 1075, loss = 0.22908058\n",
      "Iteration 1076, loss = 0.22890189\n",
      "Iteration 1077, loss = 0.22870189\n",
      "Iteration 1078, loss = 0.22861321\n",
      "Iteration 1079, loss = 0.22846854\n",
      "Iteration 1080, loss = 0.22826013\n",
      "Iteration 1081, loss = 0.22806836\n",
      "Iteration 1082, loss = 0.22793986\n",
      "Iteration 1083, loss = 0.22779348\n",
      "Iteration 1084, loss = 0.22761888\n",
      "Iteration 1085, loss = 0.22748127\n",
      "Iteration 1086, loss = 0.22734969\n",
      "Iteration 1087, loss = 0.22718283\n",
      "Iteration 1088, loss = 0.22715507\n",
      "Iteration 1089, loss = 0.22684294\n",
      "Iteration 1090, loss = 0.22671220\n",
      "Iteration 1091, loss = 0.22656682\n",
      "Iteration 1092, loss = 0.22656082\n",
      "Iteration 1093, loss = 0.22641195\n",
      "Iteration 1094, loss = 0.22616152\n",
      "Iteration 1095, loss = 0.22593177\n",
      "Iteration 1096, loss = 0.22576840\n",
      "Iteration 1097, loss = 0.22560735\n",
      "Iteration 1098, loss = 0.22565613\n",
      "Iteration 1099, loss = 0.22536029\n",
      "Iteration 1100, loss = 0.22544298\n",
      "Iteration 1101, loss = 0.22500134\n",
      "Iteration 1102, loss = 0.22486214\n",
      "Iteration 1103, loss = 0.22484559\n",
      "Iteration 1104, loss = 0.22456965\n",
      "Iteration 1105, loss = 0.22441765\n",
      "Iteration 1106, loss = 0.22431358\n",
      "Iteration 1107, loss = 0.22407915\n",
      "Iteration 1108, loss = 0.22394976\n",
      "Iteration 1109, loss = 0.22378486\n",
      "Iteration 86, loss = 0.41816037\n",
      "Iteration 87, loss = 0.41705838\n",
      "Iteration 88, loss = 0.41593602\n",
      "Iteration 89, loss = 0.41495454\n",
      "Iteration 90, loss = 0.41385074\n",
      "Iteration 91, loss = 0.41281751\n",
      "Iteration 92, loss = 0.41176140\n",
      "Iteration 93, loss = 0.41081152\n",
      "Iteration 94, loss = 0.40986847\n",
      "Iteration 95, loss = 0.40890778\n",
      "Iteration 96, loss = 0.40793784\n",
      "Iteration 97, loss = 0.40703461\n",
      "Iteration 98, loss = 0.40616198\n",
      "Iteration 99, loss = 0.40523142\n",
      "Iteration 100, loss = 0.40437998\n",
      "Iteration 101, loss = 0.40353423\n",
      "Iteration 102, loss = 0.40265729\n",
      "Iteration 103, loss = 0.40186147\n",
      "Iteration 104, loss = 0.40102128\n",
      "Iteration 105, loss = 0.40023993\n",
      "Iteration 106, loss = 0.39944562\n",
      "Iteration 107, loss = 0.39868346\n",
      "Iteration 108, loss = 0.39791705\n",
      "Iteration 109, loss = 0.39720264\n",
      "Iteration 110, loss = 0.39641853\n",
      "Iteration 111, loss = 0.39570860\n",
      "Iteration 112, loss = 0.39500852\n",
      "Iteration 113, loss = 0.39429348\n",
      "Iteration 114, loss = 0.39364333\n",
      "Iteration 115, loss = 0.39293910\n",
      "Iteration 116, loss = 0.39228082\n",
      "Iteration 117, loss = 0.39166166\n",
      "Iteration 118, loss = 0.39099653\n",
      "Iteration 119, loss = 0.39035994\n",
      "Iteration 120, loss = 0.38973553\n",
      "Iteration 121, loss = 0.38912229\n",
      "Iteration 122, loss = 0.38853722\n",
      "Iteration 123, loss = 0.38793745\n",
      "Iteration 124, loss = 0.38735320\n",
      "Iteration 125, loss = 0.38676615\n",
      "Iteration 126, loss = 0.38624409\n",
      "Iteration 127, loss = 0.38562313\n",
      "Iteration 128, loss = 0.38508601\n",
      "Iteration 129, loss = 0.38453931\n",
      "Iteration 130, loss = 0.38402667\n",
      "Iteration 131, loss = 0.38349652\n",
      "Iteration 132, loss = 0.38295846\n",
      "Iteration 133, loss = 0.38244521\n",
      "Iteration 134, loss = 0.38195846\n",
      "Iteration 135, loss = 0.38144478\n",
      "Iteration 136, loss = 0.38096218\n",
      "Iteration 137, loss = 0.38043530\n",
      "Iteration 138, loss = 0.37998178\n",
      "Iteration 139, loss = 0.37951879\n",
      "Iteration 140, loss = 0.37904778\n",
      "Iteration 141, loss = 0.37858816\n",
      "Iteration 142, loss = 0.37811390\n",
      "Iteration 143, loss = 0.37766339\n",
      "Iteration 144, loss = 0.37722867\n",
      "Iteration 145, loss = 0.37680042\n",
      "Iteration 146, loss = 0.37635317\n",
      "Iteration 147, loss = 0.37592416\n",
      "Iteration 148, loss = 0.37550921\n",
      "Iteration 149, loss = 0.37507677\n",
      "Iteration 150, loss = 0.37466694\n",
      "Iteration 151, loss = 0.37423902\n",
      "Iteration 152, loss = 0.37385008\n",
      "Iteration 153, loss = 0.37348449\n",
      "Iteration 154, loss = 0.37306873\n",
      "Iteration 155, loss = 0.37267782\n",
      "Iteration 156, loss = 0.37228964\n",
      "Iteration 157, loss = 0.37189667\n",
      "Iteration 158, loss = 0.37154414\n",
      "Iteration 159, loss = 0.37115259\n",
      "Iteration 160, loss = 0.37078676\n",
      "Iteration 161, loss = 0.37043828\n",
      "Iteration 162, loss = 0.37005751\n",
      "Iteration 163, loss = 0.36972625\n",
      "Iteration 164, loss = 0.36936969\n",
      "Iteration 165, loss = 0.36902272\n",
      "Iteration 166, loss = 0.36868338\n",
      "Iteration 167, loss = 0.36833581\n",
      "Iteration 168, loss = 0.36799551\n",
      "Iteration 169, loss = 0.36768402\n",
      "Iteration 170, loss = 0.36734425\n",
      "Iteration 171, loss = 0.36700848\n",
      "Iteration 172, loss = 0.36670938\n",
      "Iteration 173, loss = 0.36637543\n",
      "Iteration 174, loss = 0.36605403\n",
      "Iteration 175, loss = 0.36576202\n",
      "Iteration 176, loss = 0.36542968\n",
      "Iteration 177, loss = 0.36513834\n",
      "Iteration 178, loss = 0.36485207\n",
      "Iteration 179, loss = 0.36453228\n",
      "Iteration 180, loss = 0.36423814\n",
      "Iteration 181, loss = 0.36394832\n",
      "Iteration 182, loss = 0.36366014\n",
      "Iteration 183, loss = 0.36336204\n",
      "Iteration 184, loss = 0.36307468\n",
      "Iteration 185, loss = 0.36279696\n",
      "Iteration 186, loss = 0.36251434\n",
      "Iteration 187, loss = 0.36224455\n",
      "Iteration 188, loss = 0.36195399\n",
      "Iteration 189, loss = 0.36170847\n",
      "Iteration 190, loss = 0.36142075\n",
      "Iteration 191, loss = 0.36114569\n",
      "Iteration 192, loss = 0.36088046\n",
      "Iteration 193, loss = 0.36061560\n",
      "Iteration 194, loss = 0.36035946\n",
      "Iteration 195, loss = 0.36008427\n",
      "Iteration 196, loss = 0.35984201\n",
      "Iteration 197, loss = 0.35957196\n",
      "Iteration 198, loss = 0.35933057\n",
      "Iteration 199, loss = 0.35910217\n",
      "Iteration 200, loss = 0.35882059\n",
      "Iteration 201, loss = 0.35858436\n",
      "Iteration 202, loss = 0.35835135\n",
      "Iteration 203, loss = 0.35809673\n",
      "Iteration 204, loss = 0.35783919\n",
      "Iteration 205, loss = 0.35759962\n",
      "Iteration 206, loss = 0.35737081\n",
      "Iteration 207, loss = 0.35713594\n",
      "Iteration 208, loss = 0.35691421\n",
      "Iteration 209, loss = 0.35665883\n",
      "Iteration 210, loss = 0.35644061\n",
      "Iteration 211, loss = 0.35618277\n",
      "Iteration 212, loss = 0.35597511\n",
      "Iteration 213, loss = 0.35577186\n",
      "Iteration 214, loss = 0.35553256\n",
      "Iteration 215, loss = 0.35529322\n",
      "Iteration 216, loss = 0.35507224\n",
      "Iteration 217, loss = 0.35486187\n",
      "Iteration 218, loss = 0.35464623\n",
      "Iteration 219, loss = 0.35442340\n",
      "Iteration 220, loss = 0.35420142\n",
      "Iteration 221, loss = 0.35401313\n",
      "Iteration 222, loss = 0.35378993\n",
      "Iteration 223, loss = 0.35358902\n",
      "Iteration 224, loss = 0.35336588\n",
      "Iteration 225, loss = 0.35316893\n",
      "Iteration 226, loss = 0.35294859\n",
      "Iteration 227, loss = 0.35275260\n",
      "Iteration 228, loss = 0.35254777\n",
      "Iteration 229, loss = 0.35235477\n",
      "Iteration 230, loss = 0.35214606\n",
      "Iteration 231, loss = 0.35195960\n",
      "Iteration 232, loss = 0.35174537\n",
      "Iteration 233, loss = 0.35156380\n",
      "Iteration 234, loss = 0.35136290\n",
      "Iteration 235, loss = 0.35117005\n",
      "Iteration 236, loss = 0.35098950\n",
      "Iteration 237, loss = 0.35078834\n",
      "Iteration 238, loss = 0.35059358\n",
      "Iteration 239, loss = 0.35039842\n",
      "Iteration 240, loss = 0.35021702\n",
      "Iteration 241, loss = 0.35004141\n",
      "Iteration 242, loss = 0.34985138\n",
      "Iteration 243, loss = 0.34966549\n",
      "Iteration 244, loss = 0.34949273\n",
      "Iteration 245, loss = 0.34930122\n",
      "Iteration 246, loss = 0.34910829\n",
      "Iteration 247, loss = 0.34893078\n",
      "Iteration 248, loss = 0.34875667\n",
      "Iteration 249, loss = 0.34856886\n",
      "Iteration 250, loss = 0.34840134\n",
      "Iteration 251, loss = 0.34822597\n",
      "Iteration 252, loss = 0.34805194\n",
      "Iteration 253, loss = 0.34787999\n",
      "Iteration 254, loss = 0.34770060\n",
      "Iteration 255, loss = 0.34753635\n",
      "Iteration 256, loss = 0.34736788\n",
      "Iteration 257, loss = 0.34718095\n",
      "Iteration 258, loss = 0.34701089\n",
      "Iteration 259, loss = 0.34685911\n",
      "Iteration 260, loss = 0.34669082\n",
      "Iteration 261, loss = 0.34651919\n",
      "Iteration 262, loss = 0.34635160\n",
      "Iteration 263, loss = 0.34618645\n",
      "Iteration 264, loss = 0.34601521\n",
      "Iteration 265, loss = 0.34587180\n",
      "Iteration 266, loss = 0.34570038\n",
      "Iteration 267, loss = 0.34554170\n",
      "Iteration 268, loss = 0.34537497\n",
      "Iteration 269, loss = 0.34521820\n",
      "Iteration 270, loss = 0.34505487\n",
      "Iteration 271, loss = 0.34489452\n",
      "Iteration 272, loss = 0.34474628\n",
      "Iteration 273, loss = 0.34458847\n",
      "Iteration 274, loss = 0.34444424\n",
      "Iteration 275, loss = 0.34427956\n",
      "Iteration 276, loss = 0.34412191\n",
      "Iteration 277, loss = 0.34397021\n",
      "Iteration 278, loss = 0.34383027\n",
      "Iteration 279, loss = 0.34367766\n",
      "Iteration 280, loss = 0.34352909\n",
      "Iteration 281, loss = 0.34338062\n",
      "Iteration 282, loss = 0.34322908\n",
      "Iteration 283, loss = 0.34307728\n",
      "Iteration 284, loss = 0.34292965\n",
      "Iteration 285, loss = 0.34278859\n",
      "Iteration 286, loss = 0.34265285\n",
      "Iteration 287, loss = 0.34249887\n",
      "Iteration 288, loss = 0.34235273\n",
      "Iteration 289, loss = 0.34221348\n",
      "Iteration 290, loss = 0.34207507\n",
      "Iteration 291, loss = 0.34193789\n",
      "Iteration 292, loss = 0.34180832\n",
      "Iteration 293, loss = 0.34164492\n",
      "Iteration 294, loss = 0.34150992\n",
      "Iteration 295, loss = 0.34137465\n",
      "Iteration 296, loss = 0.34123009\n",
      "Iteration 297, loss = 0.34109520\n",
      "Iteration 298, loss = 0.34097223\n",
      "Iteration 299, loss = 0.34083279\n",
      "Iteration 300, loss = 0.34069067\n",
      "Iteration 301, loss = 0.34054701\n",
      "Iteration 302, loss = 0.34041185\n",
      "Iteration 303, loss = 0.34029782\n",
      "Iteration 304, loss = 0.34015941\n",
      "Iteration 305, loss = 0.34002673\n",
      "Iteration 306, loss = 0.33989869\n",
      "Iteration 307, loss = 0.33977056\n",
      "Iteration 308, loss = 0.33963613\n",
      "Iteration 309, loss = 0.33951111\n",
      "Iteration 310, loss = 0.33937511\n",
      "Iteration 311, loss = 0.33925288\n",
      "Iteration 312, loss = 0.33912157\n",
      "Iteration 313, loss = 0.33899038\n",
      "Iteration 314, loss = 0.33888433\n",
      "Iteration 315, loss = 0.33873541\n",
      "Iteration 316, loss = 0.33861836\n",
      "Iteration 317, loss = 0.33849371\n",
      "Iteration 318, loss = 0.33838529\n",
      "Iteration 319, loss = 0.33825016\n",
      "Iteration 320, loss = 0.33812666\n",
      "Iteration 321, loss = 0.33799671\n",
      "Iteration 322, loss = 0.33788088\n",
      "Iteration 323, loss = 0.33775795\n",
      "Iteration 324, loss = 0.33763969\n",
      "Iteration 325, loss = 0.33751648\n",
      "Iteration 326, loss = 0.33740382\n",
      "Iteration 327, loss = 0.33728027\n",
      "Iteration 328, loss = 0.33715994\n",
      "Iteration 329, loss = 0.33704214\n",
      "Iteration 330, loss = 0.33692401\n",
      "Iteration 331, loss = 0.33681607\n",
      "Iteration 332, loss = 0.33668558\n",
      "Iteration 333, loss = 0.33657758\n",
      "Iteration 334, loss = 0.33646095\n",
      "Iteration 335, loss = 0.33635195\n",
      "Iteration 336, loss = 0.33623041\n",
      "Iteration 337, loss = 0.33612378\n",
      "Iteration 338, loss = 0.33600744\n",
      "Iteration 339, loss = 0.33589724\n",
      "Iteration 340, loss = 0.33577743\n",
      "Iteration 341, loss = 0.33566661\n",
      "Iteration 342, loss = 0.33556040\n",
      "Iteration 343, loss = 0.33544442\n",
      "Iteration 344, loss = 0.33534990\n",
      "Iteration 345, loss = 0.33523278\n",
      "Iteration 346, loss = 0.33512022\n",
      "Iteration 347, loss = 0.33501161\n",
      "Iteration 348, loss = 0.33490831\n",
      "Iteration 349, loss = 0.33479992\n",
      "Iteration 350, loss = 0.33469672\n",
      "Iteration 351, loss = 0.33458500\n",
      "Iteration 352, loss = 0.33448310\n",
      "Iteration 353, loss = 0.33438062\n",
      "Iteration 354, loss = 0.33427469\n",
      "Iteration 355, loss = 0.33416176\n",
      "Iteration 356, loss = 0.33405330\n",
      "Iteration 357, loss = 0.33395859\n",
      "Iteration 358, loss = 0.33385324\n",
      "Iteration 359, loss = 0.33374765\n",
      "Iteration 360, loss = 0.33365370\n",
      "Iteration 361, loss = 0.33354697\n",
      "Iteration 362, loss = 0.33344726\n",
      "Iteration 363, loss = 0.33333910\n",
      "Iteration 364, loss = 0.33323636\n",
      "Iteration 365, loss = 0.33313040\n",
      "Iteration 366, loss = 0.33304563\n",
      "Iteration 367, loss = 0.33293241\n",
      "Iteration 368, loss = 0.33284727\n",
      "Iteration 369, loss = 0.33272284\n",
      "Iteration 370, loss = 0.33263336\n",
      "Iteration 371, loss = 0.33252591\n",
      "Iteration 372, loss = 0.33242263\n",
      "Iteration 373, loss = 0.33233260\n",
      "Iteration 374, loss = 0.33223193\n",
      "Iteration 375, loss = 0.33213729\n",
      "Iteration 376, loss = 0.33203755\n",
      "Iteration 377, loss = 0.33194668\n",
      "Iteration 378, loss = 0.33184188\n",
      "Iteration 379, loss = 0.33173720\n",
      "Iteration 380, loss = 0.33165548\n",
      "Iteration 381, loss = 0.33154799\n",
      "Iteration 382, loss = 0.33146036\n",
      "Iteration 383, loss = 0.33135602\n",
      "Iteration 384, loss = 0.33126974\n",
      "Iteration 385, loss = 0.33117104\n",
      "Iteration 386, loss = 0.33108831\n",
      "Iteration 387, loss = 0.33098530\n",
      "Iteration 388, loss = 0.33089886\n",
      "Iteration 389, loss = 0.33079659\n",
      "Iteration 390, loss = 0.33070908\n",
      "Iteration 391, loss = 0.33062515\n",
      "Iteration 392, loss = 0.33052063\n",
      "Iteration 393, loss = 0.33043317\n",
      "Iteration 394, loss = 0.33034249\n",
      "Iteration 395, loss = 0.33024817\n",
      "Iteration 396, loss = 0.33015587\n",
      "Iteration 397, loss = 0.33006812\n",
      "Iteration 398, loss = 0.32998028\n",
      "Iteration 399, loss = 0.32989120\n",
      "Iteration 400, loss = 0.32980358\n",
      "Iteration 401, loss = 0.32970653\n",
      "Iteration 402, loss = 0.32961491\n",
      "Iteration 403, loss = 0.32954098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70577840\n",
      "Iteration 2, loss = 0.70310377\n",
      "Iteration 3, loss = 0.69895360\n",
      "Iteration 4, loss = 0.69389313\n",
      "Iteration 5, loss = 0.68817989\n",
      "Iteration 6, loss = 0.68196911\n",
      "Iteration 7, loss = 0.67564872\n",
      "Iteration 8, loss = 0.66911347\n",
      "Iteration 9, loss = 0.66257107\n",
      "Iteration 10, loss = 0.65583547\n",
      "Iteration 11, loss = 0.64929742\n",
      "Iteration 12, loss = 0.64274939\n",
      "Iteration 13, loss = 0.63655779\n",
      "Iteration 14, loss = 0.63027517\n",
      "Iteration 15, loss = 0.62419910\n",
      "Iteration 16, loss = 0.61805513\n",
      "Iteration 17, loss = 0.61224200\n",
      "Iteration 18, loss = 0.60666398\n",
      "Iteration 19, loss = 0.60106239\n",
      "Iteration 20, loss = 0.59558785\n",
      "Iteration 21, loss = 0.59018455\n",
      "Iteration 22, loss = 0.58507822\n",
      "Iteration 23, loss = 0.58003452\n",
      "Iteration 24, loss = 0.57506588\n",
      "Iteration 25, loss = 0.57029417\n",
      "Iteration 26, loss = 0.56541138\n",
      "Iteration 27, loss = 0.56069330\n",
      "Iteration 28, loss = 0.55620859\n",
      "Iteration 29, loss = 0.55187557\n",
      "Iteration 30, loss = 0.54759410\n",
      "Iteration 31, loss = 0.54325556\n",
      "Iteration 32, loss = 0.53928113\n",
      "Iteration 33, loss = 0.53516386\n",
      "Iteration 34, loss = 0.53116901\n",
      "Iteration 35, loss = 0.52737348\n",
      "Iteration 36, loss = 0.52368005\n",
      "Iteration 37, loss = 0.51999593\n",
      "Iteration 38, loss = 0.51642748\n",
      "Iteration 39, loss = 0.51287168\n",
      "Iteration 40, loss = 0.50951824\n",
      "Iteration 41, loss = 0.50617469\n",
      "Iteration 42, loss = 0.50302740\n",
      "Iteration 43, loss = 0.49978799\n",
      "Iteration 44, loss = 0.49674733\n",
      "Iteration 45, loss = 0.49383745\n",
      "Iteration 46, loss = 0.49074587\n",
      "Iteration 47, loss = 0.48793936\n",
      "Iteration 48, loss = 0.48508407\n",
      "Iteration 49, loss = 0.48232946\n",
      "Iteration 50, loss = 0.47971288\n",
      "Iteration 51, loss = 0.47710696\n",
      "Iteration 52, loss = 0.47457202\n",
      "Iteration 53, loss = 0.47209790\n",
      "Iteration 54, loss = 0.46960630\n",
      "Iteration 55, loss = 0.46728502\n",
      "Iteration 56, loss = 0.46499097\n",
      "Iteration 57, loss = 0.46268597\n",
      "Iteration 58, loss = 0.46049899\n",
      "Iteration 59, loss = 0.45833732\n",
      "Iteration 60, loss = 0.45626127\n",
      "Iteration 61, loss = 0.45420434\n",
      "Iteration 62, loss = 0.45220019\n",
      "Iteration 63, loss = 0.45024862\n",
      "Iteration 64, loss = 0.44830346\n",
      "Iteration 65, loss = 0.44639890\n",
      "Iteration 66, loss = 0.44465273\n",
      "Iteration 67, loss = 0.44284362\n",
      "Iteration 68, loss = 0.44112517\n",
      "Iteration 69, loss = 0.43946252\n",
      "Iteration 70, loss = 0.43776789\n",
      "Iteration 71, loss = 0.43617817\n",
      "Iteration 72, loss = 0.43462117\n",
      "Iteration 73, loss = 0.43307111\n",
      "Iteration 74, loss = 0.43157314\n",
      "Iteration 75, loss = 0.43007316\n",
      "Iteration 76, loss = 0.42866158\n",
      "Iteration 77, loss = 0.42729697\n",
      "Iteration 78, loss = 0.42586875\n",
      "Iteration 79, loss = 0.42454260\n",
      "Iteration 80, loss = 0.42321878\n",
      "Iteration 81, loss = 0.42197240\n",
      "Iteration 82, loss = 0.42074266\n",
      "Iteration 83, loss = 0.41950649\n",
      "Iteration 84, loss = 0.41830524\n",
      "Iteration 85, loss = 0.41714003\n",
      "Iteration 86, loss = 0.41601478\n",
      "Iteration 87, loss = 0.41490631\n",
      "Iteration 88, loss = 0.41384282\n",
      "Iteration 89, loss = 0.41275436\n",
      "Iteration 90, loss = 0.41169287\n",
      "Iteration 91, loss = 0.41067228\n",
      "Iteration 92, loss = 0.40970338\n",
      "Iteration 93, loss = 0.40866229\n",
      "Iteration 94, loss = 0.40772754\n",
      "Iteration 95, loss = 0.40677565\n",
      "Iteration 96, loss = 0.40585615\n",
      "Iteration 97, loss = 0.40496721\n",
      "Iteration 98, loss = 0.40404384\n",
      "Iteration 99, loss = 0.40314145\n",
      "Iteration 100, loss = 0.40226389\n",
      "Iteration 101, loss = 0.40146110\n",
      "Iteration 102, loss = 0.40060030\n",
      "Iteration 103, loss = 0.39977687\n",
      "Iteration 104, loss = 0.39900122\n",
      "Iteration 105, loss = 0.39816960\n",
      "Iteration 106, loss = 0.39746288\n",
      "Iteration 107, loss = 0.39662298\n",
      "Iteration 108, loss = 0.39589575\n",
      "Iteration 109, loss = 0.39516690\n",
      "Iteration 110, loss = 0.39440626\n",
      "Iteration 111, loss = 0.39372037\n",
      "Iteration 112, loss = 0.39303741\n",
      "Iteration 113, loss = 0.39234545\n",
      "Iteration 114, loss = 0.39164558\n",
      "Iteration 115, loss = 0.39096146\n",
      "Iteration 116, loss = 0.39029952\n",
      "Iteration 117, loss = 0.38962063\n",
      "Iteration 118, loss = 0.38901313\n",
      "Iteration 119, loss = 0.38833953\n",
      "Iteration 120, loss = 0.38769158\n",
      "Iteration 121, loss = 0.38713312\n",
      "Iteration 122, loss = 0.38649548\n",
      "Iteration 123, loss = 0.38592919\n",
      "Iteration 124, loss = 0.38533129\n",
      "Iteration 125, loss = 0.38473167\n",
      "Iteration 126, loss = 0.38417296\n",
      "Iteration 127, loss = 0.38363013\n",
      "Iteration 128, loss = 0.38304529\n",
      "Iteration 129, loss = 0.38248503\n",
      "Iteration 130, loss = 0.38196322\n",
      "Iteration 131, loss = 0.38138912\n",
      "Iteration 132, loss = 0.38082713\n",
      "Iteration 133, loss = 0.38036073\n",
      "Iteration 134, loss = 0.37980533\n",
      "Iteration 135, loss = 0.37930639\n",
      "Iteration 136, loss = 0.37881043\n",
      "Iteration 137, loss = 0.37827878\n",
      "Iteration 138, loss = 0.37780049\n",
      "Iteration 139, loss = 0.37730080\n",
      "Iteration 140, loss = 0.37683047\n",
      "Iteration 141, loss = 0.37633970\n",
      "Iteration 142, loss = 0.37583281\n",
      "Iteration 143, loss = 0.37536371\n",
      "Iteration 144, loss = 0.37490751\n",
      "Iteration 145, loss = 0.37445342\n",
      "Iteration 146, loss = 0.37398510\n",
      "Iteration 147, loss = 0.37353398\n",
      "Iteration 148, loss = 0.37309569\n",
      "Iteration 149, loss = 0.37266437\n",
      "Iteration 150, loss = 0.37220629\n",
      "Iteration 151, loss = 0.37179883\n",
      "Iteration 152, loss = 0.37135125\n",
      "Iteration 153, loss = 0.37094415\n",
      "Iteration 154, loss = 0.37052891\n",
      "Iteration 155, loss = 0.37008958\n",
      "Iteration 156, loss = 0.36966733\n",
      "Iteration 157, loss = 0.36926781\n",
      "Iteration 158, loss = 0.36887246\n",
      "Iteration 159, loss = 0.36846938\n",
      "Iteration 160, loss = 0.36810970\n",
      "Iteration 161, loss = 0.36769384\n",
      "Iteration 162, loss = 0.36731465\n",
      "Iteration 163, loss = 0.36692891\n",
      "Iteration 164, loss = 0.36654753\n",
      "Iteration 165, loss = 0.36618283\n",
      "Iteration 166, loss = 0.36578410\n",
      "Iteration 167, loss = 0.36540757\n",
      "Iteration 168, loss = 0.36506074\n",
      "Iteration 169, loss = 0.36468233\n",
      "Iteration 170, loss = 0.36431224\n",
      "Iteration 171, loss = 0.36397182\n",
      "Iteration 172, loss = 0.36362040\n",
      "Iteration 173, loss = 0.36327001\n",
      "Iteration 174, loss = 0.36289713\n",
      "Iteration 175, loss = 0.36258672\n",
      "Iteration 176, loss = 0.36223276\n",
      "Iteration 177, loss = 0.36188976\n",
      "Iteration 178, loss = 0.36156600\n",
      "Iteration 495, loss = 0.30580129\n",
      "Iteration 496, loss = 0.30564604\n",
      "Iteration 497, loss = 0.30555770\n",
      "Iteration 498, loss = 0.30541728\n",
      "Iteration 499, loss = 0.30517288\n",
      "Iteration 500, loss = 0.30506883\n",
      "Iteration 501, loss = 0.30491802\n",
      "Iteration 502, loss = 0.30473399\n",
      "Iteration 503, loss = 0.30461700\n",
      "Iteration 504, loss = 0.30448546\n",
      "Iteration 505, loss = 0.30429623\n",
      "Iteration 506, loss = 0.30418368\n",
      "Iteration 507, loss = 0.30399862\n",
      "Iteration 508, loss = 0.30385922\n",
      "Iteration 509, loss = 0.30375820\n",
      "Iteration 510, loss = 0.30356727\n",
      "Iteration 511, loss = 0.30343315\n",
      "Iteration 512, loss = 0.30329741\n",
      "Iteration 513, loss = 0.30320246\n",
      "Iteration 514, loss = 0.30305947\n",
      "Iteration 515, loss = 0.30286828\n",
      "Iteration 516, loss = 0.30271508\n",
      "Iteration 517, loss = 0.30256901\n",
      "Iteration 518, loss = 0.30245677\n",
      "Iteration 519, loss = 0.30230827\n",
      "Iteration 520, loss = 0.30214435\n",
      "Iteration 521, loss = 0.30201037\n",
      "Iteration 522, loss = 0.30183801\n",
      "Iteration 523, loss = 0.30171551\n",
      "Iteration 524, loss = 0.30162821\n",
      "Iteration 525, loss = 0.30145959\n",
      "Iteration 526, loss = 0.30128377\n",
      "Iteration 527, loss = 0.30112001\n",
      "Iteration 528, loss = 0.30099495\n",
      "Iteration 529, loss = 0.30085478\n",
      "Iteration 530, loss = 0.30069206\n",
      "Iteration 531, loss = 0.30054243\n",
      "Iteration 532, loss = 0.30041519\n",
      "Iteration 533, loss = 0.30026809\n",
      "Iteration 534, loss = 0.30013189\n",
      "Iteration 535, loss = 0.29997435\n",
      "Iteration 536, loss = 0.29986125\n",
      "Iteration 537, loss = 0.29970303\n",
      "Iteration 538, loss = 0.29954225\n",
      "Iteration 539, loss = 0.29942725\n",
      "Iteration 540, loss = 0.29926259\n",
      "Iteration 541, loss = 0.29912579\n",
      "Iteration 542, loss = 0.29898583\n",
      "Iteration 543, loss = 0.29882978\n",
      "Iteration 544, loss = 0.29869819\n",
      "Iteration 545, loss = 0.29853216\n",
      "Iteration 546, loss = 0.29842247\n",
      "Iteration 547, loss = 0.29828122\n",
      "Iteration 548, loss = 0.29811787\n",
      "Iteration 549, loss = 0.29797817\n",
      "Iteration 550, loss = 0.29783529\n",
      "Iteration 551, loss = 0.29768526\n",
      "Iteration 552, loss = 0.29757416\n",
      "Iteration 553, loss = 0.29745829\n",
      "Iteration 554, loss = 0.29728749\n",
      "Iteration 555, loss = 0.29717637\n",
      "Iteration 556, loss = 0.29699520\n",
      "Iteration 557, loss = 0.29684734\n",
      "Iteration 558, loss = 0.29671101\n",
      "Iteration 559, loss = 0.29659739\n",
      "Iteration 560, loss = 0.29644843\n",
      "Iteration 561, loss = 0.29630330\n",
      "Iteration 562, loss = 0.29618042\n",
      "Iteration 563, loss = 0.29601663\n",
      "Iteration 564, loss = 0.29596400\n",
      "Iteration 565, loss = 0.29572652\n",
      "Iteration 566, loss = 0.29556850\n",
      "Iteration 567, loss = 0.29546546\n",
      "Iteration 568, loss = 0.29531733\n",
      "Iteration 569, loss = 0.29516610\n",
      "Iteration 570, loss = 0.29508633\n",
      "Iteration 571, loss = 0.29491644\n",
      "Iteration 572, loss = 0.29477182\n",
      "Iteration 573, loss = 0.29467216\n",
      "Iteration 574, loss = 0.29447193\n",
      "Iteration 575, loss = 0.29435534\n",
      "Iteration 576, loss = 0.29421927\n",
      "Iteration 577, loss = 0.29407269\n",
      "Iteration 578, loss = 0.29396071\n",
      "Iteration 579, loss = 0.29381076\n",
      "Iteration 580, loss = 0.29365709\n",
      "Iteration 581, loss = 0.29353002\n",
      "Iteration 582, loss = 0.29338181\n",
      "Iteration 583, loss = 0.29323867\n",
      "Iteration 584, loss = 0.29310418\n",
      "Iteration 585, loss = 0.29295764\n",
      "Iteration 586, loss = 0.29284403\n",
      "Iteration 587, loss = 0.29269659\n",
      "Iteration 588, loss = 0.29256712\n",
      "Iteration 589, loss = 0.29247024\n",
      "Iteration 590, loss = 0.29227905\n",
      "Iteration 591, loss = 0.29216180\n",
      "Iteration 592, loss = 0.29202759\n",
      "Iteration 593, loss = 0.29186332\n",
      "Iteration 594, loss = 0.29172959\n",
      "Iteration 595, loss = 0.29160359\n",
      "Iteration 596, loss = 0.29148680\n",
      "Iteration 597, loss = 0.29129670\n",
      "Iteration 598, loss = 0.29116590\n",
      "Iteration 599, loss = 0.29101257\n",
      "Iteration 600, loss = 0.29089464\n",
      "Iteration 601, loss = 0.29075525\n",
      "Iteration 602, loss = 0.29078196\n",
      "Iteration 603, loss = 0.29045718\n",
      "Iteration 604, loss = 0.29033217\n",
      "Iteration 605, loss = 0.29023878\n",
      "Iteration 606, loss = 0.29006289\n",
      "Iteration 607, loss = 0.28990965\n",
      "Iteration 608, loss = 0.28976882\n",
      "Iteration 609, loss = 0.28965547\n",
      "Iteration 610, loss = 0.28951511\n",
      "Iteration 611, loss = 0.28940320\n",
      "Iteration 612, loss = 0.28923000\n",
      "Iteration 613, loss = 0.28908328\n",
      "Iteration 614, loss = 0.28897895\n",
      "Iteration 615, loss = 0.28882761\n",
      "Iteration 616, loss = 0.28874311\n",
      "Iteration 617, loss = 0.28854279\n",
      "Iteration 618, loss = 0.28843463\n",
      "Iteration 619, loss = 0.28828794\n",
      "Iteration 620, loss = 0.28813023\n",
      "Iteration 621, loss = 0.28798679\n",
      "Iteration 622, loss = 0.28784939\n",
      "Iteration 623, loss = 0.28769207\n",
      "Iteration 624, loss = 0.28757315\n",
      "Iteration 625, loss = 0.28742679\n",
      "Iteration 626, loss = 0.28732673\n",
      "Iteration 627, loss = 0.28714750\n",
      "Iteration 628, loss = 0.28704253\n",
      "Iteration 629, loss = 0.28689591\n",
      "Iteration 630, loss = 0.28675836\n",
      "Iteration 631, loss = 0.28671185\n",
      "Iteration 632, loss = 0.28646592\n",
      "Iteration 633, loss = 0.28636002\n",
      "Iteration 634, loss = 0.28619220\n",
      "Iteration 635, loss = 0.28607223\n",
      "Iteration 636, loss = 0.28592436\n",
      "Iteration 637, loss = 0.28582537\n",
      "Iteration 638, loss = 0.28565657\n",
      "Iteration 639, loss = 0.28556246\n",
      "Iteration 640, loss = 0.28544345\n",
      "Iteration 641, loss = 0.28525324\n",
      "Iteration 642, loss = 0.28515181\n",
      "Iteration 643, loss = 0.28499806\n",
      "Iteration 644, loss = 0.28482389\n",
      "Iteration 645, loss = 0.28470855\n",
      "Iteration 646, loss = 0.28465579\n",
      "Iteration 647, loss = 0.28442408\n",
      "Iteration 648, loss = 0.28429736\n",
      "Iteration 649, loss = 0.28415963\n",
      "Iteration 650, loss = 0.28401813\n",
      "Iteration 651, loss = 0.28386357\n",
      "Iteration 652, loss = 0.28373990\n",
      "Iteration 653, loss = 0.28360489\n",
      "Iteration 654, loss = 0.28347157\n",
      "Iteration 655, loss = 0.28335184\n",
      "Iteration 656, loss = 0.28319672\n",
      "Iteration 657, loss = 0.28307145\n",
      "Iteration 658, loss = 0.28293217\n",
      "Iteration 659, loss = 0.28279254\n",
      "Iteration 660, loss = 0.28267586\n",
      "Iteration 661, loss = 0.28251998\n",
      "Iteration 662, loss = 0.28238030\n",
      "Iteration 663, loss = 0.28225541\n",
      "Iteration 664, loss = 0.28211622\n",
      "Iteration 665, loss = 0.28202173\n",
      "Iteration 666, loss = 0.28185580\n",
      "Iteration 667, loss = 0.28172704\n",
      "Iteration 668, loss = 0.28157217\n",
      "Iteration 669, loss = 0.28144816\n",
      "Iteration 670, loss = 0.28133834\n",
      "Iteration 671, loss = 0.28119079\n",
      "Iteration 672, loss = 0.28102737\n",
      "Iteration 673, loss = 0.28092423\n",
      "Iteration 674, loss = 0.28081780\n",
      "Iteration 675, loss = 0.28063127\n",
      "Iteration 676, loss = 0.28049343\n",
      "Iteration 677, loss = 0.28041120\n",
      "Iteration 678, loss = 0.28027890\n",
      "Iteration 679, loss = 0.28011959\n",
      "Iteration 680, loss = 0.27995903\n",
      "Iteration 681, loss = 0.27982460\n",
      "Iteration 682, loss = 0.27972185\n",
      "Iteration 683, loss = 0.27952596\n",
      "Iteration 684, loss = 0.27939747\n",
      "Iteration 685, loss = 0.27924919\n",
      "Iteration 686, loss = 0.27913409\n",
      "Iteration 687, loss = 0.27899469\n",
      "Iteration 688, loss = 0.27887039\n",
      "Iteration 689, loss = 0.27871080\n",
      "Iteration 690, loss = 0.27856915\n",
      "Iteration 691, loss = 0.27851058\n",
      "Iteration 692, loss = 0.27830621\n",
      "Iteration 693, loss = 0.27815554\n",
      "Iteration 694, loss = 0.27800949\n",
      "Iteration 695, loss = 0.27791362\n",
      "Iteration 696, loss = 0.27775174\n",
      "Iteration 697, loss = 0.27762369\n",
      "Iteration 698, loss = 0.27750263\n",
      "Iteration 699, loss = 0.27733148\n",
      "Iteration 700, loss = 0.27720216\n",
      "Iteration 701, loss = 0.27706986\n",
      "Iteration 702, loss = 0.27692390\n",
      "Iteration 703, loss = 0.27679346\n",
      "Iteration 704, loss = 0.27664683\n",
      "Iteration 705, loss = 0.27650719\n",
      "Iteration 706, loss = 0.27641512\n",
      "Iteration 707, loss = 0.27623825\n",
      "Iteration 708, loss = 0.27614274\n",
      "Iteration 709, loss = 0.27598015\n",
      "Iteration 710, loss = 0.27583556\n",
      "Iteration 711, loss = 0.27571407\n",
      "Iteration 712, loss = 0.27557025\n",
      "Iteration 713, loss = 0.27543472\n",
      "Iteration 714, loss = 0.27528573\n",
      "Iteration 715, loss = 0.27518251\n",
      "Iteration 716, loss = 0.27504637\n",
      "Iteration 717, loss = 0.27491481\n",
      "Iteration 718, loss = 0.27475096\n",
      "Iteration 719, loss = 0.27466190\n",
      "Iteration 720, loss = 0.27447731\n",
      "Iteration 721, loss = 0.27435229\n",
      "Iteration 722, loss = 0.27426043\n",
      "Iteration 723, loss = 0.27410923\n",
      "Iteration 724, loss = 0.27396285\n",
      "Iteration 725, loss = 0.27381888\n",
      "Iteration 726, loss = 0.27370285\n",
      "Iteration 727, loss = 0.27358388\n",
      "Iteration 728, loss = 0.27342706\n",
      "Iteration 729, loss = 0.27328615\n",
      "Iteration 730, loss = 0.27316604\n",
      "Iteration 731, loss = 0.27309375\n",
      "Iteration 732, loss = 0.27289111\n",
      "Iteration 733, loss = 0.27280288\n",
      "Iteration 734, loss = 0.27263720\n",
      "Iteration 735, loss = 0.27249429\n",
      "Iteration 736, loss = 0.27236958\n",
      "Iteration 737, loss = 0.27222772\n",
      "Iteration 738, loss = 0.27209942\n",
      "Iteration 739, loss = 0.27197796\n",
      "Iteration 740, loss = 0.27189304\n",
      "Iteration 741, loss = 0.27169577\n",
      "Iteration 742, loss = 0.27156547\n",
      "Iteration 743, loss = 0.27143435\n",
      "Iteration 744, loss = 0.27132612\n",
      "Iteration 745, loss = 0.27117196\n",
      "Iteration 746, loss = 0.27106346\n",
      "Iteration 747, loss = 0.27089902\n",
      "Iteration 748, loss = 0.27078008\n",
      "Iteration 749, loss = 0.27063047\n",
      "Iteration 750, loss = 0.27054552\n",
      "Iteration 751, loss = 0.27041170\n",
      "Iteration 752, loss = 0.27023764\n",
      "Iteration 753, loss = 0.27011979\n",
      "Iteration 754, loss = 0.26997858\n",
      "Iteration 755, loss = 0.26985377\n",
      "Iteration 756, loss = 0.26974243\n",
      "Iteration 757, loss = 0.26957945\n",
      "Iteration 758, loss = 0.26945376\n",
      "Iteration 759, loss = 0.26933837\n",
      "Iteration 760, loss = 0.26917281\n",
      "Iteration 761, loss = 0.26906256\n",
      "Iteration 762, loss = 0.26891159\n",
      "Iteration 763, loss = 0.26877374\n",
      "Iteration 764, loss = 0.26863957\n",
      "Iteration 765, loss = 0.26854228\n",
      "Iteration 766, loss = 0.26840635\n",
      "Iteration 767, loss = 0.26826499\n",
      "Iteration 768, loss = 0.26811414\n",
      "Iteration 769, loss = 0.26798645\n",
      "Iteration 770, loss = 0.26791460\n",
      "Iteration 771, loss = 0.26778204\n",
      "Iteration 772, loss = 0.26758613\n",
      "Iteration 773, loss = 0.26750544\n",
      "Iteration 774, loss = 0.26733874\n",
      "Iteration 775, loss = 0.26723222\n",
      "Iteration 776, loss = 0.26706505\n",
      "Iteration 777, loss = 0.26694825\n",
      "Iteration 778, loss = 0.26680532\n",
      "Iteration 779, loss = 0.26665944\n",
      "Iteration 780, loss = 0.26651927\n",
      "Iteration 781, loss = 0.26643713\n",
      "Iteration 782, loss = 0.26627809\n",
      "Iteration 783, loss = 0.26623329\n",
      "Iteration 784, loss = 0.26600747\n",
      "Iteration 785, loss = 0.26585240\n",
      "Iteration 786, loss = 0.26572216\n",
      "Iteration 787, loss = 0.26560800\n",
      "Iteration 788, loss = 0.26548040\n",
      "Iteration 789, loss = 0.26535262\n",
      "Iteration 790, loss = 0.26521109\n",
      "Iteration 791, loss = 0.26505973\n",
      "Iteration 792, loss = 0.26492722\n",
      "Iteration 793, loss = 0.26481888\n",
      "Iteration 794, loss = 0.26469722\n",
      "Iteration 795, loss = 0.26453640\n",
      "Iteration 796, loss = 0.26444564\n",
      "Iteration 797, loss = 0.26432071\n",
      "Iteration 798, loss = 0.26414824\n",
      "Iteration 799, loss = 0.26413021\n",
      "Iteration 800, loss = 0.26388639\n",
      "Iteration 801, loss = 0.26377167\n",
      "Iteration 802, loss = 0.26365377\n",
      "Iteration 803, loss = 0.26347091\n",
      "Iteration 804, loss = 0.26335547\n",
      "Iteration 805, loss = 0.26321305\n",
      "Iteration 806, loss = 0.26311877\n",
      "Iteration 807, loss = 0.26299080\n",
      "Iteration 808, loss = 0.26284222\n",
      "Iteration 809, loss = 0.26270135\n",
      "Iteration 810, loss = 0.26257213\n",
      "Iteration 811, loss = 0.26249613\n",
      "Iteration 812, loss = 0.26232907\n",
      "Iteration 813, loss = 0.26222333\n",
      "Iteration 814, loss = 0.26207865\n",
      "Iteration 815, loss = 0.26193702\n",
      "Iteration 816, loss = 0.26182437\n",
      "Iteration 817, loss = 0.26165520\n",
      "Iteration 818, loss = 0.26155382\n",
      "Iteration 819, loss = 0.26145033\n",
      "Iteration 820, loss = 0.26127221\n",
      "Iteration 821, loss = 0.26117427\n",
      "Iteration 822, loss = 0.26102863\n",
      "Iteration 823, loss = 0.26089039\n",
      "Iteration 824, loss = 0.26083433\n",
      "Iteration 825, loss = 0.26063106\n",
      "Iteration 826, loss = 0.26053493\n",
      "Iteration 827, loss = 0.26038496\n",
      "Iteration 828, loss = 0.26025017\n",
      "Iteration 829, loss = 0.26012118\n",
      "Iteration 830, loss = 0.26000101\n",
      "Iteration 831, loss = 0.25984846\n",
      "Iteration 832, loss = 0.25976696\n",
      "Iteration 833, loss = 0.25960361\n",
      "Iteration 834, loss = 0.25951610\n",
      "Iteration 835, loss = 0.25934664\n",
      "Iteration 836, loss = 0.25919657\n",
      "Iteration 837, loss = 0.25912150\n",
      "Iteration 838, loss = 0.25898359\n",
      "Iteration 839, loss = 0.25883254\n",
      "Iteration 840, loss = 0.25868841\n",
      "Iteration 841, loss = 0.25855523\n",
      "Iteration 842, loss = 0.25842813\n",
      "Iteration 843, loss = 0.25830211\n",
      "Iteration 844, loss = 0.25818057\n",
      "Iteration 845, loss = 0.25804394\n",
      "Iteration 846, loss = 0.25790290\n",
      "Iteration 847, loss = 0.25777987\n",
      "Iteration 848, loss = 0.25767244\n",
      "Iteration 849, loss = 0.25755058\n",
      "Iteration 850, loss = 0.25739748\n",
      "Iteration 851, loss = 0.25725983\n",
      "Iteration 852, loss = 0.25714308\n",
      "Iteration 853, loss = 0.25700397\n",
      "Iteration 854, loss = 0.25693890\n",
      "Iteration 855, loss = 0.25674302\n",
      "Iteration 856, loss = 0.25669402\n",
      "Iteration 857, loss = 0.25650131\n",
      "Iteration 858, loss = 0.25636084\n",
      "Iteration 859, loss = 0.25629907\n",
      "Iteration 860, loss = 0.25613532\n",
      "Iteration 861, loss = 0.25598620\n",
      "Iteration 862, loss = 0.25595745\n",
      "Iteration 863, loss = 0.25582491\n",
      "Iteration 864, loss = 0.25566178\n",
      "Iteration 865, loss = 0.25544287\n",
      "Iteration 866, loss = 0.25534683\n",
      "Iteration 867, loss = 0.25524688\n",
      "Iteration 868, loss = 0.25506451\n",
      "Iteration 869, loss = 0.25509302\n",
      "Iteration 870, loss = 0.25485635\n",
      "Iteration 871, loss = 0.25473263\n",
      "Iteration 872, loss = 0.25457190\n",
      "Iteration 873, loss = 0.25443817\n",
      "Iteration 874, loss = 0.25433698\n",
      "Iteration 875, loss = 0.25420105\n",
      "Iteration 876, loss = 0.25406922\n",
      "Iteration 877, loss = 0.25394476\n",
      "Iteration 878, loss = 0.25382681\n",
      "Iteration 879, loss = 0.25371513\n",
      "Iteration 880, loss = 0.25354027\n",
      "Iteration 881, loss = 0.25345449\n",
      "Iteration 882, loss = 0.25329207\n",
      "Iteration 883, loss = 0.25317245\n",
      "Iteration 884, loss = 0.25303327\n",
      "Iteration 885, loss = 0.25294148\n",
      "Iteration 886, loss = 0.25285453\n",
      "Iteration 887, loss = 0.25266735\n",
      "Iteration 888, loss = 0.25254457\n",
      "Iteration 889, loss = 0.25248255\n",
      "Iteration 890, loss = 0.25227241\n",
      "Iteration 891, loss = 0.25216876\n",
      "Iteration 892, loss = 0.25203844\n",
      "Iteration 893, loss = 0.25190291\n",
      "Iteration 894, loss = 0.25181792\n",
      "Iteration 895, loss = 0.25167011\n",
      "Iteration 896, loss = 0.25153140\n",
      "Iteration 897, loss = 0.25140889\n",
      "Iteration 898, loss = 0.25126638\n",
      "Iteration 899, loss = 0.25116030\n",
      "Iteration 900, loss = 0.25104422\n",
      "Iteration 901, loss = 0.25093998\n",
      "Iteration 902, loss = 0.25081883\n",
      "Iteration 903, loss = 0.25066123\n",
      "Iteration 904, loss = 0.25051813\n",
      "Iteration 905, loss = 0.25037932\n",
      "Iteration 906, loss = 0.25029848\n",
      "Iteration 907, loss = 0.25014949\n",
      "Iteration 908, loss = 0.25002298\n",
      "Iteration 909, loss = 0.24990983\n",
      "Iteration 910, loss = 0.24978080\n",
      "Iteration 911, loss = 0.24967785\n",
      "Iteration 912, loss = 0.24951593\n",
      "Iteration 913, loss = 0.24942698\n",
      "Iteration 914, loss = 0.24926004\n",
      "Iteration 915, loss = 0.24916162\n",
      "Iteration 916, loss = 0.24903829\n",
      "Iteration 917, loss = 0.24889885\n",
      "Iteration 918, loss = 0.24874427\n",
      "Iteration 919, loss = 0.24864610\n",
      "Iteration 920, loss = 0.24852421\n",
      "Iteration 921, loss = 0.24840584\n",
      "Iteration 922, loss = 0.24829356\n",
      "Iteration 923, loss = 0.24813261\n",
      "Iteration 924, loss = 0.24802274\n",
      "Iteration 925, loss = 0.24790085\n",
      "Iteration 926, loss = 0.24777811\n",
      "Iteration 927, loss = 0.24766330\n",
      "Iteration 928, loss = 0.24749513\n",
      "Iteration 929, loss = 0.24739110\n",
      "Iteration 930, loss = 0.24726451\n",
      "Iteration 931, loss = 0.24713015\n",
      "Iteration 932, loss = 0.24704480\n",
      "Iteration 933, loss = 0.24691402\n",
      "Iteration 934, loss = 0.24675085\n",
      "Iteration 935, loss = 0.24664695\n",
      "Iteration 936, loss = 0.24648954\n",
      "Iteration 937, loss = 0.24637365\n",
      "Iteration 938, loss = 0.24623849\n",
      "Iteration 939, loss = 0.24612438\n",
      "Iteration 940, loss = 0.24600090\n",
      "Iteration 941, loss = 0.24587125\n",
      "Iteration 942, loss = 0.24578808\n",
      "Iteration 943, loss = 0.24566627\n",
      "Iteration 944, loss = 0.24548588\n",
      "Iteration 945, loss = 0.24538065\n",
      "Iteration 946, loss = 0.24526036\n",
      "Iteration 947, loss = 0.24514191\n",
      "Iteration 948, loss = 0.24498743\n",
      "Iteration 949, loss = 0.24486075\n",
      "Iteration 950, loss = 0.24479746\n",
      "Iteration 951, loss = 0.24461292\n",
      "Iteration 952, loss = 0.24459888\n",
      "Iteration 953, loss = 0.24437823\n",
      "Iteration 954, loss = 0.24424720\n",
      "Iteration 955, loss = 0.24409259\n",
      "Iteration 956, loss = 0.24397862\n",
      "Iteration 957, loss = 0.24383116\n",
      "Iteration 958, loss = 0.24375517\n",
      "Iteration 959, loss = 0.24366019\n",
      "Iteration 960, loss = 0.24351684\n",
      "Iteration 961, loss = 0.24336623\n",
      "Iteration 962, loss = 0.24322704\n",
      "Iteration 963, loss = 0.24315768\n",
      "Iteration 964, loss = 0.24298474\n",
      "Iteration 965, loss = 0.24283879\n",
      "Iteration 966, loss = 0.24279757\n",
      "Iteration 967, loss = 0.24257370\n",
      "Iteration 968, loss = 0.24244891\n",
      "Iteration 969, loss = 0.24232435\n",
      "Iteration 970, loss = 0.24228624\n",
      "Iteration 971, loss = 0.24209919\n",
      "Iteration 972, loss = 0.24194544\n",
      "Iteration 973, loss = 0.24181569\n",
      "Iteration 974, loss = 0.24171317\n",
      "Iteration 975, loss = 0.24160091\n",
      "Iteration 976, loss = 0.24146482\n",
      "Iteration 977, loss = 0.24131786\n",
      "Iteration 978, loss = 0.24120585\n",
      "Iteration 979, loss = 0.24104729\n",
      "Iteration 980, loss = 0.24099196\n",
      "Iteration 981, loss = 0.24082175\n",
      "Iteration 982, loss = 0.24071523\n",
      "Iteration 983, loss = 0.24059202\n",
      "Iteration 984, loss = 0.24045924\n",
      "Iteration 985, loss = 0.24031016\n",
      "Iteration 986, loss = 0.24023034\n",
      "Iteration 987, loss = 0.24005880\n",
      "Iteration 988, loss = 0.23993362\n",
      "Iteration 989, loss = 0.23982897\n",
      "Iteration 990, loss = 0.23971196\n",
      "Iteration 143, loss = 0.38287508\n",
      "Iteration 144, loss = 0.38235127\n",
      "Iteration 145, loss = 0.38191221\n",
      "Iteration 146, loss = 0.38140033\n",
      "Iteration 147, loss = 0.38090756\n",
      "Iteration 148, loss = 0.38046845\n",
      "Iteration 149, loss = 0.37998343\n",
      "Iteration 150, loss = 0.37952962\n",
      "Iteration 151, loss = 0.37913214\n",
      "Iteration 152, loss = 0.37862175\n",
      "Iteration 153, loss = 0.37821045\n",
      "Iteration 154, loss = 0.37776243\n",
      "Iteration 155, loss = 0.37734480\n",
      "Iteration 156, loss = 0.37690588\n",
      "Iteration 157, loss = 0.37649929\n",
      "Iteration 158, loss = 0.37606267\n",
      "Iteration 159, loss = 0.37566155\n",
      "Iteration 160, loss = 0.37528356\n",
      "Iteration 161, loss = 0.37488034\n",
      "Iteration 162, loss = 0.37446568\n",
      "Iteration 163, loss = 0.37414586\n",
      "Iteration 164, loss = 0.37369982\n",
      "Iteration 165, loss = 0.37333876\n",
      "Iteration 166, loss = 0.37296060\n",
      "Iteration 167, loss = 0.37256768\n",
      "Iteration 168, loss = 0.37220562\n",
      "Iteration 169, loss = 0.37184265\n",
      "Iteration 170, loss = 0.37149354\n",
      "Iteration 171, loss = 0.37111091\n",
      "Iteration 172, loss = 0.37075917\n",
      "Iteration 173, loss = 0.37044961\n",
      "Iteration 174, loss = 0.37009031\n",
      "Iteration 175, loss = 0.36973831\n",
      "Iteration 176, loss = 0.36942787\n",
      "Iteration 177, loss = 0.36907309\n",
      "Iteration 178, loss = 0.36874831\n",
      "Iteration 179, loss = 0.36843380\n",
      "Iteration 180, loss = 0.36812239\n",
      "Iteration 181, loss = 0.36780144\n",
      "Iteration 182, loss = 0.36749426\n",
      "Iteration 183, loss = 0.36718371\n",
      "Iteration 184, loss = 0.36689527\n",
      "Iteration 185, loss = 0.36653731\n",
      "Iteration 186, loss = 0.36625715\n",
      "Iteration 187, loss = 0.36594350\n",
      "Iteration 188, loss = 0.36566228\n",
      "Iteration 189, loss = 0.36537658\n",
      "Iteration 190, loss = 0.36508031\n",
      "Iteration 191, loss = 0.36481761\n",
      "Iteration 192, loss = 0.36455999\n",
      "Iteration 193, loss = 0.36428177\n",
      "Iteration 194, loss = 0.36397487\n",
      "Iteration 195, loss = 0.36366592\n",
      "Iteration 196, loss = 0.36339035\n",
      "Iteration 197, loss = 0.36313335\n",
      "Iteration 198, loss = 0.36286037\n",
      "Iteration 199, loss = 0.36260332\n",
      "Iteration 200, loss = 0.36231363\n",
      "Iteration 201, loss = 0.36207203\n",
      "Iteration 202, loss = 0.36180851\n",
      "Iteration 203, loss = 0.36155589\n",
      "Iteration 204, loss = 0.36128961\n",
      "Iteration 205, loss = 0.36104172\n",
      "Iteration 206, loss = 0.36077902\n",
      "Iteration 207, loss = 0.36053090\n",
      "Iteration 208, loss = 0.36031371\n",
      "Iteration 209, loss = 0.36005151\n",
      "Iteration 210, loss = 0.35981194\n",
      "Iteration 211, loss = 0.35953880\n",
      "Iteration 212, loss = 0.35932862\n",
      "Iteration 213, loss = 0.35906055\n",
      "Iteration 214, loss = 0.35882950\n",
      "Iteration 215, loss = 0.35859919\n",
      "Iteration 216, loss = 0.35834618\n",
      "Iteration 217, loss = 0.35810905\n",
      "Iteration 218, loss = 0.35786460\n",
      "Iteration 219, loss = 0.35767700\n",
      "Iteration 220, loss = 0.35740108\n",
      "Iteration 221, loss = 0.35718334\n",
      "Iteration 222, loss = 0.35698178\n",
      "Iteration 223, loss = 0.35673782\n",
      "Iteration 224, loss = 0.35650133\n",
      "Iteration 225, loss = 0.35630380\n",
      "Iteration 226, loss = 0.35607550\n",
      "Iteration 227, loss = 0.35588341\n",
      "Iteration 228, loss = 0.35564874\n",
      "Iteration 229, loss = 0.35544236\n",
      "Iteration 230, loss = 0.35519617\n",
      "Iteration 231, loss = 0.35500435\n",
      "Iteration 232, loss = 0.35479918\n",
      "Iteration 233, loss = 0.35456445\n",
      "Iteration 234, loss = 0.35436854\n",
      "Iteration 235, loss = 0.35416155\n",
      "Iteration 236, loss = 0.35398713\n",
      "Iteration 237, loss = 0.35375754\n",
      "Iteration 238, loss = 0.35354332\n",
      "Iteration 239, loss = 0.35333593\n",
      "Iteration 240, loss = 0.35313954\n",
      "Iteration 241, loss = 0.35294536\n",
      "Iteration 242, loss = 0.35274531\n",
      "Iteration 243, loss = 0.35255976\n",
      "Iteration 244, loss = 0.35236769\n",
      "Iteration 245, loss = 0.35218858\n",
      "Iteration 246, loss = 0.35197785\n",
      "Iteration 247, loss = 0.35179513\n",
      "Iteration 248, loss = 0.35164962\n",
      "Iteration 249, loss = 0.35142139\n",
      "Iteration 250, loss = 0.35122177\n",
      "Iteration 251, loss = 0.35103622\n",
      "Iteration 252, loss = 0.35085455\n",
      "Iteration 253, loss = 0.35069175\n",
      "Iteration 254, loss = 0.35049933\n",
      "Iteration 255, loss = 0.35033689\n",
      "Iteration 256, loss = 0.35013775\n",
      "Iteration 257, loss = 0.34994917\n",
      "Iteration 258, loss = 0.34976245\n",
      "Iteration 259, loss = 0.34959711\n",
      "Iteration 260, loss = 0.34939528\n",
      "Iteration 261, loss = 0.34925484\n",
      "Iteration 262, loss = 0.34904946\n",
      "Iteration 263, loss = 0.34887698\n",
      "Iteration 264, loss = 0.34872598\n",
      "Iteration 265, loss = 0.34855836\n",
      "Iteration 266, loss = 0.34840171\n",
      "Iteration 267, loss = 0.34819057\n",
      "Iteration 268, loss = 0.34803173\n",
      "Iteration 269, loss = 0.34787447\n",
      "Iteration 270, loss = 0.34769722\n",
      "Iteration 271, loss = 0.34753598\n",
      "Iteration 272, loss = 0.34736048\n",
      "Iteration 273, loss = 0.34720492\n",
      "Iteration 274, loss = 0.34704726\n",
      "Iteration 275, loss = 0.34689714\n",
      "Iteration 276, loss = 0.34674690\n",
      "Iteration 277, loss = 0.34656300\n",
      "Iteration 278, loss = 0.34638555\n",
      "Iteration 279, loss = 0.34623365\n",
      "Iteration 280, loss = 0.34608030\n",
      "Iteration 281, loss = 0.34594067\n",
      "Iteration 282, loss = 0.34577983\n",
      "Iteration 283, loss = 0.34565408\n",
      "Iteration 284, loss = 0.34545177\n",
      "Iteration 285, loss = 0.34531073\n",
      "Iteration 286, loss = 0.34515458\n",
      "Iteration 287, loss = 0.34499515\n",
      "Iteration 288, loss = 0.34484550\n",
      "Iteration 289, loss = 0.34472886\n",
      "Iteration 290, loss = 0.34454699\n",
      "Iteration 291, loss = 0.34439298\n",
      "Iteration 292, loss = 0.34425062\n",
      "Iteration 293, loss = 0.34411547\n",
      "Iteration 294, loss = 0.34395809\n",
      "Iteration 295, loss = 0.34381876\n",
      "Iteration 296, loss = 0.34365357\n",
      "Iteration 297, loss = 0.34352900\n",
      "Iteration 298, loss = 0.34338385\n",
      "Iteration 299, loss = 0.34324176\n",
      "Iteration 300, loss = 0.34309699\n",
      "Iteration 301, loss = 0.34293541\n",
      "Iteration 302, loss = 0.34282264\n",
      "Iteration 303, loss = 0.34267857\n",
      "Iteration 304, loss = 0.34253803\n",
      "Iteration 305, loss = 0.34241307\n",
      "Iteration 306, loss = 0.34226093\n",
      "Iteration 307, loss = 0.34212507\n",
      "Iteration 308, loss = 0.34197201\n",
      "Iteration 309, loss = 0.34186748\n",
      "Iteration 310, loss = 0.34172950\n",
      "Iteration 311, loss = 0.34160041\n",
      "Iteration 312, loss = 0.34145498\n",
      "Iteration 313, loss = 0.34133599\n",
      "Iteration 314, loss = 0.34119009\n",
      "Iteration 315, loss = 0.34105224\n",
      "Iteration 316, loss = 0.34092258\n",
      "Iteration 317, loss = 0.34078833\n",
      "Iteration 318, loss = 0.34065893\n",
      "Iteration 319, loss = 0.34053101\n",
      "Iteration 320, loss = 0.34042063\n",
      "Iteration 321, loss = 0.34029949\n",
      "Iteration 322, loss = 0.34014780\n",
      "Iteration 323, loss = 0.34002549\n",
      "Iteration 324, loss = 0.33988214\n",
      "Iteration 325, loss = 0.33976751\n",
      "Iteration 326, loss = 0.33963445\n",
      "Iteration 327, loss = 0.33950740\n",
      "Iteration 328, loss = 0.33938508\n",
      "Iteration 329, loss = 0.33927395\n",
      "Iteration 330, loss = 0.33914380\n",
      "Iteration 331, loss = 0.33901295\n",
      "Iteration 332, loss = 0.33888385\n",
      "Iteration 333, loss = 0.33875054\n",
      "Iteration 334, loss = 0.33865300\n",
      "Iteration 335, loss = 0.33852351\n",
      "Iteration 336, loss = 0.33840088\n",
      "Iteration 337, loss = 0.33826291\n",
      "Iteration 338, loss = 0.33814754\n",
      "Iteration 339, loss = 0.33802325\n",
      "Iteration 340, loss = 0.33791780\n",
      "Iteration 341, loss = 0.33778142\n",
      "Iteration 342, loss = 0.33767028\n",
      "Iteration 343, loss = 0.33754265\n",
      "Iteration 344, loss = 0.33742437\n",
      "Iteration 345, loss = 0.33731398\n",
      "Iteration 346, loss = 0.33719828\n",
      "Iteration 347, loss = 0.33707848\n",
      "Iteration 348, loss = 0.33696794\n",
      "Iteration 349, loss = 0.33683513\n",
      "Iteration 350, loss = 0.33672935\n",
      "Iteration 351, loss = 0.33661377\n",
      "Iteration 352, loss = 0.33654821\n",
      "Iteration 353, loss = 0.33640198\n",
      "Iteration 354, loss = 0.33627089\n",
      "Iteration 355, loss = 0.33617354\n",
      "Iteration 356, loss = 0.33604563\n",
      "Iteration 357, loss = 0.33592466\n",
      "Iteration 358, loss = 0.33582522\n",
      "Iteration 359, loss = 0.33570988\n",
      "Iteration 360, loss = 0.33558473\n",
      "Iteration 361, loss = 0.33548676\n",
      "Iteration 362, loss = 0.33537755\n",
      "Iteration 363, loss = 0.33526297\n",
      "Iteration 364, loss = 0.33515339\n",
      "Iteration 365, loss = 0.33504018\n",
      "Iteration 366, loss = 0.33493907\n",
      "Iteration 367, loss = 0.33481670\n",
      "Iteration 368, loss = 0.33470902\n",
      "Iteration 369, loss = 0.33461405\n",
      "Iteration 370, loss = 0.33449275\n",
      "Iteration 371, loss = 0.33442499\n",
      "Iteration 372, loss = 0.33428225\n",
      "Iteration 373, loss = 0.33417206\n",
      "Iteration 374, loss = 0.33406164\n",
      "Iteration 375, loss = 0.33394370\n",
      "Iteration 376, loss = 0.33385771\n",
      "Iteration 377, loss = 0.33375026\n",
      "Iteration 378, loss = 0.33363690\n",
      "Iteration 379, loss = 0.33352859\n",
      "Iteration 380, loss = 0.33342721\n",
      "Iteration 381, loss = 0.33332521\n",
      "Iteration 382, loss = 0.33321220\n",
      "Iteration 383, loss = 0.33311381\n",
      "Iteration 384, loss = 0.33300824\n",
      "Iteration 385, loss = 0.33291492\n",
      "Iteration 386, loss = 0.33280113\n",
      "Iteration 387, loss = 0.33269558\n",
      "Iteration 388, loss = 0.33261239\n",
      "Iteration 389, loss = 0.33249521\n",
      "Iteration 390, loss = 0.33241310\n",
      "Iteration 391, loss = 0.33228844\n",
      "Iteration 392, loss = 0.33217782\n",
      "Iteration 393, loss = 0.33207720\n",
      "Iteration 394, loss = 0.33200521\n",
      "Iteration 395, loss = 0.33187721\n",
      "Iteration 396, loss = 0.33178555\n",
      "Iteration 397, loss = 0.33170344\n",
      "Iteration 398, loss = 0.33158359\n",
      "Iteration 399, loss = 0.33151289\n",
      "Iteration 400, loss = 0.33138685\n",
      "Iteration 401, loss = 0.33128589\n",
      "Iteration 402, loss = 0.33121422\n",
      "Iteration 403, loss = 0.33108643\n",
      "Iteration 404, loss = 0.33099087\n",
      "Iteration 405, loss = 0.33089755\n",
      "Iteration 406, loss = 0.33080266\n",
      "Iteration 407, loss = 0.33068809\n",
      "Iteration 408, loss = 0.33059154\n",
      "Iteration 409, loss = 0.33050180\n",
      "Iteration 410, loss = 0.33042471\n",
      "Iteration 411, loss = 0.33031655\n",
      "Iteration 412, loss = 0.33023382\n",
      "Iteration 413, loss = 0.33010784\n",
      "Iteration 414, loss = 0.33000838\n",
      "Iteration 415, loss = 0.32990702\n",
      "Iteration 416, loss = 0.32981442\n",
      "Iteration 417, loss = 0.32975715\n",
      "Iteration 418, loss = 0.32962959\n",
      "Iteration 419, loss = 0.32952132\n",
      "Iteration 420, loss = 0.32943364\n",
      "Iteration 421, loss = 0.32933941\n",
      "Iteration 422, loss = 0.32923432\n",
      "Iteration 423, loss = 0.32913188\n",
      "Iteration 424, loss = 0.32908180\n",
      "Iteration 425, loss = 0.32897033\n",
      "Iteration 426, loss = 0.32884923\n",
      "Iteration 427, loss = 0.32874657\n",
      "Iteration 428, loss = 0.32867320\n",
      "Iteration 429, loss = 0.32855994\n",
      "Iteration 430, loss = 0.32846222\n",
      "Iteration 431, loss = 0.32839240\n",
      "Iteration 432, loss = 0.32828204\n",
      "Iteration 433, loss = 0.32818204\n",
      "Iteration 434, loss = 0.32809392\n",
      "Iteration 435, loss = 0.32802931\n",
      "Iteration 436, loss = 0.32791664\n",
      "Iteration 437, loss = 0.32780158\n",
      "Iteration 438, loss = 0.32770666\n",
      "Iteration 439, loss = 0.32761528\n",
      "Iteration 440, loss = 0.32753108\n",
      "Iteration 441, loss = 0.32745134\n",
      "Iteration 442, loss = 0.32734292\n",
      "Iteration 443, loss = 0.32724540\n",
      "Iteration 444, loss = 0.32715233\n",
      "Iteration 445, loss = 0.32705641\n",
      "Iteration 446, loss = 0.32697630\n",
      "Iteration 447, loss = 0.32689372\n",
      "Iteration 448, loss = 0.32679294\n",
      "Iteration 449, loss = 0.32670665\n",
      "Iteration 450, loss = 0.32659789\n",
      "Iteration 451, loss = 0.32654046\n",
      "Iteration 452, loss = 0.32643351\n",
      "Iteration 453, loss = 0.32634715\n",
      "Iteration 454, loss = 0.32624721\n",
      "Iteration 455, loss = 0.32616339\n",
      "Iteration 456, loss = 0.32606861\n",
      "Iteration 457, loss = 0.32598308\n",
      "Iteration 458, loss = 0.32589377\n",
      "Iteration 459, loss = 0.32582217\n",
      "Iteration 460, loss = 0.32572251\n",
      "Iteration 461, loss = 0.32561671\n",
      "Iteration 462, loss = 0.32554774\n",
      "Iteration 463, loss = 0.32546188\n",
      "Iteration 464, loss = 0.32536979\n",
      "Iteration 465, loss = 0.32528550\n",
      "Iteration 466, loss = 0.32519418\n",
      "Iteration 467, loss = 0.32509948\n",
      "Iteration 468, loss = 0.32502865\n",
      "Iteration 469, loss = 0.32494071\n",
      "Iteration 470, loss = 0.32484066\n",
      "Iteration 471, loss = 0.32479346\n",
      "Iteration 472, loss = 0.32467288\n",
      "Iteration 473, loss = 0.32459718\n",
      "Iteration 474, loss = 0.32450219\n",
      "Iteration 475, loss = 0.32441484\n",
      "Iteration 476, loss = 0.32433787\n",
      "Iteration 477, loss = 0.32424607\n",
      "Iteration 478, loss = 0.32416352\n",
      "Iteration 479, loss = 0.32407434\n",
      "Iteration 480, loss = 0.32399700\n",
      "Iteration 481, loss = 0.32390296\n",
      "Iteration 482, loss = 0.32382118\n",
      "Iteration 483, loss = 0.32374265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78228793\n",
      "Iteration 2, loss = 0.77912444\n",
      "Iteration 3, loss = 0.77430910\n",
      "Iteration 4, loss = 0.76835273\n",
      "Iteration 5, loss = 0.76176172\n",
      "Iteration 6, loss = 0.75454490\n",
      "Iteration 7, loss = 0.74710446\n",
      "Iteration 8, loss = 0.73961897\n",
      "Iteration 9, loss = 0.73214062\n",
      "Iteration 10, loss = 0.72462670\n",
      "Iteration 11, loss = 0.71727269\n",
      "Iteration 12, loss = 0.71005331\n",
      "Iteration 13, loss = 0.70302436\n",
      "Iteration 14, loss = 0.69610219\n",
      "Iteration 15, loss = 0.68924753\n",
      "Iteration 16, loss = 0.68265390\n",
      "Iteration 17, loss = 0.67635319\n",
      "Iteration 18, loss = 0.67002433\n",
      "Iteration 19, loss = 0.66381819\n",
      "Iteration 20, loss = 0.65796584\n",
      "Iteration 21, loss = 0.65201431\n",
      "Iteration 22, loss = 0.64635735\n",
      "Iteration 23, loss = 0.64079516\n",
      "Iteration 24, loss = 0.63538795\n",
      "Iteration 25, loss = 0.62997366\n",
      "Iteration 26, loss = 0.62479221\n",
      "Iteration 27, loss = 0.61969445\n",
      "Iteration 28, loss = 0.61480045\n",
      "Iteration 29, loss = 0.60981716\n",
      "Iteration 30, loss = 0.60500102\n",
      "Iteration 31, loss = 0.60028752\n",
      "Iteration 32, loss = 0.59576400\n",
      "Iteration 33, loss = 0.59126832\n",
      "Iteration 34, loss = 0.58676727\n",
      "Iteration 35, loss = 0.58248920\n",
      "Iteration 36, loss = 0.57833875\n",
      "Iteration 37, loss = 0.57401005\n",
      "Iteration 38, loss = 0.56985249\n",
      "Iteration 39, loss = 0.56597095\n",
      "Iteration 40, loss = 0.56198705\n",
      "Iteration 41, loss = 0.55815516\n",
      "Iteration 42, loss = 0.55430028\n",
      "Iteration 43, loss = 0.55059224\n",
      "Iteration 44, loss = 0.54696324\n",
      "Iteration 45, loss = 0.54340479\n",
      "Iteration 46, loss = 0.53990261\n",
      "Iteration 47, loss = 0.53660538\n",
      "Iteration 48, loss = 0.53315229\n",
      "Iteration 49, loss = 0.52985920\n",
      "Iteration 50, loss = 0.52664321\n",
      "Iteration 51, loss = 0.52345158\n",
      "Iteration 52, loss = 0.52046736\n",
      "Iteration 53, loss = 0.51748016\n",
      "Iteration 54, loss = 0.51439768\n",
      "Iteration 55, loss = 0.51166469\n",
      "Iteration 56, loss = 0.50873651\n",
      "Iteration 57, loss = 0.50599141\n",
      "Iteration 58, loss = 0.50326208\n",
      "Iteration 59, loss = 0.50067154\n",
      "Iteration 60, loss = 0.49812158\n",
      "Iteration 61, loss = 0.49563327\n",
      "Iteration 62, loss = 0.49317508\n",
      "Iteration 63, loss = 0.49073951\n",
      "Iteration 64, loss = 0.48834650\n",
      "Iteration 65, loss = 0.48610724\n",
      "Iteration 66, loss = 0.48382380\n",
      "Iteration 67, loss = 0.48157418\n",
      "Iteration 68, loss = 0.47952541\n",
      "Iteration 69, loss = 0.47745436\n",
      "Iteration 70, loss = 0.47542884\n",
      "Iteration 71, loss = 0.47347174\n",
      "Iteration 72, loss = 0.47147000\n",
      "Iteration 73, loss = 0.46958822\n",
      "Iteration 74, loss = 0.46776463\n",
      "Iteration 75, loss = 0.46597101\n",
      "Iteration 76, loss = 0.46426450\n",
      "Iteration 77, loss = 0.46247297\n",
      "Iteration 78, loss = 0.46080337\n",
      "Iteration 79, loss = 0.45916510\n",
      "Iteration 80, loss = 0.45762173\n",
      "Iteration 81, loss = 0.45601801\n",
      "Iteration 82, loss = 0.45457431\n",
      "Iteration 83, loss = 0.45301023\n",
      "Iteration 84, loss = 0.45158993\n",
      "Iteration 85, loss = 0.45018700\n",
      "Iteration 86, loss = 0.44884260\n",
      "Iteration 87, loss = 0.44752254\n",
      "Iteration 88, loss = 0.44620218\n",
      "Iteration 89, loss = 0.44487531\n",
      "Iteration 90, loss = 0.44361307\n",
      "Iteration 91, loss = 0.44243601\n",
      "Iteration 92, loss = 0.44124892\n",
      "Iteration 93, loss = 0.44001238\n",
      "Iteration 94, loss = 0.43894312\n",
      "Iteration 95, loss = 0.43784474\n",
      "Iteration 96, loss = 0.43668657\n",
      "Iteration 97, loss = 0.43564865\n",
      "Iteration 98, loss = 0.43461073\n",
      "Iteration 99, loss = 0.43360275\n",
      "Iteration 100, loss = 0.43254902\n",
      "Iteration 101, loss = 0.43159146\n",
      "Iteration 102, loss = 0.43065837\n",
      "Iteration 103, loss = 0.42970658\n",
      "Iteration 104, loss = 0.42875887\n",
      "Iteration 105, loss = 0.42793938\n",
      "Iteration 106, loss = 0.42696642\n",
      "Iteration 107, loss = 0.42610032\n",
      "Iteration 108, loss = 0.42524236\n",
      "Iteration 109, loss = 0.42438032\n",
      "Iteration 110, loss = 0.42357644\n",
      "Iteration 111, loss = 0.42279011\n",
      "Iteration 112, loss = 0.42197639\n",
      "Iteration 113, loss = 0.42119660\n",
      "Iteration 114, loss = 0.42043286\n",
      "Iteration 115, loss = 0.41969614\n",
      "Iteration 116, loss = 0.41892225\n",
      "Iteration 117, loss = 0.41822914\n",
      "Iteration 118, loss = 0.41745743\n",
      "Iteration 119, loss = 0.41671162\n",
      "Iteration 120, loss = 0.41604759\n",
      "Iteration 121, loss = 0.41534372\n",
      "Iteration 122, loss = 0.41464602\n",
      "Iteration 123, loss = 0.41399374\n",
      "Iteration 124, loss = 0.41331746\n",
      "Iteration 125, loss = 0.41266209\n",
      "Iteration 126, loss = 0.41203849\n",
      "Iteration 127, loss = 0.41137839\n",
      "Iteration 128, loss = 0.41078100\n",
      "Iteration 129, loss = 0.41013727\n",
      "Iteration 130, loss = 0.40958640\n",
      "Iteration 131, loss = 0.40896338\n",
      "Iteration 132, loss = 0.40838308\n",
      "Iteration 133, loss = 0.40778267\n",
      "Iteration 134, loss = 0.40721249\n",
      "Iteration 135, loss = 0.40665754\n",
      "Iteration 136, loss = 0.40607775\n",
      "Iteration 137, loss = 0.40554180\n",
      "Iteration 138, loss = 0.40499300\n",
      "Iteration 139, loss = 0.40445446\n",
      "Iteration 140, loss = 0.40393709\n",
      "Iteration 141, loss = 0.40340414\n",
      "Iteration 142, loss = 0.40287966\n",
      "Iteration 143, loss = 0.40237061\n",
      "Iteration 144, loss = 0.40187762\n",
      "Iteration 145, loss = 0.40138486\n",
      "Iteration 146, loss = 0.40087041\n",
      "Iteration 147, loss = 0.40042060\n",
      "Iteration 148, loss = 0.39994260\n",
      "Iteration 149, loss = 0.39945107\n",
      "Iteration 150, loss = 0.39895718\n",
      "Iteration 151, loss = 0.39848293\n",
      "Iteration 152, loss = 0.39800236\n",
      "Iteration 153, loss = 0.39755473\n",
      "Iteration 154, loss = 0.39710947\n",
      "Iteration 155, loss = 0.39667099\n",
      "Iteration 987, loss = 0.27031977\n",
      "Iteration 988, loss = 0.27019959\n",
      "Iteration 989, loss = 0.27011572\n",
      "Iteration 990, loss = 0.26997830\n",
      "Iteration 991, loss = 0.26986159\n",
      "Iteration 992, loss = 0.26976131\n",
      "Iteration 993, loss = 0.26970990\n",
      "Iteration 994, loss = 0.26953828\n",
      "Iteration 995, loss = 0.26943010\n",
      "Iteration 996, loss = 0.26935523\n",
      "Iteration 997, loss = 0.26923491\n",
      "Iteration 998, loss = 0.26912893\n",
      "Iteration 999, loss = 0.26902473\n",
      "Iteration 1000, loss = 0.26894502\n",
      "Iteration 1001, loss = 0.26880532\n",
      "Iteration 1002, loss = 0.26873577\n",
      "Iteration 1003, loss = 0.26860191\n",
      "Iteration 1004, loss = 0.26852297\n",
      "Iteration 1005, loss = 0.26840746\n",
      "Iteration 1006, loss = 0.26830557\n",
      "Iteration 1007, loss = 0.26819538\n",
      "Iteration 1008, loss = 0.26810367\n",
      "Iteration 1009, loss = 0.26801962\n",
      "Iteration 1010, loss = 0.26791664\n",
      "Iteration 1011, loss = 0.26781226\n",
      "Iteration 1012, loss = 0.26770496\n",
      "Iteration 1013, loss = 0.26758814\n",
      "Iteration 1014, loss = 0.26749162\n",
      "Iteration 1015, loss = 0.26736815\n",
      "Iteration 1016, loss = 0.26729300\n",
      "Iteration 1017, loss = 0.26716158\n",
      "Iteration 1018, loss = 0.26720484\n",
      "Iteration 1019, loss = 0.26694183\n",
      "Iteration 1020, loss = 0.26682808\n",
      "Iteration 1021, loss = 0.26677110\n",
      "Iteration 1022, loss = 0.26660951\n",
      "Iteration 1023, loss = 0.26651768\n",
      "Iteration 1024, loss = 0.26642641\n",
      "Iteration 1025, loss = 0.26634193\n",
      "Iteration 1026, loss = 0.26620902\n",
      "Iteration 1027, loss = 0.26611540\n",
      "Iteration 1028, loss = 0.26607874\n",
      "Iteration 1029, loss = 0.26591185\n",
      "Iteration 1030, loss = 0.26579024\n",
      "Iteration 1031, loss = 0.26569775\n",
      "Iteration 1032, loss = 0.26557679\n",
      "Iteration 1033, loss = 0.26550130\n",
      "Iteration 1034, loss = 0.26541353\n",
      "Iteration 1035, loss = 0.26527658\n",
      "Iteration 1036, loss = 0.26517049\n",
      "Iteration 1037, loss = 0.26506687\n",
      "Iteration 1038, loss = 0.26508416\n",
      "Iteration 1039, loss = 0.26489306\n",
      "Iteration 1040, loss = 0.26476217\n",
      "Iteration 1041, loss = 0.26463482\n",
      "Iteration 1042, loss = 0.26454929\n",
      "Iteration 1043, loss = 0.26448419\n",
      "Iteration 1044, loss = 0.26431346\n",
      "Iteration 1045, loss = 0.26421552\n",
      "Iteration 1046, loss = 0.26422131\n",
      "Iteration 1047, loss = 0.26404172\n",
      "Iteration 1048, loss = 0.26393009\n",
      "Iteration 1049, loss = 0.26390572\n",
      "Iteration 1050, loss = 0.26372083\n",
      "Iteration 1051, loss = 0.26361449\n",
      "Iteration 1052, loss = 0.26348517\n",
      "Iteration 1053, loss = 0.26339834\n",
      "Iteration 1054, loss = 0.26334709\n",
      "Iteration 1055, loss = 0.26320388\n",
      "Iteration 1056, loss = 0.26307088\n",
      "Iteration 1057, loss = 0.26297914\n",
      "Iteration 1058, loss = 0.26285577\n",
      "Iteration 1059, loss = 0.26277387\n",
      "Iteration 1060, loss = 0.26266377\n",
      "Iteration 1061, loss = 0.26259839\n",
      "Iteration 1062, loss = 0.26243509\n",
      "Iteration 1063, loss = 0.26236725\n",
      "Iteration 1064, loss = 0.26223156\n",
      "Iteration 1065, loss = 0.26213996\n",
      "Iteration 1066, loss = 0.26207409\n",
      "Iteration 1067, loss = 0.26193481\n",
      "Iteration 1068, loss = 0.26182334\n",
      "Iteration 1069, loss = 0.26171502\n",
      "Iteration 1070, loss = 0.26163996\n",
      "Iteration 1071, loss = 0.26152327\n",
      "Iteration 1072, loss = 0.26145192\n",
      "Iteration 1073, loss = 0.26134983\n",
      "Iteration 1074, loss = 0.26118700\n",
      "Iteration 1075, loss = 0.26110619\n",
      "Iteration 1076, loss = 0.26097330\n",
      "Iteration 1077, loss = 0.26088760\n",
      "Iteration 1078, loss = 0.26080398\n",
      "Iteration 1079, loss = 0.26071669\n",
      "Iteration 1080, loss = 0.26056409\n",
      "Iteration 1081, loss = 0.26045478\n",
      "Iteration 1082, loss = 0.26042345\n",
      "Iteration 1083, loss = 0.26025690\n",
      "Iteration 1084, loss = 0.26015875\n",
      "Iteration 1085, loss = 0.26005204\n",
      "Iteration 1086, loss = 0.25994490\n",
      "Iteration 1087, loss = 0.25988036\n",
      "Iteration 1088, loss = 0.25977790\n",
      "Iteration 1089, loss = 0.25968244\n",
      "Iteration 1090, loss = 0.25954505\n",
      "Iteration 1091, loss = 0.25942094\n",
      "Iteration 1092, loss = 0.25940463\n",
      "Iteration 1093, loss = 0.25920962\n",
      "Iteration 1094, loss = 0.25918039\n",
      "Iteration 1095, loss = 0.25907044\n",
      "Iteration 1096, loss = 0.25889630\n",
      "Iteration 1097, loss = 0.25878752\n",
      "Iteration 1098, loss = 0.25876326\n",
      "Iteration 1099, loss = 0.25858232\n",
      "Iteration 1100, loss = 0.25853626\n",
      "Iteration 1101, loss = 0.25841758\n",
      "Iteration 1102, loss = 0.25833568\n",
      "Iteration 1103, loss = 0.25821563\n",
      "Iteration 1104, loss = 0.25806390\n",
      "Iteration 1105, loss = 0.25800103\n",
      "Iteration 1106, loss = 0.25787864\n",
      "Iteration 1107, loss = 0.25777356\n",
      "Iteration 1108, loss = 0.25765047\n",
      "Iteration 1109, loss = 0.25761643\n",
      "Iteration 1110, loss = 0.25744768\n",
      "Iteration 1111, loss = 0.25736397\n",
      "Iteration 1112, loss = 0.25729306\n",
      "Iteration 1113, loss = 0.25716452\n",
      "Iteration 1114, loss = 0.25702933\n",
      "Iteration 1115, loss = 0.25691907\n",
      "Iteration 1116, loss = 0.25685567\n",
      "Iteration 1117, loss = 0.25671361\n",
      "Iteration 1118, loss = 0.25662748\n",
      "Iteration 1119, loss = 0.25652117\n",
      "Iteration 1120, loss = 0.25642125\n",
      "Iteration 1121, loss = 0.25631533\n",
      "Iteration 1122, loss = 0.25621458\n",
      "Iteration 1123, loss = 0.25610631\n",
      "Iteration 1124, loss = 0.25606127\n",
      "Iteration 1125, loss = 0.25589568\n",
      "Iteration 1126, loss = 0.25581176\n",
      "Iteration 1127, loss = 0.25570831\n",
      "Iteration 1128, loss = 0.25559322\n",
      "Iteration 1129, loss = 0.25548698\n",
      "Iteration 1130, loss = 0.25539610\n",
      "Iteration 1131, loss = 0.25527932\n",
      "Iteration 1132, loss = 0.25523453\n",
      "Iteration 1133, loss = 0.25510836\n",
      "Iteration 1134, loss = 0.25497322\n",
      "Iteration 1135, loss = 0.25486100\n",
      "Iteration 1136, loss = 0.25477218\n",
      "Iteration 1137, loss = 0.25468021\n",
      "Iteration 1138, loss = 0.25453746\n",
      "Iteration 1139, loss = 0.25443445\n",
      "Iteration 1140, loss = 0.25440385\n",
      "Iteration 1141, loss = 0.25423887\n",
      "Iteration 1142, loss = 0.25413017\n",
      "Iteration 1143, loss = 0.25406862\n",
      "Iteration 1144, loss = 0.25393905\n",
      "Iteration 1145, loss = 0.25381594\n",
      "Iteration 1146, loss = 0.25371581\n",
      "Iteration 1147, loss = 0.25359480\n",
      "Iteration 1148, loss = 0.25351153\n",
      "Iteration 1149, loss = 0.25340463\n",
      "Iteration 1150, loss = 0.25329038\n",
      "Iteration 1151, loss = 0.25324625\n",
      "Iteration 1152, loss = 0.25311988\n",
      "Iteration 1153, loss = 0.25299088\n",
      "Iteration 1154, loss = 0.25288980\n",
      "Iteration 1155, loss = 0.25281471\n",
      "Iteration 1156, loss = 0.25271285\n",
      "Iteration 1157, loss = 0.25266920\n",
      "Iteration 1158, loss = 0.25250536\n",
      "Iteration 1159, loss = 0.25238509\n",
      "Iteration 1160, loss = 0.25229753\n",
      "Iteration 1161, loss = 0.25218857\n",
      "Iteration 1162, loss = 0.25209891\n",
      "Iteration 1163, loss = 0.25196876\n",
      "Iteration 1164, loss = 0.25185146\n",
      "Iteration 1165, loss = 0.25174819\n",
      "Iteration 1166, loss = 0.25169037\n",
      "Iteration 1167, loss = 0.25156650\n",
      "Iteration 1168, loss = 0.25154922\n",
      "Iteration 1169, loss = 0.25139972\n",
      "Iteration 1170, loss = 0.25124536\n",
      "Iteration 1171, loss = 0.25114208\n",
      "Iteration 1172, loss = 0.25100727\n",
      "Iteration 1173, loss = 0.25093115\n",
      "Iteration 1174, loss = 0.25090766\n",
      "Iteration 1175, loss = 0.25069860\n",
      "Iteration 1176, loss = 0.25061766\n",
      "Iteration 1177, loss = 0.25047676\n",
      "Iteration 1178, loss = 0.25044425\n",
      "Iteration 1179, loss = 0.25030452\n",
      "Iteration 1180, loss = 0.25019250\n",
      "Iteration 1181, loss = 0.25008693\n",
      "Iteration 1182, loss = 0.24998735\n",
      "Iteration 1183, loss = 0.24985833\n",
      "Iteration 1184, loss = 0.24983599\n",
      "Iteration 1185, loss = 0.24966573\n",
      "Iteration 1186, loss = 0.24953134\n",
      "Iteration 1187, loss = 0.24948535\n",
      "Iteration 1188, loss = 0.24934992\n",
      "Iteration 1189, loss = 0.24924721\n",
      "Iteration 1190, loss = 0.24917690\n",
      "Iteration 1191, loss = 0.24907553\n",
      "Iteration 1192, loss = 0.24891993\n",
      "Iteration 1193, loss = 0.24881400\n",
      "Iteration 1194, loss = 0.24873620\n",
      "Iteration 1195, loss = 0.24863407\n",
      "Iteration 1196, loss = 0.24850272\n",
      "Iteration 1197, loss = 0.24839118\n",
      "Iteration 1198, loss = 0.24831123\n",
      "Iteration 1199, loss = 0.24823567\n",
      "Iteration 1200, loss = 0.24811212\n",
      "Iteration 1201, loss = 0.24808899\n",
      "Iteration 1202, loss = 0.24788080\n",
      "Iteration 1203, loss = 0.24777045\n",
      "Iteration 1204, loss = 0.24765405\n",
      "Iteration 1205, loss = 0.24756827\n",
      "Iteration 1206, loss = 0.24749312\n",
      "Iteration 1207, loss = 0.24734698\n",
      "Iteration 1208, loss = 0.24729087\n",
      "Iteration 1209, loss = 0.24712967\n",
      "Iteration 1210, loss = 0.24704656\n",
      "Iteration 1211, loss = 0.24697871\n",
      "Iteration 1212, loss = 0.24681544\n",
      "Iteration 1213, loss = 0.24673457\n",
      "Iteration 1214, loss = 0.24662181\n",
      "Iteration 1215, loss = 0.24651861\n",
      "Iteration 1216, loss = 0.24644813\n",
      "Iteration 1217, loss = 0.24628411\n",
      "Iteration 1218, loss = 0.24626284\n",
      "Iteration 1219, loss = 0.24609547\n",
      "Iteration 1220, loss = 0.24601320\n",
      "Iteration 1221, loss = 0.24586835\n",
      "Iteration 1222, loss = 0.24578958\n",
      "Iteration 1223, loss = 0.24571754\n",
      "Iteration 1224, loss = 0.24555667\n",
      "Iteration 1225, loss = 0.24548493\n",
      "Iteration 1226, loss = 0.24537438\n",
      "Iteration 1227, loss = 0.24523056\n",
      "Iteration 1228, loss = 0.24512789\n",
      "Iteration 1229, loss = 0.24504240\n",
      "Iteration 1230, loss = 0.24497621\n",
      "Iteration 1231, loss = 0.24497476\n",
      "Iteration 1232, loss = 0.24470548\n",
      "Iteration 1233, loss = 0.24460057\n",
      "Iteration 1234, loss = 0.24450100\n",
      "Iteration 1235, loss = 0.24439989\n",
      "Iteration 1236, loss = 0.24436305\n",
      "Iteration 1237, loss = 0.24421376\n",
      "Iteration 1238, loss = 0.24407308\n",
      "Iteration 1239, loss = 0.24396290\n",
      "Iteration 1240, loss = 0.24385465\n",
      "Iteration 1241, loss = 0.24376435\n",
      "Iteration 1242, loss = 0.24365454\n",
      "Iteration 1243, loss = 0.24353879\n",
      "Iteration 1244, loss = 0.24343798\n",
      "Iteration 1245, loss = 0.24332572\n",
      "Iteration 1246, loss = 0.24322923\n",
      "Iteration 1247, loss = 0.24312583\n",
      "Iteration 1248, loss = 0.24302717\n",
      "Iteration 1249, loss = 0.24290086\n",
      "Iteration 1250, loss = 0.24282915\n",
      "Iteration 1251, loss = 0.24268931\n",
      "Iteration 1252, loss = 0.24258453\n",
      "Iteration 1253, loss = 0.24247708\n",
      "Iteration 1254, loss = 0.24235504\n",
      "Iteration 1255, loss = 0.24232653\n",
      "Iteration 1256, loss = 0.24215107\n",
      "Iteration 1257, loss = 0.24203639\n",
      "Iteration 1258, loss = 0.24194528\n",
      "Iteration 1259, loss = 0.24184769\n",
      "Iteration 1260, loss = 0.24177194\n",
      "Iteration 1261, loss = 0.24165944\n",
      "Iteration 1262, loss = 0.24151052\n",
      "Iteration 1263, loss = 0.24141665\n",
      "Iteration 1264, loss = 0.24134698\n",
      "Iteration 1265, loss = 0.24118199\n",
      "Iteration 1266, loss = 0.24111127\n",
      "Iteration 1267, loss = 0.24095854\n",
      "Iteration 1268, loss = 0.24086497\n",
      "Iteration 1269, loss = 0.24075122\n",
      "Iteration 1270, loss = 0.24065458\n",
      "Iteration 1271, loss = 0.24055311\n",
      "Iteration 1272, loss = 0.24042814\n",
      "Iteration 1273, loss = 0.24040812\n",
      "Iteration 1274, loss = 0.24021831\n",
      "Iteration 1275, loss = 0.24014806\n",
      "Iteration 1276, loss = 0.24001335\n",
      "Iteration 1277, loss = 0.23988232\n",
      "Iteration 1278, loss = 0.23981915\n",
      "Iteration 1279, loss = 0.23972155\n",
      "Iteration 1280, loss = 0.23957277\n",
      "Iteration 1281, loss = 0.23952479\n",
      "Iteration 1282, loss = 0.23936268\n",
      "Iteration 1283, loss = 0.23924542\n",
      "Iteration 1284, loss = 0.23914238\n",
      "Iteration 1285, loss = 0.23902399\n",
      "Iteration 1286, loss = 0.23905048\n",
      "Iteration 1287, loss = 0.23881546\n",
      "Iteration 1288, loss = 0.23872646\n",
      "Iteration 1289, loss = 0.23859596\n",
      "Iteration 1290, loss = 0.23851954\n",
      "Iteration 1291, loss = 0.23844543\n",
      "Iteration 1292, loss = 0.23831766\n",
      "Iteration 1293, loss = 0.23818511\n",
      "Iteration 1294, loss = 0.23806952\n",
      "Iteration 1295, loss = 0.23799894\n",
      "Iteration 1296, loss = 0.23785839\n",
      "Iteration 1297, loss = 0.23774619\n",
      "Iteration 1298, loss = 0.23766355\n",
      "Iteration 1299, loss = 0.23753996\n",
      "Iteration 1300, loss = 0.23749681\n",
      "Iteration 1301, loss = 0.23732737\n",
      "Iteration 1302, loss = 0.23718592\n",
      "Iteration 1303, loss = 0.23709194\n",
      "Iteration 1304, loss = 0.23702938\n",
      "Iteration 1305, loss = 0.23690531\n",
      "Iteration 1306, loss = 0.23677196\n",
      "Iteration 1307, loss = 0.23675892\n",
      "Iteration 1308, loss = 0.23656566\n",
      "Iteration 1309, loss = 0.23644817\n",
      "Iteration 1310, loss = 0.23635320\n",
      "Iteration 1311, loss = 0.23624458\n",
      "Iteration 1312, loss = 0.23612818\n",
      "Iteration 1313, loss = 0.23600023\n",
      "Iteration 1314, loss = 0.23598357\n",
      "Iteration 1315, loss = 0.23580254\n",
      "Iteration 1316, loss = 0.23570646\n",
      "Iteration 1317, loss = 0.23558591\n",
      "Iteration 1318, loss = 0.23555355\n",
      "Iteration 1319, loss = 0.23537316\n",
      "Iteration 1320, loss = 0.23534819\n",
      "Iteration 1321, loss = 0.23520632\n",
      "Iteration 1322, loss = 0.23505549\n",
      "Iteration 1323, loss = 0.23495250\n",
      "Iteration 1324, loss = 0.23488481\n",
      "Iteration 1325, loss = 0.23472037\n",
      "Iteration 1326, loss = 0.23462707\n",
      "Iteration 1327, loss = 0.23450397\n",
      "Iteration 1328, loss = 0.23441692\n",
      "Iteration 1329, loss = 0.23431412\n",
      "Iteration 1330, loss = 0.23419994\n",
      "Iteration 1331, loss = 0.23409384\n",
      "Iteration 1332, loss = 0.23405435\n",
      "Iteration 1333, loss = 0.23393011\n",
      "Iteration 1334, loss = 0.23382408\n",
      "Iteration 1335, loss = 0.23365184\n",
      "Iteration 1336, loss = 0.23361044\n",
      "Iteration 1337, loss = 0.23344504\n",
      "Iteration 1338, loss = 0.23339698\n",
      "Iteration 1339, loss = 0.23325750\n",
      "Iteration 1340, loss = 0.23316714\n",
      "Iteration 1341, loss = 0.23301199\n",
      "Iteration 1342, loss = 0.23288975\n",
      "Iteration 1343, loss = 0.23277909\n",
      "Iteration 1344, loss = 0.23269463\n",
      "Iteration 1345, loss = 0.23256989\n",
      "Iteration 1346, loss = 0.23250693\n",
      "Iteration 1347, loss = 0.23235803\n",
      "Iteration 1348, loss = 0.23224990\n",
      "Iteration 1349, loss = 0.23214452\n",
      "Iteration 1350, loss = 0.23202664\n",
      "Iteration 1351, loss = 0.23193508\n",
      "Iteration 1352, loss = 0.23177698\n",
      "Iteration 1353, loss = 0.23175174\n",
      "Iteration 1354, loss = 0.23158038\n",
      "Iteration 1355, loss = 0.23146971\n",
      "Iteration 1356, loss = 0.23135872\n",
      "Iteration 1357, loss = 0.23127103\n",
      "Iteration 1358, loss = 0.23114204\n",
      "Iteration 1359, loss = 0.23103307\n",
      "Iteration 1360, loss = 0.23093508\n",
      "Iteration 1361, loss = 0.23083568\n",
      "Iteration 1362, loss = 0.23070518\n",
      "Iteration 1363, loss = 0.23063268\n",
      "Iteration 1364, loss = 0.23052603\n",
      "Iteration 1365, loss = 0.23038056\n",
      "Iteration 1366, loss = 0.23030531\n",
      "Iteration 1367, loss = 0.23019713\n",
      "Iteration 1368, loss = 0.23015519\n",
      "Iteration 1369, loss = 0.22996471\n",
      "Iteration 1370, loss = 0.22985834\n",
      "Iteration 1371, loss = 0.22971634\n",
      "Iteration 1372, loss = 0.22961607\n",
      "Iteration 1373, loss = 0.22951945\n",
      "Iteration 1374, loss = 0.22943604\n",
      "Iteration 1375, loss = 0.22936162\n",
      "Iteration 1376, loss = 0.22925250\n",
      "Iteration 1377, loss = 0.22908444\n",
      "Iteration 1378, loss = 0.22902961\n",
      "Iteration 1379, loss = 0.22885916\n",
      "Iteration 1380, loss = 0.22882176\n",
      "Iteration 1381, loss = 0.22867350\n",
      "Iteration 1382, loss = 0.22854405\n",
      "Iteration 1383, loss = 0.22842875\n",
      "Iteration 1384, loss = 0.22830212\n",
      "Iteration 1385, loss = 0.22822864\n",
      "Iteration 1386, loss = 0.22809932\n",
      "Iteration 1387, loss = 0.22801344\n",
      "Iteration 1388, loss = 0.22788189\n",
      "Iteration 1389, loss = 0.22774274\n",
      "Iteration 1390, loss = 0.22768728\n",
      "Iteration 1391, loss = 0.22756988\n",
      "Iteration 1392, loss = 0.22742727\n",
      "Iteration 1393, loss = 0.22732409\n",
      "Iteration 1394, loss = 0.22721203\n",
      "Iteration 1395, loss = 0.22710384\n",
      "Iteration 1396, loss = 0.22699363\n",
      "Iteration 1397, loss = 0.22692670\n",
      "Iteration 1398, loss = 0.22676987\n",
      "Iteration 1399, loss = 0.22683182\n",
      "Iteration 1400, loss = 0.22656454\n",
      "Iteration 1401, loss = 0.22642970\n",
      "Iteration 1402, loss = 0.22637087\n",
      "Iteration 1403, loss = 0.22627303\n",
      "Iteration 1404, loss = 0.22609233\n",
      "Iteration 1405, loss = 0.22606558\n",
      "Iteration 1406, loss = 0.22592855\n",
      "Iteration 1407, loss = 0.22579609\n",
      "Iteration 1408, loss = 0.22566844\n",
      "Iteration 1409, loss = 0.22557105\n",
      "Iteration 1410, loss = 0.22546272\n",
      "Iteration 1411, loss = 0.22534811\n",
      "Iteration 1412, loss = 0.22520796\n",
      "Iteration 1413, loss = 0.22510141\n",
      "Iteration 1414, loss = 0.22503211\n",
      "Iteration 1415, loss = 0.22499413\n",
      "Iteration 1416, loss = 0.22475112\n",
      "Iteration 1417, loss = 0.22469512\n",
      "Iteration 1418, loss = 0.22462253\n",
      "Iteration 1419, loss = 0.22445232\n",
      "Iteration 1420, loss = 0.22432906\n",
      "Iteration 1421, loss = 0.22423037\n",
      "Iteration 1422, loss = 0.22410081\n",
      "Iteration 1423, loss = 0.22399430\n",
      "Iteration 1424, loss = 0.22398228\n",
      "Iteration 1425, loss = 0.22378599\n",
      "Iteration 1426, loss = 0.22366751\n",
      "Iteration 1427, loss = 0.22354626\n",
      "Iteration 1428, loss = 0.22341989\n",
      "Iteration 1429, loss = 0.22334402\n",
      "Iteration 1430, loss = 0.22322273\n",
      "Iteration 1431, loss = 0.22311130\n",
      "Iteration 1432, loss = 0.22301125\n",
      "Iteration 1433, loss = 0.22288085\n",
      "Iteration 1434, loss = 0.22289036\n",
      "Iteration 1435, loss = 0.22267353\n",
      "Iteration 1436, loss = 0.22258097\n",
      "Iteration 1437, loss = 0.22246720\n",
      "Iteration 1438, loss = 0.22238355\n",
      "Iteration 1439, loss = 0.22226408\n",
      "Iteration 1440, loss = 0.22212487\n",
      "Iteration 1441, loss = 0.22203902\n",
      "Iteration 1442, loss = 0.22189888\n",
      "Iteration 1443, loss = 0.22185246\n",
      "Iteration 1444, loss = 0.22171796\n",
      "Iteration 1445, loss = 0.22155111\n",
      "Iteration 1446, loss = 0.22146429\n",
      "Iteration 1447, loss = 0.22135801\n",
      "Iteration 1448, loss = 0.22124814\n",
      "Iteration 1449, loss = 0.22114315\n",
      "Iteration 1450, loss = 0.22100612\n",
      "Iteration 1451, loss = 0.22090389\n",
      "Iteration 1452, loss = 0.22086032\n",
      "Iteration 1453, loss = 0.22072644\n",
      "Iteration 1454, loss = 0.22065719\n",
      "Iteration 1455, loss = 0.22046594\n",
      "Iteration 1456, loss = 0.22034734\n",
      "Iteration 1457, loss = 0.22027704\n",
      "Iteration 1458, loss = 0.22020108\n",
      "Iteration 1459, loss = 0.22006633\n",
      "Iteration 1460, loss = 0.21993536\n",
      "Iteration 1461, loss = 0.21983273\n",
      "Iteration 1462, loss = 0.21974567\n",
      "Iteration 1463, loss = 0.21958048\n",
      "Iteration 1464, loss = 0.21948183\n",
      "Iteration 1465, loss = 0.21936785\n",
      "Iteration 1466, loss = 0.21929674\n",
      "Iteration 1467, loss = 0.21916167\n",
      "Iteration 1110, loss = 0.22359450\n",
      "Iteration 1111, loss = 0.22346329\n",
      "Iteration 1112, loss = 0.22363522\n",
      "Iteration 1113, loss = 0.22327592\n",
      "Iteration 1114, loss = 0.22299126\n",
      "Iteration 1115, loss = 0.22289905\n",
      "Iteration 1116, loss = 0.22271903\n",
      "Iteration 1117, loss = 0.22260281\n",
      "Iteration 1118, loss = 0.22243851\n",
      "Iteration 1119, loss = 0.22227551\n",
      "Iteration 1120, loss = 0.22214062\n",
      "Iteration 1121, loss = 0.22191169\n",
      "Iteration 1122, loss = 0.22178704\n",
      "Iteration 1123, loss = 0.22167158\n",
      "Iteration 1124, loss = 0.22152742\n",
      "Iteration 1125, loss = 0.22131143\n",
      "Iteration 1126, loss = 0.22113630\n",
      "Iteration 1127, loss = 0.22099697\n",
      "Iteration 1128, loss = 0.22081024\n",
      "Iteration 1129, loss = 0.22085565\n",
      "Iteration 1130, loss = 0.22047152\n",
      "Iteration 1131, loss = 0.22040524\n",
      "Iteration 1132, loss = 0.22042057\n",
      "Iteration 1133, loss = 0.22009150\n",
      "Iteration 1134, loss = 0.21985733\n",
      "Iteration 1135, loss = 0.21973468\n",
      "Iteration 1136, loss = 0.21960640\n",
      "Iteration 1137, loss = 0.21946290\n",
      "Iteration 1138, loss = 0.21925380\n",
      "Iteration 1139, loss = 0.21911733\n",
      "Iteration 1140, loss = 0.21896574\n",
      "Iteration 1141, loss = 0.21881011\n",
      "Iteration 1142, loss = 0.21864277\n",
      "Iteration 1143, loss = 0.21844136\n",
      "Iteration 1144, loss = 0.21862548\n",
      "Iteration 1145, loss = 0.21816279\n",
      "Iteration 1146, loss = 0.21804650\n",
      "Iteration 1147, loss = 0.21778671\n",
      "Iteration 1148, loss = 0.21766818\n",
      "Iteration 1149, loss = 0.21749851\n",
      "Iteration 1150, loss = 0.21732975\n",
      "Iteration 1151, loss = 0.21711434\n",
      "Iteration 1152, loss = 0.21697405\n",
      "Iteration 1153, loss = 0.21692603\n",
      "Iteration 1154, loss = 0.21666222\n",
      "Iteration 1155, loss = 0.21649803\n",
      "Iteration 1156, loss = 0.21630847\n",
      "Iteration 1157, loss = 0.21615123\n",
      "Iteration 1158, loss = 0.21616877\n",
      "Iteration 1159, loss = 0.21596682\n",
      "Iteration 1160, loss = 0.21570004\n",
      "Iteration 1161, loss = 0.21572238\n",
      "Iteration 1162, loss = 0.21552778\n",
      "Iteration 1163, loss = 0.21535492\n",
      "Iteration 1164, loss = 0.21512571\n",
      "Iteration 1165, loss = 0.21490521\n",
      "Iteration 1166, loss = 0.21468843\n",
      "Iteration 1167, loss = 0.21457495\n",
      "Iteration 1168, loss = 0.21441673\n",
      "Iteration 1169, loss = 0.21426740\n",
      "Iteration 1170, loss = 0.21412980\n",
      "Iteration 1171, loss = 0.21390382\n",
      "Iteration 1172, loss = 0.21373484\n",
      "Iteration 1173, loss = 0.21356855\n",
      "Iteration 1174, loss = 0.21349567\n",
      "Iteration 1175, loss = 0.21325041\n",
      "Iteration 1176, loss = 0.21306544\n",
      "Iteration 1177, loss = 0.21291234\n",
      "Iteration 1178, loss = 0.21297591\n",
      "Iteration 1179, loss = 0.21258653\n",
      "Iteration 1180, loss = 0.21244244\n",
      "Iteration 1181, loss = 0.21225142\n",
      "Iteration 1182, loss = 0.21223742\n",
      "Iteration 1183, loss = 0.21194294\n",
      "Iteration 1184, loss = 0.21191521\n",
      "Iteration 1185, loss = 0.21169591\n",
      "Iteration 1186, loss = 0.21152151\n",
      "Iteration 1187, loss = 0.21131136\n",
      "Iteration 1188, loss = 0.21115452\n",
      "Iteration 1189, loss = 0.21101065\n",
      "Iteration 1190, loss = 0.21085740\n",
      "Iteration 1191, loss = 0.21064000\n",
      "Iteration 1192, loss = 0.21059940\n",
      "Iteration 1193, loss = 0.21038357\n",
      "Iteration 1194, loss = 0.21043508\n",
      "Iteration 1195, loss = 0.21024951\n",
      "Iteration 1196, loss = 0.21011192\n",
      "Iteration 1197, loss = 0.20970699\n",
      "Iteration 1198, loss = 0.20961255\n",
      "Iteration 1199, loss = 0.20943830\n",
      "Iteration 1200, loss = 0.20923251\n",
      "Iteration 1201, loss = 0.20907380\n",
      "Iteration 1202, loss = 0.20898120\n",
      "Iteration 1203, loss = 0.20876415\n",
      "Iteration 1204, loss = 0.20860048\n",
      "Iteration 1205, loss = 0.20853050\n",
      "Iteration 1206, loss = 0.20828494\n",
      "Iteration 1207, loss = 0.20815418\n",
      "Iteration 1208, loss = 0.20826157\n",
      "Iteration 1209, loss = 0.20786411\n",
      "Iteration 1210, loss = 0.20773239\n",
      "Iteration 1211, loss = 0.20757109\n",
      "Iteration 1212, loss = 0.20736143\n",
      "Iteration 1213, loss = 0.20720058\n",
      "Iteration 1214, loss = 0.20708068\n",
      "Iteration 1215, loss = 0.20690734\n",
      "Iteration 1216, loss = 0.20686608\n",
      "Iteration 1217, loss = 0.20666794\n",
      "Iteration 1218, loss = 0.20640316\n",
      "Iteration 1219, loss = 0.20632472\n",
      "Iteration 1220, loss = 0.20608403\n",
      "Iteration 1221, loss = 0.20592395\n",
      "Iteration 1222, loss = 0.20573407\n",
      "Iteration 1223, loss = 0.20561402\n",
      "Iteration 1224, loss = 0.20546549\n",
      "Iteration 1225, loss = 0.20544603\n",
      "Iteration 1226, loss = 0.20509723\n",
      "Iteration 1227, loss = 0.20498488\n",
      "Iteration 1228, loss = 0.20476538\n",
      "Iteration 1229, loss = 0.20474314\n",
      "Iteration 1230, loss = 0.20444683\n",
      "Iteration 1231, loss = 0.20437893\n",
      "Iteration 1232, loss = 0.20421472\n",
      "Iteration 1233, loss = 0.20401085\n",
      "Iteration 1234, loss = 0.20381191\n",
      "Iteration 1235, loss = 0.20368157\n",
      "Iteration 1236, loss = 0.20360662\n",
      "Iteration 1237, loss = 0.20333021\n",
      "Iteration 1238, loss = 0.20326720\n",
      "Iteration 1239, loss = 0.20299298\n",
      "Iteration 1240, loss = 0.20286123\n",
      "Iteration 1241, loss = 0.20277941\n",
      "Iteration 1242, loss = 0.20256586\n",
      "Iteration 1243, loss = 0.20237477\n",
      "Iteration 1244, loss = 0.20264950\n",
      "Iteration 1245, loss = 0.20207342\n",
      "Iteration 1246, loss = 0.20188267\n",
      "Iteration 1247, loss = 0.20178310\n",
      "Iteration 1248, loss = 0.20160436\n",
      "Iteration 1249, loss = 0.20142036\n",
      "Iteration 1250, loss = 0.20162358\n",
      "Iteration 1251, loss = 0.20108882\n",
      "Iteration 1252, loss = 0.20107657\n",
      "Iteration 1253, loss = 0.20078065\n",
      "Iteration 1254, loss = 0.20071953\n",
      "Iteration 1255, loss = 0.20043883\n",
      "Iteration 1256, loss = 0.20025844\n",
      "Iteration 1257, loss = 0.20011615\n",
      "Iteration 1258, loss = 0.19992210\n",
      "Iteration 1259, loss = 0.19983465\n",
      "Iteration 1260, loss = 0.19971822\n",
      "Iteration 1261, loss = 0.19955712\n",
      "Iteration 1262, loss = 0.19934543\n",
      "Iteration 1263, loss = 0.19924712\n",
      "Iteration 1264, loss = 0.19897189\n",
      "Iteration 1265, loss = 0.19926555\n",
      "Iteration 1266, loss = 0.19883248\n",
      "Iteration 1267, loss = 0.19873300\n",
      "Iteration 1268, loss = 0.19834447\n",
      "Iteration 1269, loss = 0.19823318\n",
      "Iteration 1270, loss = 0.19826801\n",
      "Iteration 1271, loss = 0.19781754\n",
      "Iteration 1272, loss = 0.19780855\n",
      "Iteration 1273, loss = 0.19793414\n",
      "Iteration 1274, loss = 0.19742800\n",
      "Iteration 1275, loss = 0.19736983\n",
      "Iteration 1276, loss = 0.19720243\n",
      "Iteration 1277, loss = 0.19692273\n",
      "Iteration 1278, loss = 0.19674475\n",
      "Iteration 1279, loss = 0.19663787\n",
      "Iteration 1280, loss = 0.19638562\n",
      "Iteration 1281, loss = 0.19618062\n",
      "Iteration 1282, loss = 0.19602049\n",
      "Iteration 1283, loss = 0.19596316\n",
      "Iteration 1284, loss = 0.19574094\n",
      "Iteration 1285, loss = 0.19561282\n",
      "Iteration 1286, loss = 0.19542685\n",
      "Iteration 1287, loss = 0.19524487\n",
      "Iteration 1288, loss = 0.19530945\n",
      "Iteration 1289, loss = 0.19497611\n",
      "Iteration 1290, loss = 0.19474094\n",
      "Iteration 1291, loss = 0.19457204\n",
      "Iteration 1292, loss = 0.19440722\n",
      "Iteration 1293, loss = 0.19425755\n",
      "Iteration 1294, loss = 0.19414060\n",
      "Iteration 1295, loss = 0.19407280\n",
      "Iteration 1296, loss = 0.19397312\n",
      "Iteration 1297, loss = 0.19376300\n",
      "Iteration 1298, loss = 0.19339055\n",
      "Iteration 1299, loss = 0.19323204\n",
      "Iteration 1300, loss = 0.19327498\n",
      "Iteration 1301, loss = 0.19294341\n",
      "Iteration 1302, loss = 0.19281860\n",
      "Iteration 1303, loss = 0.19269116\n",
      "Iteration 1304, loss = 0.19245089\n",
      "Iteration 1305, loss = 0.19226149\n",
      "Iteration 1306, loss = 0.19210116\n",
      "Iteration 1307, loss = 0.19195909\n",
      "Iteration 1308, loss = 0.19178870\n",
      "Iteration 1309, loss = 0.19177774\n",
      "Iteration 1310, loss = 0.19152595\n",
      "Iteration 1311, loss = 0.19132500\n",
      "Iteration 1312, loss = 0.19109186\n",
      "Iteration 1313, loss = 0.19092752\n",
      "Iteration 1314, loss = 0.19073568\n",
      "Iteration 1315, loss = 0.19064565\n",
      "Iteration 1316, loss = 0.19061052\n",
      "Iteration 1317, loss = 0.19033164\n",
      "Iteration 1318, loss = 0.19026927\n",
      "Iteration 1319, loss = 0.18996023\n",
      "Iteration 1320, loss = 0.18984414\n",
      "Iteration 1321, loss = 0.18960428\n",
      "Iteration 1322, loss = 0.18943085\n",
      "Iteration 1323, loss = 0.18932875\n",
      "Iteration 1324, loss = 0.18913117\n",
      "Iteration 1325, loss = 0.18900303\n",
      "Iteration 1326, loss = 0.18877312\n",
      "Iteration 1327, loss = 0.18875609\n",
      "Iteration 1328, loss = 0.18856345\n",
      "Iteration 1329, loss = 0.18830558\n",
      "Iteration 1330, loss = 0.18807895\n",
      "Iteration 1331, loss = 0.18802542\n",
      "Iteration 1332, loss = 0.18785150\n",
      "Iteration 1333, loss = 0.18759224\n",
      "Iteration 1334, loss = 0.18745467\n",
      "Iteration 1335, loss = 0.18734428\n",
      "Iteration 1336, loss = 0.18717128\n",
      "Iteration 1337, loss = 0.18697319\n",
      "Iteration 1338, loss = 0.18679468\n",
      "Iteration 1339, loss = 0.18661937\n",
      "Iteration 1340, loss = 0.18644028\n",
      "Iteration 1341, loss = 0.18654467\n",
      "Iteration 1342, loss = 0.18628707\n",
      "Iteration 1343, loss = 0.18597483\n",
      "Iteration 1344, loss = 0.18586052\n",
      "Iteration 1345, loss = 0.18562816\n",
      "Iteration 1346, loss = 0.18543580\n",
      "Iteration 1347, loss = 0.18534494\n",
      "Iteration 1348, loss = 0.18515818\n",
      "Iteration 1349, loss = 0.18505841\n",
      "Iteration 1350, loss = 0.18479920\n",
      "Iteration 1351, loss = 0.18471555\n",
      "Iteration 1352, loss = 0.18443089\n",
      "Iteration 1353, loss = 0.18428462\n",
      "Iteration 1354, loss = 0.18412892\n",
      "Iteration 1355, loss = 0.18396778\n",
      "Iteration 1356, loss = 0.18384617\n",
      "Iteration 1357, loss = 0.18361924\n",
      "Iteration 1358, loss = 0.18355585\n",
      "Iteration 1359, loss = 0.18331717\n",
      "Iteration 1360, loss = 0.18311629\n",
      "Iteration 1361, loss = 0.18295667\n",
      "Iteration 1362, loss = 0.18294251\n",
      "Iteration 1363, loss = 0.18273200\n",
      "Iteration 1364, loss = 0.18258095\n",
      "Iteration 1365, loss = 0.18230919\n",
      "Iteration 1366, loss = 0.18212947\n",
      "Iteration 1367, loss = 0.18195989\n",
      "Iteration 1368, loss = 0.18181394\n",
      "Iteration 1369, loss = 0.18161687\n",
      "Iteration 1370, loss = 0.18146616\n",
      "Iteration 1371, loss = 0.18129961\n",
      "Iteration 1372, loss = 0.18109702\n",
      "Iteration 1373, loss = 0.18103165\n",
      "Iteration 1374, loss = 0.18089357\n",
      "Iteration 1375, loss = 0.18083328\n",
      "Iteration 1376, loss = 0.18044581\n",
      "Iteration 1377, loss = 0.18031812\n",
      "Iteration 1378, loss = 0.18018827\n",
      "Iteration 1379, loss = 0.17988860\n",
      "Iteration 1380, loss = 0.17993472\n",
      "Iteration 1381, loss = 0.17970678\n",
      "Iteration 1382, loss = 0.17961972\n",
      "Iteration 1383, loss = 0.17945825\n",
      "Iteration 1384, loss = 0.17919086\n",
      "Iteration 1385, loss = 0.17893580\n",
      "Iteration 1386, loss = 0.17883256\n",
      "Iteration 1387, loss = 0.17862996\n",
      "Iteration 1388, loss = 0.17867581\n",
      "Iteration 1389, loss = 0.17834987\n",
      "Iteration 1390, loss = 0.17815053\n",
      "Iteration 1391, loss = 0.17804916\n",
      "Iteration 1392, loss = 0.17781348\n",
      "Iteration 1393, loss = 0.17761211\n",
      "Iteration 1394, loss = 0.17759316\n",
      "Iteration 1395, loss = 0.17758946\n",
      "Iteration 1396, loss = 0.17705114\n",
      "Iteration 1397, loss = 0.17690825\n",
      "Iteration 1398, loss = 0.17689992\n",
      "Iteration 1399, loss = 0.17677368\n",
      "Iteration 1400, loss = 0.17652263\n",
      "Iteration 1401, loss = 0.17634235\n",
      "Iteration 1402, loss = 0.17610631\n",
      "Iteration 1403, loss = 0.17597781\n",
      "Iteration 1404, loss = 0.17590563\n",
      "Iteration 1405, loss = 0.17562112\n",
      "Iteration 1406, loss = 0.17553573\n",
      "Iteration 1407, loss = 0.17525841\n",
      "Iteration 1408, loss = 0.17503967\n",
      "Iteration 1409, loss = 0.17489037\n",
      "Iteration 1410, loss = 0.17470084\n",
      "Iteration 1411, loss = 0.17465300\n",
      "Iteration 1412, loss = 0.17451503\n",
      "Iteration 1413, loss = 0.17465460\n",
      "Iteration 1414, loss = 0.17413094\n",
      "Iteration 1415, loss = 0.17392157\n",
      "Iteration 1416, loss = 0.17373917\n",
      "Iteration 1417, loss = 0.17355225\n",
      "Iteration 1418, loss = 0.17344738\n",
      "Iteration 1419, loss = 0.17337105\n",
      "Iteration 1420, loss = 0.17347734\n",
      "Iteration 1421, loss = 0.17290998\n",
      "Iteration 1422, loss = 0.17270843\n",
      "Iteration 1423, loss = 0.17256267\n",
      "Iteration 1424, loss = 0.17237722\n",
      "Iteration 1425, loss = 0.17227816\n",
      "Iteration 1426, loss = 0.17208755\n",
      "Iteration 1427, loss = 0.17191816\n",
      "Iteration 1428, loss = 0.17169418\n",
      "Iteration 1429, loss = 0.17161802\n",
      "Iteration 1430, loss = 0.17144972\n",
      "Iteration 1431, loss = 0.17122835\n",
      "Iteration 1432, loss = 0.17127290\n",
      "Iteration 1433, loss = 0.17086960\n",
      "Iteration 1434, loss = 0.17076481\n",
      "Iteration 1435, loss = 0.17052309\n",
      "Iteration 1436, loss = 0.17036336\n",
      "Iteration 1437, loss = 0.17025311\n",
      "Iteration 1438, loss = 0.17035250\n",
      "Iteration 1439, loss = 0.16986312\n",
      "Iteration 1440, loss = 0.16982026\n",
      "Iteration 1441, loss = 0.16966256\n",
      "Iteration 1442, loss = 0.16934690\n",
      "Iteration 1443, loss = 0.16915198\n",
      "Iteration 1444, loss = 0.16909581\n",
      "Iteration 1445, loss = 0.16894444\n",
      "Iteration 1446, loss = 0.16873507\n",
      "Iteration 1447, loss = 0.16848866\n",
      "Iteration 1448, loss = 0.16832433\n",
      "Iteration 1449, loss = 0.16821982\n",
      "Iteration 1450, loss = 0.16800183\n",
      "Iteration 1451, loss = 0.16779884\n",
      "Iteration 1452, loss = 0.16760449\n",
      "Iteration 1453, loss = 0.16788480\n",
      "Iteration 1454, loss = 0.16731397\n",
      "Iteration 1455, loss = 0.16741538\n",
      "Iteration 1456, loss = 0.16709207\n",
      "Iteration 1457, loss = 0.16700447\n",
      "Iteration 1458, loss = 0.16667074\n",
      "Iteration 1459, loss = 0.16645252\n",
      "Iteration 1460, loss = 0.16636845\n",
      "Iteration 1461, loss = 0.16628540\n",
      "Iteration 1462, loss = 0.16591154\n",
      "Iteration 1463, loss = 0.16584403\n",
      "Iteration 1464, loss = 0.16576128\n",
      "Iteration 1465, loss = 0.16546027\n",
      "Iteration 1466, loss = 0.16527574\n",
      "Iteration 1467, loss = 0.16525418\n",
      "Iteration 1468, loss = 0.16493643\n",
      "Iteration 1469, loss = 0.16483927\n",
      "Iteration 1470, loss = 0.16462100\n",
      "Iteration 1471, loss = 0.16473199\n",
      "Iteration 1472, loss = 0.16443805\n",
      "Iteration 1473, loss = 0.16429618\n",
      "Iteration 1474, loss = 0.16390809\n",
      "Iteration 1475, loss = 0.16381353\n",
      "Iteration 1476, loss = 0.16365363\n",
      "Iteration 1477, loss = 0.16350101\n",
      "Iteration 1478, loss = 0.16333080\n",
      "Iteration 1479, loss = 0.16313110\n",
      "Iteration 1480, loss = 0.16294402\n",
      "Iteration 1481, loss = 0.16283609\n",
      "Iteration 1482, loss = 0.16261961\n",
      "Iteration 1483, loss = 0.16255369\n",
      "Iteration 1484, loss = 0.16231318\n",
      "Iteration 1485, loss = 0.16218948\n",
      "Iteration 1486, loss = 0.16203353\n",
      "Iteration 1487, loss = 0.16185278\n",
      "Iteration 1488, loss = 0.16163886\n",
      "Iteration 1489, loss = 0.16163180\n",
      "Iteration 1490, loss = 0.16144843\n",
      "Iteration 1491, loss = 0.16120486\n",
      "Iteration 1492, loss = 0.16107470\n",
      "Iteration 1493, loss = 0.16096501\n",
      "Iteration 1494, loss = 0.16088516\n",
      "Iteration 1495, loss = 0.16052188\n",
      "Iteration 1496, loss = 0.16046785\n",
      "Iteration 1497, loss = 0.16043549\n",
      "Iteration 1498, loss = 0.16001196\n",
      "Iteration 1499, loss = 0.15992575\n",
      "Iteration 1500, loss = 0.15971049\n",
      "Iteration 1501, loss = 0.15949888\n",
      "Iteration 1502, loss = 0.15947265\n",
      "Iteration 1503, loss = 0.15930800\n",
      "Iteration 1504, loss = 0.15929836\n",
      "Iteration 1505, loss = 0.15904996\n",
      "Iteration 1506, loss = 0.15868926\n",
      "Iteration 1507, loss = 0.15885070\n",
      "Iteration 1508, loss = 0.15857872\n",
      "Iteration 1509, loss = 0.15820219\n",
      "Iteration 1510, loss = 0.15806309\n",
      "Iteration 1511, loss = 0.15798169\n",
      "Iteration 1512, loss = 0.15801393\n",
      "Iteration 1513, loss = 0.15760799\n",
      "Iteration 1514, loss = 0.15744743\n",
      "Iteration 1515, loss = 0.15725232\n",
      "Iteration 1516, loss = 0.15719965\n",
      "Iteration 1517, loss = 0.15696243\n",
      "Iteration 1518, loss = 0.15680258\n",
      "Iteration 1519, loss = 0.15669147\n",
      "Iteration 1520, loss = 0.15649476\n",
      "Iteration 1521, loss = 0.15632496\n",
      "Iteration 1522, loss = 0.15620765\n",
      "Iteration 1523, loss = 0.15599938\n",
      "Iteration 1524, loss = 0.15591242\n",
      "Iteration 1525, loss = 0.15565619\n",
      "Iteration 1526, loss = 0.15544350\n",
      "Iteration 1527, loss = 0.15534747\n",
      "Iteration 1528, loss = 0.15526083\n",
      "Iteration 1529, loss = 0.15511487\n",
      "Iteration 1530, loss = 0.15489654\n",
      "Iteration 1531, loss = 0.15470403\n",
      "Iteration 1532, loss = 0.15468514\n",
      "Iteration 1533, loss = 0.15444884\n",
      "Iteration 1534, loss = 0.15418848\n",
      "Iteration 1535, loss = 0.15411513\n",
      "Iteration 1536, loss = 0.15398588\n",
      "Iteration 1537, loss = 0.15374022\n",
      "Iteration 1538, loss = 0.15375667\n",
      "Iteration 1539, loss = 0.15336233\n",
      "Iteration 1540, loss = 0.15326415\n",
      "Iteration 1541, loss = 0.15313191\n",
      "Iteration 1542, loss = 0.15292333\n",
      "Iteration 1543, loss = 0.15287685\n",
      "Iteration 1544, loss = 0.15299625\n",
      "Iteration 1545, loss = 0.15261795\n",
      "Iteration 1546, loss = 0.15245442\n",
      "Iteration 1547, loss = 0.15214044\n",
      "Iteration 1548, loss = 0.15201957\n",
      "Iteration 1549, loss = 0.15195978\n",
      "Iteration 1550, loss = 0.15169742\n",
      "Iteration 1551, loss = 0.15148707\n",
      "Iteration 1552, loss = 0.15137169\n",
      "Iteration 1553, loss = 0.15124339\n",
      "Iteration 1554, loss = 0.15099756\n",
      "Iteration 1555, loss = 0.15109745\n",
      "Iteration 1556, loss = 0.15081790\n",
      "Iteration 1557, loss = 0.15067633\n",
      "Iteration 1558, loss = 0.15044601\n",
      "Iteration 1559, loss = 0.15040757\n",
      "Iteration 1560, loss = 0.15015057\n",
      "Iteration 1561, loss = 0.14994421\n",
      "Iteration 1562, loss = 0.14982906\n",
      "Iteration 1563, loss = 0.14972041\n",
      "Iteration 1564, loss = 0.14965075\n",
      "Iteration 1565, loss = 0.14927471\n",
      "Iteration 1566, loss = 0.14920982\n",
      "Iteration 1567, loss = 0.14949677\n",
      "Iteration 1568, loss = 0.14899444\n",
      "Iteration 1569, loss = 0.14868429\n",
      "Iteration 1570, loss = 0.14867346\n",
      "Iteration 1571, loss = 0.14841700\n",
      "Iteration 1572, loss = 0.14830533\n",
      "Iteration 1573, loss = 0.14807265\n",
      "Iteration 1574, loss = 0.14805513\n",
      "Iteration 1575, loss = 0.14793557\n",
      "Iteration 1576, loss = 0.14778060\n",
      "Iteration 1577, loss = 0.14740871\n",
      "Iteration 1578, loss = 0.14738756\n",
      "Iteration 1579, loss = 0.14718458\n",
      "Iteration 1580, loss = 0.14700722\n",
      "Iteration 1581, loss = 0.14682210\n",
      "Iteration 1582, loss = 0.14671647\n",
      "Iteration 1583, loss = 0.14657463\n",
      "Iteration 1584, loss = 0.14649471\n",
      "Iteration 1585, loss = 0.14632121\n",
      "Iteration 1586, loss = 0.14608717\n",
      "Iteration 1587, loss = 0.14602803\n",
      "Iteration 1588, loss = 0.14591560\n",
      "Iteration 1589, loss = 0.14562828\n"
     ]
    }
   ],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    MLPClassifier(solver='sgd', max_iter=3000, learning_rate='constant', alpha=0.1, activation='relu', random_state=903967749, verbose=True),\n",
    "    X_train, y_train,\n",
    "    param_name='hidden_layer_sizes',\n",
    "    param_range=hidden_layer_sizes_range,\n",
    "    cv=5,\n",
    "    scoring=f1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "d69797dc-511e-4087-93c9-4a5babaca6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(17, 5))\n",
    "plt.plot(hidden_layer_sizes_labels, train_mean, label='Training score', color='blue')\n",
    "plt.plot(hidden_layer_sizes_labels, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve of Neural Network - Hidden Layer Sizes vs f1')\n",
    "plt.xlabel('Hidden Layer Size')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "1510075e-a7d3-439d-974b-789a5b5c497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_range = ['logistic', 'tanh', 'relu']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "13cf6591-c504-4944-8512-d40bdc7afa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60679) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60680) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60681) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60682) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60683) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60684) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60685) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(60686) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "train_scores, test_scores = validation_curve(\n",
    "    MLPClassifier(solver='sgd', max_iter=3000, learning_rate='constant', alpha=0.1, hidden_layer_sizes=(100, 50), random_state=903967749, verbose=True),\n",
    "    X_train, y_train,\n",
    "    param_name='activation',\n",
    "    param_range=activation_range,\n",
    "    cv=5,\n",
    "    scoring=f1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "404e7b4a-370c-4a21-8aa4-63c7d1d0f320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(activation_range, train_mean, label='Training score', color='blue')\n",
    "plt.plot(activation_range, test_mean, label='Cross-validation score', color='green')\n",
    "plt.title('Validation Curve of Neural Network - Activation Function vs f1')\n",
    "plt.xlabel('Activation Function')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "24be4057-4f79-4d3b-95b9-446478d999dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "mlp = MLPClassifier(solver='sgd', max_iter=4000, learning_rate='constant', alpha=0.1,\n",
    "                    hidden_layer_sizes=(100, 50), activation='relu', random_state=903967749)\n",
    "\n",
    "# Calculate learning curve\n",
    "train_sizes, train_scores, cv_scores = learning_curve(\n",
    "    mlp, X_train, y_train, cv=5, scoring=f1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "cv_mean = np.mean(cv_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training score', color='blue')\n",
    "plt.plot(train_sizes, cv_mean, label='Cross-validation score', color='green')\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Learning Curve for MLPClassifier')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "9172ac1a-aa07-49bd-a175-d862d8dee349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70765939\n",
      "Iteration 2, loss = 0.70347050\n",
      "Iteration 3, loss = 0.69712423\n",
      "Iteration 4, loss = 0.68951276\n",
      "Iteration 5, loss = 0.68119388\n",
      "Iteration 6, loss = 0.67262654\n",
      "Iteration 7, loss = 0.66362433\n",
      "Iteration 8, loss = 0.65497320\n",
      "Iteration 9, loss = 0.64628178\n",
      "Iteration 10, loss = 0.63778771\n",
      "Iteration 11, loss = 0.62963316\n",
      "Iteration 12, loss = 0.62159117\n",
      "Iteration 13, loss = 0.61365225\n",
      "Iteration 14, loss = 0.60623966\n",
      "Iteration 15, loss = 0.59901136\n",
      "Iteration 16, loss = 0.59183632\n",
      "Iteration 17, loss = 0.58495548\n",
      "Iteration 18, loss = 0.57816984\n",
      "Iteration 19, loss = 0.57175203\n",
      "Iteration 20, loss = 0.56549259\n",
      "Iteration 21, loss = 0.55934548\n",
      "Iteration 22, loss = 0.55331795\n",
      "Iteration 23, loss = 0.54752137\n",
      "Iteration 24, loss = 0.54202655\n",
      "Iteration 25, loss = 0.53663012\n",
      "Iteration 26, loss = 0.53138418\n",
      "Iteration 27, loss = 0.52617619\n",
      "Iteration 28, loss = 0.52124631\n",
      "Iteration 29, loss = 0.51646218\n",
      "Iteration 30, loss = 0.51180503\n",
      "Iteration 31, loss = 0.50729785\n",
      "Iteration 32, loss = 0.50299596\n",
      "Iteration 33, loss = 0.49870969\n",
      "Iteration 34, loss = 0.49466429\n",
      "Iteration 35, loss = 0.49069215\n",
      "Iteration 36, loss = 0.48678041\n",
      "Iteration 37, loss = 0.48311640\n",
      "Iteration 38, loss = 0.47950175\n",
      "Iteration 39, loss = 0.47607006\n",
      "Iteration 40, loss = 0.47262874\n",
      "Iteration 41, loss = 0.46935704\n",
      "Iteration 42, loss = 0.46618819\n",
      "Iteration 43, loss = 0.46304778\n",
      "Iteration 44, loss = 0.46009184\n",
      "Iteration 45, loss = 0.45718266\n",
      "Iteration 46, loss = 0.45440387\n",
      "Iteration 47, loss = 0.45173207\n",
      "Iteration 48, loss = 0.44902072\n",
      "Iteration 49, loss = 0.44652057\n",
      "Iteration 50, loss = 0.44409874\n",
      "Iteration 51, loss = 0.44173274\n",
      "Iteration 52, loss = 0.43943666\n",
      "Iteration 53, loss = 0.43727265\n",
      "Iteration 54, loss = 0.43512505\n",
      "Iteration 55, loss = 0.43301515\n",
      "Iteration 56, loss = 0.43102922\n",
      "Iteration 57, loss = 0.42909967\n",
      "Iteration 58, loss = 0.42720894\n",
      "Iteration 59, loss = 0.42537388\n",
      "Iteration 60, loss = 0.42366167\n",
      "Iteration 61, loss = 0.42195877\n",
      "Iteration 62, loss = 0.42024574\n",
      "Iteration 63, loss = 0.41870387\n",
      "Iteration 64, loss = 0.41715579\n",
      "Iteration 65, loss = 0.41560324\n",
      "Iteration 66, loss = 0.41414068\n",
      "Iteration 67, loss = 0.41274189\n",
      "Iteration 68, loss = 0.41134550\n",
      "Iteration 69, loss = 0.40997777\n",
      "Iteration 70, loss = 0.40866148\n",
      "Iteration 71, loss = 0.40732425\n",
      "Iteration 72, loss = 0.40607710\n",
      "Iteration 73, loss = 0.40487013\n",
      "Iteration 74, loss = 0.40368672\n",
      "Iteration 75, loss = 0.40252726\n",
      "Iteration 76, loss = 0.40135651\n",
      "Iteration 77, loss = 0.40031252\n",
      "Iteration 78, loss = 0.39922944\n",
      "Iteration 79, loss = 0.39824803\n",
      "Iteration 80, loss = 0.39720915\n",
      "Iteration 81, loss = 0.39622812\n",
      "Iteration 82, loss = 0.39522420\n",
      "Iteration 83, loss = 0.39427578\n",
      "Iteration 84, loss = 0.39339467\n",
      "Iteration 85, loss = 0.39246969\n",
      "Iteration 86, loss = 0.39157271\n",
      "Iteration 87, loss = 0.39070787\n",
      "Iteration 88, loss = 0.38986494\n",
      "Iteration 89, loss = 0.38906809\n",
      "Iteration 90, loss = 0.38822773\n",
      "Iteration 91, loss = 0.38744495\n",
      "Iteration 92, loss = 0.38671099\n",
      "Iteration 93, loss = 0.38595183\n",
      "Iteration 94, loss = 0.38522868\n",
      "Iteration 95, loss = 0.38448155\n",
      "Iteration 96, loss = 0.38382724\n",
      "Iteration 97, loss = 0.38311216\n",
      "Iteration 98, loss = 0.38246918\n",
      "Iteration 99, loss = 0.38176576\n",
      "Iteration 100, loss = 0.38112305\n",
      "Iteration 101, loss = 0.38053198\n",
      "Iteration 102, loss = 0.37985211\n",
      "Iteration 103, loss = 0.37929586\n",
      "Iteration 104, loss = 0.37865103\n",
      "Iteration 105, loss = 0.37805879\n",
      "Iteration 106, loss = 0.37743799\n",
      "Iteration 107, loss = 0.37684331\n",
      "Iteration 108, loss = 0.37628067\n",
      "Iteration 109, loss = 0.37570364\n",
      "Iteration 110, loss = 0.37515751\n",
      "Iteration 111, loss = 0.37461339\n",
      "Iteration 112, loss = 0.37411052\n",
      "Iteration 113, loss = 0.37352921\n",
      "Iteration 114, loss = 0.37299266\n",
      "Iteration 115, loss = 0.37248520\n",
      "Iteration 116, loss = 0.37199174\n",
      "Iteration 117, loss = 0.37147125\n",
      "Iteration 118, loss = 0.37100320\n",
      "Iteration 119, loss = 0.37050727\n",
      "Iteration 120, loss = 0.37001332\n",
      "Iteration 121, loss = 0.36957595\n",
      "Iteration 122, loss = 0.36906368\n",
      "Iteration 123, loss = 0.36863001\n",
      "Iteration 124, loss = 0.36816101\n",
      "Iteration 125, loss = 0.36771048\n",
      "Iteration 126, loss = 0.36724844\n",
      "Iteration 127, loss = 0.36682973\n",
      "Iteration 128, loss = 0.36638515\n",
      "Iteration 129, loss = 0.36596871\n",
      "Iteration 130, loss = 0.36549371\n",
      "Iteration 131, loss = 0.36512195\n",
      "Iteration 132, loss = 0.36469987\n",
      "Iteration 133, loss = 0.36424939\n",
      "Iteration 134, loss = 0.36387463\n",
      "Iteration 135, loss = 0.36344235\n",
      "Iteration 136, loss = 0.36309194\n",
      "Iteration 137, loss = 0.36264406\n",
      "Iteration 138, loss = 0.36229674\n",
      "Iteration 139, loss = 0.36191087\n",
      "Iteration 140, loss = 0.36151268\n",
      "Iteration 141, loss = 0.36116185\n",
      "Iteration 142, loss = 0.36076561\n",
      "Iteration 143, loss = 0.36041215\n",
      "Iteration 144, loss = 0.36004683\n",
      "Iteration 145, loss = 0.35969429\n",
      "Iteration 146, loss = 0.35933644\n",
      "Iteration 147, loss = 0.35899103\n",
      "Iteration 148, loss = 0.35864816\n",
      "Iteration 149, loss = 0.35832173\n",
      "Iteration 150, loss = 0.35796036\n",
      "Iteration 151, loss = 0.35763528\n",
      "Iteration 152, loss = 0.35734045\n",
      "Iteration 153, loss = 0.35699288\n",
      "Iteration 154, loss = 0.35665578\n",
      "Iteration 155, loss = 0.35633690\n",
      "Iteration 156, loss = 0.35601621\n",
      "Iteration 157, loss = 0.35570544\n",
      "Iteration 158, loss = 0.35541765\n",
      "Iteration 159, loss = 0.35509789\n",
      "Iteration 160, loss = 0.35479745\n",
      "Iteration 161, loss = 0.35448178\n",
      "Iteration 162, loss = 0.35420181\n",
      "Iteration 163, loss = 0.35393205\n",
      "Iteration 164, loss = 0.35361244\n",
      "Iteration 165, loss = 0.35330970\n",
      "Iteration 166, loss = 0.35307224\n",
      "Iteration 167, loss = 0.35275003\n",
      "Iteration 168, loss = 0.35247900\n",
      "Iteration 169, loss = 0.35220344\n",
      "Iteration 170, loss = 0.35195065\n",
      "Iteration 171, loss = 0.35165322\n",
      "Iteration 172, loss = 0.35137166\n",
      "Iteration 173, loss = 0.35111039\n",
      "Iteration 174, loss = 0.35082517\n",
      "Iteration 175, loss = 0.35059133\n",
      "Iteration 176, loss = 0.35030315\n",
      "Iteration 177, loss = 0.35005306\n",
      "Iteration 178, loss = 0.34980559\n",
      "Iteration 179, loss = 0.34952017\n",
      "Iteration 180, loss = 0.34927838\n",
      "Iteration 181, loss = 0.34905110\n",
      "Iteration 182, loss = 0.34878898\n",
      "Iteration 183, loss = 0.34855798\n",
      "Iteration 184, loss = 0.34830343\n",
      "Iteration 185, loss = 0.34803025\n",
      "Iteration 186, loss = 0.34782148\n",
      "Iteration 187, loss = 0.34758364\n",
      "Iteration 188, loss = 0.34735845\n",
      "Iteration 189, loss = 0.34711627\n",
      "Iteration 190, loss = 0.34690201\n",
      "Iteration 191, loss = 0.34666025\n",
      "Iteration 192, loss = 0.34644129\n",
      "Iteration 193, loss = 0.34622240\n",
      "Iteration 194, loss = 0.34603005\n",
      "Iteration 195, loss = 0.34577297\n",
      "Iteration 196, loss = 0.34556512\n",
      "Iteration 197, loss = 0.34537299\n",
      "Iteration 198, loss = 0.34518046\n",
      "Iteration 199, loss = 0.34492010\n",
      "Iteration 200, loss = 0.34472371\n",
      "Iteration 201, loss = 0.34450917\n",
      "Iteration 202, loss = 0.34434430\n",
      "Iteration 203, loss = 0.34412649\n",
      "Iteration 204, loss = 0.34393337\n",
      "Iteration 205, loss = 0.34370800\n",
      "Iteration 206, loss = 0.34351618\n",
      "Iteration 207, loss = 0.34332015\n",
      "Iteration 208, loss = 0.34314071\n",
      "Iteration 209, loss = 0.34294914\n",
      "Iteration 210, loss = 0.34276218\n",
      "Iteration 211, loss = 0.34256387\n",
      "Iteration 212, loss = 0.34238555\n",
      "Iteration 213, loss = 0.34218501\n",
      "Iteration 214, loss = 0.34201059\n",
      "Iteration 215, loss = 0.34183741\n",
      "Iteration 216, loss = 0.34163817\n",
      "Iteration 217, loss = 0.34145441\n",
      "Iteration 218, loss = 0.34129600\n",
      "Iteration 219, loss = 0.34110143\n",
      "Iteration 220, loss = 0.34093584\n",
      "Iteration 221, loss = 0.34075227\n",
      "Iteration 222, loss = 0.34058222\n",
      "Iteration 223, loss = 0.34042144\n",
      "Iteration 224, loss = 0.34025878\n",
      "Iteration 225, loss = 0.34006554\n",
      "Iteration 226, loss = 0.33991751\n",
      "Iteration 227, loss = 0.33973475\n",
      "Iteration 228, loss = 0.33956597\n",
      "Iteration 229, loss = 0.33943504\n",
      "Iteration 230, loss = 0.33924791\n",
      "Iteration 231, loss = 0.33906763\n",
      "Iteration 232, loss = 0.33889861\n",
      "Iteration 233, loss = 0.33874035\n",
      "Iteration 234, loss = 0.33858081\n",
      "Iteration 235, loss = 0.33841265\n",
      "Iteration 236, loss = 0.33826003\n",
      "Iteration 237, loss = 0.33809440\n",
      "Iteration 238, loss = 0.33794968\n",
      "Iteration 239, loss = 0.33776864\n",
      "Iteration 240, loss = 0.33760840\n",
      "Iteration 241, loss = 0.33745431\n",
      "Iteration 242, loss = 0.33727553\n",
      "Iteration 243, loss = 0.33712016\n",
      "Iteration 244, loss = 0.33697225\n",
      "Iteration 245, loss = 0.33681796\n",
      "Iteration 246, loss = 0.33667251\n",
      "Iteration 247, loss = 0.33654580\n",
      "Iteration 248, loss = 0.33636559\n",
      "Iteration 249, loss = 0.33624158\n",
      "Iteration 250, loss = 0.33606543\n",
      "Iteration 251, loss = 0.33591042\n",
      "Iteration 252, loss = 0.33576461\n",
      "Iteration 253, loss = 0.33560715\n",
      "Iteration 254, loss = 0.33546377\n",
      "Iteration 255, loss = 0.33532669\n",
      "Iteration 256, loss = 0.33517530\n",
      "Iteration 257, loss = 0.33503428\n",
      "Iteration 258, loss = 0.33488036\n",
      "Iteration 259, loss = 0.33473736\n",
      "Iteration 260, loss = 0.33459826\n",
      "Iteration 261, loss = 0.33444442\n",
      "Iteration 262, loss = 0.33430687\n",
      "Iteration 263, loss = 0.33417568\n",
      "Iteration 264, loss = 0.33402682\n",
      "Iteration 265, loss = 0.33388951\n",
      "Iteration 266, loss = 0.33375725\n",
      "Iteration 267, loss = 0.33360887\n",
      "Iteration 268, loss = 0.33348544\n",
      "Iteration 269, loss = 0.33336300\n",
      "Iteration 270, loss = 0.33320041\n",
      "Iteration 271, loss = 0.33307831\n",
      "Iteration 272, loss = 0.33293448\n",
      "Iteration 273, loss = 0.33283480\n",
      "Iteration 274, loss = 0.33268398\n",
      "Iteration 275, loss = 0.33254219\n",
      "Iteration 276, loss = 0.33242889\n",
      "Iteration 277, loss = 0.33229768\n",
      "Iteration 278, loss = 0.33216223\n",
      "Iteration 279, loss = 0.33202577\n",
      "Iteration 280, loss = 0.33189370\n",
      "Iteration 281, loss = 0.33177104\n",
      "Iteration 282, loss = 0.33163762\n",
      "Iteration 283, loss = 0.33153535\n",
      "Iteration 284, loss = 0.33139578\n",
      "Iteration 285, loss = 0.33125278\n",
      "Iteration 286, loss = 0.33112093\n",
      "Iteration 287, loss = 0.33100931\n",
      "Iteration 288, loss = 0.33086869\n",
      "Iteration 289, loss = 0.33074653\n",
      "Iteration 290, loss = 0.33062997\n",
      "Iteration 291, loss = 0.33049010\n",
      "Iteration 292, loss = 0.33036925\n",
      "Iteration 293, loss = 0.33023587\n",
      "Iteration 294, loss = 0.33012414\n",
      "Iteration 295, loss = 0.32999440\n",
      "Iteration 296, loss = 0.32986694\n",
      "Iteration 297, loss = 0.32974198\n",
      "Iteration 298, loss = 0.32961225\n",
      "Iteration 299, loss = 0.32949487\n",
      "Iteration 300, loss = 0.32938540\n",
      "Iteration 301, loss = 0.32926239\n",
      "Iteration 302, loss = 0.32916684\n",
      "Iteration 303, loss = 0.32901484\n",
      "Iteration 304, loss = 0.32891283\n",
      "Iteration 305, loss = 0.32878217\n",
      "Iteration 306, loss = 0.32865686\n",
      "Iteration 307, loss = 0.32857082\n",
      "Iteration 308, loss = 0.32842846\n",
      "Iteration 309, loss = 0.32832850\n",
      "Iteration 310, loss = 0.32819899\n",
      "Iteration 311, loss = 0.32808584\n",
      "Iteration 312, loss = 0.32795388\n",
      "Iteration 313, loss = 0.32783154\n",
      "Iteration 314, loss = 0.32771648\n",
      "Iteration 315, loss = 0.32762472\n",
      "Iteration 316, loss = 0.32750253\n",
      "Iteration 317, loss = 0.32737079\n",
      "Iteration 318, loss = 0.32727150\n",
      "Iteration 319, loss = 0.32714091\n",
      "Iteration 320, loss = 0.32703033\n",
      "Iteration 321, loss = 0.32692187\n",
      "Iteration 322, loss = 0.32681191\n",
      "Iteration 323, loss = 0.32669365\n",
      "Iteration 324, loss = 0.32659862\n",
      "Iteration 325, loss = 0.32646432\n",
      "Iteration 326, loss = 0.32635377\n",
      "Iteration 327, loss = 0.32623709\n",
      "Iteration 328, loss = 0.32614309\n",
      "Iteration 329, loss = 0.32603209\n",
      "Iteration 330, loss = 0.32590496\n",
      "Iteration 331, loss = 0.32580061\n",
      "Iteration 332, loss = 0.32568472\n",
      "Iteration 333, loss = 0.32556832\n",
      "Iteration 334, loss = 0.32549014\n",
      "Iteration 335, loss = 0.32537137\n",
      "Iteration 336, loss = 0.32526286\n",
      "Iteration 337, loss = 0.32513543\n",
      "Iteration 338, loss = 0.32504268\n",
      "Iteration 339, loss = 0.32493466\n",
      "Iteration 340, loss = 0.32482078\n",
      "Iteration 341, loss = 0.32472002\n",
      "Iteration 342, loss = 0.32461115\n",
      "Iteration 343, loss = 0.32449734\n",
      "Iteration 344, loss = 0.32438328\n",
      "Iteration 345, loss = 0.32428699\n",
      "Iteration 346, loss = 0.32416178\n",
      "Iteration 347, loss = 0.32405966\n",
      "Iteration 348, loss = 0.32394113\n",
      "Iteration 349, loss = 0.32385514\n",
      "Iteration 350, loss = 0.32374834\n",
      "Iteration 351, loss = 0.32362419\n",
      "Iteration 352, loss = 0.32351900\n",
      "Iteration 353, loss = 0.32343522\n",
      "Iteration 354, loss = 0.32331450\n",
      "Iteration 355, loss = 0.32320232\n",
      "Iteration 356, loss = 0.32310329\n",
      "Iteration 357, loss = 0.32301877\n",
      "Iteration 358, loss = 0.32287972\n",
      "Iteration 359, loss = 0.32277953\n",
      "Iteration 360, loss = 0.32270194\n",
      "Iteration 361, loss = 0.32257395\n",
      "Iteration 362, loss = 0.32248580\n",
      "Iteration 363, loss = 0.32237541\n",
      "Iteration 364, loss = 0.32225685\n",
      "Iteration 365, loss = 0.32219876\n",
      "Iteration 366, loss = 0.32206028\n",
      "Iteration 367, loss = 0.32196713\n",
      "Iteration 368, loss = 0.32185048\n",
      "Iteration 369, loss = 0.32176041\n",
      "Iteration 370, loss = 0.32164931\n",
      "Iteration 371, loss = 0.32156759\n",
      "Iteration 372, loss = 0.32145646\n",
      "Iteration 373, loss = 0.32134318\n",
      "Iteration 374, loss = 0.32124537\n",
      "Iteration 375, loss = 0.32114222\n",
      "Iteration 376, loss = 0.32106009\n",
      "Iteration 377, loss = 0.32093802\n",
      "Iteration 378, loss = 0.32085914\n",
      "Iteration 379, loss = 0.32074295\n",
      "Iteration 380, loss = 0.32065370\n",
      "Iteration 381, loss = 0.32054842\n",
      "Iteration 382, loss = 0.32043102\n",
      "Iteration 383, loss = 0.32033438\n",
      "Iteration 384, loss = 0.32025550\n",
      "Iteration 385, loss = 0.32012185\n",
      "Iteration 386, loss = 0.32005966\n",
      "Iteration 387, loss = 0.31995431\n",
      "Iteration 388, loss = 0.31983903\n",
      "Iteration 389, loss = 0.31974108\n",
      "Iteration 390, loss = 0.31962340\n",
      "Iteration 391, loss = 0.31953571\n",
      "Iteration 392, loss = 0.31943935\n",
      "Iteration 393, loss = 0.31932999\n",
      "Iteration 394, loss = 0.31922694\n",
      "Iteration 395, loss = 0.31913916\n",
      "Iteration 396, loss = 0.31902504\n",
      "Iteration 397, loss = 0.31895504\n",
      "Iteration 398, loss = 0.31883732\n",
      "Iteration 399, loss = 0.31873241\n",
      "Iteration 400, loss = 0.31865796\n",
      "Iteration 401, loss = 0.31853911\n",
      "Iteration 402, loss = 0.31843872\n",
      "Iteration 403, loss = 0.31833516\n",
      "Iteration 404, loss = 0.31824695\n",
      "Iteration 405, loss = 0.31814862\n",
      "Iteration 406, loss = 0.31803195\n",
      "Iteration 407, loss = 0.31796076\n",
      "Iteration 408, loss = 0.31786577\n",
      "Iteration 409, loss = 0.31774301\n",
      "Iteration 410, loss = 0.31764944\n",
      "Iteration 411, loss = 0.31754175\n",
      "Iteration 412, loss = 0.31746658\n",
      "Iteration 413, loss = 0.31735693\n",
      "Iteration 414, loss = 0.31727734\n",
      "Iteration 415, loss = 0.31717801\n",
      "Iteration 416, loss = 0.31708343\n",
      "Iteration 417, loss = 0.31697498\n",
      "Iteration 418, loss = 0.31688154\n",
      "Iteration 419, loss = 0.31682064\n",
      "Iteration 420, loss = 0.31668526\n",
      "Iteration 421, loss = 0.31658996\n",
      "Iteration 422, loss = 0.31652982\n",
      "Iteration 423, loss = 0.31641348\n",
      "Iteration 424, loss = 0.31631952\n",
      "Iteration 425, loss = 0.31623366\n",
      "Iteration 426, loss = 0.31612484\n",
      "Iteration 427, loss = 0.31604218\n",
      "Iteration 428, loss = 0.31593139\n",
      "Iteration 429, loss = 0.31586573\n",
      "Iteration 430, loss = 0.31577735\n",
      "Iteration 431, loss = 0.31566298\n",
      "Iteration 432, loss = 0.31555964\n",
      "Iteration 433, loss = 0.31547846\n",
      "Iteration 434, loss = 0.31537296\n",
      "Iteration 435, loss = 0.31529509\n",
      "Iteration 436, loss = 0.31519690\n",
      "Iteration 437, loss = 0.31510421\n",
      "Iteration 438, loss = 0.31503097\n",
      "Iteration 439, loss = 0.31492539\n",
      "Iteration 440, loss = 0.31484526\n",
      "Iteration 441, loss = 0.31475349\n",
      "Iteration 442, loss = 0.31467543\n",
      "Iteration 443, loss = 0.31458454\n",
      "Iteration 444, loss = 0.31447969\n",
      "Iteration 445, loss = 0.31439746\n",
      "Iteration 446, loss = 0.31433441\n",
      "Iteration 447, loss = 0.31421212\n",
      "Iteration 448, loss = 0.31413224\n",
      "Iteration 449, loss = 0.31405849\n",
      "Iteration 450, loss = 0.31394673\n",
      "Iteration 451, loss = 0.31386724\n",
      "Iteration 452, loss = 0.31377973\n",
      "Iteration 453, loss = 0.31370976\n",
      "Iteration 454, loss = 0.31359022\n",
      "Iteration 455, loss = 0.31352291\n",
      "Iteration 456, loss = 0.31342382\n",
      "Iteration 457, loss = 0.31337606\n",
      "Iteration 458, loss = 0.31324604\n",
      "Iteration 459, loss = 0.31316314\n",
      "Iteration 460, loss = 0.31306952\n",
      "Iteration 461, loss = 0.31299630\n",
      "Iteration 462, loss = 0.31288604\n",
      "Iteration 463, loss = 0.31282960\n",
      "Iteration 464, loss = 0.31271771\n",
      "Iteration 465, loss = 0.31263471\n",
      "Iteration 466, loss = 0.31255715\n",
      "Iteration 467, loss = 0.31247000\n",
      "Iteration 468, loss = 0.31238471\n",
      "Iteration 469, loss = 0.31228557\n",
      "Iteration 470, loss = 0.31221822\n",
      "Iteration 471, loss = 0.31211794\n",
      "Iteration 472, loss = 0.31204261\n",
      "Iteration 473, loss = 0.31195450\n",
      "Iteration 474, loss = 0.31187259\n",
      "Iteration 475, loss = 0.31179334\n",
      "Iteration 476, loss = 0.31168084\n",
      "Iteration 477, loss = 0.31159606\n",
      "Iteration 478, loss = 0.31151489\n",
      "Iteration 479, loss = 0.31143396\n",
      "Iteration 480, loss = 0.31135248\n",
      "Iteration 481, loss = 0.31125855\n",
      "Iteration 482, loss = 0.31116901\n",
      "Iteration 483, loss = 0.31109518\n",
      "Iteration 484, loss = 0.31100506\n",
      "Iteration 485, loss = 0.31092211\n",
      "Iteration 486, loss = 0.31084233\n",
      "Iteration 487, loss = 0.31076534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='sgd', max_iter=3000, learning_rate='constant', alpha=0.1,\n",
    "                    hidden_layer_sizes=(100, 50), activation='relu', random_state=903967749, verbose=True)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mlp.loss_curve_, label='Training loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve for MLPClassifier')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "461187f7-f3ad-4a39-a3d2-51c1e9683683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "2f355e85-c075-4deb-89d8-9b17b845cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='sgd', max_iter=1, learning_rate='constant', alpha=0.1,\n",
    "                    hidden_layer_sizes=(100, 50), activation='relu', warm_start=True, random_state=903967749)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "a99214c0-b9b1-4882-990d-e0b636e5bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "epochs = np.arange(1, 101)\n",
    "cv = StratifiedKFold(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "6a048ec7-4745-46f0-85b4-47b5be1144c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (33) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (33) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (33) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (33) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (33) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (36) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (36) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (36) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (36) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (36) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (37) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (37) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (37) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (37) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (37) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (39) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (39) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (39) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (39) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (39) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (43) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (43) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (43) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (43) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (43) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (44) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (44) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (44) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (44) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (44) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (45) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (45) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (45) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (45) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (45) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (46) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (46) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (46) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (46) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (46) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (47) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (47) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (47) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (47) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (47) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (54) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (54) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (54) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (54) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (54) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (56) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (56) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (56) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (56) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (56) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (57) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (57) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (57) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (57) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (57) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (58) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (58) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (58) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (58) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (58) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (60) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (60) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (60) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (60) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (60) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (61) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (61) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (61) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (61) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (61) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (63) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (63) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (63) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (63) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (63) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (65) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (65) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (65) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (65) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (65) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (66) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (66) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (66) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (66) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (66) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (67) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (67) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (67) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (67) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (67) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (68) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (68) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (68) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (68) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (68) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (69) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (69) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (69) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (69) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (69) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (71) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (71) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (71) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (71) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (71) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (72) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (72) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (72) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (72) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (72) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (73) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (73) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (73) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (73) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (73) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (74) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (74) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (74) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (74) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (74) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (75) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (75) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (75) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (75) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (75) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (76) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (76) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (76) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (76) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (76) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (78) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (78) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (78) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (78) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (78) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (79) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (79) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (79) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (79) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (79) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (81) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (81) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (81) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (81) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (81) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (82) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (82) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (82) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (82) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (82) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (83) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (83) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (83) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (83) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (83) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (84) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (84) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (84) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (84) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (84) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (85) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (85) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (85) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (85) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (85) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (88) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (88) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (88) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (88) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (88) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (89) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (89) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (89) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (89) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (89) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (90) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (90) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (90) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (90) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (90) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (92) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (92) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (92) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (92) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (92) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (93) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (93) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (93) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (93) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (93) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (94) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (94) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (94) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (94) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (94) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (95) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (95) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (95) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (95) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (95) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (96) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (96) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (96) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (96) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (96) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (97) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (97) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (97) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (97) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (97) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (98) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (98) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (98) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (98) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (98) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (99) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (99) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (99) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (99) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (99) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CS7641-ML/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71964881\n",
      "Iteration 2, loss = 0.71005539\n",
      "Iteration 3, loss = 0.69559087\n",
      "Iteration 4, loss = 0.67970732\n",
      "Iteration 5, loss = 0.66097027\n",
      "Iteration 6, loss = 0.64300300\n",
      "Iteration 7, loss = 0.62600417\n",
      "Iteration 8, loss = 0.60918368\n",
      "Iteration 9, loss = 0.59350213\n",
      "Iteration 10, loss = 0.57911107\n",
      "Iteration 11, loss = 0.56643966\n",
      "Iteration 12, loss = 0.55419748\n",
      "Iteration 13, loss = 0.54375258\n",
      "Iteration 14, loss = 0.53363870\n",
      "Iteration 15, loss = 0.52467473\n",
      "Iteration 16, loss = 0.51636002\n",
      "Iteration 17, loss = 0.50919584\n",
      "Iteration 18, loss = 0.50255089\n",
      "Iteration 19, loss = 0.49604624\n",
      "Iteration 20, loss = 0.49037761\n",
      "Iteration 21, loss = 0.48516260\n",
      "Iteration 22, loss = 0.48038253\n",
      "Iteration 23, loss = 0.47592660\n",
      "Iteration 24, loss = 0.47169451\n",
      "Iteration 25, loss = 0.46789646\n",
      "Iteration 26, loss = 0.46428441\n",
      "Iteration 27, loss = 0.46089133\n",
      "Iteration 28, loss = 0.45775117\n",
      "Iteration 29, loss = 0.45483608\n",
      "Iteration 30, loss = 0.45193381\n",
      "Iteration 31, loss = 0.44942999\n",
      "Iteration 32, loss = 0.44692940\n",
      "Iteration 33, loss = 0.44451982\n",
      "Iteration 34, loss = 0.44227628\n",
      "Iteration 35, loss = 0.44021930\n",
      "Iteration 36, loss = 0.43816341\n",
      "Iteration 37, loss = 0.43634276\n",
      "Iteration 38, loss = 0.43454868\n",
      "Iteration 39, loss = 0.43267790\n",
      "Iteration 40, loss = 0.43105903\n",
      "Iteration 41, loss = 0.42950975\n",
      "Iteration 42, loss = 0.42791422\n",
      "Iteration 43, loss = 0.42644074\n",
      "Iteration 44, loss = 0.42508487\n",
      "Iteration 45, loss = 0.42377623\n",
      "Iteration 46, loss = 0.42239111\n",
      "Iteration 47, loss = 0.42122941\n",
      "Iteration 48, loss = 0.42006216\n",
      "Iteration 49, loss = 0.41881722\n",
      "Iteration 50, loss = 0.41767847\n",
      "Iteration 51, loss = 0.41661064\n",
      "Iteration 52, loss = 0.41557711\n",
      "Iteration 53, loss = 0.41455585\n",
      "Iteration 54, loss = 0.41358913\n",
      "Iteration 55, loss = 0.41272024\n",
      "Iteration 56, loss = 0.41175657\n",
      "Iteration 57, loss = 0.41086260\n",
      "Iteration 58, loss = 0.40996413\n",
      "Iteration 59, loss = 0.40917199\n",
      "Iteration 60, loss = 0.40837816\n",
      "Iteration 61, loss = 0.40754995\n",
      "Iteration 62, loss = 0.40679369\n",
      "Iteration 63, loss = 0.40604048\n",
      "Iteration 64, loss = 0.40531769\n",
      "Iteration 65, loss = 0.40461701\n",
      "Iteration 66, loss = 0.40394103\n",
      "Iteration 67, loss = 0.40323885\n",
      "Iteration 68, loss = 0.40261875\n",
      "Iteration 69, loss = 0.40200862\n",
      "Iteration 70, loss = 0.40136511\n",
      "Iteration 71, loss = 0.40076090\n",
      "Iteration 72, loss = 0.40015355\n",
      "Iteration 73, loss = 0.39957905\n",
      "Iteration 74, loss = 0.39901244\n",
      "Iteration 75, loss = 0.39846360\n",
      "Iteration 76, loss = 0.39793743\n",
      "Iteration 77, loss = 0.39742139\n",
      "Iteration 78, loss = 0.39691920\n",
      "Iteration 79, loss = 0.39642540\n",
      "Iteration 80, loss = 0.39593785\n",
      "Iteration 81, loss = 0.39543077\n",
      "Iteration 82, loss = 0.39496220\n",
      "Iteration 83, loss = 0.39452446\n",
      "Iteration 84, loss = 0.39405804\n",
      "Iteration 85, loss = 0.39368366\n",
      "Iteration 86, loss = 0.39319504\n",
      "Iteration 87, loss = 0.39284191\n",
      "Iteration 88, loss = 0.39244970\n",
      "Iteration 89, loss = 0.39196686\n",
      "Iteration 90, loss = 0.39160080\n",
      "Iteration 91, loss = 0.39119900\n",
      "Iteration 92, loss = 0.39080283\n",
      "Iteration 93, loss = 0.39045717\n",
      "Iteration 94, loss = 0.39010047\n",
      "Iteration 95, loss = 0.38975935\n",
      "Iteration 96, loss = 0.38939229\n",
      "Iteration 97, loss = 0.38903979\n",
      "Iteration 98, loss = 0.38871589\n",
      "Iteration 99, loss = 0.38835384\n",
      "Iteration 100, loss = 0.38804426\n",
      "Iteration 101, loss = 0.38772816\n",
      "Iteration 102, loss = 0.38740933\n",
      "Iteration 103, loss = 0.38708616\n",
      "Iteration 104, loss = 0.38681135\n",
      "Iteration 105, loss = 0.38651539\n",
      "Iteration 106, loss = 0.38620756\n",
      "Iteration 107, loss = 0.38591534\n",
      "Iteration 108, loss = 0.38564697\n",
      "Iteration 109, loss = 0.38534788\n",
      "Iteration 110, loss = 0.38512550\n",
      "Iteration 111, loss = 0.38480331\n",
      "Iteration 112, loss = 0.38455503\n",
      "Iteration 113, loss = 0.38432393\n",
      "Iteration 114, loss = 0.38406831\n",
      "Iteration 115, loss = 0.38379504\n",
      "Iteration 116, loss = 0.38357406\n",
      "Iteration 117, loss = 0.38330135\n",
      "Iteration 118, loss = 0.38307530\n",
      "Iteration 119, loss = 0.38284334\n",
      "Iteration 120, loss = 0.38260954\n",
      "Iteration 121, loss = 0.38236974\n",
      "Iteration 122, loss = 0.38216567\n",
      "Iteration 123, loss = 0.38193445\n",
      "Iteration 124, loss = 0.38171989\n",
      "Iteration 125, loss = 0.38149120\n",
      "Iteration 126, loss = 0.38127849\n",
      "Iteration 127, loss = 0.38111210\n",
      "Iteration 128, loss = 0.38089673\n",
      "Iteration 129, loss = 0.38067278\n",
      "Iteration 130, loss = 0.38049202\n",
      "Iteration 131, loss = 0.38029532\n",
      "Iteration 132, loss = 0.38011257\n",
      "Iteration 133, loss = 0.37991558\n",
      "Iteration 134, loss = 0.37974748\n",
      "Iteration 135, loss = 0.37953003\n",
      "Iteration 136, loss = 0.37933068\n",
      "Iteration 137, loss = 0.37921795\n",
      "Iteration 138, loss = 0.37902568\n",
      "Iteration 139, loss = 0.37884092\n",
      "Iteration 140, loss = 0.37868268\n",
      "Iteration 141, loss = 0.37848851\n",
      "Iteration 142, loss = 0.37831981\n",
      "Iteration 143, loss = 0.37814993\n",
      "Iteration 144, loss = 0.37803268\n",
      "Iteration 145, loss = 0.37784127\n",
      "Iteration 146, loss = 0.37769037\n",
      "Iteration 147, loss = 0.37755895\n",
      "Iteration 148, loss = 0.37737753\n",
      "Iteration 149, loss = 0.37720669\n",
      "Iteration 150, loss = 0.37708908\n",
      "Iteration 151, loss = 0.37694587\n",
      "Iteration 152, loss = 0.37681093\n",
      "Iteration 153, loss = 0.37664697\n",
      "Iteration 154, loss = 0.37649654\n",
      "Iteration 155, loss = 0.37636347\n",
      "Iteration 156, loss = 0.37621591\n",
      "Iteration 157, loss = 0.37611023\n",
      "Iteration 158, loss = 0.37594385\n",
      "Iteration 159, loss = 0.37580220\n",
      "Iteration 160, loss = 0.37568429\n",
      "Iteration 161, loss = 0.37554910\n",
      "Iteration 162, loss = 0.37542008\n",
      "Iteration 163, loss = 0.37530180\n",
      "Iteration 164, loss = 0.37516931\n",
      "Iteration 165, loss = 0.37504835\n",
      "Iteration 166, loss = 0.37494577\n",
      "Iteration 167, loss = 0.37478614\n",
      "Iteration 168, loss = 0.37470608\n",
      "Iteration 169, loss = 0.37456672\n",
      "Iteration 170, loss = 0.37447058\n",
      "Iteration 171, loss = 0.37435603\n",
      "Iteration 172, loss = 0.37421540\n",
      "Iteration 173, loss = 0.37411566\n",
      "Iteration 174, loss = 0.37399284\n",
      "Iteration 175, loss = 0.37388930\n",
      "Iteration 176, loss = 0.37377930\n",
      "Iteration 177, loss = 0.37366668\n",
      "Iteration 178, loss = 0.37357578\n",
      "Iteration 179, loss = 0.37345009\n",
      "Iteration 180, loss = 0.37334774\n",
      "Iteration 181, loss = 0.37327574\n",
      "Iteration 182, loss = 0.37315942\n",
      "Iteration 183, loss = 0.37307310\n",
      "Iteration 184, loss = 0.37295997\n",
      "Iteration 185, loss = 0.37284868\n",
      "Iteration 186, loss = 0.37275889\n",
      "Iteration 187, loss = 0.37266120\n",
      "Iteration 188, loss = 0.37255246\n",
      "Iteration 189, loss = 0.37246878\n",
      "Iteration 190, loss = 0.37236332\n",
      "Iteration 191, loss = 0.37230016\n",
      "Iteration 192, loss = 0.37219132\n",
      "Iteration 193, loss = 0.37209377\n",
      "Iteration 194, loss = 0.37202075\n",
      "Iteration 195, loss = 0.37191830\n",
      "Iteration 196, loss = 0.37183555\n",
      "Iteration 197, loss = 0.37177584\n",
      "Iteration 198, loss = 0.37165987\n",
      "Iteration 199, loss = 0.37158506\n",
      "Iteration 200, loss = 0.37148778\n",
      "Iteration 201, loss = 0.37142062\n",
      "Iteration 202, loss = 0.37131389\n",
      "Iteration 203, loss = 0.37124808\n",
      "Iteration 204, loss = 0.37115252\n",
      "Iteration 205, loss = 0.37106710\n",
      "Iteration 206, loss = 0.37098329\n",
      "Iteration 207, loss = 0.37090651\n",
      "Iteration 208, loss = 0.37084491\n",
      "Iteration 209, loss = 0.37077541\n",
      "Iteration 210, loss = 0.37068706\n",
      "Iteration 211, loss = 0.37062363\n",
      "Iteration 212, loss = 0.37055104\n",
      "Iteration 213, loss = 0.37045969\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72071412\n",
      "Iteration 2, loss = 0.71055919\n",
      "Iteration 3, loss = 0.69589368\n",
      "Iteration 4, loss = 0.67862072\n",
      "Iteration 5, loss = 0.65941628\n",
      "Iteration 6, loss = 0.64033271\n",
      "Iteration 7, loss = 0.62261199\n",
      "Iteration 8, loss = 0.60463002\n",
      "Iteration 9, loss = 0.58880681\n",
      "Iteration 10, loss = 0.57338214\n",
      "Iteration 11, loss = 0.55962181\n",
      "Iteration 12, loss = 0.54711207\n",
      "Iteration 13, loss = 0.53558147\n",
      "Iteration 14, loss = 0.52509018\n",
      "Iteration 15, loss = 0.51552034\n",
      "Iteration 16, loss = 0.50696614\n",
      "Iteration 17, loss = 0.49916695\n",
      "Iteration 18, loss = 0.49195617\n",
      "Iteration 19, loss = 0.48508156\n",
      "Iteration 20, loss = 0.47907787\n",
      "Iteration 21, loss = 0.47345788\n",
      "Iteration 22, loss = 0.46833996\n",
      "Iteration 23, loss = 0.46361279\n",
      "Iteration 24, loss = 0.45906562\n",
      "Iteration 25, loss = 0.45490612\n",
      "Iteration 26, loss = 0.45112375\n",
      "Iteration 27, loss = 0.44755127\n",
      "Iteration 28, loss = 0.44415163\n",
      "Iteration 29, loss = 0.44109900\n",
      "Iteration 30, loss = 0.43822745\n",
      "Iteration 31, loss = 0.43530194\n",
      "Iteration 32, loss = 0.43256057\n",
      "Iteration 33, loss = 0.43015738\n",
      "Iteration 34, loss = 0.42781165\n",
      "Iteration 35, loss = 0.42560597\n",
      "Iteration 36, loss = 0.42343847\n",
      "Iteration 37, loss = 0.42147254\n",
      "Iteration 38, loss = 0.41961328\n",
      "Iteration 39, loss = 0.41772754\n",
      "Iteration 40, loss = 0.41599698\n",
      "Iteration 41, loss = 0.41424497\n",
      "Iteration 42, loss = 0.41273262\n",
      "Iteration 43, loss = 0.41122075\n",
      "Iteration 44, loss = 0.40970236\n",
      "Iteration 45, loss = 0.40834548\n",
      "Iteration 46, loss = 0.40700638\n",
      "Iteration 47, loss = 0.40563599\n",
      "Iteration 48, loss = 0.40447596\n",
      "Iteration 49, loss = 0.40329565\n",
      "Iteration 50, loss = 0.40207566\n",
      "Iteration 51, loss = 0.40104420\n",
      "Iteration 52, loss = 0.39991471\n",
      "Iteration 53, loss = 0.39886349\n",
      "Iteration 54, loss = 0.39782678\n",
      "Iteration 55, loss = 0.39694531\n",
      "Iteration 56, loss = 0.39597871\n",
      "Iteration 57, loss = 0.39509500\n",
      "Iteration 58, loss = 0.39415216\n",
      "Iteration 59, loss = 0.39340924\n",
      "Iteration 60, loss = 0.39251689\n",
      "Iteration 61, loss = 0.39167931\n",
      "Iteration 62, loss = 0.39098444\n",
      "Iteration 63, loss = 0.39017405\n",
      "Iteration 64, loss = 0.38943296\n",
      "Iteration 65, loss = 0.38871762\n",
      "Iteration 66, loss = 0.38801144\n",
      "Iteration 67, loss = 0.38734611\n",
      "Iteration 68, loss = 0.38671941\n",
      "Iteration 69, loss = 0.38605680\n",
      "Iteration 70, loss = 0.38541584\n",
      "Iteration 71, loss = 0.38481211\n",
      "Iteration 72, loss = 0.38428926\n",
      "Iteration 73, loss = 0.38367253\n",
      "Iteration 74, loss = 0.38307186\n",
      "Iteration 75, loss = 0.38257777\n",
      "Iteration 76, loss = 0.38202948\n",
      "Iteration 77, loss = 0.38152407\n",
      "Iteration 78, loss = 0.38097907\n",
      "Iteration 79, loss = 0.38048023\n",
      "Iteration 80, loss = 0.37997813\n",
      "Iteration 81, loss = 0.37951414\n",
      "Iteration 82, loss = 0.37903748\n",
      "Iteration 83, loss = 0.37858280\n",
      "Iteration 84, loss = 0.37818756\n",
      "Iteration 85, loss = 0.37771199\n",
      "Iteration 86, loss = 0.37726450\n",
      "Iteration 87, loss = 0.37688818\n",
      "Iteration 88, loss = 0.37644711\n",
      "Iteration 89, loss = 0.37605673\n",
      "Iteration 90, loss = 0.37566047\n",
      "Iteration 91, loss = 0.37525100\n",
      "Iteration 92, loss = 0.37491419\n",
      "Iteration 93, loss = 0.37452130\n",
      "Iteration 94, loss = 0.37418861\n",
      "Iteration 95, loss = 0.37380865\n",
      "Iteration 96, loss = 0.37346406\n",
      "Iteration 97, loss = 0.37314093\n",
      "Iteration 98, loss = 0.37280035\n",
      "Iteration 99, loss = 0.37245313\n",
      "Iteration 100, loss = 0.37214050\n",
      "Iteration 101, loss = 0.37182789\n",
      "Iteration 102, loss = 0.37152244\n",
      "Iteration 103, loss = 0.37119537\n",
      "Iteration 104, loss = 0.37091149\n",
      "Iteration 105, loss = 0.37059988\n",
      "Iteration 106, loss = 0.37031831\n",
      "Iteration 107, loss = 0.37006360\n",
      "Iteration 108, loss = 0.36981680\n",
      "Iteration 109, loss = 0.36948040\n",
      "Iteration 110, loss = 0.36922281\n",
      "Iteration 111, loss = 0.36893641\n",
      "Iteration 112, loss = 0.36871466\n",
      "Iteration 113, loss = 0.36846379\n",
      "Iteration 114, loss = 0.36816598\n",
      "Iteration 115, loss = 0.36794655\n",
      "Iteration 116, loss = 0.36771355\n",
      "Iteration 117, loss = 0.36747438\n",
      "Iteration 118, loss = 0.36723357\n",
      "Iteration 119, loss = 0.36701408\n",
      "Iteration 120, loss = 0.36676366\n",
      "Iteration 121, loss = 0.36653383\n",
      "Iteration 122, loss = 0.36631452\n",
      "Iteration 123, loss = 0.36610734\n",
      "Iteration 124, loss = 0.36589763\n",
      "Iteration 125, loss = 0.36568129\n",
      "Iteration 126, loss = 0.36547816\n",
      "Iteration 127, loss = 0.36528093\n",
      "Iteration 128, loss = 0.36509659\n",
      "Iteration 129, loss = 0.36488067\n",
      "Iteration 130, loss = 0.36469987\n",
      "Iteration 131, loss = 0.36450083\n",
      "Iteration 132, loss = 0.36429185\n",
      "Iteration 133, loss = 0.36416705\n",
      "Iteration 134, loss = 0.36394016\n",
      "Iteration 135, loss = 0.36376507\n",
      "Iteration 136, loss = 0.36361228\n",
      "Iteration 137, loss = 0.36340410\n",
      "Iteration 138, loss = 0.36326014\n",
      "Iteration 139, loss = 0.36308299\n",
      "Iteration 140, loss = 0.36291456\n",
      "Iteration 141, loss = 0.36275066\n",
      "Iteration 142, loss = 0.36258127\n",
      "Iteration 143, loss = 0.36241991\n",
      "Iteration 144, loss = 0.36225377\n",
      "Iteration 145, loss = 0.36213780\n",
      "Iteration 146, loss = 0.36196002\n",
      "Iteration 147, loss = 0.36180673\n",
      "Iteration 148, loss = 0.36167377\n",
      "Iteration 149, loss = 0.36149625\n",
      "Iteration 150, loss = 0.36137363\n",
      "Iteration 151, loss = 0.36124274\n",
      "Iteration 152, loss = 0.36111127\n",
      "Iteration 153, loss = 0.36098559\n",
      "Iteration 154, loss = 0.36086118\n",
      "Iteration 155, loss = 0.36067564\n",
      "Iteration 156, loss = 0.36055373\n",
      "Iteration 157, loss = 0.36040766\n",
      "Iteration 158, loss = 0.36030405\n",
      "Iteration 159, loss = 0.36016759\n",
      "Iteration 160, loss = 0.36003677\n",
      "Iteration 161, loss = 0.35992137\n",
      "Iteration 162, loss = 0.35981665\n",
      "Iteration 163, loss = 0.35970651\n",
      "Iteration 164, loss = 0.35956956\n",
      "Iteration 165, loss = 0.35943423\n",
      "Iteration 166, loss = 0.35930319\n",
      "Iteration 167, loss = 0.35920686\n",
      "Iteration 168, loss = 0.35908683\n",
      "Iteration 169, loss = 0.35901507\n",
      "Iteration 170, loss = 0.35890011\n",
      "Iteration 171, loss = 0.35876806\n",
      "Iteration 172, loss = 0.35865318\n",
      "Iteration 173, loss = 0.35855577\n",
      "Iteration 174, loss = 0.35844060\n",
      "Iteration 175, loss = 0.35837073\n",
      "Iteration 176, loss = 0.35824077\n",
      "Iteration 177, loss = 0.35816336\n",
      "Iteration 178, loss = 0.35805200\n",
      "Iteration 179, loss = 0.35795392\n",
      "Iteration 180, loss = 0.35783699\n",
      "Iteration 181, loss = 0.35776109\n",
      "Iteration 182, loss = 0.35766026\n",
      "Iteration 183, loss = 0.35756246\n",
      "Iteration 184, loss = 0.35746304\n",
      "Iteration 185, loss = 0.35737246\n",
      "Iteration 186, loss = 0.35729623\n",
      "Iteration 187, loss = 0.35720703\n",
      "Iteration 188, loss = 0.35711041\n",
      "Iteration 189, loss = 0.35701189\n",
      "Iteration 190, loss = 0.35692809\n",
      "Iteration 191, loss = 0.35683084\n",
      "Iteration 192, loss = 0.35676182\n",
      "Iteration 193, loss = 0.35671400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69519989\n",
      "Iteration 2, loss = 0.69518148\n",
      "Iteration 3, loss = 0.69517421\n",
      "Iteration 4, loss = 0.69513110\n",
      "Iteration 5, loss = 0.69507415\n",
      "Iteration 6, loss = 0.69504898\n",
      "Iteration 7, loss = 0.69500111\n",
      "Iteration 8, loss = 0.69499019\n",
      "Iteration 9, loss = 0.69492394\n",
      "Iteration 10, loss = 0.69493534\n",
      "Iteration 11, loss = 0.69490122\n",
      "Iteration 12, loss = 0.69483177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69516983\n",
      "Iteration 2, loss = 0.69513649\n",
      "Iteration 3, loss = 0.69511272\n",
      "Iteration 4, loss = 0.69507220\n",
      "Iteration 5, loss = 0.69503665\n",
      "Iteration 6, loss = 0.69500178\n",
      "Iteration 7, loss = 0.69496873\n",
      "Iteration 8, loss = 0.69495596\n",
      "Iteration 9, loss = 0.69492122\n",
      "Iteration 10, loss = 0.69485695\n",
      "Iteration 11, loss = 0.69484123\n",
      "Iteration 12, loss = 0.69479738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70577840\n",
      "Iteration 2, loss = 0.70310377\n",
      "Iteration 3, loss = 0.69895360\n",
      "Iteration 4, loss = 0.69389313\n",
      "Iteration 5, loss = 0.68817989\n",
      "Iteration 6, loss = 0.68196911\n",
      "Iteration 7, loss = 0.67564872\n",
      "Iteration 8, loss = 0.66911347\n",
      "Iteration 9, loss = 0.66257107\n",
      "Iteration 10, loss = 0.65583547\n",
      "Iteration 11, loss = 0.64929742\n",
      "Iteration 12, loss = 0.64274939\n",
      "Iteration 13, loss = 0.63655779\n",
      "Iteration 14, loss = 0.63027517\n",
      "Iteration 15, loss = 0.62419910\n",
      "Iteration 16, loss = 0.61805513\n",
      "Iteration 17, loss = 0.61224200\n",
      "Iteration 18, loss = 0.60666398\n",
      "Iteration 19, loss = 0.60106239\n",
      "Iteration 20, loss = 0.59558785\n",
      "Iteration 21, loss = 0.59018455\n",
      "Iteration 22, loss = 0.58507822\n",
      "Iteration 23, loss = 0.58003452\n",
      "Iteration 24, loss = 0.57506588\n",
      "Iteration 25, loss = 0.57029417\n",
      "Iteration 26, loss = 0.56541138\n",
      "Iteration 27, loss = 0.56069330\n",
      "Iteration 28, loss = 0.55620859\n",
      "Iteration 29, loss = 0.55187557\n",
      "Iteration 30, loss = 0.54759410\n",
      "Iteration 31, loss = 0.54325556\n",
      "Iteration 32, loss = 0.53928113\n",
      "Iteration 33, loss = 0.53516386\n",
      "Iteration 34, loss = 0.53116901\n",
      "Iteration 35, loss = 0.52737348\n",
      "Iteration 36, loss = 0.52368005\n",
      "Iteration 37, loss = 0.51999593\n",
      "Iteration 38, loss = 0.51642748\n",
      "Iteration 39, loss = 0.51287168\n",
      "Iteration 40, loss = 0.50951824\n",
      "Iteration 41, loss = 0.50617469\n",
      "Iteration 42, loss = 0.50302740\n",
      "Iteration 43, loss = 0.49978799\n",
      "Iteration 44, loss = 0.49674733\n",
      "Iteration 45, loss = 0.49383745\n",
      "Iteration 46, loss = 0.49074587\n",
      "Iteration 47, loss = 0.48793936\n",
      "Iteration 48, loss = 0.48508407\n",
      "Iteration 49, loss = 0.48232946\n",
      "Iteration 50, loss = 0.47971288\n",
      "Iteration 51, loss = 0.47710696\n",
      "Iteration 52, loss = 0.47457202\n",
      "Iteration 53, loss = 0.47209790\n",
      "Iteration 54, loss = 0.46960630\n",
      "Iteration 55, loss = 0.46728502\n",
      "Iteration 56, loss = 0.46499097\n",
      "Iteration 57, loss = 0.46268597\n",
      "Iteration 58, loss = 0.46049899\n",
      "Iteration 59, loss = 0.45833732\n",
      "Iteration 60, loss = 0.45626127\n",
      "Iteration 61, loss = 0.45420434\n",
      "Iteration 62, loss = 0.45220019\n",
      "Iteration 63, loss = 0.45024862\n",
      "Iteration 64, loss = 0.44830346\n",
      "Iteration 65, loss = 0.44639890\n",
      "Iteration 66, loss = 0.44465273\n",
      "Iteration 67, loss = 0.44284362\n",
      "Iteration 68, loss = 0.44112517\n",
      "Iteration 69, loss = 0.43946252\n",
      "Iteration 70, loss = 0.43776789\n",
      "Iteration 71, loss = 0.43617817\n",
      "Iteration 72, loss = 0.43462117\n",
      "Iteration 73, loss = 0.43307111\n",
      "Iteration 74, loss = 0.43157314\n",
      "Iteration 75, loss = 0.43007316\n",
      "Iteration 76, loss = 0.42866158\n",
      "Iteration 77, loss = 0.42729697\n",
      "Iteration 78, loss = 0.42586875\n",
      "Iteration 79, loss = 0.42454260\n",
      "Iteration 80, loss = 0.42321878\n",
      "Iteration 81, loss = 0.42197240\n",
      "Iteration 82, loss = 0.42074266\n",
      "Iteration 83, loss = 0.41950649\n",
      "Iteration 84, loss = 0.41830524\n",
      "Iteration 85, loss = 0.41714003\n",
      "Iteration 86, loss = 0.41601478\n",
      "Iteration 87, loss = 0.41490631\n",
      "Iteration 88, loss = 0.41384282\n",
      "Iteration 89, loss = 0.41275436\n",
      "Iteration 90, loss = 0.41169287\n",
      "Iteration 91, loss = 0.41067228\n",
      "Iteration 92, loss = 0.40970338\n",
      "Iteration 93, loss = 0.40866229\n",
      "Iteration 94, loss = 0.40772754\n",
      "Iteration 95, loss = 0.40677565\n",
      "Iteration 96, loss = 0.40585615\n",
      "Iteration 97, loss = 0.40496721\n",
      "Iteration 98, loss = 0.40404384\n",
      "Iteration 99, loss = 0.40314145\n",
      "Iteration 100, loss = 0.40226389\n",
      "Iteration 101, loss = 0.40146110\n",
      "Iteration 102, loss = 0.40060030\n",
      "Iteration 103, loss = 0.39977687\n",
      "Iteration 104, loss = 0.39900122\n",
      "Iteration 105, loss = 0.39816960\n",
      "Iteration 106, loss = 0.39746288\n",
      "Iteration 107, loss = 0.39662298\n",
      "Iteration 108, loss = 0.39589575\n",
      "Iteration 109, loss = 0.39516690\n",
      "Iteration 110, loss = 0.39440626\n",
      "Iteration 111, loss = 0.39372037\n",
      "Iteration 112, loss = 0.39303741\n",
      "Iteration 113, loss = 0.39234545\n",
      "Iteration 114, loss = 0.39164558\n",
      "Iteration 115, loss = 0.39096146\n",
      "Iteration 116, loss = 0.39029952\n",
      "Iteration 117, loss = 0.38962063\n",
      "Iteration 118, loss = 0.38901313\n",
      "Iteration 119, loss = 0.38833953\n",
      "Iteration 120, loss = 0.38769158\n",
      "Iteration 121, loss = 0.38713312\n",
      "Iteration 122, loss = 0.38649548\n",
      "Iteration 123, loss = 0.38592919\n",
      "Iteration 124, loss = 0.38533129\n",
      "Iteration 125, loss = 0.38473167\n",
      "Iteration 126, loss = 0.38417296\n",
      "Iteration 127, loss = 0.38363013\n",
      "Iteration 128, loss = 0.38304529\n",
      "Iteration 129, loss = 0.38248503\n",
      "Iteration 130, loss = 0.38196322\n",
      "Iteration 131, loss = 0.38138912\n",
      "Iteration 132, loss = 0.38082713\n",
      "Iteration 133, loss = 0.38036073\n",
      "Iteration 134, loss = 0.37980533\n",
      "Iteration 135, loss = 0.37930639\n",
      "Iteration 136, loss = 0.37881043\n",
      "Iteration 137, loss = 0.37827878\n",
      "Iteration 138, loss = 0.37780049\n",
      "Iteration 139, loss = 0.37730080\n",
      "Iteration 140, loss = 0.37683047\n",
      "Iteration 141, loss = 0.37633970\n",
      "Iteration 142, loss = 0.37583281\n",
      "Iteration 143, loss = 0.37536371\n",
      "Iteration 144, loss = 0.37490751\n",
      "Iteration 145, loss = 0.37445342\n",
      "Iteration 146, loss = 0.37398510\n",
      "Iteration 147, loss = 0.37353398\n",
      "Iteration 148, loss = 0.37309569\n",
      "Iteration 149, loss = 0.37266437\n",
      "Iteration 150, loss = 0.37220629\n",
      "Iteration 151, loss = 0.37179883\n",
      "Iteration 152, loss = 0.37135125\n",
      "Iteration 153, loss = 0.37094415\n",
      "Iteration 154, loss = 0.37052891\n",
      "Iteration 155, loss = 0.37008958\n",
      "Iteration 156, loss = 0.36966733\n",
      "Iteration 157, loss = 0.36926781\n",
      "Iteration 158, loss = 0.36887246\n",
      "Iteration 159, loss = 0.36846938\n",
      "Iteration 160, loss = 0.36810970\n",
      "Iteration 161, loss = 0.36769384\n",
      "Iteration 162, loss = 0.36731465\n",
      "Iteration 163, loss = 0.36692891\n",
      "Iteration 164, loss = 0.36654753\n",
      "Iteration 165, loss = 0.36618283\n",
      "Iteration 166, loss = 0.36578410\n",
      "Iteration 167, loss = 0.36540757\n",
      "Iteration 168, loss = 0.36506074\n",
      "Iteration 169, loss = 0.36468233\n",
      "Iteration 170, loss = 0.36431224\n",
      "Iteration 171, loss = 0.36397182\n",
      "Iteration 172, loss = 0.36362040\n",
      "Iteration 173, loss = 0.36327001\n",
      "Iteration 174, loss = 0.36289713\n",
      "Iteration 175, loss = 0.36258672\n",
      "Iteration 176, loss = 0.36223276\n",
      "Iteration 177, loss = 0.36188976\n",
      "Iteration 178, loss = 0.36156600\n",
      "Iteration 179, loss = 0.36122922\n",
      "Iteration 180, loss = 0.36091189\n",
      "Iteration 181, loss = 0.36059301\n",
      "Iteration 182, loss = 0.36024951\n",
      "Iteration 183, loss = 0.35994814\n",
      "Iteration 184, loss = 0.35963026\n",
      "Iteration 185, loss = 0.35936066\n",
      "Iteration 186, loss = 0.35900994\n",
      "Iteration 187, loss = 0.35869817\n",
      "Iteration 188, loss = 0.35840232\n",
      "Iteration 189, loss = 0.35811349\n",
      "Iteration 190, loss = 0.35782015\n",
      "Iteration 191, loss = 0.35748600\n",
      "Iteration 192, loss = 0.35723098\n",
      "Iteration 193, loss = 0.35692784\n",
      "Iteration 194, loss = 0.35661983\n",
      "Iteration 195, loss = 0.35634891\n",
      "Iteration 196, loss = 0.35603535\n",
      "Iteration 197, loss = 0.35577845\n",
      "Iteration 198, loss = 0.35547660\n",
      "Iteration 199, loss = 0.35519771\n",
      "Iteration 200, loss = 0.35492782\n",
      "Iteration 201, loss = 0.35461960\n",
      "Iteration 202, loss = 0.35435403\n",
      "Iteration 203, loss = 0.35410274\n",
      "Iteration 204, loss = 0.35382016\n",
      "Iteration 205, loss = 0.35355740\n",
      "Iteration 206, loss = 0.35325778\n",
      "Iteration 207, loss = 0.35301758\n",
      "Iteration 208, loss = 0.35274035\n",
      "Iteration 209, loss = 0.35248606\n",
      "Iteration 210, loss = 0.35224255\n",
      "Iteration 211, loss = 0.35196576\n",
      "Iteration 212, loss = 0.35171358\n",
      "Iteration 213, loss = 0.35145771\n",
      "Iteration 214, loss = 0.35120564\n",
      "Iteration 215, loss = 0.35095712\n",
      "Iteration 216, loss = 0.35071570\n",
      "Iteration 217, loss = 0.35048490\n",
      "Iteration 218, loss = 0.35021696\n",
      "Iteration 219, loss = 0.34998329\n",
      "Iteration 220, loss = 0.34972571\n",
      "Iteration 221, loss = 0.34947950\n",
      "Iteration 222, loss = 0.34925986\n",
      "Iteration 223, loss = 0.34904885\n",
      "Iteration 224, loss = 0.34878354\n",
      "Iteration 225, loss = 0.34854952\n",
      "Iteration 226, loss = 0.34833841\n",
      "Iteration 227, loss = 0.34808559\n",
      "Iteration 228, loss = 0.34785490\n",
      "Iteration 229, loss = 0.34763701\n",
      "Iteration 230, loss = 0.34740110\n",
      "Iteration 231, loss = 0.34718471\n",
      "Iteration 232, loss = 0.34696355\n",
      "Iteration 233, loss = 0.34672955\n",
      "Iteration 234, loss = 0.34650589\n",
      "Iteration 235, loss = 0.34626641\n",
      "Iteration 236, loss = 0.34607819\n",
      "Iteration 237, loss = 0.34583491\n",
      "Iteration 238, loss = 0.34561644\n",
      "Iteration 239, loss = 0.34539606\n",
      "Iteration 240, loss = 0.34519506\n",
      "Iteration 241, loss = 0.34498291\n",
      "Iteration 242, loss = 0.34476584\n",
      "Iteration 243, loss = 0.34457695\n",
      "Iteration 244, loss = 0.34434632\n",
      "Iteration 245, loss = 0.34411623\n",
      "Iteration 246, loss = 0.34390379\n",
      "Iteration 247, loss = 0.34371626\n",
      "Iteration 248, loss = 0.34352604\n",
      "Iteration 249, loss = 0.34330075\n",
      "Iteration 250, loss = 0.34308442\n",
      "Iteration 251, loss = 0.34289248\n",
      "Iteration 252, loss = 0.34268733\n",
      "Iteration 253, loss = 0.34248805\n",
      "Iteration 254, loss = 0.34228902\n",
      "Iteration 255, loss = 0.34207879\n",
      "Iteration 256, loss = 0.34189143\n",
      "Iteration 257, loss = 0.34168791\n",
      "Iteration 258, loss = 0.34150121\n",
      "Iteration 259, loss = 0.34130136\n",
      "Iteration 260, loss = 0.34109552\n",
      "Iteration 261, loss = 0.34092952\n",
      "Iteration 262, loss = 0.34071733\n",
      "Iteration 263, loss = 0.34053850\n",
      "Iteration 264, loss = 0.34033571\n",
      "Iteration 265, loss = 0.34015294\n",
      "Iteration 266, loss = 0.33998698\n",
      "Iteration 267, loss = 0.33978746\n",
      "Iteration 268, loss = 0.33959947\n",
      "Iteration 269, loss = 0.33942301\n",
      "Iteration 270, loss = 0.33922817\n",
      "Iteration 271, loss = 0.33904094\n",
      "Iteration 272, loss = 0.33885908\n",
      "Iteration 273, loss = 0.33867837\n",
      "Iteration 274, loss = 0.33849829\n",
      "Iteration 275, loss = 0.33831683\n",
      "Iteration 276, loss = 0.33816015\n",
      "Iteration 277, loss = 0.33796174\n",
      "Iteration 278, loss = 0.33781248\n",
      "Iteration 279, loss = 0.33761566\n",
      "Iteration 280, loss = 0.33745777\n",
      "Iteration 281, loss = 0.33729138\n",
      "Iteration 282, loss = 0.33711512\n",
      "Iteration 283, loss = 0.33693271\n",
      "Iteration 284, loss = 0.33676830\n",
      "Iteration 285, loss = 0.33661819\n",
      "Iteration 286, loss = 0.33643517\n",
      "Iteration 287, loss = 0.33627115\n",
      "Iteration 288, loss = 0.33609668\n",
      "Iteration 289, loss = 0.33595660\n",
      "Iteration 290, loss = 0.33578736\n",
      "Iteration 291, loss = 0.33561435\n",
      "Iteration 292, loss = 0.33548776\n",
      "Iteration 293, loss = 0.33531724\n",
      "Iteration 294, loss = 0.33518503\n",
      "Iteration 295, loss = 0.33496891\n",
      "Iteration 296, loss = 0.33481140\n",
      "Iteration 297, loss = 0.33465715\n",
      "Iteration 298, loss = 0.33451058\n",
      "Iteration 299, loss = 0.33434365\n",
      "Iteration 300, loss = 0.33417990\n",
      "Iteration 301, loss = 0.33402615\n",
      "Iteration 302, loss = 0.33385907\n",
      "Iteration 303, loss = 0.33370096\n",
      "Iteration 304, loss = 0.33355473\n",
      "Iteration 305, loss = 0.33340443\n",
      "Iteration 306, loss = 0.33324761\n",
      "Iteration 307, loss = 0.33309415\n",
      "Iteration 308, loss = 0.33294050\n",
      "Iteration 309, loss = 0.33279480\n",
      "Iteration 310, loss = 0.33264553\n",
      "Iteration 311, loss = 0.33247808\n",
      "Iteration 312, loss = 0.33234546\n",
      "Iteration 313, loss = 0.33220920\n",
      "Iteration 314, loss = 0.33204007\n",
      "Iteration 315, loss = 0.33190039\n",
      "Iteration 316, loss = 0.33175589\n",
      "Iteration 317, loss = 0.33162381\n",
      "Iteration 318, loss = 0.33146383\n",
      "Iteration 319, loss = 0.33134109\n",
      "Iteration 320, loss = 0.33119185\n",
      "Iteration 321, loss = 0.33106598\n",
      "Iteration 322, loss = 0.33089466\n",
      "Iteration 323, loss = 0.33075674\n",
      "Iteration 324, loss = 0.33062979\n",
      "Iteration 325, loss = 0.33047346\n",
      "Iteration 326, loss = 0.33033609\n",
      "Iteration 327, loss = 0.33020026\n",
      "Iteration 328, loss = 0.33005366\n",
      "Iteration 329, loss = 0.32993014\n",
      "Iteration 330, loss = 0.32980885\n",
      "Iteration 331, loss = 0.32964290\n",
      "Iteration 332, loss = 0.32951219\n",
      "Iteration 333, loss = 0.32937660\n",
      "Iteration 334, loss = 0.32923707\n",
      "Iteration 335, loss = 0.32910694\n",
      "Iteration 336, loss = 0.32897387\n",
      "Iteration 337, loss = 0.32883325\n",
      "Iteration 338, loss = 0.32871109\n",
      "Iteration 339, loss = 0.32856439\n",
      "Iteration 340, loss = 0.32843400\n",
      "Iteration 341, loss = 0.32831245\n",
      "Iteration 342, loss = 0.32817712\n",
      "Iteration 343, loss = 0.32803200\n",
      "Iteration 344, loss = 0.32789981\n",
      "Iteration 345, loss = 0.32777241\n",
      "Iteration 346, loss = 0.32765521\n",
      "Iteration 347, loss = 0.32751225\n",
      "Iteration 348, loss = 0.32738668\n",
      "Iteration 349, loss = 0.32726682\n",
      "Iteration 350, loss = 0.32712675\n",
      "Iteration 351, loss = 0.32701065\n",
      "Iteration 352, loss = 0.32686723\n",
      "Iteration 353, loss = 0.32674703\n",
      "Iteration 354, loss = 0.32663323\n",
      "Iteration 355, loss = 0.32648442\n",
      "Iteration 356, loss = 0.32635868\n",
      "Iteration 357, loss = 0.32624013\n",
      "Iteration 358, loss = 0.32611950\n",
      "Iteration 359, loss = 0.32599086\n",
      "Iteration 360, loss = 0.32587876\n",
      "Iteration 361, loss = 0.32573502\n",
      "Iteration 362, loss = 0.32560698\n",
      "Iteration 363, loss = 0.32551959\n",
      "Iteration 364, loss = 0.32536249\n",
      "Iteration 365, loss = 0.32526241\n",
      "Iteration 366, loss = 0.32512463\n",
      "Iteration 367, loss = 0.32500793\n",
      "Iteration 368, loss = 0.32487202\n",
      "Iteration 369, loss = 0.32474432\n",
      "Iteration 370, loss = 0.32462471\n",
      "Iteration 371, loss = 0.32451278\n",
      "Iteration 372, loss = 0.32437858\n",
      "Iteration 373, loss = 0.32425938\n",
      "Iteration 374, loss = 0.32414454\n",
      "Iteration 375, loss = 0.32402583\n",
      "Iteration 376, loss = 0.32388989\n",
      "Iteration 377, loss = 0.32378076\n",
      "Iteration 378, loss = 0.32365389\n",
      "Iteration 379, loss = 0.32353236\n",
      "Iteration 380, loss = 0.32341738\n",
      "Iteration 381, loss = 0.32327987\n",
      "Iteration 382, loss = 0.32316373\n",
      "Iteration 383, loss = 0.32304855\n",
      "Iteration 384, loss = 0.32293463\n",
      "Iteration 385, loss = 0.32280775\n",
      "Iteration 386, loss = 0.32269956\n",
      "Iteration 387, loss = 0.32257382\n",
      "Iteration 388, loss = 0.32245703\n",
      "Iteration 389, loss = 0.32235997\n",
      "Iteration 390, loss = 0.32221820\n",
      "Iteration 391, loss = 0.32210937\n",
      "Iteration 392, loss = 0.32198411\n",
      "Iteration 393, loss = 0.32186765\n",
      "Iteration 394, loss = 0.32177340\n",
      "Iteration 395, loss = 0.32163663\n",
      "Iteration 396, loss = 0.32156823\n",
      "Iteration 397, loss = 0.32140583\n",
      "Iteration 398, loss = 0.32130052\n",
      "Iteration 399, loss = 0.32118527\n",
      "Iteration 400, loss = 0.32105846\n",
      "Iteration 401, loss = 0.32095369\n",
      "Iteration 402, loss = 0.32084739\n",
      "Iteration 403, loss = 0.32074267\n",
      "Iteration 404, loss = 0.32060829\n",
      "Iteration 405, loss = 0.32048733\n",
      "Iteration 406, loss = 0.32039475\n",
      "Iteration 407, loss = 0.32029718\n",
      "Iteration 408, loss = 0.32015743\n",
      "Iteration 409, loss = 0.32005106\n",
      "Iteration 410, loss = 0.31993970\n",
      "Iteration 411, loss = 0.31983106\n",
      "Iteration 412, loss = 0.31975527\n",
      "Iteration 413, loss = 0.31960372\n",
      "Iteration 414, loss = 0.31950416\n",
      "Iteration 415, loss = 0.31938576\n",
      "Iteration 416, loss = 0.31928903\n",
      "Iteration 417, loss = 0.31916388\n",
      "Iteration 418, loss = 0.31906201\n",
      "Iteration 419, loss = 0.31895474\n",
      "Iteration 420, loss = 0.31883549\n",
      "Iteration 421, loss = 0.31873963\n",
      "Iteration 422, loss = 0.31862574\n",
      "Iteration 423, loss = 0.31851872\n",
      "Iteration 424, loss = 0.31842678\n",
      "Iteration 425, loss = 0.31830904\n",
      "Iteration 426, loss = 0.31822747\n",
      "Iteration 427, loss = 0.31810522\n",
      "Iteration 428, loss = 0.31798664\n",
      "Iteration 429, loss = 0.31788411\n",
      "Iteration 430, loss = 0.31779021\n",
      "Iteration 431, loss = 0.31768910\n",
      "Iteration 432, loss = 0.31757529\n",
      "Iteration 433, loss = 0.31745709\n",
      "Iteration 434, loss = 0.31736439\n",
      "Iteration 435, loss = 0.31725917\n",
      "Iteration 436, loss = 0.31716096\n",
      "Iteration 437, loss = 0.31705034\n",
      "Iteration 438, loss = 0.31694782\n",
      "Iteration 439, loss = 0.31686094\n",
      "Iteration 440, loss = 0.31674992\n",
      "Iteration 441, loss = 0.31666055\n",
      "Iteration 442, loss = 0.31654080\n",
      "Iteration 443, loss = 0.31643002\n",
      "Iteration 444, loss = 0.31636391\n",
      "Iteration 445, loss = 0.31624251\n",
      "Iteration 446, loss = 0.31614358\n",
      "Iteration 447, loss = 0.31602616\n",
      "Iteration 448, loss = 0.31593692\n",
      "Iteration 449, loss = 0.31582042\n",
      "Iteration 450, loss = 0.31572038\n",
      "Iteration 451, loss = 0.31562922\n",
      "Iteration 452, loss = 0.31552692\n",
      "Iteration 453, loss = 0.31543087\n",
      "Iteration 454, loss = 0.31531759\n",
      "Iteration 455, loss = 0.31522625\n",
      "Iteration 456, loss = 0.31513048\n",
      "Iteration 457, loss = 0.31502183\n",
      "Iteration 458, loss = 0.31492309\n",
      "Iteration 459, loss = 0.31482530\n",
      "Iteration 460, loss = 0.31472272\n",
      "Iteration 461, loss = 0.31463093\n",
      "Iteration 462, loss = 0.31455036\n",
      "Iteration 463, loss = 0.31442690\n",
      "Iteration 464, loss = 0.31433577\n",
      "Iteration 465, loss = 0.31422578\n",
      "Iteration 466, loss = 0.31412847\n",
      "Iteration 467, loss = 0.31403718\n",
      "Iteration 468, loss = 0.31393863\n",
      "Iteration 469, loss = 0.31383346\n",
      "Iteration 470, loss = 0.31373464\n",
      "Iteration 471, loss = 0.31364246\n",
      "Iteration 472, loss = 0.31354107\n",
      "Iteration 473, loss = 0.31344744\n",
      "Iteration 474, loss = 0.31335534\n",
      "Iteration 475, loss = 0.31325990\n",
      "Iteration 476, loss = 0.31314127\n",
      "Iteration 477, loss = 0.31305478\n",
      "Iteration 478, loss = 0.31294911\n",
      "Iteration 479, loss = 0.31285842\n",
      "Iteration 480, loss = 0.31275731\n",
      "Iteration 481, loss = 0.31266086\n",
      "Iteration 482, loss = 0.31255751\n",
      "Iteration 483, loss = 0.31246667\n",
      "Iteration 484, loss = 0.31235948\n",
      "Iteration 485, loss = 0.31227465\n",
      "Iteration 486, loss = 0.31217978\n",
      "Iteration 487, loss = 0.31210235\n",
      "Iteration 488, loss = 0.31199528\n",
      "Iteration 489, loss = 0.31189623\n",
      "Iteration 490, loss = 0.31179876\n",
      "Iteration 491, loss = 0.31171846\n",
      "Iteration 492, loss = 0.31162057\n",
      "Iteration 493, loss = 0.31152927\n",
      "Iteration 494, loss = 0.31141551\n",
      "Iteration 495, loss = 0.31134062\n",
      "Iteration 496, loss = 0.31123823\n",
      "Iteration 497, loss = 0.31116255\n",
      "Iteration 498, loss = 0.31105693\n",
      "Iteration 499, loss = 0.31097234\n",
      "Iteration 500, loss = 0.31087838\n",
      "Iteration 501, loss = 0.31080676\n",
      "Iteration 502, loss = 0.31068876\n",
      "Iteration 503, loss = 0.31058895\n",
      "Iteration 504, loss = 0.31050532\n",
      "Iteration 505, loss = 0.31041967\n",
      "Iteration 506, loss = 0.31036014\n",
      "Iteration 507, loss = 0.31023149\n",
      "Iteration 508, loss = 0.31013980\n",
      "Iteration 509, loss = 0.31005263\n",
      "Iteration 510, loss = 0.30995855\n",
      "Iteration 511, loss = 0.30987677\n",
      "Iteration 512, loss = 0.30979977\n",
      "Iteration 513, loss = 0.30971250\n",
      "Iteration 514, loss = 0.30959835\n",
      "Iteration 515, loss = 0.30951760\n",
      "Iteration 516, loss = 0.30941778\n",
      "Iteration 517, loss = 0.30933541\n",
      "Iteration 518, loss = 0.30925036\n",
      "Iteration 519, loss = 0.30915600\n",
      "Iteration 520, loss = 0.30905107\n",
      "Iteration 521, loss = 0.30899549\n",
      "Iteration 522, loss = 0.30887483\n",
      "Iteration 523, loss = 0.30878991\n",
      "Iteration 524, loss = 0.30870739\n",
      "Iteration 525, loss = 0.30861557\n",
      "Iteration 526, loss = 0.30853730\n",
      "Iteration 527, loss = 0.30844336\n",
      "Iteration 528, loss = 0.30835822\n",
      "Iteration 529, loss = 0.30826841\n",
      "Iteration 530, loss = 0.30818387\n",
      "Iteration 531, loss = 0.30809989\n",
      "Iteration 532, loss = 0.30800155\n",
      "Iteration 533, loss = 0.30791696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69529966\n",
      "Iteration 2, loss = 0.69530711\n",
      "Iteration 3, loss = 0.69527290\n",
      "Iteration 4, loss = 0.69522880\n",
      "Iteration 5, loss = 0.69517983\n",
      "Iteration 6, loss = 0.69514887\n",
      "Iteration 7, loss = 0.69512058\n",
      "Iteration 8, loss = 0.69508595\n",
      "Iteration 9, loss = 0.69506985\n",
      "Iteration 10, loss = 0.69501153\n",
      "Iteration 11, loss = 0.69501116\n",
      "Iteration 12, loss = 0.69495727\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71965283\n",
      "Iteration 2, loss = 0.71003753\n",
      "Iteration 3, loss = 0.69555860\n",
      "Iteration 4, loss = 0.67885192\n",
      "Iteration 5, loss = 0.66071458\n",
      "Iteration 6, loss = 0.64221928\n",
      "Iteration 7, loss = 0.62448619\n",
      "Iteration 8, loss = 0.60739912\n",
      "Iteration 9, loss = 0.59157796\n",
      "Iteration 10, loss = 0.57671037\n",
      "Iteration 11, loss = 0.56300516\n",
      "Iteration 12, loss = 0.55061798\n",
      "Iteration 13, loss = 0.53940589\n",
      "Iteration 14, loss = 0.52902584\n",
      "Iteration 15, loss = 0.51965174\n",
      "Iteration 16, loss = 0.51076089\n",
      "Iteration 17, loss = 0.50304054\n",
      "Iteration 18, loss = 0.49590993\n",
      "Iteration 19, loss = 0.48921935\n",
      "Iteration 20, loss = 0.48310851\n",
      "Iteration 21, loss = 0.47750849\n",
      "Iteration 22, loss = 0.47250587\n",
      "Iteration 23, loss = 0.46779918\n",
      "Iteration 24, loss = 0.46327683\n",
      "Iteration 25, loss = 0.45930240\n",
      "Iteration 26, loss = 0.45535765\n",
      "Iteration 27, loss = 0.45162254\n",
      "Iteration 28, loss = 0.44843541\n",
      "Iteration 29, loss = 0.44525000\n",
      "Iteration 30, loss = 0.44233074\n",
      "Iteration 31, loss = 0.43944730\n",
      "Iteration 32, loss = 0.43702875\n",
      "Iteration 33, loss = 0.43447754\n",
      "Iteration 34, loss = 0.43210425\n",
      "Iteration 35, loss = 0.42983018\n",
      "Iteration 36, loss = 0.42778887\n",
      "Iteration 37, loss = 0.42575781\n",
      "Iteration 38, loss = 0.42386801\n",
      "Iteration 39, loss = 0.42199733\n",
      "Iteration 40, loss = 0.42029702\n",
      "Iteration 41, loss = 0.41861150\n",
      "Iteration 42, loss = 0.41705327\n",
      "Iteration 43, loss = 0.41554777\n",
      "Iteration 44, loss = 0.41411163\n",
      "Iteration 45, loss = 0.41282764\n",
      "Iteration 46, loss = 0.41131123\n",
      "Iteration 47, loss = 0.41015177\n",
      "Iteration 48, loss = 0.40878787\n",
      "Iteration 49, loss = 0.40764162\n",
      "Iteration 50, loss = 0.40650136\n",
      "Iteration 51, loss = 0.40538149\n",
      "Iteration 52, loss = 0.40433046\n",
      "Iteration 53, loss = 0.40331801\n",
      "Iteration 54, loss = 0.40228906\n",
      "Iteration 55, loss = 0.40137580\n",
      "Iteration 56, loss = 0.40043719\n",
      "Iteration 57, loss = 0.39951327\n",
      "Iteration 58, loss = 0.39862805\n",
      "Iteration 59, loss = 0.39773905\n",
      "Iteration 60, loss = 0.39698306\n",
      "Iteration 61, loss = 0.39618756\n",
      "Iteration 62, loss = 0.39542610\n",
      "Iteration 63, loss = 0.39461492\n",
      "Iteration 64, loss = 0.39391984\n",
      "Iteration 65, loss = 0.39315581\n",
      "Iteration 66, loss = 0.39253307\n",
      "Iteration 67, loss = 0.39181894\n",
      "Iteration 68, loss = 0.39118038\n",
      "Iteration 69, loss = 0.39056280\n",
      "Iteration 70, loss = 0.38987759\n",
      "Iteration 71, loss = 0.38935127\n",
      "Iteration 72, loss = 0.38875794\n",
      "Iteration 73, loss = 0.38812618\n",
      "Iteration 74, loss = 0.38759979\n",
      "Iteration 75, loss = 0.38701621\n",
      "Iteration 76, loss = 0.38652862\n",
      "Iteration 77, loss = 0.38601390\n",
      "Iteration 78, loss = 0.38543615\n",
      "Iteration 79, loss = 0.38496384\n",
      "Iteration 80, loss = 0.38450235\n",
      "Iteration 81, loss = 0.38400867\n",
      "Iteration 82, loss = 0.38353709\n",
      "Iteration 83, loss = 0.38307539\n",
      "Iteration 84, loss = 0.38267734\n",
      "Iteration 85, loss = 0.38225306\n",
      "Iteration 86, loss = 0.38178747\n",
      "Iteration 87, loss = 0.38136656\n",
      "Iteration 88, loss = 0.38100023\n",
      "Iteration 89, loss = 0.38057286\n",
      "Iteration 90, loss = 0.38014574\n",
      "Iteration 91, loss = 0.37980517\n",
      "Iteration 92, loss = 0.37944929\n",
      "Iteration 93, loss = 0.37901034\n",
      "Iteration 94, loss = 0.37869440\n",
      "Iteration 95, loss = 0.37831859\n",
      "Iteration 96, loss = 0.37800474\n",
      "Iteration 97, loss = 0.37766246\n",
      "Iteration 98, loss = 0.37731643\n",
      "Iteration 99, loss = 0.37693970\n",
      "Iteration 100, loss = 0.37663914\n",
      "Iteration 101, loss = 0.37632060\n",
      "Iteration 102, loss = 0.37602165\n",
      "Iteration 103, loss = 0.37572537\n",
      "Iteration 104, loss = 0.37540780\n",
      "Iteration 105, loss = 0.37509994\n",
      "Iteration 106, loss = 0.37485512\n",
      "Iteration 107, loss = 0.37453532\n",
      "Iteration 108, loss = 0.37427021\n",
      "Iteration 109, loss = 0.37398239\n",
      "Iteration 110, loss = 0.37368735\n",
      "Iteration 111, loss = 0.37346359\n",
      "Iteration 112, loss = 0.37318681\n",
      "Iteration 113, loss = 0.37294029\n",
      "Iteration 114, loss = 0.37266134\n",
      "Iteration 115, loss = 0.37241527\n",
      "Iteration 116, loss = 0.37216572\n",
      "Iteration 117, loss = 0.37191356\n",
      "Iteration 118, loss = 0.37171915\n",
      "Iteration 119, loss = 0.37144043\n",
      "Iteration 120, loss = 0.37120464\n",
      "Iteration 121, loss = 0.37101811\n",
      "Iteration 122, loss = 0.37076846\n",
      "Iteration 123, loss = 0.37056686\n",
      "Iteration 124, loss = 0.37033720\n",
      "Iteration 125, loss = 0.37010479\n",
      "Iteration 126, loss = 0.36990490\n",
      "Iteration 127, loss = 0.36973046\n",
      "Iteration 128, loss = 0.36949242\n",
      "Iteration 129, loss = 0.36930485\n",
      "Iteration 130, loss = 0.36910295\n",
      "Iteration 131, loss = 0.36887877\n",
      "Iteration 132, loss = 0.36868770\n",
      "Iteration 133, loss = 0.36852733\n",
      "Iteration 134, loss = 0.36834029\n",
      "Iteration 135, loss = 0.36814748\n",
      "Iteration 136, loss = 0.36796695\n",
      "Iteration 137, loss = 0.36778072\n",
      "Iteration 138, loss = 0.36761028\n",
      "Iteration 139, loss = 0.36742574\n",
      "Iteration 140, loss = 0.36727256\n",
      "Iteration 141, loss = 0.36710458\n",
      "Iteration 142, loss = 0.36690575\n",
      "Iteration 143, loss = 0.36674177\n",
      "Iteration 144, loss = 0.36660260\n",
      "Iteration 145, loss = 0.36644868\n",
      "Iteration 146, loss = 0.36626986\n",
      "Iteration 147, loss = 0.36611804\n",
      "Iteration 148, loss = 0.36596349\n",
      "Iteration 149, loss = 0.36582276\n",
      "Iteration 150, loss = 0.36563969\n",
      "Iteration 151, loss = 0.36552534\n",
      "Iteration 152, loss = 0.36536548\n",
      "Iteration 153, loss = 0.36522240\n",
      "Iteration 154, loss = 0.36510344\n",
      "Iteration 155, loss = 0.36491756\n",
      "Iteration 156, loss = 0.36477795\n",
      "Iteration 157, loss = 0.36464620\n",
      "Iteration 158, loss = 0.36451109\n",
      "Iteration 159, loss = 0.36436969\n",
      "Iteration 160, loss = 0.36426381\n",
      "Iteration 161, loss = 0.36411198\n",
      "Iteration 162, loss = 0.36398703\n",
      "Iteration 163, loss = 0.36386211\n",
      "Iteration 164, loss = 0.36373381\n",
      "Iteration 165, loss = 0.36360982\n",
      "Iteration 166, loss = 0.36347050\n",
      "Iteration 167, loss = 0.36334441\n",
      "Iteration 168, loss = 0.36324673\n",
      "Iteration 169, loss = 0.36310483\n",
      "Iteration 170, loss = 0.36298216\n",
      "Iteration 171, loss = 0.36286810\n",
      "Iteration 172, loss = 0.36275715\n",
      "Iteration 173, loss = 0.36265129\n",
      "Iteration 174, loss = 0.36252439\n",
      "Iteration 175, loss = 0.36243243\n",
      "Iteration 176, loss = 0.36229963\n",
      "Iteration 177, loss = 0.36219115\n",
      "Iteration 178, loss = 0.36209209\n",
      "Iteration 179, loss = 0.36196400\n",
      "Iteration 180, loss = 0.36187370\n",
      "Iteration 181, loss = 0.36177766\n",
      "Iteration 182, loss = 0.36164093\n",
      "Iteration 183, loss = 0.36154936\n",
      "Iteration 184, loss = 0.36144874\n",
      "Iteration 185, loss = 0.36137902\n",
      "Iteration 186, loss = 0.36124244\n",
      "Iteration 187, loss = 0.36114064\n",
      "Iteration 188, loss = 0.36104016\n",
      "Iteration 189, loss = 0.36097632\n",
      "Iteration 190, loss = 0.36086573\n",
      "Iteration 191, loss = 0.36075020\n",
      "Iteration 192, loss = 0.36069068\n",
      "Iteration 193, loss = 0.36057912\n",
      "Iteration 194, loss = 0.36047217\n",
      "Iteration 195, loss = 0.36040432\n",
      "Iteration 196, loss = 0.36029042\n",
      "Iteration 197, loss = 0.36021728\n",
      "Iteration 198, loss = 0.36011075\n",
      "Iteration 199, loss = 0.36002568\n",
      "Iteration 200, loss = 0.35995249\n",
      "Iteration 201, loss = 0.35983825\n",
      "Iteration 202, loss = 0.35975542\n",
      "Iteration 203, loss = 0.35970614\n",
      "Iteration 204, loss = 0.35960181\n",
      "Iteration 205, loss = 0.35952222\n",
      "Iteration 206, loss = 0.35940290\n",
      "Iteration 207, loss = 0.35933829\n",
      "Iteration 208, loss = 0.35924615\n",
      "Iteration 209, loss = 0.35917210\n",
      "Iteration 210, loss = 0.35911352\n",
      "Iteration 211, loss = 0.35901765\n",
      "Iteration 212, loss = 0.35893085\n",
      "Iteration 213, loss = 0.35884574\n",
      "Iteration 214, loss = 0.35878075\n",
      "Iteration 215, loss = 0.35869207\n",
      "Iteration 216, loss = 0.35862421\n",
      "Iteration 217, loss = 0.35856102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72818073\n",
      "Iteration 2, loss = 0.71787088\n",
      "Iteration 3, loss = 0.70264720\n",
      "Iteration 4, loss = 0.68509105\n",
      "Iteration 5, loss = 0.66530048\n",
      "Iteration 6, loss = 0.64585177\n",
      "Iteration 7, loss = 0.62664120\n",
      "Iteration 8, loss = 0.60869275\n",
      "Iteration 9, loss = 0.59174777\n",
      "Iteration 10, loss = 0.57559451\n",
      "Iteration 11, loss = 0.56130732\n",
      "Iteration 12, loss = 0.54797241\n",
      "Iteration 13, loss = 0.53581154\n",
      "Iteration 14, loss = 0.52478445\n",
      "Iteration 15, loss = 0.51468189\n",
      "Iteration 16, loss = 0.50516956\n",
      "Iteration 17, loss = 0.49667609\n",
      "Iteration 18, loss = 0.48911692\n",
      "Iteration 19, loss = 0.48179081\n",
      "Iteration 20, loss = 0.47527742\n",
      "Iteration 21, loss = 0.46918838\n",
      "Iteration 22, loss = 0.46357250\n",
      "Iteration 23, loss = 0.45856891\n",
      "Iteration 24, loss = 0.45364050\n",
      "Iteration 25, loss = 0.44922745\n",
      "Iteration 26, loss = 0.44507516\n",
      "Iteration 27, loss = 0.44098271\n",
      "Iteration 28, loss = 0.43737808\n",
      "Iteration 29, loss = 0.43395695\n",
      "Iteration 30, loss = 0.43068930\n",
      "Iteration 31, loss = 0.42774442\n",
      "Iteration 32, loss = 0.42491164\n",
      "Iteration 33, loss = 0.42215854\n",
      "Iteration 34, loss = 0.41960086\n",
      "Iteration 35, loss = 0.41722450\n",
      "Iteration 36, loss = 0.41495158\n",
      "Iteration 37, loss = 0.41267970\n",
      "Iteration 38, loss = 0.41068383\n",
      "Iteration 39, loss = 0.40868006\n",
      "Iteration 40, loss = 0.40683591\n",
      "Iteration 41, loss = 0.40502547\n",
      "Iteration 42, loss = 0.40334716\n",
      "Iteration 43, loss = 0.40167523\n",
      "Iteration 44, loss = 0.40016441\n",
      "Iteration 45, loss = 0.39871765\n",
      "Iteration 46, loss = 0.39718723\n",
      "Iteration 47, loss = 0.39588658\n",
      "Iteration 48, loss = 0.39453888\n",
      "Iteration 49, loss = 0.39328626\n",
      "Iteration 50, loss = 0.39203815\n",
      "Iteration 51, loss = 0.39086169\n",
      "Iteration 52, loss = 0.38975617\n",
      "Iteration 53, loss = 0.38866854\n",
      "Iteration 54, loss = 0.38760894\n",
      "Iteration 55, loss = 0.38663960\n",
      "Iteration 56, loss = 0.38564078\n",
      "Iteration 57, loss = 0.38467368\n",
      "Iteration 58, loss = 0.38378961\n",
      "Iteration 59, loss = 0.38290397\n",
      "Iteration 60, loss = 0.38208290\n",
      "Iteration 61, loss = 0.38126791\n",
      "Iteration 62, loss = 0.38042489\n",
      "Iteration 63, loss = 0.37963475\n",
      "Iteration 64, loss = 0.37886928\n",
      "Iteration 65, loss = 0.37813893\n",
      "Iteration 66, loss = 0.37745016\n",
      "Iteration 67, loss = 0.37675162\n",
      "Iteration 68, loss = 0.37613220\n",
      "Iteration 69, loss = 0.37547251\n",
      "Iteration 70, loss = 0.37478314\n",
      "Iteration 71, loss = 0.37426412\n",
      "Iteration 72, loss = 0.37358847\n",
      "Iteration 73, loss = 0.37298470\n",
      "Iteration 74, loss = 0.37249359\n",
      "Iteration 75, loss = 0.37189741\n",
      "Iteration 76, loss = 0.37137454\n",
      "Iteration 77, loss = 0.37084456\n",
      "Iteration 78, loss = 0.37032440\n",
      "Iteration 79, loss = 0.36979429\n",
      "Iteration 80, loss = 0.36934784\n",
      "Iteration 81, loss = 0.36883942\n",
      "Iteration 82, loss = 0.36836334\n",
      "Iteration 83, loss = 0.36791430\n",
      "Iteration 84, loss = 0.36749528\n",
      "Iteration 85, loss = 0.36710548\n",
      "Iteration 86, loss = 0.36663330\n",
      "Iteration 87, loss = 0.36622334\n",
      "Iteration 88, loss = 0.36584651\n",
      "Iteration 89, loss = 0.36543972\n",
      "Iteration 90, loss = 0.36501597\n",
      "Iteration 91, loss = 0.36465534\n",
      "Iteration 92, loss = 0.36428651\n",
      "Iteration 93, loss = 0.36390716\n",
      "Iteration 94, loss = 0.36356231\n",
      "Iteration 95, loss = 0.36320437\n",
      "Iteration 96, loss = 0.36289146\n",
      "Iteration 97, loss = 0.36252014\n",
      "Iteration 98, loss = 0.36220157\n",
      "Iteration 99, loss = 0.36187350\n",
      "Iteration 100, loss = 0.36156563\n",
      "Iteration 101, loss = 0.36123864\n",
      "Iteration 102, loss = 0.36094131\n",
      "Iteration 103, loss = 0.36067070\n",
      "Iteration 104, loss = 0.36035513\n",
      "Iteration 105, loss = 0.36007513\n",
      "Iteration 106, loss = 0.35979243\n",
      "Iteration 107, loss = 0.35950601\n",
      "Iteration 108, loss = 0.35921576\n",
      "Iteration 109, loss = 0.35894411\n",
      "Iteration 110, loss = 0.35870711\n",
      "Iteration 111, loss = 0.35849664\n",
      "Iteration 112, loss = 0.35818465\n",
      "Iteration 113, loss = 0.35796702\n",
      "Iteration 114, loss = 0.35770232\n",
      "Iteration 115, loss = 0.35743881\n",
      "Iteration 116, loss = 0.35721213\n",
      "Iteration 117, loss = 0.35697597\n",
      "Iteration 118, loss = 0.35676211\n",
      "Iteration 119, loss = 0.35652112\n",
      "Iteration 120, loss = 0.35628422\n",
      "Iteration 121, loss = 0.35607903\n",
      "Iteration 122, loss = 0.35586803\n",
      "Iteration 123, loss = 0.35565162\n",
      "Iteration 124, loss = 0.35545379\n",
      "Iteration 125, loss = 0.35522619\n",
      "Iteration 126, loss = 0.35504224\n",
      "Iteration 127, loss = 0.35489142\n",
      "Iteration 128, loss = 0.35463823\n",
      "Iteration 129, loss = 0.35447139\n",
      "Iteration 130, loss = 0.35426924\n",
      "Iteration 131, loss = 0.35405672\n",
      "Iteration 132, loss = 0.35390737\n",
      "Iteration 133, loss = 0.35375405\n",
      "Iteration 134, loss = 0.35354432\n",
      "Iteration 135, loss = 0.35335045\n",
      "Iteration 136, loss = 0.35317582\n",
      "Iteration 137, loss = 0.35301745\n",
      "Iteration 138, loss = 0.35285514\n",
      "Iteration 139, loss = 0.35268337\n",
      "Iteration 140, loss = 0.35251135\n",
      "Iteration 141, loss = 0.35235308\n",
      "Iteration 142, loss = 0.35219661\n",
      "Iteration 143, loss = 0.35203556\n",
      "Iteration 144, loss = 0.35191361\n",
      "Iteration 145, loss = 0.35176149\n",
      "Iteration 146, loss = 0.35158452\n",
      "Iteration 147, loss = 0.35144133\n",
      "Iteration 148, loss = 0.35128289\n",
      "Iteration 149, loss = 0.35115821\n",
      "Iteration 150, loss = 0.35102004\n",
      "Iteration 151, loss = 0.35088121\n",
      "Iteration 152, loss = 0.35074490\n",
      "Iteration 153, loss = 0.35059881\n",
      "Iteration 154, loss = 0.35046811\n",
      "Iteration 155, loss = 0.35033328\n",
      "Iteration 156, loss = 0.35019093\n",
      "Iteration 157, loss = 0.35006677\n",
      "Iteration 158, loss = 0.34993384\n",
      "Iteration 159, loss = 0.34980439\n",
      "Iteration 160, loss = 0.34969649\n",
      "Iteration 161, loss = 0.34957158\n",
      "Iteration 162, loss = 0.34945249\n",
      "Iteration 163, loss = 0.34934042\n",
      "Iteration 164, loss = 0.34922056\n",
      "Iteration 165, loss = 0.34912523\n",
      "Iteration 166, loss = 0.34897664\n",
      "Iteration 167, loss = 0.34885584\n",
      "Iteration 168, loss = 0.34878746\n",
      "Iteration 169, loss = 0.34863034\n",
      "Iteration 170, loss = 0.34851708\n",
      "Iteration 171, loss = 0.34845994\n",
      "Iteration 172, loss = 0.34831672\n",
      "Iteration 173, loss = 0.34822663\n",
      "Iteration 174, loss = 0.34811974\n",
      "Iteration 175, loss = 0.34801574\n",
      "Iteration 176, loss = 0.34791331\n",
      "Iteration 177, loss = 0.34784637\n",
      "Iteration 178, loss = 0.34770519\n",
      "Iteration 179, loss = 0.34760053\n",
      "Iteration 180, loss = 0.34751942\n",
      "Iteration 181, loss = 0.34741759\n",
      "Iteration 182, loss = 0.34731025\n",
      "Iteration 183, loss = 0.34721654\n",
      "Iteration 184, loss = 0.34711625\n",
      "Iteration 185, loss = 0.34707143\n",
      "Iteration 186, loss = 0.34695505\n",
      "Iteration 187, loss = 0.34684476\n",
      "Iteration 188, loss = 0.34675536\n",
      "Iteration 189, loss = 0.34669754\n",
      "Iteration 190, loss = 0.34660120\n",
      "Iteration 191, loss = 0.34650427\n",
      "Iteration 192, loss = 0.34643322\n",
      "Iteration 193, loss = 0.34632822\n",
      "Iteration 194, loss = 0.34625738\n",
      "Iteration 195, loss = 0.34618063\n",
      "Iteration 196, loss = 0.34614759\n",
      "Iteration 197, loss = 0.34602770\n",
      "Iteration 198, loss = 0.34590847\n",
      "Iteration 199, loss = 0.34584365\n",
      "Iteration 200, loss = 0.34578360\n",
      "Iteration 201, loss = 0.34569488\n",
      "Iteration 202, loss = 0.34560331\n",
      "Iteration 203, loss = 0.34553450\n",
      "Iteration 204, loss = 0.34545498\n",
      "Iteration 205, loss = 0.34538114\n",
      "Iteration 206, loss = 0.34530513\n",
      "Iteration 207, loss = 0.34523200\n",
      "Iteration 208, loss = 0.34516260\n",
      "Iteration 209, loss = 0.34509710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70722671\n",
      "Iteration 2, loss = 0.70438061\n",
      "Iteration 3, loss = 0.69999823\n",
      "Iteration 4, loss = 0.69468223\n",
      "Iteration 5, loss = 0.68857680\n",
      "Iteration 6, loss = 0.68211504\n",
      "Iteration 7, loss = 0.67532052\n",
      "Iteration 8, loss = 0.66848828\n",
      "Iteration 9, loss = 0.66151572\n",
      "Iteration 10, loss = 0.65443160\n",
      "Iteration 11, loss = 0.64778224\n",
      "Iteration 12, loss = 0.64079268\n",
      "Iteration 13, loss = 0.63425884\n",
      "Iteration 14, loss = 0.62762203\n",
      "Iteration 15, loss = 0.62127004\n",
      "Iteration 16, loss = 0.61490879\n",
      "Iteration 17, loss = 0.60882163\n",
      "Iteration 18, loss = 0.60308923\n",
      "Iteration 19, loss = 0.59708190\n",
      "Iteration 20, loss = 0.59131601\n",
      "Iteration 21, loss = 0.58573461\n",
      "Iteration 22, loss = 0.58035479\n",
      "Iteration 23, loss = 0.57503928\n",
      "Iteration 24, loss = 0.56972293\n",
      "Iteration 25, loss = 0.56470626\n",
      "Iteration 26, loss = 0.55955930\n",
      "Iteration 27, loss = 0.55475801\n",
      "Iteration 28, loss = 0.55000503\n",
      "Iteration 29, loss = 0.54540143\n",
      "Iteration 30, loss = 0.54078634\n",
      "Iteration 31, loss = 0.53639321\n",
      "Iteration 32, loss = 0.53207493\n",
      "Iteration 33, loss = 0.52768115\n",
      "Iteration 34, loss = 0.52351848\n",
      "Iteration 35, loss = 0.51957261\n",
      "Iteration 36, loss = 0.51551763\n",
      "Iteration 37, loss = 0.51174629\n",
      "Iteration 38, loss = 0.50786973\n",
      "Iteration 39, loss = 0.50409100\n",
      "Iteration 40, loss = 0.50047605\n",
      "Iteration 41, loss = 0.49696069\n",
      "Iteration 42, loss = 0.49357433\n",
      "Iteration 43, loss = 0.49014092\n",
      "Iteration 44, loss = 0.48686994\n",
      "Iteration 45, loss = 0.48368027\n",
      "Iteration 46, loss = 0.48047682\n",
      "Iteration 47, loss = 0.47746853\n",
      "Iteration 48, loss = 0.47445757\n",
      "Iteration 49, loss = 0.47148053\n",
      "Iteration 50, loss = 0.46870936\n",
      "Iteration 51, loss = 0.46589750\n",
      "Iteration 52, loss = 0.46315630\n",
      "Iteration 53, loss = 0.46057823\n",
      "Iteration 54, loss = 0.45795137\n",
      "Iteration 55, loss = 0.45540605\n",
      "Iteration 56, loss = 0.45290199\n",
      "Iteration 57, loss = 0.45045347\n",
      "Iteration 58, loss = 0.44809543\n",
      "Iteration 59, loss = 0.44589067\n",
      "Iteration 60, loss = 0.44364368\n",
      "Iteration 61, loss = 0.44140423\n",
      "Iteration 62, loss = 0.43923080\n",
      "Iteration 63, loss = 0.43720828\n",
      "Iteration 64, loss = 0.43517831\n",
      "Iteration 65, loss = 0.43309495\n",
      "Iteration 66, loss = 0.43125662\n",
      "Iteration 67, loss = 0.42933757\n",
      "Iteration 68, loss = 0.42757929\n",
      "Iteration 69, loss = 0.42575987\n",
      "Iteration 70, loss = 0.42399716\n",
      "Iteration 71, loss = 0.42237785\n",
      "Iteration 72, loss = 0.42066645\n",
      "Iteration 73, loss = 0.41905405\n",
      "Iteration 74, loss = 0.41747014\n",
      "Iteration 75, loss = 0.41594306\n",
      "Iteration 76, loss = 0.41442853\n",
      "Iteration 77, loss = 0.41299822\n",
      "Iteration 78, loss = 0.41158876\n",
      "Iteration 79, loss = 0.41015686\n",
      "Iteration 80, loss = 0.40885175\n",
      "Iteration 81, loss = 0.40745562\n",
      "Iteration 82, loss = 0.40615832\n",
      "Iteration 83, loss = 0.40492476\n",
      "Iteration 84, loss = 0.40367510\n",
      "Iteration 85, loss = 0.40244823\n",
      "Iteration 86, loss = 0.40127151\n",
      "Iteration 87, loss = 0.40015066\n",
      "Iteration 88, loss = 0.39903134\n",
      "Iteration 89, loss = 0.39784599\n",
      "Iteration 90, loss = 0.39677064\n",
      "Iteration 91, loss = 0.39574248\n",
      "Iteration 92, loss = 0.39467378\n",
      "Iteration 93, loss = 0.39368896\n",
      "Iteration 94, loss = 0.39266852\n",
      "Iteration 95, loss = 0.39173909\n",
      "Iteration 96, loss = 0.39074570\n",
      "Iteration 97, loss = 0.38984073\n",
      "Iteration 98, loss = 0.38895450\n",
      "Iteration 99, loss = 0.38806687\n",
      "Iteration 100, loss = 0.38721294\n",
      "Iteration 101, loss = 0.38635957\n",
      "Iteration 102, loss = 0.38549104\n",
      "Iteration 103, loss = 0.38465061\n",
      "Iteration 104, loss = 0.38387043\n",
      "Iteration 105, loss = 0.38307429\n",
      "Iteration 106, loss = 0.38236886\n",
      "Iteration 107, loss = 0.38156501\n",
      "Iteration 108, loss = 0.38085673\n",
      "Iteration 109, loss = 0.38010168\n",
      "Iteration 110, loss = 0.37941625\n",
      "Iteration 111, loss = 0.37868891\n",
      "Iteration 112, loss = 0.37799449\n",
      "Iteration 113, loss = 0.37733038\n",
      "Iteration 114, loss = 0.37666976\n",
      "Iteration 115, loss = 0.37597527\n",
      "Iteration 116, loss = 0.37533359\n",
      "Iteration 117, loss = 0.37470835\n",
      "Iteration 118, loss = 0.37411351\n",
      "Iteration 119, loss = 0.37346512\n",
      "Iteration 120, loss = 0.37286407\n",
      "Iteration 121, loss = 0.37226676\n",
      "Iteration 122, loss = 0.37168081\n",
      "Iteration 123, loss = 0.37112484\n",
      "Iteration 124, loss = 0.37057579\n",
      "Iteration 125, loss = 0.36998216\n",
      "Iteration 126, loss = 0.36943693\n",
      "Iteration 127, loss = 0.36894878\n",
      "Iteration 128, loss = 0.36837153\n",
      "Iteration 129, loss = 0.36782889\n",
      "Iteration 130, loss = 0.36733966\n",
      "Iteration 131, loss = 0.36680812\n",
      "Iteration 132, loss = 0.36628183\n",
      "Iteration 133, loss = 0.36582816\n",
      "Iteration 134, loss = 0.36530958\n",
      "Iteration 135, loss = 0.36483528\n",
      "Iteration 136, loss = 0.36434450\n",
      "Iteration 137, loss = 0.36387719\n",
      "Iteration 138, loss = 0.36341581\n",
      "Iteration 139, loss = 0.36299490\n",
      "Iteration 140, loss = 0.36251392\n",
      "Iteration 141, loss = 0.36205001\n",
      "Iteration 142, loss = 0.36161416\n",
      "Iteration 143, loss = 0.36118579\n",
      "Iteration 144, loss = 0.36078508\n",
      "Iteration 145, loss = 0.36033478\n",
      "Iteration 146, loss = 0.35993012\n",
      "Iteration 147, loss = 0.35948677\n",
      "Iteration 148, loss = 0.35911006\n",
      "Iteration 149, loss = 0.35869677\n",
      "Iteration 150, loss = 0.35832257\n",
      "Iteration 151, loss = 0.35793598\n",
      "Iteration 152, loss = 0.35752453\n",
      "Iteration 153, loss = 0.35712527\n",
      "Iteration 154, loss = 0.35674628\n",
      "Iteration 155, loss = 0.35637481\n",
      "Iteration 156, loss = 0.35599634\n",
      "Iteration 157, loss = 0.35566458\n",
      "Iteration 158, loss = 0.35527931\n",
      "Iteration 159, loss = 0.35489945\n",
      "Iteration 160, loss = 0.35454574\n",
      "Iteration 161, loss = 0.35418288\n",
      "Iteration 162, loss = 0.35384985\n",
      "Iteration 163, loss = 0.35349230\n",
      "Iteration 164, loss = 0.35314617\n",
      "Iteration 165, loss = 0.35279335\n",
      "Iteration 166, loss = 0.35245542\n",
      "Iteration 167, loss = 0.35209500\n",
      "Iteration 168, loss = 0.35181715\n",
      "Iteration 169, loss = 0.35145637\n",
      "Iteration 170, loss = 0.35112801\n",
      "Iteration 171, loss = 0.35079825\n",
      "Iteration 172, loss = 0.35048101\n",
      "Iteration 173, loss = 0.35014603\n",
      "Iteration 174, loss = 0.34981470\n",
      "Iteration 175, loss = 0.34951507\n",
      "Iteration 176, loss = 0.34920565\n",
      "Iteration 177, loss = 0.34890650\n",
      "Iteration 178, loss = 0.34860720\n",
      "Iteration 179, loss = 0.34829328\n",
      "Iteration 180, loss = 0.34797252\n",
      "Iteration 181, loss = 0.34771810\n",
      "Iteration 182, loss = 0.34739009\n",
      "Iteration 183, loss = 0.34712428\n",
      "Iteration 184, loss = 0.34682836\n",
      "Iteration 185, loss = 0.34655142\n",
      "Iteration 186, loss = 0.34627124\n",
      "Iteration 187, loss = 0.34598905\n",
      "Iteration 188, loss = 0.34570238\n",
      "Iteration 189, loss = 0.34543069\n",
      "Iteration 190, loss = 0.34515357\n",
      "Iteration 191, loss = 0.34488587\n",
      "Iteration 192, loss = 0.34462101\n",
      "Iteration 193, loss = 0.34435072\n",
      "Iteration 194, loss = 0.34408590\n",
      "Iteration 195, loss = 0.34382341\n",
      "Iteration 196, loss = 0.34360769\n",
      "Iteration 197, loss = 0.34333022\n",
      "Iteration 198, loss = 0.34303086\n",
      "Iteration 199, loss = 0.34280618\n",
      "Iteration 200, loss = 0.34254961\n",
      "Iteration 201, loss = 0.34231206\n",
      "Iteration 202, loss = 0.34204029\n",
      "Iteration 203, loss = 0.34181294\n",
      "Iteration 204, loss = 0.34153832\n",
      "Iteration 205, loss = 0.34132099\n",
      "Iteration 206, loss = 0.34108415\n",
      "Iteration 207, loss = 0.34085029\n",
      "Iteration 208, loss = 0.34059792\n",
      "Iteration 209, loss = 0.34038014\n",
      "Iteration 210, loss = 0.34012241\n",
      "Iteration 211, loss = 0.33990395\n",
      "Iteration 212, loss = 0.33967231\n",
      "Iteration 213, loss = 0.33942620\n",
      "Iteration 214, loss = 0.33921142\n",
      "Iteration 215, loss = 0.33898039\n",
      "Iteration 216, loss = 0.33873826\n",
      "Iteration 217, loss = 0.33851840\n",
      "Iteration 218, loss = 0.33830388\n",
      "Iteration 219, loss = 0.33808077\n",
      "Iteration 220, loss = 0.33786147\n",
      "Iteration 221, loss = 0.33763762\n",
      "Iteration 222, loss = 0.33739129\n",
      "Iteration 223, loss = 0.33718096\n",
      "Iteration 224, loss = 0.33697495\n",
      "Iteration 225, loss = 0.33676283\n",
      "Iteration 226, loss = 0.33654726\n",
      "Iteration 227, loss = 0.33634129\n",
      "Iteration 228, loss = 0.33613828\n",
      "Iteration 229, loss = 0.33592875\n",
      "Iteration 230, loss = 0.33572471\n",
      "Iteration 231, loss = 0.33551025\n",
      "Iteration 232, loss = 0.33531716\n",
      "Iteration 233, loss = 0.33510598\n",
      "Iteration 234, loss = 0.33493165\n",
      "Iteration 235, loss = 0.33471139\n",
      "Iteration 236, loss = 0.33450303\n",
      "Iteration 237, loss = 0.33430737\n",
      "Iteration 238, loss = 0.33411328\n",
      "Iteration 239, loss = 0.33393818\n",
      "Iteration 240, loss = 0.33372013\n",
      "Iteration 241, loss = 0.33353645\n",
      "Iteration 242, loss = 0.33335169\n",
      "Iteration 243, loss = 0.33314773\n",
      "Iteration 244, loss = 0.33296755\n",
      "Iteration 245, loss = 0.33276369\n",
      "Iteration 246, loss = 0.33260364\n",
      "Iteration 247, loss = 0.33240128\n",
      "Iteration 248, loss = 0.33222398\n",
      "Iteration 249, loss = 0.33204463\n",
      "Iteration 250, loss = 0.33185456\n",
      "Iteration 251, loss = 0.33167670\n",
      "Iteration 252, loss = 0.33147113\n",
      "Iteration 253, loss = 0.33129841\n",
      "Iteration 254, loss = 0.33113991\n",
      "Iteration 255, loss = 0.33095970\n",
      "Iteration 256, loss = 0.33075912\n",
      "Iteration 257, loss = 0.33058880\n",
      "Iteration 258, loss = 0.33044306\n",
      "Iteration 259, loss = 0.33024614\n",
      "Iteration 260, loss = 0.33005471\n",
      "Iteration 261, loss = 0.32988357\n",
      "Iteration 262, loss = 0.32970618\n",
      "Iteration 263, loss = 0.32954690\n",
      "Iteration 264, loss = 0.32936985\n",
      "Iteration 265, loss = 0.32921449\n",
      "Iteration 266, loss = 0.32905030\n",
      "Iteration 267, loss = 0.32887718\n",
      "Iteration 268, loss = 0.32871103\n",
      "Iteration 269, loss = 0.32856415\n",
      "Iteration 270, loss = 0.32839537\n",
      "Iteration 271, loss = 0.32822364\n",
      "Iteration 272, loss = 0.32804392\n",
      "Iteration 273, loss = 0.32788573\n",
      "Iteration 274, loss = 0.32773998\n",
      "Iteration 275, loss = 0.32756936\n",
      "Iteration 276, loss = 0.32740479\n",
      "Iteration 277, loss = 0.32724560\n",
      "Iteration 278, loss = 0.32711647\n",
      "Iteration 279, loss = 0.32694636\n",
      "Iteration 280, loss = 0.32679150\n",
      "Iteration 281, loss = 0.32662675\n",
      "Iteration 282, loss = 0.32648402\n",
      "Iteration 283, loss = 0.32631385\n",
      "Iteration 284, loss = 0.32616946\n",
      "Iteration 285, loss = 0.32604228\n",
      "Iteration 286, loss = 0.32587206\n",
      "Iteration 287, loss = 0.32571247\n",
      "Iteration 288, loss = 0.32556383\n",
      "Iteration 289, loss = 0.32543036\n",
      "Iteration 290, loss = 0.32528144\n",
      "Iteration 291, loss = 0.32512252\n",
      "Iteration 292, loss = 0.32498809\n",
      "Iteration 293, loss = 0.32483099\n",
      "Iteration 294, loss = 0.32471936\n",
      "Iteration 295, loss = 0.32454998\n",
      "Iteration 296, loss = 0.32438430\n",
      "Iteration 297, loss = 0.32424312\n",
      "Iteration 298, loss = 0.32410101\n",
      "Iteration 299, loss = 0.32396515\n",
      "Iteration 300, loss = 0.32383316\n",
      "Iteration 301, loss = 0.32366171\n",
      "Iteration 302, loss = 0.32351455\n",
      "Iteration 303, loss = 0.32338191\n",
      "Iteration 304, loss = 0.32324089\n",
      "Iteration 305, loss = 0.32310484\n",
      "Iteration 306, loss = 0.32297121\n",
      "Iteration 307, loss = 0.32282335\n",
      "Iteration 308, loss = 0.32267997\n",
      "Iteration 309, loss = 0.32253541\n",
      "Iteration 310, loss = 0.32240841\n",
      "Iteration 311, loss = 0.32227852\n",
      "Iteration 312, loss = 0.32213986\n",
      "Iteration 313, loss = 0.32199508\n",
      "Iteration 314, loss = 0.32186591\n",
      "Iteration 315, loss = 0.32172334\n",
      "Iteration 316, loss = 0.32158607\n",
      "Iteration 317, loss = 0.32147760\n",
      "Iteration 318, loss = 0.32132417\n",
      "Iteration 319, loss = 0.32122173\n",
      "Iteration 320, loss = 0.32106512\n",
      "Iteration 321, loss = 0.32093853\n",
      "Iteration 322, loss = 0.32080187\n",
      "Iteration 323, loss = 0.32067251\n",
      "Iteration 324, loss = 0.32055238\n",
      "Iteration 325, loss = 0.32042411\n",
      "Iteration 326, loss = 0.32030039\n",
      "Iteration 327, loss = 0.32015898\n",
      "Iteration 328, loss = 0.32003016\n",
      "Iteration 329, loss = 0.31990744\n",
      "Iteration 330, loss = 0.31978481\n",
      "Iteration 331, loss = 0.31966905\n",
      "Iteration 332, loss = 0.31955123\n",
      "Iteration 333, loss = 0.31941733\n",
      "Iteration 334, loss = 0.31928371\n",
      "Iteration 335, loss = 0.31916333\n",
      "Iteration 336, loss = 0.31903499\n",
      "Iteration 337, loss = 0.31891916\n",
      "Iteration 338, loss = 0.31879150\n",
      "Iteration 339, loss = 0.31868117\n",
      "Iteration 340, loss = 0.31854787\n",
      "Iteration 341, loss = 0.31845086\n",
      "Iteration 342, loss = 0.31833291\n",
      "Iteration 343, loss = 0.31818161\n",
      "Iteration 344, loss = 0.31806711\n",
      "Iteration 345, loss = 0.31794842\n",
      "Iteration 346, loss = 0.31783320\n",
      "Iteration 347, loss = 0.31773519\n",
      "Iteration 348, loss = 0.31758492\n",
      "Iteration 349, loss = 0.31748839\n",
      "Iteration 350, loss = 0.31735233\n",
      "Iteration 351, loss = 0.31723917\n",
      "Iteration 352, loss = 0.31711474\n",
      "Iteration 353, loss = 0.31699712\n",
      "Iteration 354, loss = 0.31691685\n",
      "Iteration 355, loss = 0.31677419\n",
      "Iteration 356, loss = 0.31667386\n",
      "Iteration 357, loss = 0.31654085\n",
      "Iteration 358, loss = 0.31642505\n",
      "Iteration 359, loss = 0.31631027\n",
      "Iteration 360, loss = 0.31620323\n",
      "Iteration 361, loss = 0.31607581\n",
      "Iteration 362, loss = 0.31596714\n",
      "Iteration 363, loss = 0.31585731\n",
      "Iteration 364, loss = 0.31573690\n",
      "Iteration 365, loss = 0.31569314\n",
      "Iteration 366, loss = 0.31550873\n",
      "Iteration 367, loss = 0.31539811\n",
      "Iteration 368, loss = 0.31528633\n",
      "Iteration 369, loss = 0.31516291\n",
      "Iteration 370, loss = 0.31505545\n",
      "Iteration 371, loss = 0.31496269\n",
      "Iteration 372, loss = 0.31483747\n",
      "Iteration 373, loss = 0.31475152\n",
      "Iteration 374, loss = 0.31461138\n",
      "Iteration 375, loss = 0.31450806\n",
      "Iteration 376, loss = 0.31439744\n",
      "Iteration 377, loss = 0.31430688\n",
      "Iteration 378, loss = 0.31416803\n",
      "Iteration 379, loss = 0.31407745\n",
      "Iteration 380, loss = 0.31395236\n",
      "Iteration 381, loss = 0.31384738\n",
      "Iteration 382, loss = 0.31372750\n",
      "Iteration 383, loss = 0.31363484\n",
      "Iteration 384, loss = 0.31351790\n",
      "Iteration 385, loss = 0.31340906\n",
      "Iteration 386, loss = 0.31329494\n",
      "Iteration 387, loss = 0.31324445\n",
      "Iteration 388, loss = 0.31309180\n",
      "Iteration 389, loss = 0.31299904\n",
      "Iteration 390, loss = 0.31289916\n",
      "Iteration 391, loss = 0.31276534\n",
      "Iteration 392, loss = 0.31267586\n",
      "Iteration 393, loss = 0.31255975\n",
      "Iteration 394, loss = 0.31247362\n",
      "Iteration 395, loss = 0.31235130\n",
      "Iteration 396, loss = 0.31225801\n",
      "Iteration 397, loss = 0.31213693\n",
      "Iteration 398, loss = 0.31205829\n",
      "Iteration 399, loss = 0.31193188\n",
      "Iteration 400, loss = 0.31184668\n",
      "Iteration 401, loss = 0.31173869\n",
      "Iteration 402, loss = 0.31164701\n",
      "Iteration 403, loss = 0.31152393\n",
      "Iteration 404, loss = 0.31142848\n",
      "Iteration 405, loss = 0.31132310\n",
      "Iteration 406, loss = 0.31124320\n",
      "Iteration 407, loss = 0.31113366\n",
      "Iteration 408, loss = 0.31102447\n",
      "Iteration 409, loss = 0.31091915\n",
      "Iteration 410, loss = 0.31082371\n",
      "Iteration 411, loss = 0.31071457\n",
      "Iteration 412, loss = 0.31062064\n",
      "Iteration 413, loss = 0.31054046\n",
      "Iteration 414, loss = 0.31041293\n",
      "Iteration 415, loss = 0.31032631\n",
      "Iteration 416, loss = 0.31022208\n",
      "Iteration 417, loss = 0.31010883\n",
      "Iteration 418, loss = 0.31002538\n",
      "Iteration 419, loss = 0.30992211\n",
      "Iteration 420, loss = 0.30982127\n",
      "Iteration 421, loss = 0.30971552\n",
      "Iteration 422, loss = 0.30961374\n",
      "Iteration 423, loss = 0.30956129\n",
      "Iteration 424, loss = 0.30942735\n",
      "Iteration 425, loss = 0.30933893\n",
      "Iteration 426, loss = 0.30922922\n",
      "Iteration 427, loss = 0.30911624\n",
      "Iteration 428, loss = 0.30902469\n",
      "Iteration 429, loss = 0.30892679\n",
      "Iteration 430, loss = 0.30883640\n",
      "Iteration 431, loss = 0.30871736\n",
      "Iteration 432, loss = 0.30862376\n",
      "Iteration 433, loss = 0.30852402\n",
      "Iteration 434, loss = 0.30842269\n",
      "Iteration 435, loss = 0.30832658\n",
      "Iteration 436, loss = 0.30821139\n",
      "Iteration 437, loss = 0.30811473\n",
      "Iteration 438, loss = 0.30803475\n",
      "Iteration 439, loss = 0.30791389\n",
      "Iteration 440, loss = 0.30783239\n",
      "Iteration 441, loss = 0.30773722\n",
      "Iteration 442, loss = 0.30761387\n",
      "Iteration 443, loss = 0.30752275\n",
      "Iteration 444, loss = 0.30741052\n",
      "Iteration 445, loss = 0.30731873\n",
      "Iteration 446, loss = 0.30723797\n",
      "Iteration 447, loss = 0.30712271\n",
      "Iteration 448, loss = 0.30702328\n",
      "Iteration 449, loss = 0.30693664\n",
      "Iteration 450, loss = 0.30682954\n",
      "Iteration 451, loss = 0.30674143\n",
      "Iteration 452, loss = 0.30667358\n",
      "Iteration 453, loss = 0.30654709\n",
      "Iteration 454, loss = 0.30644365\n",
      "Iteration 455, loss = 0.30634320\n",
      "Iteration 456, loss = 0.30625536\n",
      "Iteration 457, loss = 0.30615555\n",
      "Iteration 458, loss = 0.30606183\n",
      "Iteration 459, loss = 0.30596410\n",
      "Iteration 460, loss = 0.30585931\n",
      "Iteration 461, loss = 0.30576259\n",
      "Iteration 462, loss = 0.30568596\n",
      "Iteration 463, loss = 0.30560648\n",
      "Iteration 464, loss = 0.30549061\n",
      "Iteration 465, loss = 0.30539133\n",
      "Iteration 466, loss = 0.30529416\n",
      "Iteration 467, loss = 0.30520011\n",
      "Iteration 468, loss = 0.30510521\n",
      "Iteration 469, loss = 0.30499849\n",
      "Iteration 470, loss = 0.30490730\n",
      "Iteration 471, loss = 0.30482199\n",
      "Iteration 472, loss = 0.30472010\n",
      "Iteration 473, loss = 0.30461742\n",
      "Iteration 474, loss = 0.30452902\n",
      "Iteration 475, loss = 0.30444800\n",
      "Iteration 476, loss = 0.30434642\n",
      "Iteration 477, loss = 0.30424230\n",
      "Iteration 478, loss = 0.30415014\n",
      "Iteration 479, loss = 0.30405943\n",
      "Iteration 480, loss = 0.30397387\n",
      "Iteration 481, loss = 0.30386685\n",
      "Iteration 482, loss = 0.30378639\n",
      "Iteration 483, loss = 0.30370651\n",
      "Iteration 484, loss = 0.30359798\n",
      "Iteration 485, loss = 0.30350752\n",
      "Iteration 486, loss = 0.30341527\n",
      "Iteration 487, loss = 0.30333587\n",
      "Iteration 488, loss = 0.30322964\n",
      "Iteration 489, loss = 0.30313093\n",
      "Iteration 490, loss = 0.30304644\n",
      "Iteration 491, loss = 0.30296777\n",
      "Iteration 492, loss = 0.30286254\n",
      "Iteration 493, loss = 0.30280222\n",
      "Iteration 494, loss = 0.30267863\n",
      "Iteration 495, loss = 0.30260357\n",
      "Iteration 496, loss = 0.30249246\n",
      "Iteration 497, loss = 0.30243658\n",
      "Iteration 498, loss = 0.30231967\n",
      "Iteration 499, loss = 0.30223256\n",
      "Iteration 500, loss = 0.30213238\n",
      "Iteration 501, loss = 0.30204376\n",
      "Iteration 502, loss = 0.30195886\n",
      "Iteration 503, loss = 0.30187051\n",
      "Iteration 504, loss = 0.30177790\n",
      "Iteration 505, loss = 0.30168278\n",
      "Iteration 506, loss = 0.30158865\n",
      "Iteration 507, loss = 0.30152619\n",
      "Iteration 508, loss = 0.30141358\n",
      "Iteration 509, loss = 0.30132767\n",
      "Iteration 510, loss = 0.30123261\n",
      "Iteration 511, loss = 0.30114821\n",
      "Iteration 512, loss = 0.30107451\n",
      "Iteration 513, loss = 0.30096863\n",
      "Iteration 514, loss = 0.30087551\n",
      "Iteration 515, loss = 0.30080746\n",
      "Iteration 516, loss = 0.30070452\n",
      "Iteration 517, loss = 0.30061115\n",
      "Iteration 518, loss = 0.30052985\n",
      "Iteration 519, loss = 0.30043805\n",
      "Iteration 520, loss = 0.30034970\n",
      "Iteration 521, loss = 0.30025722\n",
      "Iteration 522, loss = 0.30016579\n",
      "Iteration 523, loss = 0.30008936\n",
      "Iteration 524, loss = 0.29999877\n",
      "Iteration 525, loss = 0.29991158\n",
      "Iteration 526, loss = 0.29981425\n",
      "Iteration 527, loss = 0.29972961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72039515\n",
      "Iteration 2, loss = 0.71000565\n",
      "Iteration 3, loss = 0.69422661\n",
      "Iteration 4, loss = 0.67640568\n",
      "Iteration 5, loss = 0.65625667\n",
      "Iteration 6, loss = 0.63673190\n",
      "Iteration 7, loss = 0.61751249\n",
      "Iteration 8, loss = 0.59920392\n",
      "Iteration 9, loss = 0.58200978\n",
      "Iteration 10, loss = 0.56597020\n",
      "Iteration 11, loss = 0.55202253\n",
      "Iteration 12, loss = 0.53845566\n",
      "Iteration 13, loss = 0.52664040\n",
      "Iteration 14, loss = 0.51544603\n",
      "Iteration 15, loss = 0.50554614\n",
      "Iteration 16, loss = 0.49637846\n",
      "Iteration 17, loss = 0.48822030\n",
      "Iteration 18, loss = 0.48096455\n",
      "Iteration 19, loss = 0.47371634\n",
      "Iteration 20, loss = 0.46728444\n",
      "Iteration 21, loss = 0.46163602\n",
      "Iteration 22, loss = 0.45636361\n",
      "Iteration 23, loss = 0.45146088\n",
      "Iteration 24, loss = 0.44673979\n",
      "Iteration 25, loss = 0.44262383\n",
      "Iteration 26, loss = 0.43863889\n",
      "Iteration 27, loss = 0.43491151\n",
      "Iteration 28, loss = 0.43151681\n",
      "Iteration 29, loss = 0.42829394\n",
      "Iteration 30, loss = 0.42528498\n",
      "Iteration 31, loss = 0.42256943\n",
      "Iteration 32, loss = 0.41993297\n",
      "Iteration 33, loss = 0.41735200\n",
      "Iteration 34, loss = 0.41495691\n",
      "Iteration 35, loss = 0.41280512\n",
      "Iteration 36, loss = 0.41063441\n",
      "Iteration 37, loss = 0.40877040\n",
      "Iteration 38, loss = 0.40682414\n",
      "Iteration 39, loss = 0.40499784\n",
      "Iteration 40, loss = 0.40324507\n",
      "Iteration 41, loss = 0.40168342\n",
      "Iteration 42, loss = 0.40012819\n",
      "Iteration 43, loss = 0.39862918\n",
      "Iteration 44, loss = 0.39719743\n",
      "Iteration 45, loss = 0.39590908\n",
      "Iteration 46, loss = 0.39450726\n",
      "Iteration 47, loss = 0.39332906\n",
      "Iteration 48, loss = 0.39214206\n",
      "Iteration 49, loss = 0.39095972\n",
      "Iteration 50, loss = 0.38989444\n",
      "Iteration 51, loss = 0.38882418\n",
      "Iteration 52, loss = 0.38783287\n",
      "Iteration 53, loss = 0.38684748\n",
      "Iteration 54, loss = 0.38590214\n",
      "Iteration 55, loss = 0.38504467\n",
      "Iteration 56, loss = 0.38412069\n",
      "Iteration 57, loss = 0.38326047\n",
      "Iteration 58, loss = 0.38239270\n",
      "Iteration 59, loss = 0.38167823\n",
      "Iteration 60, loss = 0.38093656\n",
      "Iteration 61, loss = 0.38014937\n",
      "Iteration 62, loss = 0.37944766\n",
      "Iteration 63, loss = 0.37874767\n",
      "Iteration 64, loss = 0.37806776\n",
      "Iteration 65, loss = 0.37738647\n",
      "Iteration 66, loss = 0.37678149\n",
      "Iteration 67, loss = 0.37615169\n",
      "Iteration 68, loss = 0.37559296\n",
      "Iteration 69, loss = 0.37500958\n",
      "Iteration 70, loss = 0.37439870\n",
      "Iteration 71, loss = 0.37390743\n",
      "Iteration 72, loss = 0.37332784\n",
      "Iteration 73, loss = 0.37279234\n",
      "Iteration 74, loss = 0.37228409\n",
      "Iteration 75, loss = 0.37180023\n",
      "Iteration 76, loss = 0.37130761\n",
      "Iteration 77, loss = 0.37084541\n",
      "Iteration 78, loss = 0.37039434\n",
      "Iteration 79, loss = 0.36994822\n",
      "Iteration 80, loss = 0.36954477\n",
      "Iteration 81, loss = 0.36905461\n",
      "Iteration 82, loss = 0.36863488\n",
      "Iteration 83, loss = 0.36824880\n",
      "Iteration 84, loss = 0.36785151\n",
      "Iteration 85, loss = 0.36751111\n",
      "Iteration 86, loss = 0.36707021\n",
      "Iteration 87, loss = 0.36673741\n",
      "Iteration 88, loss = 0.36638811\n",
      "Iteration 89, loss = 0.36596518\n",
      "Iteration 90, loss = 0.36564262\n",
      "Iteration 91, loss = 0.36532166\n",
      "Iteration 92, loss = 0.36495388\n",
      "Iteration 93, loss = 0.36466062\n",
      "Iteration 94, loss = 0.36431189\n",
      "Iteration 95, loss = 0.36401832\n",
      "Iteration 96, loss = 0.36369679\n",
      "Iteration 97, loss = 0.36337266\n",
      "Iteration 98, loss = 0.36309901\n",
      "Iteration 99, loss = 0.36281058\n",
      "Iteration 100, loss = 0.36254627\n",
      "Iteration 101, loss = 0.36225074\n",
      "Iteration 102, loss = 0.36197034\n",
      "Iteration 103, loss = 0.36166361\n",
      "Iteration 104, loss = 0.36142191\n",
      "Iteration 105, loss = 0.36114873\n",
      "Iteration 106, loss = 0.36091699\n",
      "Iteration 107, loss = 0.36063492\n",
      "Iteration 108, loss = 0.36040936\n",
      "Iteration 109, loss = 0.36015242\n",
      "Iteration 110, loss = 0.35995838\n",
      "Iteration 111, loss = 0.35970637\n",
      "Iteration 112, loss = 0.35945221\n",
      "Iteration 113, loss = 0.35924272\n",
      "Iteration 114, loss = 0.35902159\n",
      "Iteration 115, loss = 0.35878689\n",
      "Iteration 116, loss = 0.35856398\n",
      "Iteration 117, loss = 0.35836512\n",
      "Iteration 118, loss = 0.35817289\n",
      "Iteration 119, loss = 0.35795529\n",
      "Iteration 120, loss = 0.35774100\n",
      "Iteration 121, loss = 0.35753514\n",
      "Iteration 122, loss = 0.35734803\n",
      "Iteration 123, loss = 0.35717675\n",
      "Iteration 124, loss = 0.35699396\n",
      "Iteration 125, loss = 0.35678463\n",
      "Iteration 126, loss = 0.35659327\n",
      "Iteration 127, loss = 0.35645188\n",
      "Iteration 128, loss = 0.35624445\n",
      "Iteration 129, loss = 0.35606822\n",
      "Iteration 130, loss = 0.35590808\n",
      "Iteration 131, loss = 0.35573247\n",
      "Iteration 132, loss = 0.35555466\n",
      "Iteration 133, loss = 0.35542576\n",
      "Iteration 134, loss = 0.35524423\n",
      "Iteration 135, loss = 0.35507716\n",
      "Iteration 136, loss = 0.35490073\n",
      "Iteration 137, loss = 0.35476193\n",
      "Iteration 138, loss = 0.35460631\n",
      "Iteration 139, loss = 0.35447126\n",
      "Iteration 140, loss = 0.35430402\n",
      "Iteration 141, loss = 0.35414444\n",
      "Iteration 142, loss = 0.35399289\n",
      "Iteration 143, loss = 0.35385247\n",
      "Iteration 144, loss = 0.35375309\n",
      "Iteration 145, loss = 0.35357757\n",
      "Iteration 146, loss = 0.35344595\n",
      "Iteration 147, loss = 0.35330995\n",
      "Iteration 148, loss = 0.35316779\n",
      "Iteration 149, loss = 0.35303086\n",
      "Iteration 150, loss = 0.35291670\n",
      "Iteration 151, loss = 0.35280637\n",
      "Iteration 152, loss = 0.35265987\n",
      "Iteration 153, loss = 0.35251642\n",
      "Iteration 154, loss = 0.35240411\n",
      "Iteration 155, loss = 0.35228005\n",
      "Iteration 156, loss = 0.35216083\n",
      "Iteration 157, loss = 0.35206288\n",
      "Iteration 158, loss = 0.35190904\n",
      "Iteration 159, loss = 0.35179711\n",
      "Iteration 160, loss = 0.35168425\n",
      "Iteration 161, loss = 0.35156947\n",
      "Iteration 162, loss = 0.35146985\n",
      "Iteration 163, loss = 0.35135926\n",
      "Iteration 164, loss = 0.35124595\n",
      "Iteration 165, loss = 0.35113460\n",
      "Iteration 166, loss = 0.35101651\n",
      "Iteration 167, loss = 0.35089817\n",
      "Iteration 168, loss = 0.35085399\n",
      "Iteration 169, loss = 0.35071269\n",
      "Iteration 170, loss = 0.35061726\n",
      "Iteration 171, loss = 0.35051763\n",
      "Iteration 172, loss = 0.35042334\n",
      "Iteration 173, loss = 0.35030404\n",
      "Iteration 174, loss = 0.35020052\n",
      "Iteration 175, loss = 0.35011313\n",
      "Iteration 176, loss = 0.35001078\n",
      "Iteration 177, loss = 0.34992996\n",
      "Iteration 178, loss = 0.34984395\n",
      "Iteration 179, loss = 0.34973383\n",
      "Iteration 180, loss = 0.34962994\n",
      "Iteration 181, loss = 0.34956459\n",
      "Iteration 182, loss = 0.34945857\n",
      "Iteration 183, loss = 0.34937688\n",
      "Iteration 184, loss = 0.34929225\n",
      "Iteration 185, loss = 0.34921655\n",
      "Iteration 186, loss = 0.34912337\n",
      "Iteration 187, loss = 0.34902436\n",
      "Iteration 188, loss = 0.34894144\n",
      "Iteration 189, loss = 0.34887606\n",
      "Iteration 190, loss = 0.34877693\n",
      "Iteration 191, loss = 0.34871522\n",
      "Iteration 192, loss = 0.34862939\n",
      "Iteration 193, loss = 0.34853791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69542144\n",
      "Iteration 2, loss = 0.69541783\n",
      "Iteration 3, loss = 0.69536791\n",
      "Iteration 4, loss = 0.69532288\n",
      "Iteration 5, loss = 0.69531863\n",
      "Iteration 6, loss = 0.69531867\n",
      "Iteration 7, loss = 0.69519837\n",
      "Iteration 8, loss = 0.69516561\n",
      "Iteration 9, loss = 0.69513350\n",
      "Iteration 10, loss = 0.69510906\n",
      "Iteration 11, loss = 0.69506911\n",
      "Iteration 12, loss = 0.69504696\n",
      "Iteration 13, loss = 0.69502349\n",
      "Iteration 14, loss = 0.69497734\n",
      "Iteration 15, loss = 0.69497900\n",
      "Iteration 16, loss = 0.69493036\n",
      "Iteration 17, loss = 0.69489983\n",
      "Iteration 18, loss = 0.69487308\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70485339\n",
      "Iteration 2, loss = 0.70215648\n",
      "Iteration 3, loss = 0.69802750\n",
      "Iteration 4, loss = 0.69296598\n",
      "Iteration 5, loss = 0.68720198\n",
      "Iteration 6, loss = 0.68098220\n",
      "Iteration 7, loss = 0.67466404\n",
      "Iteration 8, loss = 0.66804648\n",
      "Iteration 9, loss = 0.66152120\n",
      "Iteration 10, loss = 0.65492288\n",
      "Iteration 11, loss = 0.64828906\n",
      "Iteration 12, loss = 0.64189404\n",
      "Iteration 13, loss = 0.63556455\n",
      "Iteration 14, loss = 0.62932410\n",
      "Iteration 15, loss = 0.62312500\n",
      "Iteration 16, loss = 0.61724651\n",
      "Iteration 17, loss = 0.61143023\n",
      "Iteration 18, loss = 0.60574786\n",
      "Iteration 19, loss = 0.60003128\n",
      "Iteration 20, loss = 0.59463539\n",
      "Iteration 21, loss = 0.58921951\n",
      "Iteration 22, loss = 0.58406062\n",
      "Iteration 23, loss = 0.57892405\n",
      "Iteration 24, loss = 0.57390309\n",
      "Iteration 25, loss = 0.56895105\n",
      "Iteration 26, loss = 0.56430153\n",
      "Iteration 27, loss = 0.55955811\n",
      "Iteration 28, loss = 0.55499861\n",
      "Iteration 29, loss = 0.55067751\n",
      "Iteration 30, loss = 0.54630027\n",
      "Iteration 31, loss = 0.54206133\n",
      "Iteration 32, loss = 0.53783428\n",
      "Iteration 33, loss = 0.53386808\n",
      "Iteration 34, loss = 0.52987494\n",
      "Iteration 35, loss = 0.52611784\n",
      "Iteration 36, loss = 0.52223574\n",
      "Iteration 37, loss = 0.51861108\n",
      "Iteration 38, loss = 0.51504739\n",
      "Iteration 39, loss = 0.51147496\n",
      "Iteration 40, loss = 0.50807154\n",
      "Iteration 41, loss = 0.50469288\n",
      "Iteration 42, loss = 0.50148324\n",
      "Iteration 43, loss = 0.49822462\n",
      "Iteration 44, loss = 0.49513782\n",
      "Iteration 45, loss = 0.49211453\n",
      "Iteration 46, loss = 0.48914446\n",
      "Iteration 47, loss = 0.48614900\n",
      "Iteration 48, loss = 0.48341569\n",
      "Iteration 49, loss = 0.48066004\n",
      "Iteration 50, loss = 0.47794522\n",
      "Iteration 51, loss = 0.47529368\n",
      "Iteration 52, loss = 0.47271397\n",
      "Iteration 53, loss = 0.47023406\n",
      "Iteration 54, loss = 0.46777489\n",
      "Iteration 55, loss = 0.46547699\n",
      "Iteration 56, loss = 0.46304214\n",
      "Iteration 57, loss = 0.46086981\n",
      "Iteration 58, loss = 0.45854220\n",
      "Iteration 59, loss = 0.45654203\n",
      "Iteration 60, loss = 0.45429284\n",
      "Iteration 61, loss = 0.45224782\n",
      "Iteration 62, loss = 0.45034110\n",
      "Iteration 63, loss = 0.44818710\n",
      "Iteration 64, loss = 0.44630061\n",
      "Iteration 65, loss = 0.44447091\n",
      "Iteration 66, loss = 0.44260413\n",
      "Iteration 67, loss = 0.44083184\n",
      "Iteration 68, loss = 0.43909222\n",
      "Iteration 69, loss = 0.43732298\n",
      "Iteration 70, loss = 0.43567464\n",
      "Iteration 71, loss = 0.43406198\n",
      "Iteration 72, loss = 0.43252611\n",
      "Iteration 73, loss = 0.43097182\n",
      "Iteration 74, loss = 0.42941958\n",
      "Iteration 75, loss = 0.42799177\n",
      "Iteration 76, loss = 0.42657246\n",
      "Iteration 77, loss = 0.42514533\n",
      "Iteration 78, loss = 0.42372224\n",
      "Iteration 79, loss = 0.42244328\n",
      "Iteration 80, loss = 0.42106975\n",
      "Iteration 81, loss = 0.41982075\n",
      "Iteration 82, loss = 0.41856675\n",
      "Iteration 83, loss = 0.41735476\n",
      "Iteration 84, loss = 0.41618383\n",
      "Iteration 85, loss = 0.41498924\n",
      "Iteration 86, loss = 0.41381679\n",
      "Iteration 87, loss = 0.41270691\n",
      "Iteration 88, loss = 0.41161426\n",
      "Iteration 89, loss = 0.41052942\n",
      "Iteration 90, loss = 0.40945752\n",
      "Iteration 91, loss = 0.40839685\n",
      "Iteration 92, loss = 0.40747139\n",
      "Iteration 93, loss = 0.40642215\n",
      "Iteration 94, loss = 0.40544521\n",
      "Iteration 95, loss = 0.40447316\n",
      "Iteration 96, loss = 0.40351591\n",
      "Iteration 97, loss = 0.40260215\n",
      "Iteration 98, loss = 0.40173515\n",
      "Iteration 99, loss = 0.40083227\n",
      "Iteration 100, loss = 0.39995393\n",
      "Iteration 101, loss = 0.39910290\n",
      "Iteration 102, loss = 0.39829883\n",
      "Iteration 103, loss = 0.39745370\n",
      "Iteration 104, loss = 0.39668044\n",
      "Iteration 105, loss = 0.39587896\n",
      "Iteration 106, loss = 0.39509562\n",
      "Iteration 107, loss = 0.39436434\n",
      "Iteration 108, loss = 0.39366271\n",
      "Iteration 109, loss = 0.39287874\n",
      "Iteration 110, loss = 0.39218161\n",
      "Iteration 111, loss = 0.39144033\n",
      "Iteration 112, loss = 0.39077446\n",
      "Iteration 113, loss = 0.39009987\n",
      "Iteration 114, loss = 0.38942362\n",
      "Iteration 115, loss = 0.38876065\n",
      "Iteration 116, loss = 0.38813441\n",
      "Iteration 117, loss = 0.38748235\n",
      "Iteration 118, loss = 0.38684004\n",
      "Iteration 119, loss = 0.38623307\n",
      "Iteration 120, loss = 0.38559042\n",
      "Iteration 121, loss = 0.38499913\n",
      "Iteration 122, loss = 0.38441251\n",
      "Iteration 123, loss = 0.38383078\n",
      "Iteration 124, loss = 0.38325978\n",
      "Iteration 125, loss = 0.38267908\n",
      "Iteration 126, loss = 0.38214768\n",
      "Iteration 127, loss = 0.38160045\n",
      "Iteration 128, loss = 0.38108981\n",
      "Iteration 129, loss = 0.38053730\n",
      "Iteration 130, loss = 0.38001520\n",
      "Iteration 131, loss = 0.37950878\n",
      "Iteration 132, loss = 0.37896420\n",
      "Iteration 133, loss = 0.37851716\n",
      "Iteration 134, loss = 0.37800601\n",
      "Iteration 135, loss = 0.37749416\n",
      "Iteration 136, loss = 0.37706244\n",
      "Iteration 137, loss = 0.37655060\n",
      "Iteration 138, loss = 0.37614623\n",
      "Iteration 139, loss = 0.37564639\n",
      "Iteration 140, loss = 0.37517531\n",
      "Iteration 141, loss = 0.37471483\n",
      "Iteration 142, loss = 0.37428124\n",
      "Iteration 143, loss = 0.37383742\n",
      "Iteration 144, loss = 0.37338225\n",
      "Iteration 145, loss = 0.37300660\n",
      "Iteration 146, loss = 0.37254531\n",
      "Iteration 147, loss = 0.37211184\n",
      "Iteration 148, loss = 0.37171304\n",
      "Iteration 149, loss = 0.37127523\n",
      "Iteration 150, loss = 0.37088659\n",
      "Iteration 151, loss = 0.37050811\n",
      "Iteration 152, loss = 0.37009525\n",
      "Iteration 153, loss = 0.36972546\n",
      "Iteration 154, loss = 0.36935418\n",
      "Iteration 155, loss = 0.36890230\n",
      "Iteration 156, loss = 0.36853793\n",
      "Iteration 157, loss = 0.36814804\n",
      "Iteration 158, loss = 0.36779358\n",
      "Iteration 159, loss = 0.36741236\n",
      "Iteration 160, loss = 0.36705603\n",
      "Iteration 161, loss = 0.36670359\n",
      "Iteration 162, loss = 0.36635674\n",
      "Iteration 163, loss = 0.36603068\n",
      "Iteration 164, loss = 0.36565540\n",
      "Iteration 165, loss = 0.36529319\n",
      "Iteration 166, loss = 0.36492319\n",
      "Iteration 167, loss = 0.36462112\n",
      "Iteration 168, loss = 0.36427513\n",
      "Iteration 169, loss = 0.36394700\n",
      "Iteration 170, loss = 0.36363311\n",
      "Iteration 171, loss = 0.36328105\n",
      "Iteration 172, loss = 0.36294641\n",
      "Iteration 173, loss = 0.36263992\n",
      "Iteration 174, loss = 0.36229577\n",
      "Iteration 175, loss = 0.36201843\n",
      "Iteration 176, loss = 0.36167289\n",
      "Iteration 177, loss = 0.36137533\n",
      "Iteration 178, loss = 0.36104852\n",
      "Iteration 179, loss = 0.36073546\n",
      "Iteration 180, loss = 0.36040911\n",
      "Iteration 181, loss = 0.36012859\n",
      "Iteration 182, loss = 0.35983094\n",
      "Iteration 183, loss = 0.35951904\n",
      "Iteration 184, loss = 0.35922284\n",
      "Iteration 185, loss = 0.35891869\n",
      "Iteration 186, loss = 0.35864934\n",
      "Iteration 187, loss = 0.35835347\n",
      "Iteration 188, loss = 0.35807131\n",
      "Iteration 189, loss = 0.35775955\n",
      "Iteration 190, loss = 0.35747423\n",
      "Iteration 191, loss = 0.35719417\n",
      "Iteration 192, loss = 0.35692431\n",
      "Iteration 193, loss = 0.35668109\n",
      "Iteration 194, loss = 0.35638061\n",
      "Iteration 195, loss = 0.35609493\n",
      "Iteration 196, loss = 0.35583251\n",
      "Iteration 197, loss = 0.35554180\n",
      "Iteration 198, loss = 0.35530169\n",
      "Iteration 199, loss = 0.35501784\n",
      "Iteration 200, loss = 0.35478340\n",
      "Iteration 201, loss = 0.35449778\n",
      "Iteration 202, loss = 0.35423975\n",
      "Iteration 203, loss = 0.35395439\n",
      "Iteration 204, loss = 0.35372118\n",
      "Iteration 205, loss = 0.35347369\n",
      "Iteration 206, loss = 0.35320475\n",
      "Iteration 207, loss = 0.35296674\n",
      "Iteration 208, loss = 0.35270946\n",
      "Iteration 209, loss = 0.35247323\n",
      "Iteration 210, loss = 0.35223270\n",
      "Iteration 211, loss = 0.35200757\n",
      "Iteration 212, loss = 0.35173114\n",
      "Iteration 213, loss = 0.35149741\n",
      "Iteration 214, loss = 0.35128189\n",
      "Iteration 215, loss = 0.35103243\n",
      "Iteration 216, loss = 0.35080876\n",
      "Iteration 217, loss = 0.35057221\n",
      "Iteration 218, loss = 0.35035160\n",
      "Iteration 219, loss = 0.35009877\n",
      "Iteration 220, loss = 0.34986760\n",
      "Iteration 221, loss = 0.34964653\n",
      "Iteration 222, loss = 0.34943303\n",
      "Iteration 223, loss = 0.34922143\n",
      "Iteration 224, loss = 0.34897540\n",
      "Iteration 225, loss = 0.34874917\n",
      "Iteration 226, loss = 0.34853595\n",
      "Iteration 227, loss = 0.34831237\n",
      "Iteration 228, loss = 0.34809312\n",
      "Iteration 229, loss = 0.34788835\n",
      "Iteration 230, loss = 0.34771357\n",
      "Iteration 231, loss = 0.34745176\n",
      "Iteration 232, loss = 0.34724434\n",
      "Iteration 233, loss = 0.34702660\n",
      "Iteration 234, loss = 0.34681753\n",
      "Iteration 235, loss = 0.34660029\n",
      "Iteration 236, loss = 0.34639529\n",
      "Iteration 237, loss = 0.34619206\n",
      "Iteration 238, loss = 0.34598917\n",
      "Iteration 239, loss = 0.34577525\n",
      "Iteration 240, loss = 0.34558513\n",
      "Iteration 241, loss = 0.34536739\n",
      "Iteration 242, loss = 0.34518860\n",
      "Iteration 243, loss = 0.34498922\n",
      "Iteration 244, loss = 0.34478176\n",
      "Iteration 245, loss = 0.34458035\n",
      "Iteration 246, loss = 0.34438139\n",
      "Iteration 247, loss = 0.34420675\n",
      "Iteration 248, loss = 0.34400128\n",
      "Iteration 249, loss = 0.34384361\n",
      "Iteration 250, loss = 0.34363024\n",
      "Iteration 251, loss = 0.34345491\n",
      "Iteration 252, loss = 0.34325411\n",
      "Iteration 253, loss = 0.34307287\n",
      "Iteration 254, loss = 0.34289356\n",
      "Iteration 255, loss = 0.34270777\n",
      "Iteration 256, loss = 0.34252628\n",
      "Iteration 257, loss = 0.34236009\n",
      "Iteration 258, loss = 0.34217796\n",
      "Iteration 259, loss = 0.34198233\n",
      "Iteration 260, loss = 0.34180986\n",
      "Iteration 261, loss = 0.34165866\n",
      "Iteration 262, loss = 0.34144731\n",
      "Iteration 263, loss = 0.34130327\n",
      "Iteration 264, loss = 0.34113675\n",
      "Iteration 265, loss = 0.34094335\n",
      "Iteration 266, loss = 0.34079407\n",
      "Iteration 267, loss = 0.34060104\n",
      "Iteration 268, loss = 0.34044055\n",
      "Iteration 269, loss = 0.34028186\n",
      "Iteration 270, loss = 0.34009229\n",
      "Iteration 271, loss = 0.33994053\n",
      "Iteration 272, loss = 0.33976106\n",
      "Iteration 273, loss = 0.33963216\n",
      "Iteration 274, loss = 0.33943808\n",
      "Iteration 275, loss = 0.33927515\n",
      "Iteration 276, loss = 0.33916185\n",
      "Iteration 277, loss = 0.33895285\n",
      "Iteration 278, loss = 0.33879569\n",
      "Iteration 279, loss = 0.33864043\n",
      "Iteration 280, loss = 0.33846809\n",
      "Iteration 281, loss = 0.33832281\n",
      "Iteration 282, loss = 0.33815663\n",
      "Iteration 283, loss = 0.33803002\n",
      "Iteration 284, loss = 0.33784495\n",
      "Iteration 285, loss = 0.33771076\n",
      "Iteration 286, loss = 0.33753949\n",
      "Iteration 287, loss = 0.33738394\n",
      "Iteration 288, loss = 0.33723252\n",
      "Iteration 289, loss = 0.33709505\n",
      "Iteration 290, loss = 0.33694607\n",
      "Iteration 291, loss = 0.33679099\n",
      "Iteration 292, loss = 0.33664098\n",
      "Iteration 293, loss = 0.33649723\n",
      "Iteration 294, loss = 0.33635808\n",
      "Iteration 295, loss = 0.33619688\n",
      "Iteration 296, loss = 0.33606305\n",
      "Iteration 297, loss = 0.33590158\n",
      "Iteration 298, loss = 0.33577872\n",
      "Iteration 299, loss = 0.33563772\n",
      "Iteration 300, loss = 0.33550565\n",
      "Iteration 301, loss = 0.33534160\n",
      "Iteration 302, loss = 0.33520721\n",
      "Iteration 303, loss = 0.33506811\n",
      "Iteration 304, loss = 0.33490880\n",
      "Iteration 305, loss = 0.33478025\n",
      "Iteration 306, loss = 0.33463408\n",
      "Iteration 307, loss = 0.33449515\n",
      "Iteration 308, loss = 0.33441182\n",
      "Iteration 309, loss = 0.33423619\n",
      "Iteration 310, loss = 0.33409865\n",
      "Iteration 311, loss = 0.33397630\n",
      "Iteration 312, loss = 0.33381923\n",
      "Iteration 313, loss = 0.33370670\n",
      "Iteration 314, loss = 0.33355148\n",
      "Iteration 315, loss = 0.33343493\n",
      "Iteration 316, loss = 0.33329440\n",
      "Iteration 317, loss = 0.33316941\n",
      "Iteration 318, loss = 0.33304717\n",
      "Iteration 319, loss = 0.33290407\n",
      "Iteration 320, loss = 0.33278680\n",
      "Iteration 321, loss = 0.33265189\n",
      "Iteration 322, loss = 0.33252661\n",
      "Iteration 323, loss = 0.33240741\n",
      "Iteration 324, loss = 0.33228025\n",
      "Iteration 325, loss = 0.33213544\n",
      "Iteration 326, loss = 0.33201222\n",
      "Iteration 327, loss = 0.33190064\n",
      "Iteration 328, loss = 0.33175220\n",
      "Iteration 329, loss = 0.33166890\n",
      "Iteration 330, loss = 0.33152671\n",
      "Iteration 331, loss = 0.33138924\n",
      "Iteration 332, loss = 0.33127215\n",
      "Iteration 333, loss = 0.33115984\n",
      "Iteration 334, loss = 0.33102514\n",
      "Iteration 335, loss = 0.33091393\n",
      "Iteration 336, loss = 0.33078171\n",
      "Iteration 337, loss = 0.33066329\n",
      "Iteration 338, loss = 0.33056089\n",
      "Iteration 339, loss = 0.33042409\n",
      "Iteration 340, loss = 0.33030324\n",
      "Iteration 341, loss = 0.33019696\n",
      "Iteration 342, loss = 0.33006418\n",
      "Iteration 343, loss = 0.32994816\n",
      "Iteration 344, loss = 0.32983537\n",
      "Iteration 345, loss = 0.32972046\n",
      "Iteration 346, loss = 0.32961170\n",
      "Iteration 347, loss = 0.32949462\n",
      "Iteration 348, loss = 0.32937580\n",
      "Iteration 349, loss = 0.32927156\n",
      "Iteration 350, loss = 0.32917336\n",
      "Iteration 351, loss = 0.32903303\n",
      "Iteration 352, loss = 0.32893595\n",
      "Iteration 353, loss = 0.32883536\n",
      "Iteration 354, loss = 0.32870676\n",
      "Iteration 355, loss = 0.32859052\n",
      "Iteration 356, loss = 0.32848028\n",
      "Iteration 357, loss = 0.32836889\n",
      "Iteration 358, loss = 0.32824694\n",
      "Iteration 359, loss = 0.32817898\n",
      "Iteration 360, loss = 0.32804174\n",
      "Iteration 361, loss = 0.32792083\n",
      "Iteration 362, loss = 0.32784390\n",
      "Iteration 363, loss = 0.32771170\n",
      "Iteration 364, loss = 0.32759189\n",
      "Iteration 365, loss = 0.32748569\n",
      "Iteration 366, loss = 0.32739615\n",
      "Iteration 367, loss = 0.32727342\n",
      "Iteration 368, loss = 0.32717714\n",
      "Iteration 369, loss = 0.32708714\n",
      "Iteration 370, loss = 0.32695900\n",
      "Iteration 371, loss = 0.32684482\n",
      "Iteration 372, loss = 0.32675339\n",
      "Iteration 373, loss = 0.32664861\n",
      "Iteration 374, loss = 0.32656106\n",
      "Iteration 375, loss = 0.32643661\n",
      "Iteration 376, loss = 0.32634218\n",
      "Iteration 377, loss = 0.32621765\n",
      "Iteration 378, loss = 0.32612184\n",
      "Iteration 379, loss = 0.32601810\n",
      "Iteration 380, loss = 0.32590176\n",
      "Iteration 381, loss = 0.32580936\n",
      "Iteration 382, loss = 0.32569707\n",
      "Iteration 383, loss = 0.32559684\n",
      "Iteration 384, loss = 0.32548486\n",
      "Iteration 385, loss = 0.32539633\n",
      "Iteration 386, loss = 0.32530087\n",
      "Iteration 387, loss = 0.32518269\n",
      "Iteration 388, loss = 0.32507996\n",
      "Iteration 389, loss = 0.32500502\n",
      "Iteration 390, loss = 0.32488123\n",
      "Iteration 391, loss = 0.32478317\n",
      "Iteration 392, loss = 0.32468904\n",
      "Iteration 393, loss = 0.32456959\n",
      "Iteration 394, loss = 0.32448339\n",
      "Iteration 395, loss = 0.32438357\n",
      "Iteration 396, loss = 0.32427496\n",
      "Iteration 397, loss = 0.32416983\n",
      "Iteration 398, loss = 0.32406260\n",
      "Iteration 399, loss = 0.32397269\n",
      "Iteration 400, loss = 0.32387856\n",
      "Iteration 401, loss = 0.32376295\n",
      "Iteration 402, loss = 0.32367745\n",
      "Iteration 403, loss = 0.32356252\n",
      "Iteration 404, loss = 0.32346730\n",
      "Iteration 405, loss = 0.32337643\n",
      "Iteration 406, loss = 0.32326201\n",
      "Iteration 407, loss = 0.32317092\n",
      "Iteration 408, loss = 0.32307582\n",
      "Iteration 409, loss = 0.32297847\n",
      "Iteration 410, loss = 0.32287662\n",
      "Iteration 411, loss = 0.32277345\n",
      "Iteration 412, loss = 0.32268783\n",
      "Iteration 413, loss = 0.32258992\n",
      "Iteration 414, loss = 0.32248033\n",
      "Iteration 415, loss = 0.32243009\n",
      "Iteration 416, loss = 0.32229518\n",
      "Iteration 417, loss = 0.32221085\n",
      "Iteration 418, loss = 0.32210870\n",
      "Iteration 419, loss = 0.32202038\n",
      "Iteration 420, loss = 0.32191982\n",
      "Iteration 421, loss = 0.32185208\n",
      "Iteration 422, loss = 0.32174391\n",
      "Iteration 423, loss = 0.32164846\n",
      "Iteration 424, loss = 0.32154229\n",
      "Iteration 425, loss = 0.32144745\n",
      "Iteration 426, loss = 0.32135296\n",
      "Iteration 427, loss = 0.32125222\n",
      "Iteration 428, loss = 0.32116403\n",
      "Iteration 429, loss = 0.32107066\n",
      "Iteration 430, loss = 0.32097647\n",
      "Iteration 431, loss = 0.32088032\n",
      "Iteration 432, loss = 0.32078529\n",
      "Iteration 433, loss = 0.32068638\n",
      "Iteration 434, loss = 0.32059668\n",
      "Iteration 435, loss = 0.32053067\n",
      "Iteration 436, loss = 0.32040486\n",
      "Iteration 437, loss = 0.32032525\n",
      "Iteration 438, loss = 0.32021533\n",
      "Iteration 439, loss = 0.32013228\n",
      "Iteration 440, loss = 0.32004765\n",
      "Iteration 441, loss = 0.31994347\n",
      "Iteration 442, loss = 0.31987255\n",
      "Iteration 443, loss = 0.31976600\n",
      "Iteration 444, loss = 0.31968167\n",
      "Iteration 445, loss = 0.31958177\n",
      "Iteration 446, loss = 0.31948829\n",
      "Iteration 447, loss = 0.31940389\n",
      "Iteration 448, loss = 0.31931085\n",
      "Iteration 449, loss = 0.31921368\n",
      "Iteration 450, loss = 0.31912900\n",
      "Iteration 451, loss = 0.31904406\n",
      "Iteration 452, loss = 0.31894427\n",
      "Iteration 453, loss = 0.31886244\n",
      "Iteration 454, loss = 0.31876705\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70458312\n",
      "Iteration 2, loss = 0.70199832\n",
      "Iteration 3, loss = 0.69806014\n",
      "Iteration 4, loss = 0.69329298\n",
      "Iteration 5, loss = 0.68768737\n",
      "Iteration 6, loss = 0.68188749\n",
      "Iteration 7, loss = 0.67575761\n",
      "Iteration 8, loss = 0.66948568\n",
      "Iteration 9, loss = 0.66313996\n",
      "Iteration 10, loss = 0.65668736\n",
      "Iteration 11, loss = 0.65067534\n",
      "Iteration 12, loss = 0.64437847\n",
      "Iteration 13, loss = 0.63844839\n",
      "Iteration 14, loss = 0.63247123\n",
      "Iteration 15, loss = 0.62665251\n",
      "Iteration 16, loss = 0.62083526\n",
      "Iteration 17, loss = 0.61532602\n",
      "Iteration 18, loss = 0.61009413\n",
      "Iteration 19, loss = 0.60466360\n",
      "Iteration 20, loss = 0.59951029\n",
      "Iteration 21, loss = 0.59450300\n",
      "Iteration 22, loss = 0.58962595\n",
      "Iteration 23, loss = 0.58478974\n",
      "Iteration 24, loss = 0.58008646\n",
      "Iteration 25, loss = 0.57554419\n",
      "Iteration 26, loss = 0.57089641\n",
      "Iteration 27, loss = 0.56662495\n",
      "Iteration 28, loss = 0.56234269\n",
      "Iteration 29, loss = 0.55817724\n",
      "Iteration 30, loss = 0.55402467\n",
      "Iteration 31, loss = 0.55010125\n",
      "Iteration 32, loss = 0.54623190\n",
      "Iteration 33, loss = 0.54234252\n",
      "Iteration 34, loss = 0.53860778\n",
      "Iteration 35, loss = 0.53504869\n",
      "Iteration 36, loss = 0.53146217\n",
      "Iteration 37, loss = 0.52805916\n",
      "Iteration 38, loss = 0.52466487\n",
      "Iteration 39, loss = 0.52119951\n",
      "Iteration 40, loss = 0.51802728\n",
      "Iteration 41, loss = 0.51485955\n",
      "Iteration 42, loss = 0.51175788\n",
      "Iteration 43, loss = 0.50871001\n",
      "Iteration 44, loss = 0.50583009\n",
      "Iteration 45, loss = 0.50297381\n",
      "Iteration 46, loss = 0.50006869\n",
      "Iteration 47, loss = 0.49744839\n",
      "Iteration 48, loss = 0.49472447\n",
      "Iteration 49, loss = 0.49206770\n",
      "Iteration 50, loss = 0.48951562\n",
      "Iteration 51, loss = 0.48701714\n",
      "Iteration 52, loss = 0.48456001\n",
      "Iteration 53, loss = 0.48228792\n",
      "Iteration 54, loss = 0.47996603\n",
      "Iteration 55, loss = 0.47767015\n",
      "Iteration 56, loss = 0.47548646\n",
      "Iteration 57, loss = 0.47331162\n",
      "Iteration 58, loss = 0.47117170\n",
      "Iteration 59, loss = 0.46920247\n",
      "Iteration 60, loss = 0.46716560\n",
      "Iteration 61, loss = 0.46520013\n",
      "Iteration 62, loss = 0.46328335\n",
      "Iteration 63, loss = 0.46141798\n",
      "Iteration 64, loss = 0.45954344\n",
      "Iteration 65, loss = 0.45770581\n",
      "Iteration 66, loss = 0.45600442\n",
      "Iteration 67, loss = 0.45422611\n",
      "Iteration 68, loss = 0.45262383\n",
      "Iteration 69, loss = 0.45100313\n",
      "Iteration 70, loss = 0.44935951\n",
      "Iteration 71, loss = 0.44784574\n",
      "Iteration 72, loss = 0.44628240\n",
      "Iteration 73, loss = 0.44477857\n",
      "Iteration 74, loss = 0.44335086\n",
      "Iteration 75, loss = 0.44189705\n",
      "Iteration 76, loss = 0.44050985\n",
      "Iteration 77, loss = 0.43914480\n",
      "Iteration 78, loss = 0.43784702\n",
      "Iteration 79, loss = 0.43652518\n",
      "Iteration 80, loss = 0.43528498\n",
      "Iteration 81, loss = 0.43398746\n",
      "Iteration 82, loss = 0.43277591\n",
      "Iteration 83, loss = 0.43161738\n",
      "Iteration 84, loss = 0.43042759\n",
      "Iteration 85, loss = 0.42930147\n",
      "Iteration 86, loss = 0.42817847\n",
      "Iteration 87, loss = 0.42712056\n",
      "Iteration 88, loss = 0.42611073\n",
      "Iteration 89, loss = 0.42493606\n",
      "Iteration 90, loss = 0.42394431\n",
      "Iteration 91, loss = 0.42292180\n",
      "Iteration 92, loss = 0.42189500\n",
      "Iteration 93, loss = 0.42096586\n",
      "Iteration 94, loss = 0.41996920\n",
      "Iteration 95, loss = 0.41908366\n",
      "Iteration 96, loss = 0.41810019\n",
      "Iteration 97, loss = 0.41723067\n",
      "Iteration 98, loss = 0.41634776\n",
      "Iteration 99, loss = 0.41542221\n",
      "Iteration 100, loss = 0.41460393\n",
      "Iteration 101, loss = 0.41376978\n",
      "Iteration 102, loss = 0.41289540\n",
      "Iteration 103, loss = 0.41205978\n",
      "Iteration 104, loss = 0.41131757\n",
      "Iteration 105, loss = 0.41051596\n",
      "Iteration 106, loss = 0.40978869\n",
      "Iteration 107, loss = 0.40900789\n",
      "Iteration 108, loss = 0.40829422\n",
      "Iteration 109, loss = 0.40751959\n",
      "Iteration 110, loss = 0.40685374\n",
      "Iteration 111, loss = 0.40608760\n",
      "Iteration 112, loss = 0.40541681\n",
      "Iteration 113, loss = 0.40479104\n",
      "Iteration 114, loss = 0.40408618\n",
      "Iteration 115, loss = 0.40338344\n",
      "Iteration 116, loss = 0.40275387\n",
      "Iteration 117, loss = 0.40208372\n",
      "Iteration 118, loss = 0.40150307\n",
      "Iteration 119, loss = 0.40084265\n",
      "Iteration 120, loss = 0.40023513\n",
      "Iteration 121, loss = 0.39961688\n",
      "Iteration 122, loss = 0.39902227\n",
      "Iteration 123, loss = 0.39843988\n",
      "Iteration 124, loss = 0.39785561\n",
      "Iteration 125, loss = 0.39725102\n",
      "Iteration 126, loss = 0.39668945\n",
      "Iteration 127, loss = 0.39617032\n",
      "Iteration 128, loss = 0.39559808\n",
      "Iteration 129, loss = 0.39499741\n",
      "Iteration 130, loss = 0.39449252\n",
      "Iteration 131, loss = 0.39393916\n",
      "Iteration 132, loss = 0.39341283\n",
      "Iteration 133, loss = 0.39289685\n",
      "Iteration 134, loss = 0.39236861\n",
      "Iteration 135, loss = 0.39186060\n",
      "Iteration 136, loss = 0.39131385\n",
      "Iteration 137, loss = 0.39088862\n",
      "Iteration 138, loss = 0.39035995\n",
      "Iteration 139, loss = 0.38987678\n",
      "Iteration 140, loss = 0.38939304\n",
      "Iteration 141, loss = 0.38888517\n",
      "Iteration 142, loss = 0.38840499\n",
      "Iteration 143, loss = 0.38795192\n",
      "Iteration 144, loss = 0.38751486\n",
      "Iteration 145, loss = 0.38702175\n",
      "Iteration 146, loss = 0.38658012\n",
      "Iteration 147, loss = 0.38612483\n",
      "Iteration 148, loss = 0.38569120\n",
      "Iteration 149, loss = 0.38520645\n",
      "Iteration 150, loss = 0.38480079\n",
      "Iteration 151, loss = 0.38437237\n",
      "Iteration 152, loss = 0.38392942\n",
      "Iteration 153, loss = 0.38347455\n",
      "Iteration 154, loss = 0.38304798\n",
      "Iteration 155, loss = 0.38263043\n",
      "Iteration 156, loss = 0.38220044\n",
      "Iteration 157, loss = 0.38181132\n",
      "Iteration 158, loss = 0.38138622\n",
      "Iteration 159, loss = 0.38094404\n",
      "Iteration 160, loss = 0.38056252\n",
      "Iteration 161, loss = 0.38015232\n",
      "Iteration 162, loss = 0.37974944\n",
      "Iteration 163, loss = 0.37938556\n",
      "Iteration 164, loss = 0.37897638\n",
      "Iteration 165, loss = 0.37859414\n",
      "Iteration 166, loss = 0.37823554\n",
      "Iteration 167, loss = 0.37781039\n",
      "Iteration 168, loss = 0.37748369\n",
      "Iteration 169, loss = 0.37710111\n",
      "Iteration 170, loss = 0.37675466\n",
      "Iteration 171, loss = 0.37640061\n",
      "Iteration 172, loss = 0.37600435\n",
      "Iteration 173, loss = 0.37565680\n",
      "Iteration 174, loss = 0.37528457\n",
      "Iteration 175, loss = 0.37495539\n",
      "Iteration 176, loss = 0.37461926\n",
      "Iteration 177, loss = 0.37427043\n",
      "Iteration 178, loss = 0.37392982\n",
      "Iteration 179, loss = 0.37358966\n",
      "Iteration 180, loss = 0.37324021\n",
      "Iteration 181, loss = 0.37296921\n",
      "Iteration 182, loss = 0.37260650\n",
      "Iteration 183, loss = 0.37231240\n",
      "Iteration 184, loss = 0.37197972\n",
      "Iteration 185, loss = 0.37165101\n",
      "Iteration 186, loss = 0.37134934\n",
      "Iteration 187, loss = 0.37104604\n",
      "Iteration 188, loss = 0.37072334\n",
      "Iteration 189, loss = 0.37042003\n",
      "Iteration 190, loss = 0.37011303\n",
      "Iteration 191, loss = 0.36983652\n",
      "Iteration 192, loss = 0.36952991\n",
      "Iteration 193, loss = 0.36924447\n",
      "Iteration 194, loss = 0.36897647\n",
      "Iteration 195, loss = 0.36866278\n",
      "Iteration 196, loss = 0.36839342\n",
      "Iteration 197, loss = 0.36811700\n",
      "Iteration 198, loss = 0.36779948\n",
      "Iteration 199, loss = 0.36753971\n",
      "Iteration 200, loss = 0.36726009\n",
      "Iteration 201, loss = 0.36699296\n",
      "Iteration 202, loss = 0.36671371\n",
      "Iteration 203, loss = 0.36646158\n",
      "Iteration 204, loss = 0.36615360\n",
      "Iteration 205, loss = 0.36590847\n",
      "Iteration 206, loss = 0.36564369\n",
      "Iteration 207, loss = 0.36538368\n",
      "Iteration 208, loss = 0.36513251\n",
      "Iteration 209, loss = 0.36489352\n",
      "Iteration 210, loss = 0.36462334\n",
      "Iteration 211, loss = 0.36438568\n",
      "Iteration 212, loss = 0.36414202\n",
      "Iteration 213, loss = 0.36387954\n",
      "Iteration 214, loss = 0.36362848\n",
      "Iteration 215, loss = 0.36337878\n",
      "Iteration 216, loss = 0.36313480\n",
      "Iteration 217, loss = 0.36289324\n",
      "Iteration 218, loss = 0.36268397\n",
      "Iteration 219, loss = 0.36241389\n",
      "Iteration 220, loss = 0.36219162\n",
      "Iteration 221, loss = 0.36198798\n",
      "Iteration 222, loss = 0.36173650\n",
      "Iteration 223, loss = 0.36151643\n",
      "Iteration 224, loss = 0.36128890\n",
      "Iteration 225, loss = 0.36104936\n",
      "Iteration 226, loss = 0.36084103\n",
      "Iteration 227, loss = 0.36062842\n",
      "Iteration 228, loss = 0.36041379\n",
      "Iteration 229, loss = 0.36018548\n",
      "Iteration 230, loss = 0.35996964\n",
      "Iteration 231, loss = 0.35975061\n",
      "Iteration 232, loss = 0.35954583\n",
      "Iteration 233, loss = 0.35933655\n",
      "Iteration 234, loss = 0.35912615\n",
      "Iteration 235, loss = 0.35890473\n",
      "Iteration 236, loss = 0.35869032\n",
      "Iteration 237, loss = 0.35851071\n",
      "Iteration 238, loss = 0.35829610\n",
      "Iteration 239, loss = 0.35812590\n",
      "Iteration 240, loss = 0.35788791\n",
      "Iteration 241, loss = 0.35768276\n",
      "Iteration 242, loss = 0.35751656\n",
      "Iteration 243, loss = 0.35729491\n",
      "Iteration 244, loss = 0.35709518\n",
      "Iteration 245, loss = 0.35688224\n",
      "Iteration 246, loss = 0.35671498\n",
      "Iteration 247, loss = 0.35651253\n",
      "Iteration 248, loss = 0.35632846\n",
      "Iteration 249, loss = 0.35612985\n",
      "Iteration 250, loss = 0.35594574\n",
      "Iteration 251, loss = 0.35575159\n",
      "Iteration 252, loss = 0.35556218\n",
      "Iteration 253, loss = 0.35539104\n",
      "Iteration 254, loss = 0.35521210\n",
      "Iteration 255, loss = 0.35503839\n",
      "Iteration 256, loss = 0.35480518\n",
      "Iteration 257, loss = 0.35462258\n",
      "Iteration 258, loss = 0.35446628\n",
      "Iteration 259, loss = 0.35428261\n",
      "Iteration 260, loss = 0.35409366\n",
      "Iteration 261, loss = 0.35390991\n",
      "Iteration 262, loss = 0.35372274\n",
      "Iteration 263, loss = 0.35357127\n",
      "Iteration 264, loss = 0.35340372\n",
      "Iteration 265, loss = 0.35321056\n",
      "Iteration 266, loss = 0.35304752\n",
      "Iteration 267, loss = 0.35286328\n",
      "Iteration 268, loss = 0.35268502\n",
      "Iteration 269, loss = 0.35253195\n",
      "Iteration 270, loss = 0.35236296\n",
      "Iteration 271, loss = 0.35218783\n",
      "Iteration 272, loss = 0.35201433\n",
      "Iteration 273, loss = 0.35185035\n",
      "Iteration 274, loss = 0.35170051\n",
      "Iteration 275, loss = 0.35151658\n",
      "Iteration 276, loss = 0.35134822\n",
      "Iteration 277, loss = 0.35118657\n",
      "Iteration 278, loss = 0.35107275\n",
      "Iteration 279, loss = 0.35087699\n",
      "Iteration 280, loss = 0.35071609\n",
      "Iteration 281, loss = 0.35055192\n",
      "Iteration 282, loss = 0.35039042\n",
      "Iteration 283, loss = 0.35022489\n",
      "Iteration 284, loss = 0.35008353\n",
      "Iteration 285, loss = 0.34994204\n",
      "Iteration 286, loss = 0.34977187\n",
      "Iteration 287, loss = 0.34961123\n",
      "Iteration 288, loss = 0.34944889\n",
      "Iteration 289, loss = 0.34929328\n",
      "Iteration 290, loss = 0.34915522\n",
      "Iteration 291, loss = 0.34900015\n",
      "Iteration 292, loss = 0.34884651\n",
      "Iteration 293, loss = 0.34870027\n",
      "Iteration 294, loss = 0.34859683\n",
      "Iteration 295, loss = 0.34840595\n",
      "Iteration 296, loss = 0.34825174\n",
      "Iteration 297, loss = 0.34810390\n",
      "Iteration 298, loss = 0.34800870\n",
      "Iteration 299, loss = 0.34780721\n",
      "Iteration 300, loss = 0.34768535\n",
      "Iteration 301, loss = 0.34752200\n",
      "Iteration 302, loss = 0.34737202\n",
      "Iteration 303, loss = 0.34723587\n",
      "Iteration 304, loss = 0.34708817\n",
      "Iteration 305, loss = 0.34697190\n",
      "Iteration 306, loss = 0.34680891\n",
      "Iteration 307, loss = 0.34666284\n",
      "Iteration 308, loss = 0.34655391\n",
      "Iteration 309, loss = 0.34638241\n",
      "Iteration 310, loss = 0.34625385\n",
      "Iteration 311, loss = 0.34610917\n",
      "Iteration 312, loss = 0.34597514\n",
      "Iteration 313, loss = 0.34583291\n",
      "Iteration 314, loss = 0.34572487\n",
      "Iteration 315, loss = 0.34557802\n",
      "Iteration 316, loss = 0.34543134\n",
      "Iteration 317, loss = 0.34529225\n",
      "Iteration 318, loss = 0.34516510\n",
      "Iteration 319, loss = 0.34505431\n",
      "Iteration 320, loss = 0.34490612\n",
      "Iteration 321, loss = 0.34477222\n",
      "Iteration 322, loss = 0.34463415\n",
      "Iteration 323, loss = 0.34450259\n",
      "Iteration 324, loss = 0.34437911\n",
      "Iteration 325, loss = 0.34426109\n",
      "Iteration 326, loss = 0.34412541\n",
      "Iteration 327, loss = 0.34398737\n",
      "Iteration 328, loss = 0.34387001\n",
      "Iteration 329, loss = 0.34373898\n",
      "Iteration 330, loss = 0.34360462\n",
      "Iteration 331, loss = 0.34347872\n",
      "Iteration 332, loss = 0.34336843\n",
      "Iteration 333, loss = 0.34323315\n",
      "Iteration 334, loss = 0.34310199\n",
      "Iteration 335, loss = 0.34297959\n",
      "Iteration 336, loss = 0.34285930\n",
      "Iteration 337, loss = 0.34272909\n",
      "Iteration 338, loss = 0.34260969\n",
      "Iteration 339, loss = 0.34249123\n",
      "Iteration 340, loss = 0.34236650\n",
      "Iteration 341, loss = 0.34226237\n",
      "Iteration 342, loss = 0.34214509\n",
      "Iteration 343, loss = 0.34199903\n",
      "Iteration 344, loss = 0.34188482\n",
      "Iteration 345, loss = 0.34176734\n",
      "Iteration 346, loss = 0.34164661\n",
      "Iteration 347, loss = 0.34152303\n",
      "Iteration 348, loss = 0.34139836\n",
      "Iteration 349, loss = 0.34130855\n",
      "Iteration 350, loss = 0.34116353\n",
      "Iteration 351, loss = 0.34105234\n",
      "Iteration 352, loss = 0.34092817\n",
      "Iteration 353, loss = 0.34080577\n",
      "Iteration 354, loss = 0.34071120\n",
      "Iteration 355, loss = 0.34057840\n",
      "Iteration 356, loss = 0.34048320\n",
      "Iteration 357, loss = 0.34036621\n",
      "Iteration 358, loss = 0.34022407\n",
      "Iteration 359, loss = 0.34009906\n",
      "Iteration 360, loss = 0.33998892\n",
      "Iteration 361, loss = 0.33988420\n",
      "Iteration 362, loss = 0.33978692\n",
      "Iteration 363, loss = 0.33965719\n",
      "Iteration 364, loss = 0.33955762\n",
      "Iteration 365, loss = 0.33945503\n",
      "Iteration 366, loss = 0.33931422\n",
      "Iteration 367, loss = 0.33918936\n",
      "Iteration 368, loss = 0.33908053\n",
      "Iteration 369, loss = 0.33897872\n",
      "Iteration 370, loss = 0.33886118\n",
      "Iteration 371, loss = 0.33874876\n",
      "Iteration 372, loss = 0.33863140\n",
      "Iteration 373, loss = 0.33853042\n",
      "Iteration 374, loss = 0.33840243\n",
      "Iteration 375, loss = 0.33831569\n",
      "Iteration 376, loss = 0.33820743\n",
      "Iteration 377, loss = 0.33810442\n",
      "Iteration 378, loss = 0.33796199\n",
      "Iteration 379, loss = 0.33785818\n",
      "Iteration 380, loss = 0.33775335\n",
      "Iteration 381, loss = 0.33764957\n",
      "Iteration 382, loss = 0.33752960\n",
      "Iteration 383, loss = 0.33743818\n",
      "Iteration 384, loss = 0.33732087\n",
      "Iteration 385, loss = 0.33720285\n",
      "Iteration 386, loss = 0.33709005\n",
      "Iteration 387, loss = 0.33701490\n",
      "Iteration 388, loss = 0.33689002\n",
      "Iteration 389, loss = 0.33678994\n",
      "Iteration 390, loss = 0.33669735\n",
      "Iteration 391, loss = 0.33655920\n",
      "Iteration 392, loss = 0.33648367\n",
      "Iteration 393, loss = 0.33637453\n",
      "Iteration 394, loss = 0.33624008\n",
      "Iteration 395, loss = 0.33614228\n",
      "Iteration 396, loss = 0.33604135\n",
      "Iteration 397, loss = 0.33592347\n",
      "Iteration 398, loss = 0.33584072\n",
      "Iteration 399, loss = 0.33573353\n",
      "Iteration 400, loss = 0.33563731\n",
      "Iteration 401, loss = 0.33553003\n",
      "Iteration 402, loss = 0.33541435\n",
      "Iteration 403, loss = 0.33531363\n",
      "Iteration 404, loss = 0.33521700\n",
      "Iteration 405, loss = 0.33510513\n",
      "Iteration 406, loss = 0.33503588\n",
      "Iteration 407, loss = 0.33493805\n",
      "Iteration 408, loss = 0.33481821\n",
      "Iteration 409, loss = 0.33470280\n",
      "Iteration 410, loss = 0.33459954\n",
      "Iteration 411, loss = 0.33450690\n",
      "Iteration 412, loss = 0.33439813\n",
      "Iteration 413, loss = 0.33431819\n",
      "Iteration 414, loss = 0.33419669\n",
      "Iteration 415, loss = 0.33410317\n",
      "Iteration 416, loss = 0.33400361\n",
      "Iteration 417, loss = 0.33389158\n",
      "Iteration 418, loss = 0.33380727\n",
      "Iteration 419, loss = 0.33370791\n",
      "Iteration 420, loss = 0.33360956\n",
      "Iteration 421, loss = 0.33352240\n",
      "Iteration 422, loss = 0.33339855\n",
      "Iteration 423, loss = 0.33332726\n",
      "Iteration 424, loss = 0.33322808\n",
      "Iteration 425, loss = 0.33314024\n",
      "Iteration 426, loss = 0.33302981\n",
      "Iteration 427, loss = 0.33292804\n",
      "Iteration 428, loss = 0.33282585\n",
      "Iteration 429, loss = 0.33273613\n",
      "Iteration 430, loss = 0.33262981\n",
      "Iteration 431, loss = 0.33254871\n",
      "Iteration 432, loss = 0.33244040\n",
      "Iteration 433, loss = 0.33234582\n",
      "Iteration 434, loss = 0.33224549\n",
      "Iteration 435, loss = 0.33216024\n",
      "Iteration 436, loss = 0.33204993\n",
      "Iteration 437, loss = 0.33195435\n",
      "Iteration 438, loss = 0.33186899\n",
      "Iteration 439, loss = 0.33176387\n",
      "Iteration 440, loss = 0.33168095\n",
      "Iteration 441, loss = 0.33160085\n",
      "Iteration 442, loss = 0.33147624\n",
      "Iteration 443, loss = 0.33139983\n",
      "Iteration 444, loss = 0.33128945\n",
      "Iteration 445, loss = 0.33121810\n",
      "Iteration 446, loss = 0.33113289\n",
      "Iteration 447, loss = 0.33101243\n",
      "Iteration 448, loss = 0.33092660\n",
      "Iteration 449, loss = 0.33082311\n",
      "Iteration 450, loss = 0.33074333\n",
      "Iteration 451, loss = 0.33064387\n",
      "Iteration 452, loss = 0.33058817\n",
      "Iteration 453, loss = 0.33046115\n",
      "Iteration 454, loss = 0.33039298\n",
      "Iteration 455, loss = 0.33028471\n",
      "Iteration 456, loss = 0.33019092\n",
      "Iteration 457, loss = 0.33010157\n",
      "Iteration 458, loss = 0.33001794\n",
      "Iteration 459, loss = 0.32993277\n",
      "Iteration 460, loss = 0.32982117\n",
      "Iteration 461, loss = 0.32974942\n",
      "Iteration 462, loss = 0.32964903\n",
      "Iteration 463, loss = 0.32957657\n",
      "Iteration 464, loss = 0.32948649\n",
      "Iteration 465, loss = 0.32937806\n",
      "Iteration 466, loss = 0.32930314\n",
      "Iteration 467, loss = 0.32921923\n",
      "Iteration 468, loss = 0.32910879\n",
      "Iteration 469, loss = 0.32902354\n",
      "Iteration 470, loss = 0.32892934\n",
      "Iteration 471, loss = 0.32884193\n",
      "Iteration 472, loss = 0.32875378\n",
      "Iteration 473, loss = 0.32867697\n",
      "Iteration 474, loss = 0.32858852\n",
      "Iteration 475, loss = 0.32850056\n",
      "Iteration 476, loss = 0.32840736\n",
      "Iteration 477, loss = 0.32831366\n",
      "Iteration 478, loss = 0.32823670\n",
      "Iteration 479, loss = 0.32813590\n",
      "Iteration 480, loss = 0.32804902\n",
      "Iteration 481, loss = 0.32795973\n",
      "Iteration 482, loss = 0.32788148\n",
      "Iteration 483, loss = 0.32779797\n",
      "Iteration 484, loss = 0.32770438\n",
      "Iteration 485, loss = 0.32760511\n",
      "Iteration 486, loss = 0.32752246\n",
      "Iteration 487, loss = 0.32743659\n",
      "Iteration 488, loss = 0.32734575\n",
      "Iteration 489, loss = 0.32725808\n",
      "Iteration 490, loss = 0.32718401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69564723\n",
      "Iteration 2, loss = 0.69562869\n",
      "Iteration 3, loss = 0.69563324\n",
      "Iteration 4, loss = 0.69557401\n",
      "Iteration 5, loss = 0.69550739\n",
      "Iteration 6, loss = 0.69550060\n",
      "Iteration 7, loss = 0.69540814\n",
      "Iteration 8, loss = 0.69537629\n",
      "Iteration 9, loss = 0.69534074\n",
      "Iteration 10, loss = 0.69530517\n",
      "Iteration 11, loss = 0.69533687\n",
      "Iteration 12, loss = 0.69524507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70868025\n",
      "Iteration 2, loss = 0.70599165\n",
      "Iteration 3, loss = 0.70187834\n",
      "Iteration 4, loss = 0.69680927\n",
      "Iteration 5, loss = 0.69099706\n",
      "Iteration 6, loss = 0.68488374\n",
      "Iteration 7, loss = 0.67833372\n",
      "Iteration 8, loss = 0.67184373\n",
      "Iteration 9, loss = 0.66527639\n",
      "Iteration 10, loss = 0.65838837\n",
      "Iteration 11, loss = 0.65192967\n",
      "Iteration 12, loss = 0.64527205\n",
      "Iteration 13, loss = 0.63888899\n",
      "Iteration 14, loss = 0.63259115\n",
      "Iteration 15, loss = 0.62642464\n",
      "Iteration 16, loss = 0.62015398\n",
      "Iteration 17, loss = 0.61418200\n",
      "Iteration 18, loss = 0.60857130\n",
      "Iteration 19, loss = 0.60281714\n",
      "Iteration 20, loss = 0.59725983\n",
      "Iteration 21, loss = 0.59175439\n",
      "Iteration 22, loss = 0.58645985\n",
      "Iteration 23, loss = 0.58133346\n",
      "Iteration 24, loss = 0.57616219\n",
      "Iteration 25, loss = 0.57120135\n",
      "Iteration 26, loss = 0.56621876\n",
      "Iteration 27, loss = 0.56138235\n",
      "Iteration 28, loss = 0.55666634\n",
      "Iteration 29, loss = 0.55214830\n",
      "Iteration 30, loss = 0.54758695\n",
      "Iteration 31, loss = 0.54318815\n",
      "Iteration 32, loss = 0.53886414\n",
      "Iteration 33, loss = 0.53453350\n",
      "Iteration 34, loss = 0.53034037\n",
      "Iteration 35, loss = 0.52639431\n",
      "Iteration 36, loss = 0.52237513\n",
      "Iteration 37, loss = 0.51843766\n",
      "Iteration 38, loss = 0.51466378\n",
      "Iteration 39, loss = 0.51083071\n",
      "Iteration 40, loss = 0.50720871\n",
      "Iteration 41, loss = 0.50362474\n",
      "Iteration 42, loss = 0.50020780\n",
      "Iteration 43, loss = 0.49670359\n",
      "Iteration 44, loss = 0.49339388\n",
      "Iteration 45, loss = 0.49015610\n",
      "Iteration 46, loss = 0.48685623\n",
      "Iteration 47, loss = 0.48375399\n",
      "Iteration 48, loss = 0.48065069\n",
      "Iteration 49, loss = 0.47768048\n",
      "Iteration 50, loss = 0.47475305\n",
      "Iteration 51, loss = 0.47185791\n",
      "Iteration 52, loss = 0.46904575\n",
      "Iteration 53, loss = 0.46639139\n",
      "Iteration 54, loss = 0.46365013\n",
      "Iteration 55, loss = 0.46106828\n",
      "Iteration 56, loss = 0.45845934\n",
      "Iteration 57, loss = 0.45593705\n",
      "Iteration 58, loss = 0.45359794\n",
      "Iteration 59, loss = 0.45121710\n",
      "Iteration 60, loss = 0.44887685\n",
      "Iteration 61, loss = 0.44659737\n",
      "Iteration 62, loss = 0.44433749\n",
      "Iteration 63, loss = 0.44215581\n",
      "Iteration 64, loss = 0.44002537\n",
      "Iteration 65, loss = 0.43792708\n",
      "Iteration 66, loss = 0.43595025\n",
      "Iteration 67, loss = 0.43393312\n",
      "Iteration 68, loss = 0.43209857\n",
      "Iteration 69, loss = 0.43016861\n",
      "Iteration 70, loss = 0.42833074\n",
      "Iteration 71, loss = 0.42659784\n",
      "Iteration 72, loss = 0.42476624\n",
      "Iteration 73, loss = 0.42306733\n",
      "Iteration 74, loss = 0.42146751\n",
      "Iteration 75, loss = 0.41981274\n",
      "Iteration 76, loss = 0.41821011\n",
      "Iteration 77, loss = 0.41670825\n",
      "Iteration 78, loss = 0.41517621\n",
      "Iteration 79, loss = 0.41366908\n",
      "Iteration 80, loss = 0.41226504\n",
      "Iteration 81, loss = 0.41084974\n",
      "Iteration 82, loss = 0.40946878\n",
      "Iteration 83, loss = 0.40814027\n",
      "Iteration 84, loss = 0.40681380\n",
      "Iteration 85, loss = 0.40556609\n",
      "Iteration 86, loss = 0.40430055\n",
      "Iteration 87, loss = 0.40309851\n",
      "Iteration 88, loss = 0.40191600\n",
      "Iteration 89, loss = 0.40071040\n",
      "Iteration 90, loss = 0.39954758\n",
      "Iteration 91, loss = 0.39843484\n",
      "Iteration 92, loss = 0.39734373\n",
      "Iteration 93, loss = 0.39624473\n",
      "Iteration 94, loss = 0.39523864\n",
      "Iteration 95, loss = 0.39420448\n",
      "Iteration 96, loss = 0.39321407\n",
      "Iteration 97, loss = 0.39221651\n",
      "Iteration 98, loss = 0.39124859\n",
      "Iteration 99, loss = 0.39031499\n",
      "Iteration 100, loss = 0.38940582\n",
      "Iteration 101, loss = 0.38850013\n",
      "Iteration 102, loss = 0.38757698\n",
      "Iteration 103, loss = 0.38671091\n",
      "Iteration 104, loss = 0.38589063\n",
      "Iteration 105, loss = 0.38502492\n",
      "Iteration 106, loss = 0.38425785\n",
      "Iteration 107, loss = 0.38341517\n",
      "Iteration 108, loss = 0.38260928\n",
      "Iteration 109, loss = 0.38187567\n",
      "Iteration 110, loss = 0.38112507\n",
      "Iteration 111, loss = 0.38044673\n",
      "Iteration 112, loss = 0.37965440\n",
      "Iteration 113, loss = 0.37898405\n",
      "Iteration 114, loss = 0.37825075\n",
      "Iteration 115, loss = 0.37754135\n",
      "Iteration 116, loss = 0.37686533\n",
      "Iteration 117, loss = 0.37620416\n",
      "Iteration 118, loss = 0.37555409\n",
      "Iteration 119, loss = 0.37488574\n",
      "Iteration 120, loss = 0.37425363\n",
      "Iteration 121, loss = 0.37364408\n",
      "Iteration 122, loss = 0.37302591\n",
      "Iteration 123, loss = 0.37243500\n",
      "Iteration 124, loss = 0.37186554\n",
      "Iteration 125, loss = 0.37125944\n",
      "Iteration 126, loss = 0.37070314\n",
      "Iteration 127, loss = 0.37017630\n",
      "Iteration 128, loss = 0.36956554\n",
      "Iteration 129, loss = 0.36902195\n",
      "Iteration 130, loss = 0.36850595\n",
      "Iteration 131, loss = 0.36793540\n",
      "Iteration 132, loss = 0.36742716\n",
      "Iteration 133, loss = 0.36696176\n",
      "Iteration 134, loss = 0.36638953\n",
      "Iteration 135, loss = 0.36590313\n",
      "Iteration 136, loss = 0.36543163\n",
      "Iteration 137, loss = 0.36493571\n",
      "Iteration 138, loss = 0.36445376\n",
      "Iteration 139, loss = 0.36399372\n",
      "Iteration 140, loss = 0.36352417\n",
      "Iteration 141, loss = 0.36304976\n",
      "Iteration 142, loss = 0.36259671\n",
      "Iteration 143, loss = 0.36215621\n",
      "Iteration 144, loss = 0.36172310\n",
      "Iteration 145, loss = 0.36128080\n",
      "Iteration 146, loss = 0.36084281\n",
      "Iteration 147, loss = 0.36039350\n",
      "Iteration 148, loss = 0.35997347\n",
      "Iteration 149, loss = 0.35957080\n",
      "Iteration 150, loss = 0.35917107\n",
      "Iteration 151, loss = 0.35875941\n",
      "Iteration 152, loss = 0.35834567\n",
      "Iteration 153, loss = 0.35794133\n",
      "Iteration 154, loss = 0.35754879\n",
      "Iteration 155, loss = 0.35715899\n",
      "Iteration 156, loss = 0.35676088\n",
      "Iteration 157, loss = 0.35637813\n",
      "Iteration 158, loss = 0.35600315\n",
      "Iteration 159, loss = 0.35563017\n",
      "Iteration 160, loss = 0.35527611\n",
      "Iteration 161, loss = 0.35490234\n",
      "Iteration 162, loss = 0.35454254\n",
      "Iteration 163, loss = 0.35419649\n",
      "Iteration 164, loss = 0.35384318\n",
      "Iteration 165, loss = 0.35350815\n",
      "Iteration 166, loss = 0.35314300\n",
      "Iteration 167, loss = 0.35279635\n",
      "Iteration 168, loss = 0.35249606\n",
      "Iteration 169, loss = 0.35212800\n",
      "Iteration 170, loss = 0.35179999\n",
      "Iteration 171, loss = 0.35152232\n",
      "Iteration 172, loss = 0.35117755\n",
      "Iteration 173, loss = 0.35085810\n",
      "Iteration 174, loss = 0.35054138\n",
      "Iteration 175, loss = 0.35023752\n",
      "Iteration 176, loss = 0.34993097\n",
      "Iteration 177, loss = 0.34965366\n",
      "Iteration 178, loss = 0.34931490\n",
      "Iteration 179, loss = 0.34901810\n",
      "Iteration 180, loss = 0.34871940\n",
      "Iteration 181, loss = 0.34843410\n",
      "Iteration 182, loss = 0.34812914\n",
      "Iteration 183, loss = 0.34784154\n",
      "Iteration 184, loss = 0.34754057\n",
      "Iteration 185, loss = 0.34729263\n",
      "Iteration 186, loss = 0.34698588\n",
      "Iteration 187, loss = 0.34669476\n",
      "Iteration 188, loss = 0.34641925\n",
      "Iteration 189, loss = 0.34614478\n",
      "Iteration 190, loss = 0.34588005\n",
      "Iteration 191, loss = 0.34557597\n",
      "Iteration 192, loss = 0.34532094\n",
      "Iteration 193, loss = 0.34502467\n",
      "Iteration 194, loss = 0.34478262\n",
      "Iteration 195, loss = 0.34449353\n",
      "Iteration 196, loss = 0.34427370\n",
      "Iteration 197, loss = 0.34399349\n",
      "Iteration 198, loss = 0.34368809\n",
      "Iteration 199, loss = 0.34344487\n",
      "Iteration 200, loss = 0.34320161\n",
      "Iteration 201, loss = 0.34293593\n",
      "Iteration 202, loss = 0.34267470\n",
      "Iteration 203, loss = 0.34243266\n",
      "Iteration 204, loss = 0.34218276\n",
      "Iteration 205, loss = 0.34193961\n",
      "Iteration 206, loss = 0.34169181\n",
      "Iteration 207, loss = 0.34145898\n",
      "Iteration 208, loss = 0.34121434\n",
      "Iteration 209, loss = 0.34097579\n",
      "Iteration 210, loss = 0.34073902\n",
      "Iteration 211, loss = 0.34049160\n",
      "Iteration 212, loss = 0.34027792\n",
      "Iteration 213, loss = 0.34002617\n",
      "Iteration 214, loss = 0.33981371\n",
      "Iteration 215, loss = 0.33957051\n",
      "Iteration 216, loss = 0.33934980\n",
      "Iteration 217, loss = 0.33913430\n",
      "Iteration 218, loss = 0.33889894\n",
      "Iteration 219, loss = 0.33869047\n",
      "Iteration 220, loss = 0.33845901\n",
      "Iteration 221, loss = 0.33822508\n",
      "Iteration 222, loss = 0.33801578\n",
      "Iteration 223, loss = 0.33780370\n",
      "Iteration 224, loss = 0.33757206\n",
      "Iteration 225, loss = 0.33735236\n",
      "Iteration 226, loss = 0.33715060\n",
      "Iteration 227, loss = 0.33693456\n",
      "Iteration 228, loss = 0.33672685\n",
      "Iteration 229, loss = 0.33649711\n",
      "Iteration 230, loss = 0.33630084\n",
      "Iteration 231, loss = 0.33610130\n",
      "Iteration 232, loss = 0.33588571\n",
      "Iteration 233, loss = 0.33571424\n",
      "Iteration 234, loss = 0.33550173\n",
      "Iteration 235, loss = 0.33527956\n",
      "Iteration 236, loss = 0.33510607\n",
      "Iteration 237, loss = 0.33489255\n",
      "Iteration 238, loss = 0.33469091\n",
      "Iteration 239, loss = 0.33450035\n",
      "Iteration 240, loss = 0.33431729\n",
      "Iteration 241, loss = 0.33412445\n",
      "Iteration 242, loss = 0.33395154\n",
      "Iteration 243, loss = 0.33373963\n",
      "Iteration 244, loss = 0.33357266\n",
      "Iteration 245, loss = 0.33336803\n",
      "Iteration 246, loss = 0.33317240\n",
      "Iteration 247, loss = 0.33300334\n",
      "Iteration 248, loss = 0.33282658\n",
      "Iteration 249, loss = 0.33265541\n",
      "Iteration 250, loss = 0.33245663\n",
      "Iteration 251, loss = 0.33228792\n",
      "Iteration 252, loss = 0.33209381\n",
      "Iteration 253, loss = 0.33193444\n",
      "Iteration 254, loss = 0.33175174\n",
      "Iteration 255, loss = 0.33157923\n",
      "Iteration 256, loss = 0.33141204\n",
      "Iteration 257, loss = 0.33122845\n",
      "Iteration 258, loss = 0.33108012\n",
      "Iteration 259, loss = 0.33088881\n",
      "Iteration 260, loss = 0.33071185\n",
      "Iteration 261, loss = 0.33058205\n",
      "Iteration 262, loss = 0.33039691\n",
      "Iteration 263, loss = 0.33024664\n",
      "Iteration 264, loss = 0.33004596\n",
      "Iteration 265, loss = 0.32990437\n",
      "Iteration 266, loss = 0.32974400\n",
      "Iteration 267, loss = 0.32957279\n",
      "Iteration 268, loss = 0.32941573\n",
      "Iteration 269, loss = 0.32927608\n",
      "Iteration 270, loss = 0.32910399\n",
      "Iteration 271, loss = 0.32893778\n",
      "Iteration 272, loss = 0.32880775\n",
      "Iteration 273, loss = 0.32863119\n",
      "Iteration 274, loss = 0.32847584\n",
      "Iteration 275, loss = 0.32831037\n",
      "Iteration 276, loss = 0.32816969\n",
      "Iteration 277, loss = 0.32800132\n",
      "Iteration 278, loss = 0.32788304\n",
      "Iteration 279, loss = 0.32769743\n",
      "Iteration 280, loss = 0.32755238\n",
      "Iteration 281, loss = 0.32740834\n",
      "Iteration 282, loss = 0.32725159\n",
      "Iteration 283, loss = 0.32709634\n",
      "Iteration 284, loss = 0.32695926\n",
      "Iteration 285, loss = 0.32684330\n",
      "Iteration 286, loss = 0.32666566\n",
      "Iteration 287, loss = 0.32651270\n",
      "Iteration 288, loss = 0.32637098\n",
      "Iteration 289, loss = 0.32623957\n",
      "Iteration 290, loss = 0.32609534\n",
      "Iteration 291, loss = 0.32596896\n",
      "Iteration 292, loss = 0.32582145\n",
      "Iteration 293, loss = 0.32568385\n",
      "Iteration 294, loss = 0.32559027\n",
      "Iteration 295, loss = 0.32539774\n",
      "Iteration 296, loss = 0.32525241\n",
      "Iteration 297, loss = 0.32513313\n",
      "Iteration 298, loss = 0.32499186\n",
      "Iteration 299, loss = 0.32484594\n",
      "Iteration 300, loss = 0.32474232\n",
      "Iteration 301, loss = 0.32458486\n",
      "Iteration 302, loss = 0.32443449\n",
      "Iteration 303, loss = 0.32430135\n",
      "Iteration 304, loss = 0.32418739\n",
      "Iteration 305, loss = 0.32405528\n",
      "Iteration 306, loss = 0.32391875\n",
      "Iteration 307, loss = 0.32378149\n",
      "Iteration 308, loss = 0.32365514\n",
      "Iteration 309, loss = 0.32352491\n",
      "Iteration 310, loss = 0.32340385\n",
      "Iteration 311, loss = 0.32326659\n",
      "Iteration 312, loss = 0.32314922\n",
      "Iteration 313, loss = 0.32301481\n",
      "Iteration 314, loss = 0.32288026\n",
      "Iteration 315, loss = 0.32275184\n",
      "Iteration 316, loss = 0.32262694\n",
      "Iteration 317, loss = 0.32250770\n",
      "Iteration 318, loss = 0.32237355\n",
      "Iteration 319, loss = 0.32225498\n",
      "Iteration 320, loss = 0.32212491\n",
      "Iteration 321, loss = 0.32200389\n",
      "Iteration 322, loss = 0.32187496\n",
      "Iteration 323, loss = 0.32176036\n",
      "Iteration 324, loss = 0.32164696\n",
      "Iteration 325, loss = 0.32151263\n",
      "Iteration 326, loss = 0.32138556\n",
      "Iteration 327, loss = 0.32125776\n",
      "Iteration 328, loss = 0.32112826\n",
      "Iteration 329, loss = 0.32102664\n",
      "Iteration 330, loss = 0.32089666\n",
      "Iteration 331, loss = 0.32078587\n",
      "Iteration 332, loss = 0.32066565\n",
      "Iteration 333, loss = 0.32054749\n",
      "Iteration 334, loss = 0.32043185\n",
      "Iteration 335, loss = 0.32031928\n",
      "Iteration 336, loss = 0.32018910\n",
      "Iteration 337, loss = 0.32007110\n",
      "Iteration 338, loss = 0.31995659\n",
      "Iteration 339, loss = 0.31986178\n",
      "Iteration 340, loss = 0.31972115\n",
      "Iteration 341, loss = 0.31960888\n",
      "Iteration 342, loss = 0.31951493\n",
      "Iteration 343, loss = 0.31938268\n",
      "Iteration 344, loss = 0.31926772\n",
      "Iteration 345, loss = 0.31915959\n",
      "Iteration 346, loss = 0.31907677\n",
      "Iteration 347, loss = 0.31893376\n",
      "Iteration 348, loss = 0.31881506\n",
      "Iteration 349, loss = 0.31871460\n",
      "Iteration 350, loss = 0.31858902\n",
      "Iteration 351, loss = 0.31847344\n",
      "Iteration 352, loss = 0.31836085\n",
      "Iteration 353, loss = 0.31825041\n",
      "Iteration 354, loss = 0.31817993\n",
      "Iteration 355, loss = 0.31802841\n",
      "Iteration 356, loss = 0.31793955\n",
      "Iteration 357, loss = 0.31782102\n",
      "Iteration 358, loss = 0.31770170\n",
      "Iteration 359, loss = 0.31758753\n",
      "Iteration 360, loss = 0.31751081\n",
      "Iteration 361, loss = 0.31737497\n",
      "Iteration 362, loss = 0.31726806\n",
      "Iteration 363, loss = 0.31717551\n",
      "Iteration 364, loss = 0.31705755\n",
      "Iteration 365, loss = 0.31697878\n",
      "Iteration 366, loss = 0.31684475\n",
      "Iteration 367, loss = 0.31674401\n",
      "Iteration 368, loss = 0.31662769\n",
      "Iteration 369, loss = 0.31653503\n",
      "Iteration 370, loss = 0.31641704\n",
      "Iteration 371, loss = 0.31632417\n",
      "Iteration 372, loss = 0.31620564\n",
      "Iteration 373, loss = 0.31610866\n",
      "Iteration 374, loss = 0.31600484\n",
      "Iteration 375, loss = 0.31588925\n",
      "Iteration 376, loss = 0.31579758\n",
      "Iteration 377, loss = 0.31568530\n",
      "Iteration 378, loss = 0.31557834\n",
      "Iteration 379, loss = 0.31548369\n",
      "Iteration 380, loss = 0.31538457\n",
      "Iteration 381, loss = 0.31526969\n",
      "Iteration 382, loss = 0.31517212\n",
      "Iteration 383, loss = 0.31508165\n",
      "Iteration 384, loss = 0.31497413\n",
      "Iteration 385, loss = 0.31486879\n",
      "Iteration 386, loss = 0.31476881\n",
      "Iteration 387, loss = 0.31468284\n",
      "Iteration 388, loss = 0.31456730\n",
      "Iteration 389, loss = 0.31448108\n",
      "Iteration 390, loss = 0.31437920\n",
      "Iteration 391, loss = 0.31425508\n",
      "Iteration 392, loss = 0.31416275\n",
      "Iteration 393, loss = 0.31405927\n",
      "Iteration 394, loss = 0.31398453\n",
      "Iteration 395, loss = 0.31385871\n",
      "Iteration 396, loss = 0.31381658\n",
      "Iteration 397, loss = 0.31366555\n",
      "Iteration 398, loss = 0.31357859\n",
      "Iteration 399, loss = 0.31347345\n",
      "Iteration 400, loss = 0.31341895\n",
      "Iteration 401, loss = 0.31328925\n",
      "Iteration 402, loss = 0.31319506\n",
      "Iteration 403, loss = 0.31308565\n",
      "Iteration 404, loss = 0.31298689\n",
      "Iteration 405, loss = 0.31288992\n",
      "Iteration 406, loss = 0.31281083\n",
      "Iteration 407, loss = 0.31271133\n",
      "Iteration 408, loss = 0.31260114\n",
      "Iteration 409, loss = 0.31250989\n",
      "Iteration 410, loss = 0.31240538\n",
      "Iteration 411, loss = 0.31230252\n",
      "Iteration 412, loss = 0.31223316\n",
      "Iteration 413, loss = 0.31211686\n",
      "Iteration 414, loss = 0.31201981\n",
      "Iteration 415, loss = 0.31193764\n",
      "Iteration 416, loss = 0.31183811\n",
      "Iteration 417, loss = 0.31171850\n",
      "Iteration 418, loss = 0.31164394\n",
      "Iteration 419, loss = 0.31153780\n",
      "Iteration 420, loss = 0.31143290\n",
      "Iteration 421, loss = 0.31135436\n",
      "Iteration 422, loss = 0.31124553\n",
      "Iteration 423, loss = 0.31115725\n",
      "Iteration 424, loss = 0.31106070\n",
      "Iteration 425, loss = 0.31098185\n",
      "Iteration 426, loss = 0.31087909\n",
      "Iteration 427, loss = 0.31077954\n",
      "Iteration 428, loss = 0.31068217\n",
      "Iteration 429, loss = 0.31058917\n",
      "Iteration 430, loss = 0.31051424\n",
      "Iteration 431, loss = 0.31042490\n",
      "Iteration 432, loss = 0.31032028\n",
      "Iteration 433, loss = 0.31022887\n",
      "Iteration 434, loss = 0.31013391\n",
      "Iteration 435, loss = 0.31004830\n",
      "Iteration 436, loss = 0.30994908\n",
      "Iteration 437, loss = 0.30985641\n",
      "Iteration 438, loss = 0.30979969\n",
      "Iteration 439, loss = 0.30967369\n",
      "Iteration 440, loss = 0.30959664\n",
      "Iteration 441, loss = 0.30950511\n",
      "Iteration 442, loss = 0.30940105\n",
      "Iteration 443, loss = 0.30932870\n",
      "Iteration 444, loss = 0.30922752\n",
      "Iteration 445, loss = 0.30914956\n",
      "Iteration 446, loss = 0.30905644\n",
      "Iteration 447, loss = 0.30895048\n",
      "Iteration 448, loss = 0.30888053\n",
      "Iteration 449, loss = 0.30877071\n",
      "Iteration 450, loss = 0.30868378\n",
      "Iteration 451, loss = 0.30861191\n",
      "Iteration 452, loss = 0.30854757\n",
      "Iteration 453, loss = 0.30842515\n",
      "Iteration 454, loss = 0.30833179\n",
      "Iteration 455, loss = 0.30824627\n",
      "Iteration 456, loss = 0.30815969\n",
      "Iteration 457, loss = 0.30807115\n",
      "Iteration 458, loss = 0.30800458\n",
      "Iteration 459, loss = 0.30789562\n",
      "Iteration 460, loss = 0.30780450\n",
      "Iteration 461, loss = 0.30772132\n",
      "Iteration 462, loss = 0.30764126\n",
      "Iteration 463, loss = 0.30754474\n",
      "Iteration 464, loss = 0.30747640\n",
      "Iteration 465, loss = 0.30737666\n",
      "Iteration 466, loss = 0.30730094\n",
      "Iteration 467, loss = 0.30720593\n",
      "Iteration 468, loss = 0.30711382\n",
      "Iteration 469, loss = 0.30702782\n",
      "Iteration 470, loss = 0.30693799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "for epoch in epochs:\n",
    "    mlp.max_iter = epoch\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    train_score = f1_score(y_train, mlp.predict(X_train), average='weighted')\n",
    "    train_scores.append(train_score)\n",
    "\n",
    "    cv_score = cross_val_score(mlp, X_train, y_train, cv=cv, scoring=f1).mean()\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "\n",
    "print(len(epochs), len(train_scores))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_scores, label='Training score', color='blue')\n",
    "plt.plot(epochs, cv_scores, label='Cross-validation score', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Learning Curve for MLPClassifier')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "a44f0052-94ec-4092-a6b7-67e7a152e611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='sgd', max_iter=3000, learning_rate='adaptive', alpha=1, hidden_layer_sizes=(100, 50), activation='relu', random_state=903967749)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mlp.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "d31e9b81-40ad-4355-819c-80994d48acb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart Disease Training times\n",
      "SVM: 0.054599 seconds\n",
      "k-NN: 0.014547 seconds\n",
      "Neural Network: 8.624016 seconds\n",
      "\n",
      "Heart Disease Testing times\n",
      "SVM: 0.005916 seconds\n",
      "k-NN: 0.004937 seconds\n",
      "Neural Network: 0.000446 seconds\n"
     ]
    }
   ],
   "source": [
    "algorithms = {\n",
    "    'SVM': svc,\n",
    "    'k-NN': knn,\n",
    "    'Neural Network': mlp\n",
    "}\n",
    "\n",
    "# Dictionary to store wall clock times\n",
    "wall_clock_times_training = {}\n",
    "wall_clock_times_testing = {}\n",
    "\n",
    "# Measure wall clock times for each algorithm\n",
    "for name, model in algorithms.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate wall clock time\n",
    "    wall_clock_times_training[name] = end_time - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate wall clock time\n",
    "    wall_clock_times_testing[name] = end_time - start_time\n",
    "\n",
    "# Display wall clock times\n",
    "print(\"Heart Disease Training times\")\n",
    "for name, duration in wall_clock_times_training.items():\n",
    "    print(f\"{name}: {duration:.6f} seconds\")\n",
    "\n",
    "print(\"\\nHeart Disease Testing times\")\n",
    "for name, duration in wall_clock_times_testing.items():\n",
    "    print(f\"{name}: {duration:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075b54f-7b89-48ec-8cc6-2956998b1017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
